{"Playing SNES in the Retro Learning Environment": {"container_type": "Publication", "bib": {"title": "Playing SNES in the retro learning environment", "author": ["N Bhonker", "S Rozenberg", "I Hubara"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02205", "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE)(Bellemare et al., 2013) has become a commonly used benchmark"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02205", "author_id": ["eqMeaz8AAAAJ", "ExX3aEwAAAAJ", "dyYryZYAAAAJ"], "url_scholarbib": "/scholar?q=info:7y8AXT2UoDYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPlaying%2BSNES%2Bin%2Bthe%2BRetro%2BLearning%2BEnvironment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7y8AXT2UoDYJ&ei=TQ9kYoSwK-iSy9YPp-OyiAE&json=", "num_citations": 21, "citedby_url": "/scholar?cites=3936309065596022767&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7y8AXT2UoDYJ:scholar.google.com/&scioq=Playing+SNES+in+the+Retro+Learning+Environment&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02205"}, "Conditional Image Synthesis With Auxiliary Classifier GANs": {"container_type": "Publication", "bib": {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["A Odena", "C Olah", "J Shlens"], "pub_year": "2017", "venue": "International conference on \u2026", "abstract": "In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in $128\\times 128$ resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v70/odena17a.html", "author_id": ["EHQHNdEAAAAJ", "6dskOSUAAAAJ", "sm1q2bYAAAAJ"], "url_scholarbib": "/scholar?q=info:huQnqF-syM0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConditional%2BImage%2BSynthesis%2BWith%2BAuxiliary%2BClassifier%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=huQnqF-syM0J&ei=Ug9kYo29I4OEmgHx-5DADA&json=", "num_citations": 2520, "citedby_url": "/scholar?cites=14828291299960415366&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:huQnqF-syM0J:scholar.google.com/&scioq=Conditional+Image+Synthesis+With+Auxiliary+Classifier+GANs&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/odena17a/odena17a.pdf"}, "PREDICTION OF POTENTIAL HUMAN INTENTION USING SUPERVISED COMPETITIVE LEARNING": {"container_type": "Publication", "bib": {"title": "PREDICTION OF POTENTIAL HUMAN INTENTION USING SUPERVISED COMPETITIVE LEARNING", "author": ["M Ishikawa", "M Okude", "T Nishida", "K Muto"], "pub_year": "2016", "venue": "NA", "abstract": "We propose a learning method to quantify human intention. Generally, a human being will imagine several potential actions for a given scene, but only one of these actions will subsequently be taken. This makes it difficult to quantify human intentions. To solve this problem, we apply competitive learning to human behavior prediction as supervised learning. In our approach, competitive learning generates several outputs that are then associated with several potential situations imagined by a human. We applied the proposed"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkqMSCHxe", "author_id": ["", "", "", ""], "url_scholarbib": "/scholar?q=info:ZCD_pdGQSYkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPREDICTION%2BOF%2BPOTENTIAL%2BHUMAN%2BINTENTION%2BUSING%2BSUPERVISED%2BCOMPETITIVE%2BLEARNING%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZCD_pdGQSYkJ&ei=VQ9kYouIDI2ymgHg1rfQDQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:ZCD_pdGQSYkJ:scholar.google.com/&scioq=PREDICTION+OF+POTENTIAL+HUMAN+INTENTION+USING+SUPERVISED+COMPETITIVE+LEARNING&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkqMSCHxe"}, "Two Methods for Wild Variational Inference": {"container_type": "Publication", "bib": {"title": "Two methods for wild variational inference", "author": ["Q Liu", "Y Feng"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.00081", "abstract": "Variational inference provides a powerful tool for approximate probabilistic in-ference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable proba-bility density functions. This largely limits the design and implementation of vari-ational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.00081", "author_id": ["XEx1fZkAAAAJ", ""], "url_scholarbib": "/scholar?q=info:dnBfMVoKl-MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTwo%2BMethods%2Bfor%2BWild%2BVariational%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dnBfMVoKl-MJ&ei=YA9kYu_fLe-Sy9YPs_mY8AM&json=", "num_citations": 25, "citedby_url": "/scholar?cites=16399587950584623222&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dnBfMVoKl-MJ:scholar.google.com/&scioq=Two+Methods+for+Wild+Variational+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.00081"}, " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer": {"container_type": "Publication", "bib": {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["N Shazeer", "A Mirhoseini", "K Maziarz", "A Davis"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.06538", "author_id": ["wsGvgA8AAAAJ", "PjEHjPEAAAAJ", "BA8bBVkAAAAJ", ""], "url_scholarbib": "/scholar?q=info:SMYGx8W4edIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BOutrageously%2BLarge%2BNeural%2BNetworks:%2BThe%2BSparsely-Gated%2BMixture-of-Experts%2BLayer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SMYGx8W4edIJ&ei=Yw9kYqXyGeHDywSSipaYAg&json=", "num_citations": 870, "citedby_url": "/scholar?cites=15166356379734033992&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SMYGx8W4edIJ:scholar.google.com/&scioq=+Outrageously+Large+Neural+Networks:+The+Sparsely-Gated+Mixture-of-Experts+Layer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.06538.pdf%22%20%5Ct%20%22_blank"}, "A Simple yet Effective Method to Prune Dense Layers of Neural Networks": {"container_type": "Publication", "bib": {"title": "A simple yet effective method to prune dense layers of neural networks", "author": ["M Babaeizadeh", "P Smaragdis", "RH Campbell"], "pub_year": "2016", "venue": "NA", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJIY0E9ge&noteId=HJIY0E9ge&ref=https://githubhelp.com", "author_id": ["3Y4egcYAAAAJ", "AYpm7KcAAAAJ", "2ftJYXMAAAAJ"], "url_scholarbib": "/scholar?q=info:U58Jqz-kmMcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BSimple%2Byet%2BEffective%2BMethod%2Bto%2BPrune%2BDense%2BLayers%2Bof%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=U58Jqz-kmMcJ&ei=Zw9kYqC2CLKO6rQPy-CRsA8&json=", "num_citations": 6, "citedby_url": "/scholar?cites=14382426003367108435&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:U58Jqz-kmMcJ:scholar.google.com/&scioq=A+Simple+yet+Effective+Method+to+Prune+Dense+Layers+of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJIY0E9ge"}, "Chess Game Concepts Emerge under Weak Supervision: A Case Study of Tic-tac-toe": {"container_type": "Publication", "bib": {"title": "Chess Game Concepts Emerge under Weak Supervision: A Case Study of Tic-tac-toe", "author": ["H Zhao", "M Lu", "A Yao", "Y Chen", "L Zhang"], "pub_year": "2016", "venue": "NA", "abstract": "This paper explores the possibility of learning chess game concepts under weak supervision with convolutional neural networks, which is a topic that has not been visited to the best of our knowledge. We put this task in three different backgrounds:(1) deep reinforcement learning has shown an amazing capability to learn a mapping from visual inputs to most rewarding actions, without knowing the concepts of a video game. But how could we confirm that the network understands these concepts or it just does not?(2) cross-modal supervision"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJo9n9Feg", "author_id": ["", "", "b9hCmPYAAAAJ", "MKRyHXsAAAAJ", ""], "url_scholarbib": "/scholar?q=info:nu5-4unpXrQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DChess%2BGame%2BConcepts%2BEmerge%2Bunder%2BWeak%2BSupervision:%2BA%2BCase%2BStudy%2Bof%2BTic-tac-toe%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nu5-4unpXrQJ&ei=ag9kYtLDA-iSy9YPp-OyiAE&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:nu5-4unpXrQJ:scholar.google.com/&scioq=Chess+Game+Concepts+Emerge+under+Weak+Supervision:+A+Case+Study+of+Tic-tac-toe&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJo9n9Feg"}, "Charged Point Normalization: An Efficient Solution to the Saddle Point Problem": {"container_type": "Publication", "bib": {"title": "Charged Point Normalization: An Efficient Solution to the Saddle Point Problem", "author": ["A Aghajanyan"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.09522", "abstract": "Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.09522", "author_id": ["KxQfzRcAAAAJ"], "url_scholarbib": "/scholar?q=info:WIv4m6IP1w8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCharged%2BPoint%2BNormalization:%2BAn%2BEfficient%2BSolution%2Bto%2Bthe%2BSaddle%2BPoint%2BProblem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WIv4m6IP1w8J&ei=bA9kYsXGIIuKmgGY1YjABQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:WIv4m6IP1w8J:scholar.google.com/&scioq=Charged+Point+Normalization:+An+Efficient+Solution+to+the+Saddle+Point+Problem&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.09522"}, "Bidirectional Attention Flow for Machine Comprehension": {"container_type": "Publication", "bib": {"title": "Bidirectional attention flow for machine comprehension", "author": ["M Seo", "A Kembhavi", "A Farhadi", "H Hajishirzi"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01603", "author_id": ["zYze5fIAAAAJ", "JnUevM0AAAAJ", "jeOFRDsAAAAJ", "LOV6_WIAAAAJ"], "url_scholarbib": "/scholar?q=info:tG0XhfOfwwIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBidirectional%2BAttention%2BFlow%2Bfor%2BMachine%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tG0XhfOfwwIJ&ei=cw9kYvStDo2ymgHg1rfQDQ&json=", "num_citations": 1690, "citedby_url": "/scholar?cites=199178676793208244&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tG0XhfOfwwIJ:scholar.google.com/&scioq=Bidirectional+Attention+Flow+for+Machine+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01603.pdf?source=post_page---------------------------"}, "Dynamic Coattention Networks For Question Answering": {"container_type": "Publication", "bib": {"title": "Dynamic coattention networks for question answering", "author": ["C Xiong", "V Zhong", "R Socher"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01604", "abstract": "to incorrect answers. To address this problem, we introduce the Dynamic Coattention  Network (DCN) for question answering. The DCN first fuses co-dependent representations of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01604", "author_id": ["vaSdahkAAAAJ", "lT3YoNkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:YCSv6jFRf2kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BCoattention%2BNetworks%2BFor%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YCSv6jFRf2kJ&ei=fA9kYoe9AfmQ6rQP5OqKqAo&json=", "num_citations": 582, "citedby_url": "/scholar?cites=7601883970857280608&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YCSv6jFRf2kJ:scholar.google.com/&scioq=Dynamic+Coattention+Networks+For+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01604"}, "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension": {"container_type": "Publication", "bib": {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Y Yu", "W Zhang", "K Hasan", "M Yu", "B Xiang"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.09996", "author_id": ["3fjXq0MAAAAJ", "_b4LTZ8AAAAJ", "1E7muHwAAAAJ", "vC8DssQAAAAJ", "A6yjdJAAAAAJ"], "url_scholarbib": "/scholar?q=info:pI9Hp6UcfLgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnd-to-End%2BAnswer%2BChunk%2BExtraction%2Band%2BRanking%2Bfor%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pI9Hp6UcfLgJ&ei=gQ9kYrWtMsLZmQHnraWYCA&json=", "num_citations": 53, "citedby_url": "/scholar?cites=13293531697892528036&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pI9Hp6UcfLgJ:scholar.google.com/&scioq=End-to-End+Answer+Chunk+Extraction+and+Ranking+for+Reading+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.09996"}, "Exploring Sparsity in Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Exploring sparsity in recurrent neural networks", "author": ["S Narang", "E Elsen", "G Diamos", "S Sengupta"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent Neural Networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.05119", "author_id": ["CWOixywAAAAJ", "5d2Ss9EAAAAJ", "Qq70O4UAAAAJ", "pTvhQDQAAAAJ"], "url_scholarbib": "/scholar?q=info:ndKNfuFPZdkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploring%2BSparsity%2Bin%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ndKNfuFPZdkJ&ei=iQ9kYv_LFo2ymgHg1rfQDQ&json=", "num_citations": 239, "citedby_url": "/scholar?cites=15665014708787597981&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ndKNfuFPZdkJ:scholar.google.com/&scioq=Exploring+Sparsity+in+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.05119"}, "Taming the waves: sine as activation function in deep neural networks": {"container_type": "Publication", "bib": {"title": "Taming the waves: sine as activation function in deep neural networks", "author": ["G Parascandolo", "H Huttunen", "T Virtanen"], "pub_year": "2016", "venue": "NA", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least quasiconvex\u2014activation functions. While sinusoidal activation functions have been successfully used for specific applications, they remain largely ignored and regarded as difficult to train. In this paper we formally characterize why these networks can indeed often be difficult to train even in very simple scenarios, and describe how the presence of infinitely many and shallow local minima emerges from the architecture. We also provide an explanation to the good"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Sks3zF9eg", "author_id": ["1zCDX_UAAAAJ", "8_du6qgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:hUDHwocKfokJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTaming%2Bthe%2Bwaves:%2Bsine%2Bas%2Bactivation%2Bfunction%2Bin%2Bdeep%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hUDHwocKfokJ&ei=jQ9kYu-zDY6pywTd4KPADw&json=", "num_citations": 38, "citedby_url": "/scholar?cites=9907367808466370693&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hUDHwocKfokJ:scholar.google.com/&scioq=Taming+the+waves:+sine+as+activation+function+in+deep+neural+networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Sks3zF9eg"}, "Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes": {"container_type": "Publication", "bib": {"title": "Normalizing the normalizers: Comparing and extending network normalization schemes", "author": ["M Ren", "R Liao", "R Urtasun", "FH Sinz"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work well in the recurrent setting. In this paper we propose a unified view of normalization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.04520", "author_id": ["XcQ9WqMAAAAJ", "2wrS35MAAAAJ", "jyxO2akAAAAJ", "xpwMxy8AAAAJ"], "url_scholarbib": "/scholar?q=info:biiGJJ3GGZMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNormalizing%2Bthe%2BNormalizers:%2BComparing%2Band%2BExtending%2BNetwork%2BNormalization%2BSchemes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=biiGJJ3GGZMJ&ei=kA9kYvbtDoySyASZk6HgCA&json=", "num_citations": 84, "citedby_url": "/scholar?cites=10599721576218110062&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:biiGJJ3GGZMJ:scholar.google.com/&scioq=Normalizing+the+Normalizers:+Comparing+and+Extending+Network+Normalization+Schemes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.04520"}, "Identity Matters in Deep Learning": {"container_type": "Publication", "bib": {"title": "Identity matters in deep learning", "author": ["M Hardt", "T Ma"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.04231", "abstract": "in deep learning is that each layer of a deep artificial neural network should be able to easily  express the identity  In this work, we put the principle of identity parameterization on a more"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.04231", "author_id": ["adnTgaAAAAAJ", "i38QlUwAAAAJ"], "url_scholarbib": "/scholar?q=info:-m50qy492bwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIdentity%2BMatters%2Bin%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-m50qy492bwJ&ei=mw9kYvrIMu-Sy9YPs_mY8AM&json=", "num_citations": 309, "citedby_url": "/scholar?cites=13607975019730988794&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-m50qy492bwJ:scholar.google.com/&scioq=Identity+Matters+in+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.04231.pdf,"}, "Semi-supervised deep learning by metric embedding": {"container_type": "Publication", "bib": {"title": "Semi-supervised deep learning by metric embedding", "author": ["E Hoffer", "N Ailon"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01449", "abstract": "Deep networks are successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01449", "author_id": ["iEfTH7AAAAAJ", "MpckH9YAAAAJ"], "url_scholarbib": "/scholar?q=info:VxN0FpKncJoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-supervised%2Bdeep%2Blearning%2Bby%2Bmetric%2Bembedding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VxN0FpKncJoJ&ei=pA9kYqD0HIuKmgGY1YjABQ&json=", "num_citations": 35, "citedby_url": "/scholar?cites=11128578925116265303&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VxN0FpKncJoJ:scholar.google.com/&scioq=Semi-supervised+deep+learning+by+metric+embedding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01449"}, "Multi-task learning with deep model based reinforcement learning": {"container_type": "Publication", "bib": {"title": "Multi-task learning with deep model based reinforcement learning", "author": ["A Mujika"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01457", "abstract": "In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01457", "author_id": ["jUvb8coAAAAJ"], "url_scholarbib": "/scholar?q=info:x-gYdgbfJ1kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-task%2Blearning%2Bwith%2Bdeep%2Bmodel%2Bbased%2Breinforcement%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x-gYdgbfJ1kJ&ei=qQ9kYs2ND5yO6rQP_qe3mAs&json=", "num_citations": 12, "citedby_url": "/scholar?cites=6424348612311443655&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x-gYdgbfJ1kJ:scholar.google.com/&scioq=Multi-task+learning+with+deep+model+based+reinforcement+learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01457"}, "Divide and Conquer with Neural Networks": {"container_type": "Publication", "bib": {"title": "Divide and conquer neural networks", "author": ["SG Romaniuk", "LO Hall"], "pub_year": "1993", "venue": "Neural Networks", "abstract": "classified in the conquer phase, an example is randomly removed from the train set and  the conquer cell is not used in the network. So, we start over in the divide phase with only one"}, "filled": false, "gsrank": 1, "pub_url": "https://www.sciencedirect.com/science/article/pii/S0893608009800221", "author_id": ["", "AKHplAUAAAAJ"], "url_scholarbib": "/scholar?q=info:sAAg82j4PXkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDivide%2Band%2BConquer%2Bwith%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sAAg82j4PXkJ&ei=rA9kYrvzDIuKmgGY1YjABQ&json=", "num_citations": 69, "citedby_url": "/scholar?cites=8736411981807878320&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sAAg82j4PXkJ:scholar.google.com/&scioq=Divide+and+Conquer+with+Neural+Networks&hl=en&as_sdt=0,33"}, "Designing Neural Network Architectures using Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Designing neural network architectures using reinforcement learning", "author": ["B Baker", "O Gupta", "N Naik", "R Raskar"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02167", "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $ Q $-learning with an $\\epsilon $-greedy exploration strategy and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02167", "author_id": ["bMfPYdYAAAAJ", "2aEdHz0AAAAJ", "M1IgIyMAAAAJ", "8hpOmVgAAAAJ"], "url_scholarbib": "/scholar?q=info:exkJIy-tOBQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDesigning%2BNeural%2BNetwork%2BArchitectures%2Busing%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=exkJIy-tOBQJ&ei=sA9kYtmJFJGJmwGY-qmYDQ&json=", "num_citations": 1171, "citedby_url": "/scholar?cites=1457104897417222523&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:exkJIy-tOBQJ:scholar.google.com/&scioq=Designing+Neural+Network+Architectures+using+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02167.pdf).I"}, "Neural Code Completion": {"container_type": "Publication", "bib": {"title": "Neural code completion", "author": ["C Liu", "X Wang", "R Shin", "JE Gonzalez", "D Song"], "pub_year": "2016", "venue": "NA", "abstract": "investigation into developing neural network approaches for the code completion problem.   field of neural network-based code completion, but is also an important step toward neural"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJbPBt9lg", "author_id": ["Zrbs8hIAAAAJ", "e9gUdKwAAAAJ", "xPnkc80AAAAJ", "B96GkdgAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:mq4ktavHqGwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BCode%2BCompletion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mq4ktavHqGwJ&ei=sw9kYu3gDoyuyAT-mrWwCA&json=", "num_citations": 25, "citedby_url": "/scholar?cites=7829727492476022426&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mq4ktavHqGwJ:scholar.google.com/&scioq=Neural+Code+Completion&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJbPBt9lg"}, "Online Structure Learning for Sum-Product Networks with Gaussian Leaves": {"container_type": "Publication", "bib": {"title": "Online structure learning for sum-product networks with gaussian leaves", "author": ["W Hsu", "A Kalra", "P Poupart"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.05265", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (ie, completeness and decomposability) that must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.05265", "author_id": ["", "tcbzdzAAAAAJ", "KhAJWroAAAAJ"], "url_scholarbib": "/scholar?q=info:kNGGXMW-VcEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOnline%2BStructure%2BLearning%2Bfor%2BSum-Product%2BNetworks%2Bwith%2BGaussian%2BLeaves%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kNGGXMW-VcEJ&ei=vg9kYoytHIOEmgHx-5DADA&json=", "num_citations": 24, "citedby_url": "/scholar?cites=13931250777210671504&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kNGGXMW-VcEJ:scholar.google.com/&scioq=Online+Structure+Learning+for+Sum-Product+Networks+with+Gaussian+Leaves&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.05265"}, "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks": {"container_type": "Publication", "bib": {"title": "Transfer learning for sequence tagging with hierarchical recurrent networks", "author": ["Z Yang", "R Salakhutdinov", "WW Cohen"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.06345", "abstract": "with plentiful annotations (eg, POS tagging on Penn Treebank) is used to  tagging for  microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.06345", "author_id": ["7qXxyJkAAAAJ", "ITZ1e7MAAAAJ", "8ys-38kAAAAJ"], "url_scholarbib": "/scholar?q=info:CrciT5yfDg0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransfer%2BLearning%2Bfor%2BSequence%2BTagging%2Bwith%2BHierarchical%2BRecurrent%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CrciT5yfDg0J&ei=wQ9kYpucIo6pywTd4KPADw&json=", "num_citations": 324, "citedby_url": "/scholar?cites=940864865858402058&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CrciT5yfDg0J:scholar.google.com/&scioq=Transfer+Learning+for+Sequence+Tagging+with+Hierarchical+Recurrent+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.06345.pdf'"}, "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD": {"container_type": "Publication", "bib": {"title": "Neurogenesis-inspired dictionary learning: Online model adaption in a changing world", "author": ["S Garg", "I Rish", "G Cecchi", "A Lozano"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.06106", "abstract": "In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.06106", "author_id": ["Sz2mNx0AAAAJ", "Avse5gIAAAAJ", "pQZaTGEAAAAJ", "4wTGaDsAAAAJ"], "url_scholarbib": "/scholar?q=info:21v7Z7D-PaUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNEUROGENESIS-INSPIRED%2BDICTIONARY%2BLEARNING:%2BONLINE%2BMODEL%2BADAPTION%2BIN%2BA%2BCHANGING%2BWORLD%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=21v7Z7D-PaUJ&ei=ww9kYuSFO6KUy9YP_JONiAY&json=", "num_citations": 5, "citedby_url": "/scholar?cites=11906953023449684955&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:21v7Z7D-PaUJ:scholar.google.com/&scioq=NEUROGENESIS-INSPIRED+DICTIONARY+LEARNING:+ONLINE+MODEL+ADAPTION+IN+A+CHANGING+WORLD&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.06106"}, "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys": {"container_type": "Publication", "bib": {"title": "Entropy-sgd: Biasing gradient descent into wide valleys", "author": ["P Chaudhari", "A Choromanska", "S Soatto"], "pub_year": "2019", "venue": "Journal of Statistical \u2026", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding"}, "filled": false, "gsrank": 1, "pub_url": "https://iopscience.iop.org/article/10.1088/1742-5468/ab39d9/meta", "author_id": ["c_z5hWEAAAAJ", "l-mlF7YAAAAJ", "lH1PdF8AAAAJ"], "url_scholarbib": "/scholar?q=info:bDenAOWm4wYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEntropy-SGD:%2BBiasing%2BGradient%2BDescent%2BInto%2BWide%2BValleys%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bDenAOWm4wYJ&ei=xg9kYtrdK5LeyQTE46-QAg&json=", "num_citations": 488, "citedby_url": "/scholar?cites=496423886429566828&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bDenAOWm4wYJ:scholar.google.com/&scioq=Entropy-SGD:+Biasing+Gradient+Descent+Into+Wide+Valleys&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01838"}, "The Neural Noisy Channel": {"container_type": "Publication", "bib": {"title": "The neural noisy channel", "author": ["L Yu", "P Blunsom", "C Dyer", "E Grefenstette"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "noisy channel decoding problem and use recurrent neural networks to parameterise the  source and channel  effects during training, noisy channel models must produce outputs that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02554", "author_id": ["gX5JBc4AAAAJ", "eJwbbXEAAAAJ", "W2DsnAkAAAAJ", "ezllEwMAAAAJ"], "url_scholarbib": "/scholar?q=info:a0B13YpiaMQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BNeural%2BNoisy%2BChannel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a0B13YpiaMQJ&ei=yg9kYqniKc6E6rQP5-KmKA&json=", "num_citations": 58, "citedby_url": "/scholar?cites=14152670177572241515&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a0B13YpiaMQJ:scholar.google.com/&scioq=The+Neural+Noisy+Channel&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02554"}, "Offline bilingual word vectors, orthogonal transformations and the inverted softmax": {"container_type": "Publication", "bib": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "author": ["SL Smith", "DHP Turban", "S Hamblin"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Usually bilingual word vectors are trained\" online\". Mikolov et al. showed they can also be found\" offline\", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel\" inverted softmax\" for identifying translation pairs, with which we improve the precision@ 1 of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.03859", "author_id": ["fyEqU5oAAAAJ", "hcdj-IwAAAAJ", "BNmAuswAAAAJ"], "url_scholarbib": "/scholar?q=info:IUzFkq9dOU4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOffline%2Bbilingual%2Bword%2Bvectors,%2Borthogonal%2Btransformations%2Band%2Bthe%2Binverted%2Bsoftmax%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IUzFkq9dOU4J&ei=zQ9kYuOiMZLeyQTE46-QAg&json=", "num_citations": 461, "citedby_url": "/scholar?cites=5636639417293949985&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IUzFkq9dOU4J:scholar.google.com/&scioq=Offline+bilingual+word+vectors,+orthogonal+transformations+and+the+inverted+softmax&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.03859?ref=https://githubhelp.com"}, "Learning to Generate Samples from Noise through Infusion Training": {"container_type": "Publication", "bib": {"title": "Learning to generate samples from noise through infusion training", "author": ["F Bordes", "S Honari", "P Vincent"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.06975", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.06975", "author_id": ["OADfWhUAAAAJ", "8uou2n4AAAAJ", "WBCKQMsAAAAJ"], "url_scholarbib": "/scholar?q=info:SS6I3BDnYTcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BGenerate%2BSamples%2Bfrom%2BNoise%2Bthrough%2BInfusion%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SS6I3BDnYTcJ&ei=0A9kYqiNFpGJmwGY-qmYDQ&json=", "num_citations": 28, "citedby_url": "/scholar?cites=3990724804432375369&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SS6I3BDnYTcJ:scholar.google.com/&scioq=Learning+to+Generate+Samples+from+Noise+through+Infusion+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.06975"}, "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?": {"container_type": "Publication", "bib": {"title": "Learning locomotion skills using deeprl: Does the choice of action space matter?", "author": ["XB Peng", "M van de Panne"], "pub_year": "2017", "venue": "Proceedings of the ACM SIGGRAPH \u2026", "abstract": "The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts learning and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gait-cycle imitation task for multiple planar articulated figures and multiple"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3099564.3099567?casa_token=RPKzxKWQMOEAAAAA:A-SLL4oCtaRQ0_ieQcq0_3wGePI0pNH_ejjttbCs9tapykCZ-IseFAP4ZOCDjjLB3sW2__b5lrC2Xw", "author_id": ["FwxfQosAAAAJ", "lJwPbcUAAAAJ"], "url_scholarbib": "/scholar?q=info:6lEBIJCM7nUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLocomotion%2BSkills%2BUsing%2BDeepRL:%2BDoes%2Bthe%2BChoice%2Bof%2BAction%2BSpace%2BMatter%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6lEBIJCM7nUJ&ei=0w9kYtW7Ls6E6rQP5-KmKA&json=", "num_citations": 111, "citedby_url": "/scholar?cites=8497884097534841322&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6lEBIJCM7nUJ:scholar.google.com/&scioq=Learning+Locomotion+Skills+Using+DeepRL:+Does+the+Choice+of+Action+Space+Matter%3F&hl=en&as_sdt=0,33", "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3099564.3099567?casa_token=oWcRLFV19eAAAAAA:nS7-datRYMD2AEPt1lceL38OznKqPTZmXPTV6-l3gMNU1qut6-GzmPiCi2REq3t_gXueFvRDRifkzw"}, "Learning Efficient Algorithms with Hierarchical Attentive Memory": {"container_type": "Publication", "bib": {"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["M Andrychowicz", "K Kurach"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1602.03218", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O (log n) complexity, which is a significant improvement over the standard attention mechanism that requires O (n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1602.03218", "author_id": ["n9K1v-cAAAAJ", "oD_Ea7EAAAAJ"], "url_scholarbib": "/scholar?q=info:sn-wk8fHg3QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BEfficient%2BAlgorithms%2Bwith%2BHierarchical%2BAttentive%2BMemory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sn-wk8fHg3QJ&ei=3g9kYoeTF5yO6rQP_qe3mAs&json=", "num_citations": 49, "citedby_url": "/scholar?cites=8395773790338973618&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sn-wk8fHg3QJ:scholar.google.com/&scioq=Learning+Efficient+Algorithms+with+Hierarchical+Attentive+Memory&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1602.03218"}, "Online Bayesian Transfer Learning for Sequential Data Modeling": {"container_type": "Publication", "bib": {"title": "Online Bayesian transfer learning for sequential data modeling", "author": ["P Jaini", "Z Chen", "P Carbajal", "E Law", "L Middleton"], "pub_year": "2016", "venue": "NA", "abstract": "We consider the problem of inferring a sequence of hidden states associated with a sequence of observations produced by an individual within a population. Instead of learning a single sequence model for the population (which does not account for variations within the population), we learn a set of basis sequence models based on different individuals. The sequence of hidden states for a new individual is inferred in an online fashion by estimating a distribution over the basis models that best explain the sequence of observations of this"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByqiJIqxg", "author_id": ["keg9BGEAAAAJ", "", "", "OOssnq8AAAAJ", "HGEQv7cAAAAJ"], "url_scholarbib": "/scholar?q=info:uvre0onTmNwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOnline%2BBayesian%2BTransfer%2BLearning%2Bfor%2BSequential%2BData%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uvre0onTmNwJ&ei=4g9kYszMMouKmgGY1YjABQ&json=", "num_citations": 18, "citedby_url": "/scholar?cites=15895687473705974458&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uvre0onTmNwJ:scholar.google.com/&scioq=Online+Bayesian+Transfer+Learning+for+Sequential+Data+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByqiJIqxg"}, "Neural Combinatorial Optimization with Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Neural combinatorial optimization with reinforcement learning", "author": ["I Bello", "H Pham", "QV Le", "M Norouzi", "S Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09940", "author_id": ["mY6p8gcAAAAJ", "GpcGdRkAAAAJ", "vfT6-XIAAAAJ", "Lncr-VoAAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:c6mNLzAUaMEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BCombinatorial%2BOptimization%2Bwith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c6mNLzAUaMEJ&ei=7g9kYtj6EYuKmgGY1YjABQ&json=", "num_citations": 847, "citedby_url": "/scholar?cites=13936411244086798707&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c6mNLzAUaMEJ:scholar.google.com/&scioq=Neural+Combinatorial+Optimization+with+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09940"}, "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Surprise-based intrinsic motivation for deep reinforcement learning", "author": ["J Achiam", "S Sastry"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.01732", "abstract": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\\epsilon $-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.01732", "author_id": ["jRKEUjkAAAAJ", "KgZxzjsAAAAJ"], "url_scholarbib": "/scholar?q=info:4jh5xMeQuo8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSurprise-Based%2BIntrinsic%2BMotivation%2Bfor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4jh5xMeQuo8J&ei=8Q9kYrLgFpLeyQTE46-QAg&json=", "num_citations": 167, "citedby_url": "/scholar?cites=10356749480761047266&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4jh5xMeQuo8J:scholar.google.com/&scioq=Surprise-Based+Intrinsic+Motivation+for+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.01732.pdf%20http://arxiv.org/abs/1703.01732"}, "Lie-Access Neural Turing Machines": {"container_type": "Publication", "bib": {"title": "Lie access neural turing machine", "author": ["G Yang"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1602.08671", "abstract": "We call the latter \u201cLie access,\u201d with the meaning parametrized  concept of \u201cLie access\u201d a Lie  Access Neural Turing Machine, or  not the only ways of instantiating the \u201cLie access\u201d concept."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1602.08671", "author_id": ["Xz4RAJkAAAAJ"], "url_scholarbib": "/scholar?q=info:ZFttK2snvbIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLie-Access%2BNeural%2BTuring%2BMachines%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZFttK2snvbIJ&ei=-g9kYrDUNZqSy9YP8pKNsAE&json=", "num_citations": 18, "citedby_url": "/scholar?cites=12879493850593057636&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZFttK2snvbIJ:scholar.google.com/&scioq=Lie-Access+Neural+Turing+Machines&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1602.08671"}, "Learning a Natural Language Interface with Neural Programmer": {"container_type": "Publication", "bib": {"title": "Learning a natural language interface with neural programmer", "author": ["A Neelakantan", "QV Le", "M Abadi", "A McCallum"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.08945", "author_id": ["ygTCc6cAAAAJ", "vfT6-XIAAAAJ", "vWTI60AAAAAJ", "yILa1y0AAAAJ"], "url_scholarbib": "/scholar?q=info:gwm4S4kX_NgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ba%2BNatural%2BLanguage%2BInterface%2Bwith%2BNeural%2BProgrammer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gwm4S4kX_NgJ&ei=ABBkYpb7CqKUy9YP_JONiAY&json=", "num_citations": 118, "citedby_url": "/scholar?cites=15635397884771830147&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gwm4S4kX_NgJ:scholar.google.com/&scioq=Learning+a+Natural+Language+Interface+with+Neural+Programmer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.08945"}, "Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension": {"container_type": "Publication", "bib": {"title": "Finding a jack-of-all-trades: An examination of semi-supervised learning in reading comprehension", "author": ["R Kadlec", "O Bajgar", "P Hrincar", "J Kleindienst"], "pub_year": "2016", "venue": "NA", "abstract": "Deep learning has proven useful on many NLP tasks including reading comprehension. However it requires a lot of training data which are not available in some domains of application. Hence we examine the possibility of using data-rich domains to pre-train models and then apply them in domains where training data are harder to get. Specifically, we train a neural-network-based model on two context-question-answer datasets, the BookTest and CNN/Daily Mail, and we monitor transfer to subsets of bAbI, a set of artificial tasks designed"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJM69B5xx", "author_id": ["ULYx6voAAAAJ", "", "", "xKxv70cAAAAJ"], "url_scholarbib": "/scholar?q=info:NKELLNvOba4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFinding%2Ba%2BJack-of-All-Trades:%2BAn%2BExamination%2Bof%2BSemi-supervised%2BLearning%2Bin%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NKELLNvOba4J&ei=AxBkYufdApyO6rQP_qe3mAs&json=", "num_citations": 2, "citedby_url": "/scholar?cites=12568929575793041716&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NKELLNvOba4J:scholar.google.com/&scioq=Finding+a+Jack-of-All-Trades:+An+Examination+of+Semi-supervised+Learning+in+Reading+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJM69B5xx"}, "FractalNet: Ultra-Deep Neural Networks without Residuals": {"container_type": "Publication", "bib": {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["G Larsson", "M Maire", "G Shakhnarovich"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.07648", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.07648", "author_id": ["8hIYYuEAAAAJ", "HXowq5YAAAAJ", "YLOz1kgAAAAJ"], "url_scholarbib": "/scholar?q=info:JHyEXx5KV9QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFractalNet:%2BUltra-Deep%2BNeural%2BNetworks%2Bwithout%2BResiduals%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JHyEXx5KV9QJ&ei=ChBkYtagDZGJmwGY-qmYDQ&json=", "num_citations": 819, "citedby_url": "/scholar?cites=15300779753326541860&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JHyEXx5KV9QJ:scholar.google.com/&scioq=FractalNet:+Ultra-Deep+Neural+Networks+without+Residuals&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.07648"}, "Structured Attention Networks": {"container_type": "Publication", "bib": {"title": "Structured attention networks", "author": ["Y Kim", "C Denton", "L Hoang", "AM Rush"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.00887", "abstract": "structured internal attention layers within deep networks,  We focus on two classes of  structured attention: linear-chain  with structured attention outperform standard attention models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.00887", "author_id": ["n_ts4eYAAAAJ", "", "U-IT6XoAAAAJ", "LIjnUGgAAAAJ"], "url_scholarbib": "/scholar?q=info:y_8ieBv-0x8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructured%2BAttention%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y_8ieBv-0x8J&ei=DxBkYuLpCLKO6rQPy-CRsA8&json=", "num_citations": 378, "citedby_url": "/scholar?cites=2293456029194846155&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y_8ieBv-0x8J:scholar.google.com/&scioq=Structured+Attention+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.00887"}, "Binary Paragraph Vectors": {"container_type": "Publication", "bib": {"title": "Binary paragraph vectors", "author": ["K Grzegorczyk", "M Kurdziel"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01116", "abstract": "Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can   we present Binary Paragraph Vector models: simple neural networks that learn short binary"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01116", "author_id": ["tyldmhAAAAAJ", "F-IzFZIAAAAJ"], "url_scholarbib": "/scholar?q=info:DU62p-HB1PYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBinary%2BParagraph%2BVectors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DU62p-HB1PYJ&ei=GhBkYrPwN--Sy9YPs_mY8AM&json=", "num_citations": 6, "citedby_url": "/scholar?cites=17786054003318476301&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DU62p-HB1PYJ:scholar.google.com/&scioq=Binary+Paragraph+Vectors&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01116"}, "Non-linear Dimensionality Regularizer for Solving Inverse Problems": {"container_type": "Publication", "bib": {"title": "Non-linear dimensionality regularizer for solving inverse problems", "author": ["R Garg", "A Eriksson", "I Reid"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1603.05015", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (un-known) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regulariza-tion technique for solving such problems without pre-training. We re-formulate Kernel-PCA as an energy minimization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1603.05015", "author_id": ["2dvDXjkAAAAJ", "", "ATkNLcQAAAAJ"], "url_scholarbib": "/scholar?q=info:YcJqpMM9MbUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNon-linear%2BDimensionality%2BRegularizer%2Bfor%2BSolving%2BInverse%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YcJqpMM9MbUJ&ei=HxBkYr2xBfmQ6rQP5OqKqAo&json=", "num_citations": 10, "citedby_url": "/scholar?cites=13056284705210155617&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YcJqpMM9MbUJ:scholar.google.com/&scioq=Non-linear+Dimensionality+Regularizer+for+Solving+Inverse+Problems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1603.05015"}, "Dynamic Steerable Frame Networks": {"container_type": "Publication", "bib": {"title": "Dynamic steerable blocks in deep residual networks", "author": ["JH Jacobsen", "B De Brabandere"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Secondly, we introduce the theoretical framework of frame-based CNNs and steerable two-  framebased Resnet blocks to dynamic steerable blocks that have the ability to dynamically"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.00598", "author_id": ["c1FYGAQAAAAJ", "KcMb_7EAAAAJ"], "url_scholarbib": "/scholar?q=info:gQpDGYAYdkwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BSteerable%2BFrame%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gQpDGYAYdkwJ&ei=KhBkYsrCFo6pywTd4KPADw&json=", "num_citations": 24, "citedby_url": "/scholar?cites=5509618132593085057&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gQpDGYAYdkwJ:scholar.google.com/&scioq=Dynamic+Steerable+Frame+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.00598"}, "Revisiting Distributed Synchronous SGD": {"container_type": "Publication", "bib": {"title": "Revisiting distributed synchronous SGD", "author": ["J Chen", "X Pan", "R Monga", "S Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We revisit these conventional beliefs in this paper, and examine the weaknesses of both   we revisit synchronous learning, and propose a method for mitigating stragglers in synchronous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1604.00981", "author_id": ["", "Zldo9CAAAAAJ", "W8yZCNsAAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:p6m8OjbJlbAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRevisiting%2BDistributed%2BSynchronous%2BSGD%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p6m8OjbJlbAJ&ei=LRBkYuzQKOHDywSSipaYAg&json=", "num_citations": 625, "citedby_url": "/scholar?cites=12724297556956064167&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p6m8OjbJlbAJ:scholar.google.com/&scioq=Revisiting+Distributed+Synchronous+SGD&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1604.00981.pdf?ref=https://githubhelp.com"}, "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs": {"container_type": "Publication", "bib": {"title": "A way out of the odyssey: Analyzing and combining recent insights for lstms", "author": ["S Longpre", "S Pradhan", "C Xiong", "R Socher"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of augmentations and modifications to LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, average pooling, and residual connections, along with four other"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.05104", "author_id": ["ADd_YfkAAAAJ", "5mJUkI4AAAAJ", "vaSdahkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:J8hfpmE_-uIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BWay%2Bout%2Bof%2Bthe%2BOdyssey:%2BAnalyzing%2Band%2BCombining%2BRecent%2BInsights%2Bfor%2BLSTMs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J8hfpmE_-uIJ&ei=MxBkYprrEfmQ6rQP5OqKqAo&json=", "num_citations": 11, "citedby_url": "/scholar?cites=16355454685385050151&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:J8hfpmE_-uIJ:scholar.google.com/&scioq=A+Way+out+of+the+Odyssey:+Analyzing+and+Combining+Recent+Insights+for+LSTMs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.05104"}, "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima": {"container_type": "Publication", "bib": {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["NS Keskar", "D Mudigere", "J Nocedal"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "generalization gap, sharp minima, and possible modifications to make large-batch training   In Appendix E, we present some attempts to overcome the problems of large-batch training."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.04836", "author_id": ["CJ-_cEEAAAAJ", "p2R-FRoAAAAJ", "gGNxMZ0AAAAJ"], "url_scholarbib": "/scholar?q=info:JYVTtjInECMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BLarge-Batch%2BTraining%2Bfor%2BDeep%2BLearning:%2BGeneralization%2BGap%2Band%2BSharp%2BMinima%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JYVTtjInECMJ&ei=OBBkYsqXI4uKmgGY1YjABQ&json=", "num_citations": 1906, "citedby_url": "/scholar?cites=2526562489715623205&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JYVTtjInECMJ:scholar.google.com/&scioq=On+Large-Batch+Training+for+Deep+Learning:+Generalization+Gap+and+Sharp+Minima&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.04836.pdf,"}, "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis": {"container_type": "Publication", "bib": {"title": "Visualizing deep neural network decisions: Prediction difference analysis", "author": ["LM Zintgraf", "TS Cohen", "T Adel", "M Welling"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "prediction difference analysis method for visualizing the response of a deep neural network   and provides great additional insight into the decision making process of classifiers. Making"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.04595", "author_id": ["lEzcLFwAAAAJ", "a3q4YxEAAAAJ", "IkhgmfEAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:NMrsvFI43rgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisualizing%2BDeep%2BNeural%2BNetwork%2BDecisions:%2BPrediction%2BDifference%2BAnalysis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NMrsvFI43rgJ&ei=QRBkYqaNAfmQ6rQP5OqKqAo&json=", "num_citations": 545, "citedby_url": "/scholar?cites=13321146675816614452&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NMrsvFI43rgJ:scholar.google.com/&scioq=Visualizing+Deep+Neural+Network+Decisions:+Prediction+Difference+Analysis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.04595"}, "Learning Invariant Representations Of Planar Curves ": {"container_type": "Publication", "bib": {"title": "Learning invariant representations of planar curves", "author": ["G Pai", "A Wetzler", "R Kimmel"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.07807", "abstract": "We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Eucledian and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.07807", "author_id": ["weJ9q1UAAAAJ", "oSGdRWoAAAAJ", "yV7LaW8AAAAJ"], "url_scholarbib": "/scholar?q=info:gIXxrwD1Q30J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BInvariant%2BRepresentations%2BOf%2BPlanar%2BCurves%2B%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gIXxrwD1Q30J&ei=RRBkYsu-JIOEmgHx-5DADA&json=", "num_citations": 6, "citedby_url": "/scholar?cites=9026327461481252224&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gIXxrwD1Q30J:scholar.google.com/&scioq=Learning+Invariant+Representations+Of+Planar+Curves+&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.07807"}, "Universality in halting time": {"container_type": "Publication", "bib": {"title": "Universality in numerical computations with random data", "author": ["PA Deift", "G Menon", "S Olver"], "pub_year": "2014", "venue": "Proceedings of the \u2026", "abstract": "universality is observed for the fluctuations of the halting time\u2014ie, the histogram for the halting  times the sample variance, collapses to a universal curve, independent of the input data"}, "filled": false, "gsrank": 1, "pub_url": "https://www.pnas.org/content/111/42/14973.short", "author_id": ["-QJLMYYAAAAJ", "", "ZszMLyAAAAAJ"], "url_scholarbib": "/scholar?q=info:FVs4a1iomVUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversality%2Bin%2Bhalting%2Btime%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FVs4a1iomVUJ&ei=SRBkYrGkB46pywTd4KPADw&json=", "num_citations": 30, "citedby_url": "/scholar?cites=6168146262370048789&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FVs4a1iomVUJ:scholar.google.com/&scioq=Universality+in+halting+time&hl=en&as_sdt=0,33", "eprint_url": "https://www.pnas.org/content/pnas/111/42/14973.full.pdf"}, "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset": {"container_type": "Publication", "bib": {"title": "MS MARCO: A human generated machine reading comprehension dataset", "author": ["T Nguyen", "M Rosenberg", "X Song", "J Gao", "S Tiwary"], "pub_year": "2016", "venue": "CoCo@ NIPS", "abstract": "sample of those in MS MARCO. We believe a characteristic of reading comprehension is to   the form of a complete sentence, so the workers may write a longform passage on their own."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJ-Qj8-_ZH", "author_id": ["RfcYK8QAAAAJ", "", "0aPSv9kAAAAJ", "CQ1cqKkAAAAJ", ""], "url_scholarbib": "/scholar?q=info:_ZWSD6NMLkoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMS%2BMARCO:%2BA%2BHuman-Generated%2BMAchine%2BReading%2BCOmprehension%2BDataset%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_ZWSD6NMLkoJ&ei=TRBkYq3pA-HDywSSipaYAg&json=", "num_citations": 770, "citedby_url": "/scholar?cites=5345294070960002557&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_ZWSD6NMLkoJ:scholar.google.com/&scioq=MS+MARCO:+A+Human-Generated+MAchine+Reading+COmprehension+Dataset&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJ-Qj8-_ZH"}, "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax": {"container_type": "Publication", "bib": {"title": "An information-theoretic framework for fast and robust unsupervised learning via neural population infomax", "author": ["W Huang", "K Zhang"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01886", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01886", "author_id": ["", "XrqAC5kAAAAJ"], "url_scholarbib": "/scholar?q=info:JzM8fUS6TB4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BInformation-Theoretic%2BFramework%2Bfor%2BFast%2Band%2BRobust%2BUnsupervised%2BLearning%2Bvia%2BNeural%2BPopulation%2BInfomax%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JzM8fUS6TB4J&ei=UBBkYsa7AYOEmgHx-5DADA&json=", "num_citations": 5, "citedby_url": "/scholar?cites=2183324722689487655&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JzM8fUS6TB4J:scholar.google.com/&scioq=An+Information-Theoretic+Framework+for+Fast+and+Robust+Unsupervised+Learning+via+Neural+Population+Infomax&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01886"}, "Tuning Recurrent Neural Networks with Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Tuning recurrent neural networks with reinforcement learning", "author": ["N Jaques", "S Gu", "RE Turner", "D Eck"], "pub_year": "2017", "venue": "NA", "abstract": "The approach of training sequence models using supervised learning and next-step prediction suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Syyv2e-Kx", "author_id": ["8iCb2TwAAAAJ", "B8wslVsAAAAJ", "DgLEyZgAAAAJ", "bLb3VdIAAAAJ"], "url_scholarbib": "/scholar?q=info:rY5YYSO9MdsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTuning%2BRecurrent%2BNeural%2BNetworks%2Bwith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rY5YYSO9MdsJ&ei=UhBkYrL2LeiSy9YPp-OyiAE&json=", "num_citations": 82, "citedby_url": "/scholar?cites=15794613327819738797&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rY5YYSO9MdsJ:scholar.google.com/&scioq=Tuning+Recurrent+Neural+Networks+with+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Syyv2e-Kx"}, "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "Deep unsupervised clustering with gaussian mixture variational autoencoders", "author": ["N Dilokthanakul", "PAM Mediano", "M Garnelo"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02648", "author_id": ["ouce6eWAbloC", "I9-416MAAAAJ", "Hr3zNQUAAAAJ"], "url_scholarbib": "/scholar?q=info:UQ2tyqAeEE0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BUnsupervised%2BClustering%2Bwith%2BGaussian%2BMixture%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UQ2tyqAeEE0J&ei=VRBkYqvEEqKUy9YP_JONiAY&json=", "num_citations": 476, "citedby_url": "/scholar?cites=5552972016491760977&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UQ2tyqAeEE0J:scholar.google.com/&scioq=Deep+Unsupervised+Clustering+with+Gaussian+Mixture+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02648"}, "Temporal Ensembling for Semi-Supervised Learning": {"container_type": "Publication", "bib": {"title": "Temporal ensembling for semi-supervised learning", "author": ["S Laine", "T Aila"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.02242", "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.02242", "author_id": ["UCXJOTUAAAAJ", "e7abmgkAAAAJ"], "url_scholarbib": "/scholar?q=info:ONU6BsKS17AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTemporal%2BEnsembling%2Bfor%2BSemi-Supervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ONU6BsKS17AJ&ei=WBBkYuPjLIySyASZk6HgCA&json=", "num_citations": 1433, "citedby_url": "/scholar?cites=12742815032693937464&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ONU6BsKS17AJ:scholar.google.com/&scioq=Temporal+Ensembling+for+Semi-Supervised+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.02242.pdf?ref=https://githubhelp.com"}, "Efficient iterative policy optimization": {"container_type": "Publication", "bib": {"title": "Efficient iterative policy optimization", "author": ["NL Roux"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.08967", "abstract": "In this paper, we shall focus on how to efficiently use these logs to obtain a good policy. In   initial policy is outside the scope of this paper. There are many ways to learn good policies"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.08967", "author_id": ["LmKtwk8AAAAJ"], "url_scholarbib": "/scholar?q=info:w7tz7_0ktgwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2Biterative%2Bpolicy%2Boptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w7tz7_0ktgwJ&ei=XBBkYqu3E5qSy9YP8pKNsAE&json=", "num_citations": 4, "citedby_url": "/scholar?cites=915960247279139779&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w7tz7_0ktgwJ:scholar.google.com/&scioq=Efficient+iterative+policy+optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.08967"}, "Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses": {"container_type": "Publication", "bib": {"title": "Multilayer recurrent network models of primate retinal ganglion cell responses", "author": ["E Batty", "J Merel", "N Brackbill", "A Heitman", "A Sher", "A Litke"], "pub_year": "2016", "venue": "NA", "abstract": "Developing accurate predictive models of sensory neurons is vital to understanding sensory processing and brain computations. The current standard approach to modeling neurons is to start with simple models and to incrementally add interpretable features. An alternative approach is to start with a more complex model that captures responses accurately, and then probe the fitted model structure to understand the neural computations. Here, we show that a multitask recurrent neural network (RNN) framework provides the flexibility necessary"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkEI22jeg", "author_id": ["Zyvyp-8AAAAJ", "K4OcFXUAAAAJ", "xtcjYc8AAAAJ", "", "", ""], "url_scholarbib": "/scholar?q=info:SD5V6b6Ps1EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultilayer%2BRecurrent%2BNetwork%2BModels%2Bof%2BPrimate%2BRetinal%2BGanglion%2BCell%2BResponses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SD5V6b6Ps1EJ&ei=bBBkYsvACO-Sy9YPs_mY8AM&json=", "num_citations": 52, "citedby_url": "/scholar?cites=5887207188024606280&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SD5V6b6Ps1EJ:scholar.google.com/&scioq=Multilayer+Recurrent+Network+Models+of+Primate+Retinal+Ganglion+Cell+Responses&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkEI22jeg"}, "Encoding and Decoding Representations with Sum- and Max-Product Networks": {"container_type": "Publication", "bib": {"title": "Encoding and decoding representations with sum-And max-product networks", "author": ["A Vergari", "R Peharz", "N Di Mauro", "F Esposito"], "pub_year": "2017", "venue": "NA", "abstract": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation Learning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkndY2VYx", "author_id": ["YK0NLaUAAAAJ", "ywkqnqMAAAAJ", "WqfpBgkAAAAJ", "ptnjVPAAAAAJ"], "url_scholarbib": "/scholar?q=info:zMqpfDCXWBcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEncoding%2Band%2BDecoding%2BRepresentations%2Bwith%2BSum-%2Band%2BMax-Product%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zMqpfDCXWBcJ&ei=dhBkYrusDpLeyQTE46-QAg&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:zMqpfDCXWBcJ:scholar.google.com/&scioq=Encoding+and+Decoding+Representations+with+Sum-+and+Max-Product+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkndY2VYx"}, "Simple Black-Box Adversarial Perturbations for Deep Networks": {"container_type": "Publication", "bib": {"title": "Simple black-box adversarial perturbations for deep networks", "author": ["N Narodytska", "SP Kasiviswanathan"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.06299", "abstract": "Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to carefully crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.06299", "author_id": ["pQw6xK4AAAAJ", "XnHdkZUAAAAJ"], "url_scholarbib": "/scholar?q=info:TM2sFclX-ZYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimple%2BBlack-Box%2BAdversarial%2BPerturbations%2Bfor%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TM2sFclX-ZYJ&ei=gBBkYobaHeHDywSSipaYAg&json=", "num_citations": 188, "citedby_url": "/scholar?cites=10878822896053833036&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TM2sFclX-ZYJ:scholar.google.com/&scioq=Simple+Black-Box+Adversarial+Perturbations+for+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.06299.pdf?ref=https://githubhelp.com"}, "Here's My Point: Argumentation Mining with Pointer Networks": {"container_type": "Publication", "bib": {"title": "Here's My Point: Argumentation Mining with Pointer Networks", "author": ["P Potash", "A Romanov", "A Rumshisky"], "pub_year": "2016", "venue": "NA", "abstract": "One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on extracting links between"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1KBHtcel", "author_id": ["JZ38E6gAAAAJ", "huBJSMwAAAAJ", "_Q1uzVYAAAAJ"], "url_scholarbib": "/scholar?q=info:6UWLTNwHCPwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHere%2527s%2BMy%2BPoint:%2BArgumentation%2BMining%2Bwith%2BPointer%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6UWLTNwHCPwJ&ei=hBBkYpbaBZqSy9YP8pKNsAE&json=", "num_citations": 3, "citedby_url": "/scholar?cites=18160774140129920489&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6UWLTNwHCPwJ:scholar.google.com/&scioq=Here%27s+My+Point:+Argumentation+Mining+with+Pointer+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1KBHtcel"}, "Learning Identity Mappings with Residual Gates": {"container_type": "Publication", "bib": {"title": "Learning identity mappings with residual gates", "author": ["PHP Savarese", "LO Mazza", "DR Figueiredo"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a new layer design by adding a linear gating mechanism to shortcut connections. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01260", "author_id": ["PwQHMlwAAAAJ", "WHKvYisAAAAJ", "j4YbANwAAAAJ"], "url_scholarbib": "/scholar?q=info:2YmIR-pbH98J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BIdentity%2BMappings%2Bwith%2BResidual%2BGates%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2YmIR-pbH98J&ei=hhBkYqb4Iu-Sy9YPs_mY8AM&json=", "num_citations": 9, "citedby_url": "/scholar?cites=16077670256516565465&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2YmIR-pbH98J:scholar.google.com/&scioq=Learning+Identity+Mappings+with+Residual+Gates&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01260"}, "Attentive Recurrent Comparators": {"container_type": "Publication", "bib": {"title": "Attentive recurrent comparators", "author": ["P Shyam", "S Gupta", "A Dukkipati"], "pub_year": "2017", "venue": "\u2026 Conference on Machine \u2026", "abstract": "Rapid learning requires flexible representations to quickly adopt to new evidence. We  develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/shyam17a.html?ref=https://githubhelp.com", "author_id": ["TAomEzQAAAAJ", "Nt-tK2UAAAAJ", "0y2aAvgAAAAJ"], "url_scholarbib": "/scholar?q=info:_plmS9xvW98J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttentive%2BRecurrent%2BComparators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_plmS9xvW98J&ei=ihBkYsWyFrKO6rQPy-CRsA8&json=", "num_citations": 99, "citedby_url": "/scholar?cites=16094580685287102974&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_plmS9xvW98J:scholar.google.com/&scioq=Attentive+Recurrent+Comparators&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/shyam17a/shyam17a.pdf"}, "Rotation Plane Doubly Orthogonal Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "author": ["Z McCarthy", "A Bai", "X Chen", "P Abbeel"], "pub_year": "2016", "venue": "NA", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem. The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ry_4vpixl", "author_id": ["lsbreWwAAAAJ", "", "", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:FyNwGHrmIvEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRotation%2BPlane%2BDoubly%2BOrthogonal%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FyNwGHrmIvEJ&ei=jRBkYoGlCuHDywSSipaYAg&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:FyNwGHrmIvEJ:scholar.google.com/&scioq=Rotation+Plane+Doubly+Orthogonal+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ry_4vpixl"}, "Learning to Navigate in Complex Environments": {"container_type": "Publication", "bib": {"title": "Learning to navigate in complex environments", "author": ["P Mirowski", "R Pascanu", "F Viola", "H Soyer"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.03673", "author_id": ["uKXVH54AAAAJ", "eSPY8LwAAAAJ", "-cCry1cAAAAJ", "HtYyb5sAAAAJ"], "url_scholarbib": "/scholar?q=info:PZSWOOlQ1_QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BNavigate%2Bin%2BComplex%2BEnvironments%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PZSWOOlQ1_QJ&ei=kRBkYoDeAc6E6rQP5-KmKA&json=", "num_citations": 701, "citedby_url": "/scholar?cites=17642659027854201917&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PZSWOOlQ1_QJ:scholar.google.com/&scioq=Learning+to+Navigate+in+Complex+Environments&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.03673.pdf?ref=https://githubhelp.com"}, "Understanding Trainable Sparse Coding with Matrix Factorization": {"container_type": "Publication", "bib": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "author": ["TMCENS Paris-Saclay", "J Bruna", "J Audiffren"], "venue": "NA", "pub_year": "NA", "abstract": "LISTA is the unfolded version of the RNN of ISTA, trainable with back-propagation.   Sparse coding for the PASCAL 08 datasets over the Haar wavelets family. The sparse  coding is performed for patches of size 8 \u00d7 8."}, "filled": false, "gsrank": 1, "pub_url": "https://tommoral.github.io/talks/17_11_Google_ZRH.pdf", "author_id": ["", "L4bNmsMAAAAJ", ""], "url_scholarbib": "/scholar?q=info:ak4g4OSqAAsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BTrainable%2BSparse%2BCoding%2Bwith%2BMatrix%2BFactorization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ak4g4OSqAAsJ&ei=lBBkYtH1B6KUy9YP_JONiAY&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:ak4g4OSqAAsJ:scholar.google.com/&scioq=Understanding+Trainable+Sparse+Coding+with+Matrix+Factorization&hl=en&as_sdt=0,33", "eprint_url": "https://tommoral.github.io/talks/17_11_Google_ZRH.pdf"}, "ReasoNet: Learning to Stop Reading in Machine Comprehension": {"container_type": "Publication", "bib": {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Y Shen", "PS Huang", "J Gao", "W Chen"], "pub_year": "2017", "venue": "Proceedings of the 23rd ACM \u2026", "abstract": "Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3097983.3098177", "author_id": ["S6OFEFEAAAAJ", "4oJB32YAAAAJ", "CQ1cqKkAAAAJ", "LG_E-4EAAAAJ"], "url_scholarbib": "/scholar?q=info:Ob0iIGFyjpkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReasoNet:%2BLearning%2Bto%2BStop%2BReading%2Bin%2BMachine%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ob0iIGFyjpkJ&ei=mBBkYpmrAeiSy9YPp-OyiAE&json=", "num_citations": 281, "citedby_url": "/scholar?cites=11064907095972429113&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ob0iIGFyjpkJ:scholar.google.com/&scioq=ReasoNet:+Learning+to+Stop+Reading+in+Machine+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.05284?ref=https://githubhelp.com"}, "Capacity and Trainability in Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Capacity and trainability in recurrent neural networks", "author": ["J Collins", "J Sohl-Dickstein", "D Sussillo"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.09913", "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09913", "author_id": ["QOmjMZ0AAAAJ", "-3zYIjQAAAAJ", "ebBgMSkAAAAJ"], "url_scholarbib": "/scholar?q=info:I4oHB_YmlRUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCapacity%2Band%2BTrainability%2Bin%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=I4oHB_YmlRUJ&ei=mxBkYqm6GI2ymgHg1rfQDQ&json=", "num_citations": 176, "citedby_url": "/scholar?cites=1555192084448119331&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:I4oHB_YmlRUJ:scholar.google.com/&scioq=Capacity+and+Trainability+in+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09913"}, "BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL": {"container_type": "Publication", "bib": {"title": "BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL", "author": ["V Roger", "M Bartcus", "F Chamroukhi", "H Glotin"], "pub_year": "2016", "venue": "NA", "abstract": "Understanding the communication between different animals by analysing their acoustic signals is an important topic in bioacoustics. It can be a powerful tool for the preservation of ecological diversity. We investigate probabilistic models to analyse signals issued from real-world bioacoustic sound scenes. We study a Bayesian non-parametric sequential models based on Hierarchical Dirichlet Process Hidden Markov Models (HDP-HMM). The model is able to infer hidden states, that are referred here as song units. However, using such a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hk1l9Xqxe", "author_id": ["", "", "AOxrOs0AAAAJ", "DqieizcAAAAJ"], "url_scholarbib": "/scholar?q=info:PpSdYTlX7NYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBIOACOUSTIC%2BSEGMENTATION%2BBY%2BHIERARCHICAL%2BDIRICHLET%2BPROCESS%2BHIDDEN%2BMARKOV%2BMODEL%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PpSdYTlX7NYJ&ei=oBBkYquZHZqSy9YP8pKNsAE&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:PpSdYTlX7NYJ:scholar.google.com/&scioq=BIOACOUSTIC+SEGMENTATION+BY+HIERARCHICAL+DIRICHLET+PROCESS+HIDDEN+MARKOV+MODEL&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hk1l9Xqxe"}, "Learning Approximate Distribution-Sensitive Data Structures": {"container_type": "Publication", "bib": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "author": ["Z Tavares", "A Solar-Lezama"], "pub_year": "2016", "venue": "NA", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, ie, which exploit non-uniformity in their usage patterns to reduce time or space complexity. Abstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior. We extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJVEEF9lx", "author_id": ["bK6k2gcAAAAJ", "8BX3BokAAAAJ"], "url_scholarbib": "/scholar?q=info:RD6nRAmc5ScJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BApproximate%2BDistribution-Sensitive%2BData%2BStructures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RD6nRAmc5ScJ&ei=pBBkYonYBJLeyQTE46-QAg&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:RD6nRAmc5ScJ:scholar.google.com/&scioq=Learning+Approximate+Distribution-Sensitive+Data+Structures&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJVEEF9lx"}, "Learning Features of Music From Scratch": {"container_type": "Publication", "bib": {"title": "Learning features of music from scratch", "author": ["J Thickstun", "Z Harchaoui", "S Kakade"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.09827", "abstract": "This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09827", "author_id": ["RkuzIZMAAAAJ", "yCyR-TsAAAAJ", "wb-DKCIAAAAJ"], "url_scholarbib": "/scholar?q=info:j-RFQ8h1OUoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFeatures%2Bof%2BMusic%2BFrom%2BScratch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j-RFQ8h1OUoJ&ei=phBkYraEPe-Sy9YPs_mY8AM&json=", "num_citations": 146, "citedby_url": "/scholar?cites=5348435535461737615&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:j-RFQ8h1OUoJ:scholar.google.com/&scioq=Learning+Features+of+Music+From+Scratch&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09827"}, "Adversarial Machine Learning at Scale": {"container_type": "Publication", "bib": {"title": "Adversarial machine learning at scale", "author": ["A Kurakin", "I Goodfellow", "S Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01236", "abstract": "Adversarial training was originally developed for small models that did not use batch  normalization. To scale adversarial training to ImageNet, we recommend using batch normalization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01236", "author_id": ["nCh4qyMAAAAJ", "iYN86KEAAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:5tTjtqmdF3IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BMachine%2BLearning%2Bat%2BScale%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5tTjtqmdF3IJ&ei=qhBkYtSPCY2ymgHg1rfQDQ&json=", "num_citations": 2067, "citedby_url": "/scholar?cites=8221212997031548134&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5tTjtqmdF3IJ:scholar.google.com/&scioq=Adversarial+Machine+Learning+at+Scale&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01236.pdf?ref=https://githubhelp.com"}, "Emergence of foveal image sampling from learning to attend in visual scenes": {"container_type": "Publication", "bib": {"title": "Emergence of foveal image sampling from learning to attend in visual scenes", "author": ["B Cheung", "E Weiss", "B Olshausen"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.09430", "abstract": "We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09430", "author_id": ["7N-ethYAAAAJ", "", "4aqK_74AAAAJ"], "url_scholarbib": "/scholar?q=info:mORxHjbF0UgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergence%2Bof%2Bfoveal%2Bimage%2Bsampling%2Bfrom%2Blearning%2Bto%2Battend%2Bin%2Bvisual%2Bscenes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mORxHjbF0UgJ&ei=rhBkYsKPAsLZmQHnraWYCA&json=", "num_citations": 34, "citedby_url": "/scholar?cites=5247191877093024920&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mORxHjbF0UgJ:scholar.google.com/&scioq=Emergence+of+foveal+image+sampling+from+learning+to+attend+in+visual+scenes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09430"}, "Towards the Limit of Network Quantization": {"container_type": "Publication", "bib": {"title": "Towards the limit of network quantization", "author": ["Y Choi", "M El-Khamy", "J Lee"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.01543", "abstract": "Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.01543", "author_id": ["haggDAwAAAAJ", "qxPC268AAAAJ", "Cya7Va8AAAAJ"], "url_scholarbib": "/scholar?q=info:qaRCiOfWvgkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2Bthe%2BLimit%2Bof%2BNetwork%2BQuantization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qaRCiOfWvgkJ&ei=sRBkYolng4SaAfH7kMAM&json=", "num_citations": 149, "citedby_url": "/scholar?cites=702234881828234409&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qaRCiOfWvgkJ:scholar.google.com/&scioq=Towards+the+Limit+of+Network+Quantization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.01543?ref=https://githubhelp.com"}, "Near-Data Processing for Machine Learning": {"container_type": "Publication", "bib": {"title": "Near-data processing for differentiable machine learning models", "author": ["H Choe", "S Lee", "H Nam", "S Park", "S Kim"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "ISP of machine learning workloads and evaluate the potential of NDP for machine learning  in ISP.  SSD that can execute various machine learning algorithms using the data stored on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.02273", "author_id": ["FpmqgJEAAAAJ", "-ERi6-cAAAAJ", "", "j8qTMtoAAAAJ", "ovimNFsAAAAJ"], "url_scholarbib": "/scholar?q=info:2sVODzS1LHYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNear-Data%2BProcessing%2Bfor%2BMachine%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2sVODzS1LHYJ&ei=sxBkYt7bJZLeyQTE46-QAg&json=", "num_citations": 18, "citedby_url": "/scholar?cites=8515380230650512858&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2sVODzS1LHYJ:scholar.google.com/&scioq=Near-Data+Processing+for+Machine+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.02273"}, "A Differentiable Physics Engine for Deep Learning in Robotics": {"container_type": "Publication", "bib": {"title": "A differentiable physics engine for deep learning in robotics", "author": ["J Degrave", "M Hermans", "J Dambre"], "pub_year": "2019", "venue": "Frontiers in neurorobotics", "abstract": ", such as found in deep learning. We propose the implementation of a modern physics engine,  which can differentiate control parameters. This engine is implemented for both CPU and"}, "filled": false, "gsrank": 1, "pub_url": "https://www.frontiersin.org/articles/10.3389/fnbot.2019.00006/full?ref=https://githubhelp.com", "author_id": ["M3bkh8MAAAAJ", "WVT7QKoAAAAJ", "0zJ4w-MAAAAJ"], "url_scholarbib": "/scholar?q=info:9fk9Z9AdsloJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDifferentiable%2BPhysics%2BEngine%2Bfor%2BDeep%2BLearning%2Bin%2BRobotics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9fk9Z9AdsloJ&ei=xRBkYvG9L6KUy9YP_JONiAY&json=", "num_citations": 137, "citedby_url": "/scholar?cites=6535318790190529013&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9fk9Z9AdsloJ:scholar.google.com/&scioq=A+Differentiable+Physics+Engine+for+Deep+Learning+in+Robotics&hl=en&as_sdt=0,33", "eprint_url": "https://www.frontiersin.org/articles/10.3389/fnbot.2019.00006/full?ref=https://githubhelp.com"}, "An Actor-critic Algorithm for Learning Rate Learning": {"container_type": "Publication", "bib": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "author": ["C Xu", "T Qin", "G Wang", "TY Liu"], "pub_year": "2016", "venue": "NA", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Sy7m72Ogg", "author_id": ["", "Bl4SRU0AAAAJ", "", "Nh832fgAAAAJ"], "url_scholarbib": "/scholar?q=info:E7doXH7G0igJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BActor-critic%2BAlgorithm%2Bfor%2BLearning%2BRate%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=E7doXH7G0igJ&ei=yRBkYrapCOiSy9YPp-OyiAE&json=", "num_citations": 1, "citedby_url": "/scholar?cites=2941631752644900627&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:E7doXH7G0igJ:scholar.google.com/&scioq=An+Actor-critic+Algorithm+for+Learning+Rate+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Sy7m72Ogg"}, "Higher Order Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Higher order recurrent neural networks", "author": ["R Soltani", "H Jiang"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.00064", "abstract": ", we call these new recurrent structures as higher order recurrent neural networks (HORNNs).  At  In section 3, we first present the key idea of higher order RNNs (HORNNs) in detail, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.00064", "author_id": ["s2V5NbYAAAAJ", "lQi05ZkAAAAJ"], "url_scholarbib": "/scholar?q=info:HvQRBxfReMYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHigher%2BOrder%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HvQRBxfReMYJ&ei=zRBkYtmoDY6pywTd4KPADw&json=", "num_citations": 59, "citedby_url": "/scholar?cites=14301410513548080158&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HvQRBxfReMYJ:scholar.google.com/&scioq=Higher+Order+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.00064"}, "Exponential Machines": {"container_type": "Publication", "bib": {"title": "Exponential machines", "author": ["A Novikov", "M Trofimov", "I Oseledets"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.03795", "abstract": "Modeling interactions between features improves the performance of machine learning  solutions in many domains (eg recommender systems or sentiment analysis). In this paper, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.03795", "author_id": ["jMUkLqwAAAAJ", "5TILEvEAAAAJ", "5kMqBQEAAAAJ"], "url_scholarbib": "/scholar?q=info:BH5N18-Ik8wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExponential%2BMachines%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BH5N18-Ik8wJ&ei=0BBkYpKRKo2ymgHg1rfQDQ&json=", "num_citations": 80, "citedby_url": "/scholar?cites=14741276431565553156&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BH5N18-Ik8wJ:scholar.google.com/&scioq=Exponential+Machines&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.03795"}, "Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning": {"container_type": "Publication", "bib": {"title": "Training agent for first-person shooter game with actor-critic curriculum learning", "author": ["Y Wu", "Y Tian"], "pub_year": "2016", "venue": "NA", "abstract": "In this paper, we propose a novel framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents' information. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hk3mPK5gg", "author_id": ["mJQI-gUAAAAJ", "0mgEF28AAAAJ"], "url_scholarbib": "/scholar?q=info:f4q2pII1h9sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BAgent%2Bfor%2BFirst-Person%2BShooter%2BGame%2Bwith%2BActor-Critic%2BCurriculum%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=f4q2pII1h9sJ&ei=0xBkYo3cIPmQ6rQP5OqKqAo&json=", "num_citations": 164, "citedby_url": "/scholar?cites=15818671051387603583&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:f4q2pII1h9sJ:scholar.google.com/&scioq=Training+Agent+for+First-Person+Shooter+Game+with+Actor-Critic+Curriculum+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hk3mPK5gg"}, "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks": {"container_type": "Publication", "bib": {"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Y Adi", "E Kermany", "Y Belinkov", "O Lavi"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.04207", "author_id": ["4W-HuYYAAAAJ", "", "K-6ujU4AAAAJ", "89o6AvkAAAAJ"], "url_scholarbib": "/scholar?q=info:VH_OxaGIkFcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFine-grained%2BAnalysis%2Bof%2BSentence%2BEmbeddings%2BUsing%2BAuxiliary%2BPrediction%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VH_OxaGIkFcJ&ei=3RBkYorlH--Sy9YPs_mY8AM&json=", "num_citations": 410, "citedby_url": "/scholar?cites=6309693306335821652&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VH_OxaGIkFcJ:scholar.google.com/&scioq=Fine-grained+Analysis+of+Sentence+Embeddings+Using+Auxiliary+Prediction+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.04207.pdf?ref=https://githubhelp.com"}, "Steerable CNNs": {"container_type": "Publication", "bib": {"title": "Steerable cnns", "author": ["TS Cohen", "M Welling"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.08498", "abstract": "kinds of steerable CNN, obtained by replacing the convolution layers by steerable convolution   We first tested steerable CNNs that consist entirely of a single kind of capsule. We found"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.08498", "author_id": ["a3q4YxEAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:_tlPAoJ31KQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSteerable%2BCNNs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_tlPAoJ31KQJ&ei=4BBkYtj_J7KO6rQPy-CRsA8&json=", "num_citations": 285, "citedby_url": "/scholar?cites=11877249517551081982&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_tlPAoJ31KQJ:scholar.google.com/&scioq=Steerable+CNNs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.08498"}, "Generative Adversarial Networks as Variational Training of Energy Based Models": {"container_type": "Publication", "bib": {"title": "Generative adversarial networks as variational training of energy based models", "author": ["S Zhai", "Y Cheng", "R Feris", "Z Zhang"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01799", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $ p (\\mathbf {x}) $ is approximated by a variational distribution $ q (\\mathbf {x}) $ that is easy to sample from. The training of VGAN takes a two step procedure: given $ p (\\mathbf {x}) $, $ q (\\mathbf {x}) $ is updated to maximize the lower bound; $ p (\\mathbf {x}) $ is then updated one step with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01799", "author_id": ["G6vdBYsAAAAJ", "ORPxbV4AAAAJ", "xt3XLjcAAAAJ", "RM7y-rcAAAAJ"], "url_scholarbib": "/scholar?q=info:HA3QWbUzPp8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BAdversarial%2BNetworks%2Bas%2BVariational%2BTraining%2Bof%2BEnergy%2BBased%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HA3QWbUzPp8J&ei=4xBkYr_QD42ymgHg1rfQDQ&json=", "num_citations": 29, "citedby_url": "/scholar?cites=11474665754575506716&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HA3QWbUzPp8J:scholar.google.com/&scioq=Generative+Adversarial+Networks+as+Variational+Training+of+Energy+Based+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01799.pdf?ref=https://githubhelp.com"}, "FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS": {"container_type": "Publication", "bib": {"title": "Filter shaping for convolutional neural networks", "author": ["X Li", "F Li", "X Fern", "R Raich"], "pub_year": "2016", "venue": "NA", "abstract": "Convolutional neural networks (CNNs) are powerful tools for classification of visual inputs. An important property of CNN is its restriction to local connections and sharing of local weights among different locations. In this paper, we consider the definition of appropriate local neighborhoods in CNN. We provide a theoretical analysis that justifies the traditional square filter used in CNN for analyzing natural images. The analysis also provides a principle for designing customized filter shapes for application domains that do not resemble"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1TER2oll", "author_id": ["33PUgaAAAAAJ", "snDpfA0AAAAJ", "rnDD_oEAAAAJ", "XkBkmnwAAAAJ"], "url_scholarbib": "/scholar?q=info:Y5uEd9uVIvIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFILTER%2BSHAPING%2BFOR%2BCONVOLUTIONAL%2BNEURAL%2BNETWORKS%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Y5uEd9uVIvIJ&ei=5RBkYv2tPM6E6rQP5-KmKA&json=", "num_citations": 24, "citedby_url": "/scholar?cites=17447672676222278499&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Y5uEd9uVIvIJ:scholar.google.com/&scioq=FILTER+SHAPING+FOR+CONVOLUTIONAL+NEURAL+NETWORKS&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1TER2oll"}, "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["A Gupta", "C Devin", "YX Liu", "P Abbeel"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (eg, different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.02949", "author_id": ["1wLVDP4AAAAJ", "Ivot3fkAAAAJ", "zUJus70AAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:-3VKDM8xPWwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BInvariant%2BFeature%2BSpaces%2Bto%2BTransfer%2BSkills%2Bwith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-3VKDM8xPWwJ&ei=7xBkYvXiErKO6rQPy-CRsA8&json=", "num_citations": 199, "citedby_url": "/scholar?cites=7799444895009764859&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-3VKDM8xPWwJ:scholar.google.com/&scioq=Learning+Invariant+Feature+Spaces+to+Transfer+Skills+with+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.02949.pdf),"}, "CONTENT2VEC: SPECIALIZING JOINT REPRESENTATIONS OF PRODUCT IMAGES AND TEXT FOR THE TASK OF PRODUCT RECOMMENDATION": {"container_type": "Publication", "bib": {"title": "Content2vec: Specializing joint representations of product images and text for the task of product recommendation", "author": ["T Nedelec", "E Smirnova", "F Vasile"], "pub_year": "2016", "venue": "NA", "abstract": "We propose a unified product embedded representation that is optimized for the task of retrieval-based product recommendation. We generate this representation using Content2Vec, a new deep architecture that merges product content infor-mation such as text and image and we analyze its performance on hard recom-mendation setups such as cold-start and cross-category recommendations. In the case of a normal recommendation regime where collaborative information signal is available we merge the product co-occurence"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryTYxh5ll", "author_id": ["DiLQ2X8AAAAJ", "1hxYNEIAAAAJ", "L7qb6ToAAAAJ"], "url_scholarbib": "/scholar?q=info:zpGPJU7bmroJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCONTENT2VEC:%2BSPECIALIZING%2BJOINT%2BREPRESENTATIONS%2BOF%2BPRODUCT%2BIMAGES%2BAND%2BTEXT%2BFOR%2BTHE%2BTASK%2BOF%2BPRODUCT%2BRECOMMENDATION%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zpGPJU7bmroJ&ei=_BBkYsOPJIOEmgHx-5DADA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=13446300766152135118&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zpGPJU7bmroJ:scholar.google.com/&scioq=CONTENT2VEC:+SPECIALIZING+JOINT+REPRESENTATIONS+OF+PRODUCT+IMAGES+AND+TEXT+FOR+THE+TASK+OF+PRODUCT+RECOMMENDATION&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryTYxh5ll"}, "Boosting Image Captioning with Attributes": {"container_type": "Publication", "bib": {"title": "Boosting image captioning with attributes", "author": ["T Yao", "Y Pan", "Y Li", "Z Qiu", "T Mei"], "pub_year": "2017", "venue": "Proceedings of the IEEE \u2026", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A)-a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. Particularly, the learning of attributes is strengthened by integrating inter"}, "filled": false, "gsrank": 1, "pub_url": "http://openaccess.thecvf.com/content_iccv_2017/html/Yao_Boosting_Image_Captioning_ICCV_2017_paper.html", "author_id": ["7Yc6yssAAAAJ", "2RxXFPoAAAAJ", "7_1gqKgAAAAJ", "mvPyFLoAAAAJ", "7Yq4wf4AAAAJ"], "url_scholarbib": "/scholar?q=info:ERNh0taA8vkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosting%2BImage%2BCaptioning%2Bwith%2BAttributes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ERNh0taA8vkJ&ei=_xBkYvLJNe-Sy9YPs_mY8AM&json=", "num_citations": 570, "citedby_url": "/scholar?cites=18010599519948968721&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ERNh0taA8vkJ:scholar.google.com/&scioq=Boosting+Image+Captioning+with+Attributes&hl=en&as_sdt=0,33", "eprint_url": "https://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf"}, "Program Synthesis for Character Level Language Modeling": {"container_type": "Publication", "bib": {"title": "Program synthesis for character level language modeling", "author": ["P Bielik", "V Raychev", "M Vechev"], "pub_year": "2016", "venue": "NA", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases:(i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data-the process is done via counting, as in simple language"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ry_sjFqgx", "author_id": ["JNTwnawAAAAJ", "8JzPv9UAAAAJ", "aZ1Rh50AAAAJ"], "url_scholarbib": "/scholar?q=info:YwH2Y1PzLKYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgram%2BSynthesis%2Bfor%2BCharacter%2BLevel%2BLanguage%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YwH2Y1PzLKYJ&ei=AxFkYt24J7KO6rQPy-CRsA8&json=", "num_citations": 6, "citedby_url": "/scholar?cites=11974213048756207971&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YwH2Y1PzLKYJ:scholar.google.com/&scioq=Program+Synthesis+for+Character+Level+Language+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ry_sjFqgx"}, "Improving Sampling from Generative Autoencoders with Markov Chains": {"container_type": "Publication", "bib": {"title": "Improving sampling from generative autoencoders with markov chains", "author": ["K Arulkumaran", "A Creswell", "AA Bharath"], "pub_year": "2016", "venue": "NA", "abstract": "We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. We define generative autoencoders as autoencoders which are trained to softly enforce a prior on the latent distribution learned by the model. However, the model does not necessarily learn to match the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively encoding and decoding, which allows us to sample from the learned latent"}, "filled": false, "gsrank": 1, "pub_url": "https://spiral.imperial.ac.uk/handle/10044/1/42493", "author_id": ["QKCypSoAAAAJ", "faiFBhoAAAAJ", "3TeLlQsAAAAJ"], "url_scholarbib": "/scholar?q=info:WlEiZGU1cW0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BSampling%2Bfrom%2BGenerative%2BAutoencoders%2Bwith%2BMarkov%2BChains%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WlEiZGU1cW0J&ei=BhFkYruiPIyuyAT-mrWwCA&json=", "num_citations": 20, "citedby_url": "/scholar?cites=7886143132090388826&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WlEiZGU1cW0J:scholar.google.com/&scioq=Improving+Sampling+from+Generative+Autoencoders+with+Markov+Chains&hl=en&as_sdt=0,33", "eprint_url": "https://spiral.imperial.ac.uk/bitstream/10044/1/42493/2/1610.09296v1.pdf"}, "Efficient Representation of Low-Dimensional Manifolds using Deep Networks": {"container_type": "Publication", "bib": {"title": "Efficient representation of low-dimensional manifolds using deep networks", "author": ["R Basri", "D Jacobs"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1602.04723", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data. We first show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably, the network can do this using an almost optimal number of parameters"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1602.04723", "author_id": ["d6vuvHIAAAAJ", "WH2KmRgAAAAJ"], "url_scholarbib": "/scholar?q=info:Zld2I0d-kiYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BRepresentation%2Bof%2BLow-Dimensional%2BManifolds%2Busing%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Zld2I0d-kiYJ&ei=ChFkYtf7G7KO6rQPy-CRsA8&json=", "num_citations": 35, "citedby_url": "/scholar?cites=2779422764043753318&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Zld2I0d-kiYJ:scholar.google.com/&scioq=Efficient+Representation+of+Low-Dimensional+Manifolds+using+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1602.04723"}, "NEWSQA: A MACHINE COMPREHENSION DATASET": {"container_type": "Publication", "bib": {"title": "Newsqa: A machine comprehension dataset", "author": ["A Trischler", "T Wang", "X Yuan", "J Harris", "A Sordoni"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "NewsQA, a challenging machine comprehension dataset of over  news articles from CNN,  with answers consisting of spans of text from the corresponding articles. We collect this dataset"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09830", "author_id": ["EvUM6UUAAAAJ", "cINVaEoAAAAJ", "dS3LulEAAAAJ", "w9QAUcAAAAAJ", "DJon7w4AAAAJ"], "url_scholarbib": "/scholar?q=info:mEYuvk8AlR0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNEWSQA:%2BA%2BMACHINE%2BCOMPREHENSION%2BDATASET%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mEYuvk8AlR0J&ei=EBFkYvuwI5qSy9YP8pKNsAE&json=", "num_citations": 552, "citedby_url": "/scholar?cites=2131610341122918040&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mEYuvk8AlR0J:scholar.google.com/&scioq=NEWSQA:+A+MACHINE+COMPREHENSION+DATASET&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09830.pdf%5D"}, "Tree-structured decoding with doubly-recurrent neural networks": {"container_type": "Publication", "bib": {"title": "Tree-structured decoding with doubly-recurrent neural networks", "author": ["D Alvarez-Melis", "TS Jaakkola"], "pub_year": "2016", "venue": "NA", "abstract": "We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly-recurrent neural network that models separately the width and depth recurrences across the tree, and combines them inside each cell to generate an output. The topology of the tree is explicitly modeled, allowing the network to predict both content and topology of the tree when decoding. That is, given only an encoded vector representation, the network is able to simultaneously generate"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkYhZDqxg", "author_id": ["XsxZrYYAAAAJ", "Ao4gtsYAAAAJ"], "url_scholarbib": "/scholar?q=info:6i-37ECYoqAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTree-structured%2Bdecoding%2Bwith%2Bdoubly-recurrent%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6i-37ECYoqAJ&ei=ExFkYo7eB7KO6rQPy-CRsA8&json=", "num_citations": 83, "citedby_url": "/scholar?cites=11574981396912353258&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6i-37ECYoqAJ:scholar.google.com/&scioq=Tree-structured+decoding+with+doubly-recurrent+neural+networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkYhZDqxg"}, "Unrolled Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Unrolled generative adversarial networks", "author": ["L Metz", "B Poole", "D Pfau", "J Sohl-Dickstein"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative Adversarial Networks (GANs) by defining the generator objective with respect to  an unrolled  We believe unrolling effectively increases the capacity of the discriminator. The"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02163", "author_id": ["jCOmCb4AAAAJ", "i5FMLA4AAAAJ", "rgRibJwAAAAJ", "-3zYIjQAAAAJ"], "url_scholarbib": "/scholar?q=info:ZMgPR_uaPbkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnrolled%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZMgPR_uaPbkJ&ei=FxFkYp6zIPmQ6rQP5OqKqAo&json=", "num_citations": 897, "citedby_url": "/scholar?cites=13347995274615703652&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZMgPR_uaPbkJ:scholar.google.com/&scioq=Unrolled+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02163.pdf)"}, "Exploring LOTS in Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Exploring LOTS in Deep Neural Networks", "author": ["A Rozsa", "M Gunther", "TE Boult"], "pub_year": "2016", "venue": "NA", "abstract": "Deep neural networks have recently demonstrated excellent performance on various tasks. Despite recent advances, our understanding of these learning models is still incomplete, at least, as their unexpected vulnerability to imperceptibly small, non-random perturbations revealed. The existence of these so-called adversarial examples presents a serious problem of the application of vulnerable machine learning models. In this paper, we introduce the layerwise origin-target synthesis (LOTS) that can serve multiple purposes. First, we can use"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkCILwqex", "author_id": ["", "", "fF1B7mAAAAAJ"], "url_scholarbib": "/scholar?q=info:APKI5vkb4b0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploring%2BLOTS%2Bin%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=APKI5vkb4b0J&ei=GxFkYpqgI5yO6rQP_qe3mAs&json=", "num_citations": 31, "citedby_url": "/scholar?cites=13682247903056818688&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:APKI5vkb4b0J:scholar.google.com/&scioq=Exploring+LOTS+in+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkCILwqex"}, "Quasi-Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Quasi-recurrent neural networks", "author": ["J Bradbury", "S Merity", "C Xiong", "R Socher"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01576", "abstract": "Each layer of a quasi-recurrent neural network consists of two kinds of subcomponents,  analogous to convolution and pooling layers in CNNs. The convolutional component, like"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01576", "author_id": ["GprA5UsAAAAJ", "AolIi4QAAAAJ", "vaSdahkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:naXVfUvyYDgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuasi-Recurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=naXVfUvyYDgJ&ei=HRFkYoyrOoOEmgHx-5DADA&json=", "num_citations": 455, "citedby_url": "/scholar?cites=4062513269935809949&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:naXVfUvyYDgJ:scholar.google.com/&scioq=Quasi-Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01576.pdf?fbclid=IwAR3hreOvBGmJZe54-631X49XedcbsQoDYIRu87BcCHEBf_vMKF8FDKK_7Nw"}, "Trusting SVM for Piecewise Linear CNNs": {"container_type": "Publication", "bib": {"title": "Trusting SVM for piecewise linear CNNs", "author": ["L Berrada", "A Zisserman", "MP Kumar"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02185", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the problem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02185", "author_id": ["zoae83AAAAAJ", "UZ5wscMAAAAJ", "BfmcfEAAAAAJ"], "url_scholarbib": "/scholar?q=info:RDpNUZDwYOQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTrusting%2BSVM%2Bfor%2BPiecewise%2BLinear%2BCNNs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RDpNUZDwYOQJ&ei=IxFkYumvD-HDywSSipaYAg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=16456417541041764932&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RDpNUZDwYOQJ:scholar.google.com/&scioq=Trusting+SVM+for+Piecewise+Linear+CNNs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02185"}, "DeepCoder: Learning to Write Programs": {"container_type": "Publication", "bib": {"title": "Deepcoder: Learning to write programs", "author": ["M Balog", "AL Gaunt", "M Brockschmidt", "S Nowozin"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01989", "author_id": ["5pF1dBwAAAAJ", "IzwvqPIAAAAJ", "pF27eLMAAAAJ", "7-B7aQkAAAAJ"], "url_scholarbib": "/scholar?q=info:rK8f51_8fssJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeepCoder:%2BLearning%2Bto%2BWrite%2BPrograms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rK8f51_8fssJ&ei=JhFkYsPxH5GJmwGY-qmYDQ&json=", "num_citations": 455, "citedby_url": "/scholar?cites=14663434925594619820&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rK8f51_8fssJ:scholar.google.com/&scioq=DeepCoder:+Learning+to+Write+Programs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01989)"}, "Paleo: A Performance Model for Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Paleo: A performance model for deep neural networks", "author": ["H Qi", "ER Sparks", "A Talwalkar"], "pub_year": "2016", "venue": "NA", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration. In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SyVVJ85lg", "author_id": ["72jdrSUAAAAJ", "", "TW7U1W0AAAAJ"], "url_scholarbib": "/scholar?q=info:jQ95T_UlHhcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPaleo:%2BA%2BPerformance%2BModel%2Bfor%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jQ95T_UlHhcJ&ei=LRFkYt3xGe-Sy9YPs_mY8AM&json=", "num_citations": 113, "citedby_url": "/scholar?cites=1665810647704211341&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jQ95T_UlHhcJ:scholar.google.com/&scioq=Paleo:+A+Performance+Model+for+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SyVVJ85lg"}, "Amortised MAP Inference for Image Super-resolution": {"container_type": "Publication", "bib": {"title": "Amortised map inference for image super-resolution", "author": ["CK S\u00f8nderby", "J Caballero", "L Theis", "W Shi"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "the image prior from samples. Here we introduce new methods for amortised MAP inference  whereby we calculate the MAP  Image super-resolution (SR) is the underdetermined inverse"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.04490", "author_id": ["yzGdbKoAAAAJ", "Y81PB6IAAAAJ", "dgVYYngAAAAJ", "cQYo4SkAAAAJ"], "url_scholarbib": "/scholar?q=info:aOIBlKAVQsYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAmortised%2BMAP%2BInference%2Bfor%2BImage%2BSuper-resolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aOIBlKAVQsYJ&ei=MBFkYob5I4yuyAT-mrWwCA&json=", "num_citations": 396, "citedby_url": "/scholar?cites=14286004747394736744&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aOIBlKAVQsYJ:scholar.google.com/&scioq=Amortised+MAP+Inference+for+Image+Super-resolution&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.04490.pdf?ref=https://githubhelp.com"}, "Tensorial Mixture Models": {"container_type": "Publication", "bib": {"title": "Tensorial mixture models", "author": ["O Sharir", "R Tamari", "N Cohen", "A Shashua"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We refer to Ad1,...,dN \u2261P(d1,...,dN ) as the prior tensor. Under this perspective, eq.  mixture  model with tensorial mixing weights, thus we call the arising models Tensorial Mixture Models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.04167", "author_id": ["2y5Am34AAAAJ", "AW26zZcAAAAJ", "DmzoCRMAAAAJ", "dwi5wvYAAAAJ"], "url_scholarbib": "/scholar?q=info:1BlydffDHEIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTensorial%2BMixture%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1BlydffDHEIJ&ei=NBFkYoeTOoyuyAT-mrWwCA&json=", "num_citations": 19, "citedby_url": "/scholar?cites=4763897973445892564&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1BlydffDHEIJ:scholar.google.com/&scioq=Tensorial+Mixture+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.04167"}, "Pedestrian Detection Based On Fast R-CNN and Batch Normalization ": {"container_type": "Publication", "bib": {"title": "Pedestrian detection based on fast R-CNN and batch normalization", "author": ["ZQ Zhao", "H Bian", "D Hu", "W Cheng", "H Glotin"], "pub_year": "2017", "venue": "International Conference on \u2026", "abstract": "Most of the pedestrian detection methods are based on hand-crafted features which produce low accuracy on complex scenes. With the development of deep learning method, pedestrian detection has achieved great success. In this paper, we take advantage of a convolutional neural network which is based on Fast R-CNN framework to extract robust pedestrian features for efficient and effective pedestrian detection in complicated environments. We use the EdgeBoxes algorithm to generate effective region proposals from"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-319-63309-1_65", "author_id": ["yELU6JcAAAAJ", "", "4BeJ_84AAAAJ", "", "DqieizcAAAAJ"], "url_scholarbib": "/scholar?q=info:JBl4LzYmdzgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPedestrian%2BDetection%2BBased%2BOn%2BFast%2BR-CNN%2Band%2BBatch%2BNormalization%2B%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JBl4LzYmdzgJ&ei=ORFkYsqSLY2ymgHg1rfQDQ&json=", "num_citations": 34, "citedby_url": "/scholar?cites=4068762802519021860&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JBl4LzYmdzgJ:scholar.google.com/&scioq=Pedestrian+Detection+Based+On+Fast+R-CNN+and+Batch+Normalization+&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJ6idTdgg"}, "Learning Python Code Suggestion with a Sparse Pointer Network": {"container_type": "Publication", "bib": {"title": "Learning python code suggestion with a sparse pointer network", "author": ["A Bhoopchand", "T Rockt\u00e4schel", "E Barr"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "can improve code suggestion systems by learning from  model with a sparse pointer network  aimed at capturing very  -scale code suggestion corpus of 41M lines of Python code crawled"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.08307", "author_id": ["ccfKPnsAAAAJ", "mWBY8aIAAAAJ", "lSvqKPkAAAAJ"], "url_scholarbib": "/scholar?q=info:T1HHvmlIEZYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BPython%2BCode%2BSuggestion%2Bwith%2Ba%2BSparse%2BPointer%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=T1HHvmlIEZYJ&ei=PRFkYrSPJ4uKmgGY1YjABQ&json=", "num_citations": 73, "citedby_url": "/scholar?cites=10813503799302771023&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:T1HHvmlIEZYJ:scholar.google.com/&scioq=Learning+Python+Code+Suggestion+with+a+Sparse+Pointer+Network&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.08307"}, "Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep Learning Networks by Exploiting Numerical Precision Variability": {"container_type": "Publication", "bib": {"title": "Tartan: Accelerating fully-connected and convolutional layers in deep learning networks by exploiting numerical precision variability", "author": ["A Delmas", "S Sharify", "P Judd", "A Moshovos"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs), is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.09068", "author_id": ["", "1QehFwQAAAAJ", "", "D2VLt-8AAAAJ"], "url_scholarbib": "/scholar?q=info:6-RjC5zwAcYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTartan:%2BAccelerating%2BFully-Connected%2Band%2BConvolutional%2BLayers%2Bin%2BDeep%2BLearning%2BNetworks%2Bby%2BExploiting%2BNumerical%2BPrecision%2BVariability%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6-RjC5zwAcYJ&ei=QBFkYvCIDI6pywTd4KPADw&json=", "num_citations": 14, "citedby_url": "/scholar?cites=14267949647483102443&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6-RjC5zwAcYJ:scholar.google.com/&scioq=Tartan:+Accelerating+Fully-Connected+and+Convolutional+Layers+in+Deep+Learning+Networks+by+Exploiting+Numerical+Precision+Variability&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.09068.pdf?ref=https://githubhelp.com"}, "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations": {"container_type": "Publication", "bib": {"title": "Automated generation of multilingual clusters for the evaluation of distributed representations", "author": ["P Blair", "Y Merhav", "J Barry"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01547", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of\" outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01547", "author_id": ["8174fbsAAAAJ", "wyQFX6AAAAAJ", ""], "url_scholarbib": "/scholar?q=info:bvtD8xSvUdAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomated%2BGeneration%2Bof%2BMultilingual%2BClusters%2Bfor%2Bthe%2BEvaluation%2Bof%2BDistributed%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bvtD8xSvUdAJ&ei=QxFkYrOhL4uKmgGY1YjABQ&json=", "num_citations": 17, "citedby_url": "/scholar?cites=15010971537518099310&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bvtD8xSvUdAJ:scholar.google.com/&scioq=Automated+Generation+of+Multilingual+Clusters+for+the+Evaluation+of+Distributed+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01547"}, "Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning": {"container_type": "Publication", "bib": {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["D Wang", "Q Liu"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01722", "abstract": "We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01722", "author_id": ["dmTy9EIAAAAJ", "XEx1fZkAAAAJ"], "url_scholarbib": "/scholar?q=info:Iq7V2-KSupsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BDraw%2BSamples:%2BWith%2BApplication%2Bto%2BAmortized%2BMLE%2Bfor%2BGenerative%2BAdversarial%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Iq7V2-KSupsJ&ei=RxFkYvC6Go6pywTd4KPADw&json=", "num_citations": 156, "citedby_url": "/scholar?cites=11221442924595490338&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Iq7V2-KSupsJ:scholar.google.com/&scioq=Learning+to+Draw+Samples:+With+Application+to+Amortized+MLE+for+Generative+Adversarial+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01722"}, "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification": {"container_type": "Publication", "bib": {"title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "author": ["S Zhang", "S Bahrampour", "N Ramakrishnan"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.01254", "author_id": ["cycK6u8AAAAJ", "8Hm3ZO8AAAAJ", ""], "url_scholarbib": "/scholar?q=info:4JskXGNP2jUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BSymbolic%2BRepresentation%2BLearning%2Bfor%2BHeterogeneous%2BTime-series%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4JskXGNP2jUJ&ei=UA9kYvCzHJyO6rQP_qe3mAs&json=", "num_citations": 3, "citedby_url": "/scholar?cites=3880501317099363296&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4JskXGNP2jUJ:scholar.google.com/&scioq=Deep+Symbolic+Representation+Learning+for+Heterogeneous+Time-series+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.01254"}, "End-to-End Learnable Histogram Filters": {"container_type": "Publication", "bib": {"title": "End-to-end learnable histogram filters", "author": ["R Jonschkowski", "O Brock"], "pub_year": "2016", "venue": "NA", "abstract": "Similarly to supervised end-to-end learning, we apply the filter on different subsequences  of length C, but then we follow this with D steps without performing the measurement update ("}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByvJuTigl", "author_id": ["5ErX8dMAAAAJ", "83OxU_4AAAAJ"], "url_scholarbib": "/scholar?q=info:gl9G1u65MmEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnd-to-End%2BLearnable%2BHistogram%2BFilters%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gl9G1u65MmEJ&ei=Uw9kYo3UAZGJmwGY-qmYDQ&json=", "num_citations": 31, "citedby_url": "/scholar?cites=7003864805962833794&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gl9G1u65MmEJ:scholar.google.com/&scioq=End-to-End+Learnable+Histogram+Filters&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByvJuTigl"}, "Learning through Dialogue Interactions by Asking Questions": {"container_type": "Publication", "bib": {"title": "Learning through dialogue interactions by asking questions", "author": ["J Li", "AH Miller", "S Chopra", "MA Ranzato"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interaction. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and demonstrate that the learner improves when asking"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.04936", "author_id": ["PwU16JEAAAAJ", "3b0l5LEAAAAJ", "ETU-ePAAAAAJ", "NbXF7T8AAAAJ"], "url_scholarbib": "/scholar?q=info:NNCiFIYQCv0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bthrough%2BDialogue%2BInteractions%2Bby%2BAsking%2BQuestions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NNCiFIYQCv0J&ei=Vg9kYvjEKcLZmQHnraWYCA&json=", "num_citations": 213, "citedby_url": "/scholar?cites=18233404209420750900&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NNCiFIYQCv0J:scholar.google.com/&scioq=Learning+through+Dialogue+Interactions+by+Asking+Questions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.04936"}, "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech": {"container_type": "Publication", "bib": {"title": "Fast Chirplet transform to enhance CNN machine listening-validation on animal calls and speech", "author": ["H Glotin", "J Ricard", "R Balestriero"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.08749", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels. Convolutional Neural Net (CNN) can be seen as an optimal kernel decomposition, nevertheless it requires large amount of training data to learn its kernels. We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrain CNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.08749", "author_id": ["DqieizcAAAAJ", "", "S1x_xqcAAAAJ"], "url_scholarbib": "/scholar?q=info:d1YbsAyiQpkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFast%2BChirplet%2BTransform%2Bto%2BEnhance%2BCNN%2BMachine%2BListening%2B-%2BValidation%2Bon%2BAnimal%2Bcalls%2Band%2BSpeech%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d1YbsAyiQpkJ&ei=WQ9kYpClE5LeyQTE46-QAg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=11043567411643766391&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:d1YbsAyiQpkJ:scholar.google.com/&scioq=Fast+Chirplet+Transform+to+Enhance+CNN+Machine+Listening+-+Validation+on+Animal+calls+and+Speech&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.08749"}, "Learning End-to-End Goal-Oriented Dialog": {"container_type": "Publication", "bib": {"title": "Learning end-to-end goal-oriented dialog", "author": ["A Bordes", "YL Boureau", "J Weston"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.07683", "abstract": "chit-chat dialog may not carry over to goal-oriented settings. This  of end-to-end dialog systems  in goal-oriented applications. Set in  We show that an end-to-end dialog system based on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.07683", "author_id": ["jWWx33IAAAAJ", "GfcBlpUAAAAJ", "lMkTx0EAAAAJ"], "url_scholarbib": "/scholar?q=info:BiCos2G-9MoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BEnd-to-End%2BGoal-Oriented%2BDialog%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BiCos2G-9MoJ&ei=XA9kYoupG_mQ6rQP5OqKqAo&json=", "num_citations": 747, "citedby_url": "/scholar?cites=14624523216814088198&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BiCos2G-9MoJ:scholar.google.com/&scioq=Learning+End-to-End+Goal-Oriented+Dialog&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.07683"}, "Extrapolation and learning equations": {"container_type": "Publication", "bib": {"title": "Extrapolation and learning equations", "author": ["G Martius", "CH Lampert"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.02995", "abstract": "of learning a system of equations that govern a physical system and can extrapolate to new   their absence in the considered physical equations. Other, predominantly local nonlinearities"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.02995", "author_id": ["b-JF-UIAAAAJ", "iCf3SwgAAAAJ"], "url_scholarbib": "/scholar?q=info:8kmC0On_MyEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExtrapolation%2Band%2Blearning%2Bequations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8kmC0On_MyEJ&ei=YA9kYrXtOcLZmQHnraWYCA&json=", "num_citations": 61, "citedby_url": "/scholar?cites=2392537206754527730&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8kmC0On_MyEJ:scholar.google.com/&scioq=Extrapolation+and+learning+equations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.02995"}, "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations": {"container_type": "Publication", "bib": {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["D Krueger", "T Maharaj", "J Kram\u00e1r", "M Pezeshki"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.01305", "author_id": ["5Uz70IoAAAAJ", "XpscC-EAAAAJ", "iW_lUIkAAAAJ", "HT85tXsAAAAJ"], "url_scholarbib": "/scholar?q=info:o_FI0u2lhr8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DZoneout:%2BRegularizing%2BRNNs%2Bby%2BRandomly%2BPreserving%2BHidden%2BActivations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o_FI0u2lhr8J&ei=ZA9kYpDrIJyO6rQP_qe3mAs&json=", "num_citations": 297, "citedby_url": "/scholar?cites=13800900548977291683&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o_FI0u2lhr8J:scholar.google.com/&scioq=Zoneout:+Regularizing+RNNs+by+Randomly+Preserving+Hidden+Activations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.01305.pdf?ref=https://githubhelp.com"}, "Multi-label learning with semantic embeddings": {"container_type": "Publication", "bib": {"title": "Multi-label learning with semantic embeddings", "author": ["L Jing", "MM Cheng", "L Yang", "A Gittens", "MW Mahoney"], "pub_year": "2016", "venue": "NA", "abstract": "Multi-label learning aims to automatically assign to an instance (eg, an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryAe2WBee", "author_id": ["zStEDu4AAAAJ", "", "", "ysiqoUoAAAAJ", "QXyvv94AAAAJ"], "url_scholarbib": "/scholar?q=info:lMzb0bLZMwUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-label%2Blearning%2Bwith%2Bsemantic%2Bembeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lMzb0bLZMwUJ&ei=aA9kYsTCD86E6rQP5-KmKA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:lMzb0bLZMwUJ:scholar.google.com/&scioq=Multi-label+learning+with+semantic+embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryAe2WBee"}, "Learning Visual Servoing with Deep Features and Fitted Q-Iteration": {"container_type": "Publication", "bib": {"title": "Learning visual servoing with deep features and fitted q-iteration", "author": ["AX Lee", "S Levine", "P Abbeel"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.11000", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.11000", "author_id": ["8-p9CLsAAAAJ", "8R35rCwAAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:0g0XSsNTfiwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BVisual%2BServoing%2Bwith%2BDeep%2BFeatures%2Band%2BFitted%2BQ-Iteration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0g0XSsNTfiwJ&ei=dQ9kYpCpBu-Sy9YPs_mY8AM&json=", "num_citations": 69, "citedby_url": "/scholar?cites=3206092082961124818&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0g0XSsNTfiwJ:scholar.google.com/&scioq=Learning+Visual+Servoing+with+Deep+Features+and+Fitted+Q-Iteration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.11000"}, "OMG: Orthogonal Method of Grouping With Application of K-Shot Learning": {"container_type": "Publication", "bib": {"title": "OMG: Orthogonal Method of Grouping With Application of K-Shot Learning", "author": ["H Fan", "Y Zhang", "KM Kitani"], "pub_year": "2016", "venue": "NA", "abstract": "Training a classifier with only a few examples remains a significant barrier when using neural networks with large number of parameters. Though various specialized network architectures have been proposed for these k-shot learning tasks to avoid overfitting, a question remains: is there a generalizable framework for the k-shot learning problem that can leverage existing deep models as well as avoid model overfitting? In this paper, we proposed a generalizable k-shot learning framework that can be used on any pre-trained"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1HcOI5le", "author_id": ["", "", "yv3sH74AAAAJ"], "url_scholarbib": "/scholar?q=info:iaGty7S56IgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOMG:%2BOrthogonal%2BMethod%2Bof%2BGrouping%2BWith%2BApplication%2Bof%2BK-Shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iaGty7S56IgJ&ei=eA9kYrF975LL1g-z-ZjwAw&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:iaGty7S56IgJ:scholar.google.com/&scioq=OMG:+Orthogonal+Method+of+Grouping+With+Application+of+K-Shot+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1HcOI5le"}, "Energy-based Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Energy-based generative adversarial network", "author": ["J Zhao", "M Mathieu", "Y LeCun"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.03126", "abstract": "We introduce the\" Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.03126", "author_id": ["XSNOLAkAAAAJ", "SSTIBK0AAAAJ", "WLN3QrAAAAAJ"], "url_scholarbib": "/scholar?q=info:rZKJgS3QFtYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnergy-based%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rZKJgS3QFtYJ&ei=hA9kYqn7NIySyASZk6HgCA&json=", "num_citations": 1195, "citedby_url": "/scholar?cites=15426746467469595309&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rZKJgS3QFtYJ:scholar.google.com/&scioq=Energy-based+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.03126.pdf?source=post_page---------------------------"}, "Fuzzy paraphrases in learning word representations with a lexicon": {"container_type": "Publication", "bib": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "author": ["Y Ke", "M Hagiwara"], "pub_year": "2016", "venue": "NA", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. In this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1osyr_xg", "author_id": ["SA0WWBQAAAAJ", ""], "url_scholarbib": "/scholar?q=info:Ei6iHs14628J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFuzzy%2Bparaphrases%2Bin%2Blearning%2Bword%2Brepresentations%2Bwith%2Ba%2Blexicon%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ei6iHs14628J&ei=iA9kYpuCFZqSy9YP8pKNsAE&json=", "num_citations": 1, "citedby_url": "/scholar?cites=8064672380114578962&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ei6iHs14628J:scholar.google.com/&scioq=Fuzzy+paraphrases+in+learning+word+representations+with+a+lexicon&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1osyr_xg"}, "Riemannian Optimization for Skip-Gram Negative Sampling": {"container_type": "Publication", "bib": {"title": "Riemannian optimization for skip-gram negative sampling", "author": ["A Fonarev", "O Hrinchuk", "G Gusev", "P Serdyukov"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in\" word2vec\" software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.08059", "author_id": ["", "Z8GCLksAAAAJ", "RWX4sYcAAAAJ", ""], "url_scholarbib": "/scholar?q=info:4gMlmuHcoEYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRiemannian%2BOptimization%2Bfor%2BSkip-Gram%2BNegative%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4gMlmuHcoEYJ&ei=iw9kYsO8LZqSy9YP8pKNsAE&json=", "num_citations": 12, "citedby_url": "/scholar?cites=5089310440440529890&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4gMlmuHcoEYJ:scholar.google.com/&scioq=Riemannian+Optimization+for+Skip-Gram+Negative+Sampling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.08059"}, "A Compare-Aggregate Model for Matching Text Sequences": {"container_type": "Publication", "bib": {"title": "A compare-aggregate model for matching text sequences", "author": ["S Wang", "J Jiang"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01747", "abstract": "Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general\" compare-aggregate\" framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01747", "author_id": ["mN-IO6wAAAAJ", "hVTK2YwAAAAJ"], "url_scholarbib": "/scholar?q=info:e2lUnqRfhi8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCompare-Aggregate%2BModel%2Bfor%2BMatching%2BText%2BSequences%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e2lUnqRfhi8J&ei=jg9kYqLNH8LZmQHnraWYCA&json=", "num_citations": 248, "citedby_url": "/scholar?cites=3424529727297448315&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e2lUnqRfhi8J:scholar.google.com/&scioq=A+Compare-Aggregate+Model+for+Matching+Text+Sequences&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01747"}, "Boosted Generative Models": {"container_type": "Publication", "bib": {"title": "Boosted generative models", "author": ["A Grover", "S Ermon"], "pub_year": "2018", "venue": "Proceedings of the AAAI Conference on Artificial \u2026", "abstract": "We propose a novel approach for using unsupervised boosting to create an ensemble of  generative models, where models are trained in sequence to correct earlier mistakes. Our meta"}, "filled": false, "gsrank": 1, "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/11827", "author_id": ["oOhnPUgAAAAJ", "ogXTOZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:YmWUybXsjGMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosted%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YmWUybXsjGMJ&ei=lg9kYvuYBYuKmgGY1YjABQ&json=", "num_citations": 38, "citedby_url": "/scholar?cites=7173368572009538914&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YmWUybXsjGMJ:scholar.google.com/&scioq=Boosted+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/view/11827/11686"}, "Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets": {"container_type": "Publication", "bib": {"title": "Semi-supervised detection of extreme weather events in large climate datasets", "author": ["E Racah", "C Beckham", "T Maharaj", "C Pal"], "pub_year": "2016", "venue": "NA", "abstract": "The detection and identification of extreme weather events in large scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, there are many different types of spatially localized climate patterns"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJ_QCYqle", "author_id": ["jI4ATW4AAAAJ", "PpD3zNYAAAAJ", "XpscC-EAAAAJ", "1ScWJOoAAAAJ"], "url_scholarbib": "/scholar?q=info:3vfVwPVbBPwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-Supervised%2BDetection%2Bof%2BExtreme%2BWeather%2BEvents%2Bin%2BLarge%2BClimate%2BDatasets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3vfVwPVbBPwJ&ei=mg9kYvrkIrKO6rQPy-CRsA8&json=", "num_citations": 18, "citedby_url": "/scholar?cites=18159740708525045726&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3vfVwPVbBPwJ:scholar.google.com/&scioq=Semi-Supervised+Detection+of+Extreme+Weather+Events+in+Large+Climate+Datasets&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJ_QCYqle"}, "On Detecting Adversarial Perturbations": {"container_type": "Publication", "bib": {"title": "On detecting adversarial perturbations", "author": ["JH Metzen", "T Genewein", "V Fischer"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "adversarial perturbations, which has mostly focused on making the classification network itself  more robust. We show empirically that adversarial perturbations can be detected  to detect"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.04267", "author_id": ["w047VfEAAAAJ", "peNTK9oAAAAJ", "r6JrCToAAAAJ"], "url_scholarbib": "/scholar?q=info:PLIN_eCNcSAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BDetecting%2BAdversarial%2BPerturbations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PLIN_eCNcSAJ&ei=nQ9kYomiIIySyASZk6HgCA&json=", "num_citations": 750, "citedby_url": "/scholar?cites=2337805679039722044&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PLIN_eCNcSAJ:scholar.google.com/&scioq=On+Detecting+Adversarial+Perturbations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.04267"}, "The Predictron: End-To-End Learning and Planning": {"container_type": "Publication", "bib": {"title": "The predictron: End-to-end learning and planning", "author": ["D Silver", "H Hasselt", "M Hessel"], "pub_year": "2017", "venue": "International \u2026", "abstract": "One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \u201cimagined\u201d planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/silver17a.html", "author_id": ["-8DNE4UAAAAJ", "W80oBMkAAAAJ", "odVYodIAAAAJ"], "url_scholarbib": "/scholar?q=info:X0YGLR4TtQEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BPredictron:%2BEnd-To-End%2BLearning%2Band%2BPlanning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=X0YGLR4TtQEJ&ei=nw9kYp3PPIyuyAT-mrWwCA&json=", "num_citations": 229, "citedby_url": "/scholar?cites=123025585147889247&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:X0YGLR4TtQEJ:scholar.google.com/&scioq=The+Predictron:+End-To-End+Learning+and+Planning&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/silver17a/silver17a.pdf"}, "End-to-end Optimized Image Compression": {"container_type": "Publication", "bib": {"title": "End-to-end optimized image compression", "author": ["J Ball\u00e9", "V Laparra", "EP Simoncelli"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01704", "abstract": "We have developed a framework for end-to-end optimization of an image compression  model based on nonlinear transforms (figure 1). Previously, we demonstrated that a model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01704", "author_id": ["uKDe38UAAAAJ", "dNt_xikAAAAJ", "MplR7_cAAAAJ"], "url_scholarbib": "/scholar?q=info:5a9SN7Pt_Q8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnd-to-end%2BOptimized%2BImage%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5a9SN7Pt_Q8J&ei=og9kYpu1M86E6rQP5-KmKA&json=", "num_citations": 823, "citedby_url": "/scholar?cites=1152338433659809765&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5a9SN7Pt_Q8J:scholar.google.com/&scioq=End-to-end+Optimized+Image+Compression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01704"}, "Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment": {"container_type": "Publication", "bib": {"title": "Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment", "author": ["A Shahriari"], "pub_year": "2016", "venue": "NA", "abstract": "Transfer learning is a popular practice in deep neural networks, but fine-tuning of a large number of parameters is a hard challenge due to the complex wiring of neurons between splitting layers and imbalance class distributions of original and transferred domains. Recent advances in evidence theory show that in an imbalance multiclass learning problem, optimizing of proper objective functions based on contingency tables prevents biases towards high-prior classes. Transfer learning usually deals with highly non-convex"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJJN38cge", "author_id": [""], "url_scholarbib": "/scholar?q=info:apf5rR49GKUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BTransfer%2BLearning%2Bfor%2BDeep%2BConvolutional%2BNeural%2BNetworks%2Bby%2BBasic%2BProbability%2BAssignment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=apf5rR49GKUJ&ei=pg9kYoDsLbKO6rQPy-CRsA8&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:apf5rR49GKUJ:scholar.google.com/&scioq=Distributed+Transfer+Learning+for+Deep+Convolutional+Neural+Networks+by+Basic+Probability+Assignment&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJJN38cge"}, "Learning to Compose Words into Sentences with Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning to compose words into sentences with reinforcement learning", "author": ["D Yogatama", "P Blunsom", "C Dyer", "E Grefenstette"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09100", "author_id": ["IBlMTLwAAAAJ", "eJwbbXEAAAAJ", "W2DsnAkAAAAJ", "ezllEwMAAAAJ"], "url_scholarbib": "/scholar?q=info:Z6uwHmIORPYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BCompose%2BWords%2Binto%2BSentences%2Bwith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z6uwHmIORPYJ&ei=qQ9kYuDgF5qSy9YP8pKNsAE&json=", "num_citations": 160, "citedby_url": "/scholar?cites=17745324246331075431&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Z6uwHmIORPYJ:scholar.google.com/&scioq=Learning+to+Compose+Words+into+Sentences+with+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09100"}, "Reinforcement Learning with Unsupervised Auxiliary Tasks": {"container_type": "Publication", "bib": {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M Jaderberg", "V Mnih", "WM Czarnecki", "T Schaul"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.05397", "author_id": ["JeZoMicAAAAJ", "rLdfJ1gAAAAJ", "aOvr9eMAAAAJ", "vDimc-4AAAAJ"], "url_scholarbib": "/scholar?q=info:tj5uz7Opn84J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforcement%2BLearning%2Bwith%2BUnsupervised%2BAuxiliary%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tj5uz7Opn84J&ei=rA9kYuXpDIyuyAT-mrWwCA&json=", "num_citations": 991, "citedby_url": "/scholar?cites=14888805482854497974&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tj5uz7Opn84J:scholar.google.com/&scioq=Reinforcement+Learning+with+Unsupervised+Auxiliary+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.05397.pdf?ref=https://githubhelp.com"}, "Extensions and Limitations of the Neural GPU": {"container_type": "Publication", "bib": {"title": "Extensions and limitations of the neural gpu", "author": ["E Price", "W Zaremba", "I Sutskever"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.00736", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques increase the set of algorithmic problems that can be solved by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.00736", "author_id": ["UE6z_m8AAAAJ", "XCZpOcAAAAAJ", "x04W_mMAAAAJ"], "url_scholarbib": "/scholar?q=info:jobGIGuiev8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExtensions%2Band%2BLimitations%2Bof%2Bthe%2BNeural%2BGPU%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jobGIGuiev8J&ei=rg9kYv3nMoOEmgHx-5DADA&json=", "num_citations": 14, "citedby_url": "/scholar?cites=18409205007825405582&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jobGIGuiev8J:scholar.google.com/&scioq=Extensions+and+Limitations+of+the+Neural+GPU&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.00736"}, "Multiagent System for Layer Free Network": {"container_type": "Publication", "bib": {"title": "Multiagent System for Layer Free Network", "author": ["H Kurotaki", "K Nakayama", "Y Matsuo"], "pub_year": "2016", "venue": "NA", "abstract": "We propose a multiagent system that have feedforward networks as its subset while free from layer structure with matrix-vector scheme. Deep networks are often compared to the brain neocortex or visual perception system. One of the largest difference from human brain is the use of matrix-vector multiplication based on layer architecture. It would help understanding the way human brain works if we manage to develop good deep network model without the layer architecture while preserving their performance. The brain neocortex"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1PA8fqeg", "author_id": ["", "MT4jukMAAAAJ", ""], "url_scholarbib": "/scholar?q=info:ihvdQB7u9uEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultiagent%2BSystem%2Bfor%2BLayer%2BFree%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ihvdQB7u9uEJ&ei=sw9kYtaROO-Sy9YPs_mY8AM&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:ihvdQB7u9uEJ:scholar.google.com/&scioq=Multiagent+System+for+Layer+Free+Network&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1PA8fqeg"}, "Efficient Softmax Approximation for GPUs": {"container_type": "Publication", "bib": {"title": "Efficient softmax approximation for gpus", "author": ["A Joulin", "M Ciss\u00e9", "D Grangier"], "pub_year": "2017", "venue": "\u2026 conference on machine \u2026", "abstract": "We propose an approximate strategy to efficiently train neural network based language  models over very large vocabularies. Our approach, called adaptive softmax, circumvents the"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/grave17a.html?ref=https://githubhelp.com", "author_id": ["kRJkDakAAAAJ", "", "CIQEGCYAAAAJ"], "url_scholarbib": "/scholar?q=info:18h44sShhw4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BSoftmax%2BApproximation%2Bfor%2BGPUs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=18h44sShhw4J&ei=tg9kYouUEIySyASZk6HgCA&json=", "num_citations": 224, "citedby_url": "/scholar?cites=1046983305372158167&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:18h44sShhw4J:scholar.google.com/&scioq=Efficient+Softmax+Approximation+for+GPUs&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/grave17a/grave17a.pdf"}, "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables": {"container_type": "Publication", "bib": {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["CJ Maddison", "A Mnih", "YW Teh"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.00712", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.00712", "author_id": ["WjCG3owAAAAJ", "mxiO4IkAAAAJ", "y-nUzMwAAAAJ"], "url_scholarbib": "/scholar?q=info:viaoPU2jvOQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BConcrete%2BDistribution:%2BA%2BContinuous%2BRelaxation%2Bof%2BDiscrete%2BRandom%2BVariables%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=viaoPU2jvOQJ&ei=uQ9kYqmbOe-Sy9YPs_mY8AM&json=", "num_citations": 1634, "citedby_url": "/scholar?cites=16482228288411412158&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:viaoPU2jvOQJ:scholar.google.com/&scioq=The+Concrete+Distribution:+A+Continuous+Relaxation+of+Discrete+Random+Variables&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.00712"}, "Local minima in training of deep networks": {"container_type": "Publication", "bib": {"title": "Local minima in training of deep networks", "author": ["G Swirszcz", "WM Czarnecki", "R Pascanu"], "pub_year": "2016", "venue": "NA", "abstract": "There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Syoiqwcxx", "author_id": ["YeefX_oAAAAJ", "aOvr9eMAAAAJ", "eSPY8LwAAAAJ"], "url_scholarbib": "/scholar?q=info:w0cF1tUc9_IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLocal%2Bminima%2Bin%2Btraining%2Bof%2Bdeep%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w0cF1tUc9_IJ&ei=vQ9kYti-EZyO6rQP_qe3mAs&json=", "num_citations": 32, "citedby_url": "/scholar?cites=17507493781170374595&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w0cF1tUc9_IJ:scholar.google.com/&scioq=Local+minima+in+training+of+deep+networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Syoiqwcxx"}, "Human perception in computer vision": {"container_type": "Publication", "bib": {"title": "Human perception in computer vision", "author": ["R Dekel"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.04674", "abstract": "Computer vision has made remarkable progress in recent years. Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning. Biological vision (learned in life and through evolution) is also accurate and general-purpose. Is it possible that these different learning regimes converge to similar problem-dependent optimal computations? We therefore asked"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.04674", "author_id": ["ABOIhk4AAAAJ"], "url_scholarbib": "/scholar?q=info:VfWjMRiDjN4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHuman%2Bperception%2Bin%2Bcomputer%2Bvision%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VfWjMRiDjN4J&ei=xQ9kYrmwGOiSy9YPp-OyiAE&json=", "num_citations": 10, "citedby_url": "/scholar?cites=16036336513094776149&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VfWjMRiDjN4J:scholar.google.com/&scioq=Human+perception+in+computer+vision&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.04674"}, "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles": {"container_type": "Publication", "bib": {"title": "Epopt: Learning robust neural network policies using model ensembles", "author": ["A Rajeswaran", "S Ghotra", "B Ravindran"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.01283", "author_id": ["_EJrRVAAAAAJ", "fVcC6sQAAAAJ", "nGUcGrYAAAAJ"], "url_scholarbib": "/scholar?q=info:RlEwbM8jq8wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEPOpt:%2BLearning%2BRobust%2BNeural%2BNetwork%2BPolicies%2BUsing%2BModel%2BEnsembles%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RlEwbM8jq8wJ&ei=yA9kYu7mG4ySyASZk6HgCA&json=", "num_citations": 272, "citedby_url": "/scholar?cites=14747920778535129414&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RlEwbM8jq8wJ:scholar.google.com/&scioq=EPOpt:+Learning+Robust+Neural+Network+Policies+Using+Model+Ensembles&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.01283"}, "Training deep neural-networks using a noise adaptation layer": {"container_type": "Publication", "bib": {"title": "Training deep neural-networks using a noise adaptation layer", "author": ["J Goldberger", "E Ben-Reuven"], "pub_year": "2016", "venue": "NA", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H12GRgcxg", "author_id": ["vgzrOK4AAAAJ", "9LFRvA8AAAAJ"], "url_scholarbib": "/scholar?q=info:avalz1mkNrIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2Bdeep%2Bneural-networks%2Busing%2Ba%2Bnoise%2Badaptation%2Blayer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=avalz1mkNrIJ&ei=0w9kYrvSBuHDywSSipaYAg&json=", "num_citations": 379, "citedby_url": "/scholar?cites=12841632093136352874&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:avalz1mkNrIJ:scholar.google.com/&scioq=Training+deep+neural-networks+using+a+noise+adaptation+layer&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H12GRgcxg"}, "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning": {"container_type": "Publication", "bib": {"title": "Deep predictive coding networks for video prediction and unsupervised learning", "author": ["W Lotter", "G Kreiman", "D Cox"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.08104", "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning-leveraging unlabeled examples to learn about the structure of a domain-remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\" PredNet\") architecture that is inspired by the concept of\" predictive coding\" from the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.08104", "author_id": ["NaSe78YAAAAJ", "WxZ_6nsAAAAJ", "6S-WgLkAAAAJ"], "url_scholarbib": "/scholar?q=info:v0yMd4FZMZwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BPredictive%2BCoding%2BNetworks%2Bfor%2BVideo%2BPrediction%2Band%2BUnsupervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v0yMd4FZMZwJ&ei=1Q9kYuqVKu-Sy9YPs_mY8AM&json=", "num_citations": 748, "citedby_url": "/scholar?cites=11254875356366916799&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v0yMd4FZMZwJ:scholar.google.com/&scioq=Deep+Predictive+Coding+Networks+for+Video+Prediction+and+Unsupervised+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.08104"}, "FastText.zip: Compressing text classification models": {"container_type": "Publication", "bib": {"title": "Fasttext. zip: Compressing text classification models", "author": ["A Joulin", "E Grave", "P Bojanowski", "M Douze"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.03651", "author_id": ["kRJkDakAAAAJ", "7UV4ET4AAAAJ", "lJ_oh2EAAAAJ", "0eFZtREAAAAJ"], "url_scholarbib": "/scholar?q=info:vWmIsrgljaAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFastText.zip:%2BCompressing%2Btext%2Bclassification%2Bmodels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vWmIsrgljaAJ&ei=2A9kYsKxIJyO6rQP_qe3mAs&json=", "num_citations": 831, "citedby_url": "/scholar?cites=11568944492984166845&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vWmIsrgljaAJ:scholar.google.com/&scioq=FastText.zip:+Compressing+text+classification+models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.03651.pdf?ref=https://githubhelp.com"}, "Generating Interpretable Images with Controllable Structure": {"container_type": "Publication", "bib": {"title": "Generating interpretable images with controllable structure", "author": ["S Reed", "A van den Oord", "N Kalchbrenner", "V Bapst"], "pub_year": "2016", "venue": "NA", "abstract": "We demonstrate improved text-to-image synthesis with controllable object locations using an extension of Pixel Convolutional Neural Networks (PixelCNN). In addition to conditioning on text, we show how the model can generate images conditioned on part keypoints and segmentation masks. The character-level text encoder and image generation network are jointly trained end-to-end via maximum likelihood. We establish quantitative baselines in terms of text and structure-conditional pixel log-likelihood for three data sets: Caltech-UCSD"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hyvw0L9el", "author_id": ["jEANvfgAAAAJ", "TqUN-LwAAAAJ", "LFyg0tAAAAAJ", "95mnc80AAAAJ"], "url_scholarbib": "/scholar?q=info:5wbLU7w6OLQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BInterpretable%2BImages%2Bwith%2BControllable%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5wbLU7w6OLQJ&ei=3A9kYq2tEpGJmwGY-qmYDQ&json=", "num_citations": 54, "citedby_url": "/scholar?cites=12986194106056902375&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5wbLU7w6OLQJ:scholar.google.com/&scioq=Generating+Interpretable+Images+with+Controllable+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hyvw0L9el"}, "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Sparsely-connected neural networks: towards efficient vlsi implementation of deep neural networks", "author": ["A Ardakani", "C Condo", "WJ Gross"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01427", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01427", "author_id": ["QFXIpuIAAAAJ", "E4SEQjUAAAAJ", "xSvOcF8AAAAJ"], "url_scholarbib": "/scholar?q=info:VMwjbpiOoHsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSparsely-Connected%2BNeural%2BNetworks:%2BTowards%2BEfficient%2BVLSI%2BImplementation%2Bof%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VMwjbpiOoHsJ&ei=3w9kYpSrPM6E6rQP5-KmKA&json=", "num_citations": 68, "citedby_url": "/scholar?cites=8908276848272854100&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VMwjbpiOoHsJ:scholar.google.com/&scioq=Sparsely-Connected+Neural+Networks:+Towards+Efficient+VLSI+Implementation+of+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01427"}, "Recurrent Normalization Propagation": {"container_type": "Publication", "bib": {"title": "Recurrent normalization propagation", "author": ["C Laurent", "N Ballas", "P Vincent"], "pub_year": "2017", "venue": "NA", "abstract": "a brief overview of the Batch-Normalized LSTM, in section 3 we derive our Normalized  LSTM, section 4 investigates the impact of such normalization on the gradient flow, section 5"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1GKzP5xx", "author_id": ["Ev9dOhQAAAAJ", "euUV4iUAAAAJ", "WBCKQMsAAAAJ"], "url_scholarbib": "/scholar?q=info:AyeyThBZsG4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BNormalization%2BPropagation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AyeyThBZsG4J&ei=4w9kYvHxL7KO6rQPy-CRsA8&json=", "num_citations": 2, "citedby_url": "/scholar?cites=7975972866647795459&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AyeyThBZsG4J:scholar.google.com/&scioq=Recurrent+Normalization+Propagation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1hNW-MYg"}, "Transformation-based Models of Video Sequences": {"container_type": "Publication", "bib": {"title": "Transformation-based models of video sequences", "author": ["J Van Amersfoort", "A Kannan", "MA Ranzato"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model. In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.08435", "author_id": ["C0LaV8IAAAAJ", "eoBHpj4AAAAJ", "NbXF7T8AAAAJ"], "url_scholarbib": "/scholar?q=info:s31I_ihwp6gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransformation-based%2BModels%2Bof%2BVideo%2BSequences%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s31I_ihwp6gJ&ei=5w9kYuKUJ--Sy9YPs_mY8AM&json=", "num_citations": 54, "citedby_url": "/scholar?cites=12152805440849739187&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s31I_ihwp6gJ:scholar.google.com/&scioq=Transformation-based+Models+of+Video+Sequences&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.08435"}, "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models": {"container_type": "Publication", "bib": {"title": "Diverse beam search: Decoding diverse solutions from neural sequence models", "author": ["AK Vijayakumar", "M Cogswell", "RR Selvaraju"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates-resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.02424", "author_id": ["KYHL9aIAAAAJ", "9e2wRsoAAAAJ", "yNFGwa4AAAAJ"], "url_scholarbib": "/scholar?q=info:wX-Dk9EBtLcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiverse%2BBeam%2BSearch:%2BDecoding%2BDiverse%2BSolutions%2Bfrom%2BNeural%2BSequence%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wX-Dk9EBtLcJ&ei=6w9kYpa9IoOEmgHx-5DADA&json=", "num_citations": 279, "citedby_url": "/scholar?cites=13237207204383391681&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wX-Dk9EBtLcJ:scholar.google.com/&scioq=Diverse+Beam+Search:+Decoding+Diverse+Solutions+from+Neural+Sequence+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.02424.pdf?ref=https://githubhelp.com"}, "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context": {"container_type": "Publication", "bib": {"title": "Beyond bilingual: Multi-sense word embeddings using multilingual context", "author": ["S Upadhyay", "KW Chang", "M Taddy", "A Kalai"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (ie, more than two languages) corpora to significantly improve sense embeddings beyond what one"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.08160", "author_id": ["i7lw4LwAAAAJ", "fqDBtzYAAAAJ", "", "gRxBNZoAAAAJ"], "url_scholarbib": "/scholar?q=info:wDJYh4qnmgkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BBilingual:%2BMulti-sense%2BWord%2BEmbeddings%2Busing%2BMultilingual%2BContext%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wDJYh4qnmgkJ&ei=8w9kYov8JJqSy9YP8pKNsAE&json=", "num_citations": 28, "citedby_url": "/scholar?cites=692049706172822208&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wDJYh4qnmgkJ:scholar.google.com/&scioq=Beyond+Bilingual:+Multi-sense+Word+Embeddings+using+Multilingual+Context&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.08160"}, "Query-Reduction Networks for Question Answering": {"container_type": "Publication", "bib": {"title": "Query-reduction networks for question answering", "author": ["M Seo", "S Min", "A Farhadi", "H Hajishirzi"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1606.04582", "abstract": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.04582", "author_id": ["zYze5fIAAAAJ", "jU4IZs4AAAAJ", "jeOFRDsAAAAJ", "LOV6_WIAAAAJ"], "url_scholarbib": "/scholar?q=info:xD5N-JszLGMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuery-Reduction%2BNetworks%2Bfor%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xD5N-JszLGMJ&ei=-A9kYsqKOO-Sy9YPs_mY8AM&json=", "num_citations": 99, "citedby_url": "/scholar?cites=7146143453708893892&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xD5N-JszLGMJ:scholar.google.com/&scioq=Query-Reduction+Networks+for+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.04582.pdf?ref=https://githubhelp.com"}, "CAN AI GENERATE LOVE ADVICE?: TOWARD NEURAL ANSWER GENERATION FOR NON-FACTOID QUESTIONS": {"container_type": "Publication", "bib": {"title": "Can AI Generate Love Advice?: Toward Neural Answer Generation for Non-Factoid Questions", "author": ["M Nakatsuji"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.10163", "abstract": "Deep learning methods that extract answers for non-factoid questions from QA sites are seen as critical since they can assist users in reaching their next decisions through conversations with AI systems. The current methods, however, have the following two problems:(1) They can not understand the ambiguous use of words in the questions as word usage can strongly depend on the context. As a result, the accuracies of their answer selections are not good enough.(2) The current methods can only select from among the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10163", "author_id": ["NILKxCIAAAAJ"], "url_scholarbib": "/scholar?q=info:8iSO0ZPW900J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCAN%2BAI%2BGENERATE%2BLOVE%2BADVICE%253F:%2BTOWARD%2BNEURAL%2BANSWER%2BGENERATION%2BFOR%2BNON-FACTOID%2BQUESTIONS%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8iSO0ZPW900J&ei=_A9kYpaJMu-Sy9YPs_mY8AM&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:8iSO0ZPW900J:scholar.google.com/&scioq=CAN+AI+GENERATE+LOVE+ADVICE%3F:+TOWARD+NEURAL+ANSWER+GENERATION+FOR+NON-FACTOID+QUESTIONS&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10163"}, "An Analysis of Deep Neural Network Models for Practical Applications": {"container_type": "Publication", "bib": {"title": "An analysis of deep neural network models for practical applications", "author": ["A Canziani", "A Paszke", "E Culurciello"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.07678", "abstract": "Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.07678", "author_id": ["dU5bJOMAAAAJ", "LkVtZkQAAAAJ", "SeGmqkIAAAAJ"], "url_scholarbib": "/scholar?q=info:0bMLOX5x0mIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BAnalysis%2Bof%2BDeep%2BNeural%2BNetwork%2BModels%2Bfor%2BPractical%2BApplications%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0bMLOX5x0mIJ&ei=_w9kYqj-FJGJmwGY-qmYDQ&json=", "num_citations": 1055, "citedby_url": "/scholar?cites=7120878747763061713&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0bMLOX5x0mIJ:scholar.google.com/&scioq=An+Analysis+of+Deep+Neural+Network+Models+for+Practical+Applications&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.07678.pdf?source=post_page---------------------------"}, "Multiplicative LSTM for sequence modelling": {"container_type": "Publication", "bib": {"title": "Multiplicative LSTM for sequence modelling", "author": ["B Krause", "L Lu", "I Murray", "S Renals"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.07959", "abstract": "introduce multiplicative LSTM (mLSTM), a recurrent neural network architecture for sequence  modelling  We compare this multiplicative LSTM hybrid architecture with other variants of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.07959", "author_id": ["ONNif60AAAAJ", "DkC5aRAAAAAJ", "4BEvaw8AAAAJ", "hERDiOEAAAAJ"], "url_scholarbib": "/scholar?q=info:WJz-HGzZn9AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultiplicative%2BLSTM%2Bfor%2Bsequence%2Bmodelling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WJz-HGzZn9AJ&ei=AxBkYpGGOIuKmgGY1YjABQ&json=", "num_citations": 166, "citedby_url": "/scholar?cites=15032973139552148568&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WJz-HGzZn9AJ:scholar.google.com/&scioq=Multiplicative+LSTM+for+sequence+modelling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.07959"}, "Improving Neural Language Models with a Continuous Cache": {"container_type": "Publication", "bib": {"title": "Improving neural language models with a continuous cache", "author": ["E Grave", "A Joulin", "N Usunier"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.04426", "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.04426", "author_id": ["7UV4ET4AAAAJ", "kRJkDakAAAAJ", "tYro5N8AAAAJ"], "url_scholarbib": "/scholar?q=info:tMJvjWNtmDQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BNeural%2BLanguage%2BModels%2Bwith%2Ba%2BContinuous%2BCache%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tMJvjWNtmDQJ&ei=BxBkYp_lBJGJmwGY-qmYDQ&json=", "num_citations": 245, "citedby_url": "/scholar?cites=3789899360774374068&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tMJvjWNtmDQJ:scholar.google.com/&scioq=Improving+Neural+Language+Models+with+a+Continuous+Cache&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.04426"}, "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING": {"container_type": "Publication", "bib": {"title": "A structured self-attentive sentence embedding", "author": ["Z Lin", "M Feng", "CN Santos", "M Yu", "B Xiang"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.03130", "author_id": ["LNZ4efwAAAAJ", "i0EuhA4AAAAJ", "sx8MOL8AAAAJ", "vC8DssQAAAAJ", "A6yjdJAAAAAJ"], "url_scholarbib": "/scholar?q=info:cwcDZgJA4zIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BSTRUCTURED%2BSELF-ATTENTIVE%2BSENTENCE%2BEMBEDDING%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cwcDZgJA4zIJ&ei=ExBkYvLbKo2ymgHg1rfQDQ&json=", "num_citations": 1819, "citedby_url": "/scholar?cites=3666844900655302515&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cwcDZgJA4zIJ:scholar.google.com/&scioq=A+STRUCTURED+SELF-ATTENTIVE+SENTENCE+EMBEDDING&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.03130.pdf?source=post_page---------------------------"}, "Information Dropout: learning optimal representations through noise": {"container_type": "Publication", "bib": {"title": "Information dropout: learning optimal representations through noise", "author": ["A Achille", "S Soatto"], "pub_year": "2016", "venue": "NA", "abstract": "We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJa0ECFxe", "author_id": ["LiwtZz4AAAAJ", "lH1PdF8AAAAJ"], "url_scholarbib": "/scholar?q=info:WIoxCf8CuoAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation%2BDropout:%2Blearning%2Boptimal%2Brepresentations%2Bthrough%2Bnoise%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WIoxCf8CuoAJ&ei=HhBkYszzGYOEmgHx-5DADA&json=", "num_citations": 22, "citedby_url": "/scholar?cites=9275729676917115480&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WIoxCf8CuoAJ:scholar.google.com/&scioq=Information+Dropout:+learning+optimal+representations+through+noise&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJa0ECFxe"}, "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?": {"container_type": "Publication", "bib": {"title": "What is the best practice for cnns applied to visual instance retrieval?", "author": ["J Hao", "J Dong", "W Wang", "T Tan"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01640", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01640", "author_id": ["", "", "Om9jSl8AAAAJ", "W-FGd_UAAAAJ"], "url_scholarbib": "/scholar?q=info:Gt8yzcewNOIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhat%2BIs%2Bthe%2BBest%2BPractice%2Bfor%2BCNNs%2BApplied%2Bto%2BVisual%2BInstance%2BRetrieval%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Gt8yzcewNOIJ&ei=IRBkYtmRBc6E6rQP5-KmKA&json=", "num_citations": 9, "citedby_url": "/scholar?cites=16299847323548311322&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Gt8yzcewNOIJ:scholar.google.com/&scioq=What+Is+the+Best+Practice+for+CNNs+Applied+to+Visual+Instance+Retrieval%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01640"}, "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Alternating direction method of multipliers for sparse convolutional neural networks", "author": ["F Kiaee", "C Gagn\u00e9", "M Abbasi"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01590", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01590", "author_id": ["", "egixsbEAAAAJ", "IIJuvsYAAAAJ"], "url_scholarbib": "/scholar?q=info:TXY7oIcb6QsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAlternating%2BDirection%2BMethod%2Bof%2BMultipliers%2Bfor%2BSparse%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TXY7oIcb6QsJ&ei=JBBkYurAPM6E6rQP5-KmKA&json=", "num_citations": 19, "citedby_url": "/scholar?cites=858247473313576525&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TXY7oIcb6QsJ:scholar.google.com/&scioq=Alternating+Direction+Method+of+Multipliers+for+Sparse+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01590"}, "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?": {"container_type": "Publication", "bib": {"title": "Do deep convolutional nets really need to be deep and convolutional?", "author": ["G Urban", "KJ Geras", "SE Kahou", "O Aslan", "S Wang"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1603.05691", "author_id": ["1ZNBxEcAAAAJ", "ZMCGp48AAAAJ", "F99FuaAAAAAJ", "LoOCs1kAAAAJ", "hLxJ02wAAAAJ"], "url_scholarbib": "/scholar?q=info:RFt92GGj6ZsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDo%2BDeep%2BConvolutional%2BNets%2BReally%2BNeed%2Bto%2Bbe%2BDeep%2Band%2BConvolutional%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RFt92GGj6ZsJ&ei=KBBkYujNEu-Sy9YPs_mY8AM&json=", "num_citations": 225, "citedby_url": "/scholar?cites=11234690386091662148&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RFt92GGj6ZsJ:scholar.google.com/&scioq=Do+Deep+Convolutional+Nets+Really+Need+to+be+Deep+and+Convolutional%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1603.05691.pdf?ref=https://githubhelp.com"}, "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views": {"container_type": "Publication", "bib": {"title": "An information retrieval approach for finding dependent subspaces of multiple views", "author": ["Z Lin", "J Peltonen"], "pub_year": "2017", "venue": "International Conference on Machine Learning and \u2026", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-319-62416-7_1", "author_id": ["", "WFYU6DkAAAAJ"], "url_scholarbib": "/scholar?q=info:oGTx7Vun5pUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BInformation%2BRetrieval%2BApproach%2Bfor%2BFinding%2BDependent%2BSubspaces%2Bof%2BMultiple%2BViews%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oGTx7Vun5pUJ&ei=LBBkYrGbDYyuyAT-mrWwCA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=10801504769570596000&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oGTx7Vun5pUJ:scholar.google.com/&scioq=An+Information+Retrieval+Approach+for+Finding+Dependent+Subspaces+of+Multiple+Views&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1511.06423"}, "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size": {"container_type": "Publication", "bib": {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size", "author": ["FN Iandola", "S Han", "MW Moskewicz", "K Ashraf"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages:(1) Smaller DNNs require less communication across servers during distributed training.(2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car.(3) Smaller DNNs are more feasible to deploy on FPGAs and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1602.07360", "author_id": ["y1lVpBEAAAAJ", "E0iCaa4AAAAJ", "XnhYW0MAAAAJ", "-lQSzAwAAAAJ"], "url_scholarbib": "/scholar?q=info:RzdzST68wO0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSqueezeNet:%2BAlexNet-level%2Baccuracy%2Bwith%2B50x%2Bfewer%2Bparameters%2Band%2B%253C0.5MB%2Bmodel%2Bsize%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RzdzST68wO0J&ei=LxBkYuzXGu-Sy9YPs_mY8AM&json=", "num_citations": 5539, "citedby_url": "/scholar?cites=17131899958223648583&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RzdzST68wO0J:scholar.google.com/&scioq=SqueezeNet:+AlexNet-level+accuracy+with+50x+fewer+parameters+and+%3C0.5MB+model+size&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1602.07360.pdf?ref=https://githubhelp.com"}, "Snapshot Ensembles: Train 1, Get M for Free": {"container_type": "Publication", "bib": {"title": "Snapshot ensembles: Train 1, get m for free", "author": ["G Huang", "Y Li", "G Pleiss", "Z Liu", "JE Hopcroft"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In fact, Snapshot Ensembles can even be  , an ensemble of K Snapshot Ensembles yields  K \u00d7 M models at K times the training cost. We evaluate the efficacy of Snapshot Ensembles on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.00109", "author_id": ["-P9LwcgAAAAJ", "QSTd1oUAAAAJ", "XO8T-Y4AAAAJ", "7OTD-LEAAAAJ", "4Z6vo5QAAAAJ"], "url_scholarbib": "/scholar?q=info:XE9lSNKsALgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSnapshot%2BEnsembles:%2BTrain%2B1,%2BGet%2BM%2Bfor%2BFree%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XE9lSNKsALgJ&ei=MRBkYua3N_mQ6rQP5OqKqAo&json=", "num_citations": 617, "citedby_url": "/scholar?cites=13258787322136448860&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XE9lSNKsALgJ:scholar.google.com/&scioq=Snapshot+Ensembles:+Train+1,+Get+M+for+Free&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.00109.pdf?ref=https://githubhelp.com"}, "Adversarial examples for generative models": {"container_type": "Publication", "bib": {"title": "Adversarial examples for generative models", "author": ["J Kos", "I Fischer", "D Song"], "pub_year": "2018", "venue": "2018 ieee security and privacy \u2026", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8424630/", "author_id": ["fszzlckAAAAJ", "Z63Zf_0AAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:wnar1tYp6fEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2Bexamples%2Bfor%2Bgenerative%2Bmodels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wnar1tYp6fEJ&ei=NBBkYqyiNIOEmgHx-5DADA&json=", "num_citations": 212, "citedby_url": "/scholar?cites=17431509835415516866&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wnar1tYp6fEJ:scholar.google.com/&scioq=Adversarial+examples+for+generative+models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.06832.pdf?ref=https://githubhelp.com"}, "Learning Recurrent Representations for Hierarchical Behavior Modeling": {"container_type": "Publication", "bib": {"title": "Learning recurrent representations for hierarchical behavior modeling", "author": ["E Eyjolfsdottir", "K Branson", "Y Yue", "P Perona"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The network has a discriminative part (classifying actions) and a generative part (predicting motion), whose recurrent cells are laterally connected, allowing higher levels of the network to represent high level phenomena. We test our framework on two types of data, fruit fly behavior and online handwriting. Our results show that 1) taking advantage of unlabeled"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.00094", "author_id": ["lROfnUQAAAAJ", "g558OVoAAAAJ", "tEk4qo8AAAAJ", "j29kMCwAAAAJ"], "url_scholarbib": "/scholar?q=info:Fd0x2Q4FVcIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRecurrent%2BRepresentations%2Bfor%2BHierarchical%2BBehavior%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Fd0x2Q4FVcIJ&ei=RRBkYrqcNo2ymgHg1rfQDQ&json=", "num_citations": 38, "citedby_url": "/scholar?cites=14003104177710030101&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Fd0x2Q4FVcIJ:scholar.google.com/&scioq=Learning+Recurrent+Representations+for+Hierarchical+Behavior+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.00094"}, "Improving Stochastic Gradient Descent with Feedback": {"container_type": "Publication", "bib": {"title": "Improving stochastic gradient descent with feedback", "author": ["J Koushik", "H Hayashi"], "pub_year": "2016", "venue": "NA", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1WUqIceg", "author_id": ["XTqgW-EAAAAJ", "dhdP1fIAAAAJ"], "url_scholarbib": "/scholar?q=info:rPVbVufM2KIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BStochastic%2BGradient%2BDescent%2Bwith%2BFeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rPVbVufM2KIJ&ei=SBBkYpKzD--Sy9YPs_mY8AM&json=", "num_citations": 32, "citedby_url": "/scholar?cites=11734354123072206252&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rPVbVufM2KIJ:scholar.google.com/&scioq=Improving+Stochastic+Gradient+Descent+with+Feedback&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1WUqIceg"}, "Tighter bounds lead to improved classifiers": {"container_type": "Publication", "bib": {"title": "Tighter bounds lead to improved classifiers", "author": ["NL Roux"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1606.09202", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.09202", "author_id": ["LmKtwk8AAAAJ"], "url_scholarbib": "/scholar?q=info:UDmz9mDVwNkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTighter%2Bbounds%2Blead%2Bto%2Bimproved%2Bclassifiers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UDmz9mDVwNkJ&ei=SxBkYpOUOO-Sy9YPs_mY8AM&json=", "num_citations": 7, "citedby_url": "/scholar?cites=15690775714191325520&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UDmz9mDVwNkJ:scholar.google.com/&scioq=Tighter+bounds+lead+to+improved+classifiers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.09202"}, "New Learning Approach By Genetic Algorithm In A Convolutional Neural Network For Pattern Recognition": {"container_type": "Publication", "bib": {"title": "New Learning Approach By Genetic Algorithm In A Convolutional Neural Network For Pattern Recognition", "author": ["MA Mehrolhassani", "M Mohammadi"], "pub_year": "2016", "venue": "NA", "abstract": "Almost all of the presented articles in the CNN are based on the error backpropagation algorithm and calculation of derivations of error, our innovative proposal refers to engaging TICA filters and NSGA-II genetic algorithms to train the LeNet-5 CNN network. Consequently, genetic algorithm updates the weights of LeNet-5 CNN network similar to chromosome update. In our approach the weights of LeNet-5 are obtained in two stages. The first is pre-training and the second is fine-tuning. As a result, our approach impacts in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJqaCVLxx", "author_id": ["", ""], "url_scholarbib": "/scholar?q=info:6UqYdWhQQq4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNew%2BLearning%2BApproach%2BBy%2BGenetic%2BAlgorithm%2BIn%2BA%2BConvolutional%2BNeural%2BNetwork%2BFor%2BPattern%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6UqYdWhQQq4J&ei=TxBkYvaAGIyuyAT-mrWwCA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:6UqYdWhQQq4J:scholar.google.com/&scioq=New+Learning+Approach+By+Genetic+Algorithm+In+A+Convolutional+Neural+Network+For+Pattern+Recognition&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJqaCVLxx"}, "Dynamic Partition Models": {"container_type": "Publication", "bib": {"title": "Predictive macro-finance with dynamic partition models", "author": ["D Zantedeschi", "P Damien"], "pub_year": "2011", "venue": "Journal of the American \u2026", "abstract": "We extend the dynamic N\u2013S approach in a number of ways. First, we propose a dynamic  product partition model (PPM) to allow for an unknown number of change points. This ad"}, "filled": false, "gsrank": 1, "pub_url": "https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.ap09732", "author_id": ["", ""], "url_scholarbib": "/scholar?q=info:a7lEIKVm6WMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BPartition%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a7lEIKVm6WMJ&ei=WhBkYsjwCZyO6rQP_qe3mAs&json=", "num_citations": 20, "citedby_url": "/scholar?cites=7199398338725460331&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a7lEIKVm6WMJ:scholar.google.com/&scioq=Dynamic+Partition+Models&hl=en&as_sdt=0,33", "eprint_url": "http://www.igier.unibocconi.it/files/zantedeschi.pdf"}, "Improving Generative Adversarial Networks with Denoising Feature Matching": {"container_type": "Publication", "bib": {"title": "Improving generative adversarial networks with denoising feature matching", "author": ["D Warde-Farley", "Y Bengio"], "pub_year": "2016", "venue": "NA", "abstract": "We propose an augmented training procedure for generative adversarial networks designed to address shortcomings of the original by directing the generator towards probable configurations of abstract discriminator features. We estimate and track the distribution of these features, as computed from data, with a denoising auto-encoder, and use it to propose high-level targets for the generator. We combine this new loss with the original and evaluate the hybrid criterion on the task of unsupervised image synthesis from datasets comprising a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1X7nhsxl", "author_id": ["MOgfm8oAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:ko3LUcnYcjUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGenerative%2BAdversarial%2BNetworks%2Bwith%2BDenoising%2BFeature%2BMatching%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ko3LUcnYcjUJ&ei=XRBkYpHmCLKO6rQPy-CRsA8&json=", "num_citations": 154, "citedby_url": "/scholar?cites=3851378990527516050&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ko3LUcnYcjUJ:scholar.google.com/&scioq=Improving+Generative+Adversarial+Networks+with+Denoising+Feature+Matching&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1X7nhsxl"}, "Words or Characters? Fine-grained Gating for Reading Comprehension": {"container_type": "Publication", "bib": {"title": "Words or characters? fine-grained gating for reading comprehension", "author": ["Z Yang", "B Dhingra", "Y Yuan", "J Hu", "WW Cohen"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01724", "author_id": ["7qXxyJkAAAAJ", "SLISZFAAAAAJ", "", "j-42gHYAAAAJ", "8ys-38kAAAAJ"], "url_scholarbib": "/scholar?q=info:Vgnf1xfvICkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWords%2Bor%2BCharacters%253F%2BFine-grained%2BGating%2Bfor%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Vgnf1xfvICkJ&ei=XxBkYoabKpGJmwGY-qmYDQ&json=", "num_citations": 89, "citedby_url": "/scholar?cites=2963631440494790998&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Vgnf1xfvICkJ:scholar.google.com/&scioq=Words+or+Characters%3F+Fine-grained+Gating+for+Reading+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01724"}, "Machine Solver for Physics Word Problems": {"container_type": "Publication", "bib": {"title": "Machine solver for physics word problems", "author": ["M Leszczynski", "J Moreira"], "pub_year": "2016", "venue": "NA", "abstract": "We build a machine solver for word problems on the physics of a free falling object under constant acceleration of gravity. Each problem consists of a formulation part, describing the setting, and a question part asking for the value of an unknown. Our solver consists of two long short-term memory recurrent neural networks and a numerical integrator. The first neural network (the labeler) labels each word of the problem, identifying the physical parameters and the question part of the problem. The second neural network (the classifier)"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyFkG45gl", "author_id": ["IVvSxFYAAAAJ", "5f3dXLkAAAAJ"], "url_scholarbib": "/scholar?q=info:shvaFLii_V0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMachine%2BSolver%2Bfor%2BPhysics%2BWord%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=shvaFLii_V0J&ei=YxBkYp6NEIuKmgGY1YjABQ&json=", "num_citations": 3, "citedby_url": "/scholar?cites=6772748326142614450&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:shvaFLii_V0J:scholar.google.com/&scioq=Machine+Solver+for+Physics+Word+Problems&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyFkG45gl"}, "A Convolutional Encoder Model for Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "A convolutional encoder model for neural machine translation", "author": ["J Gehring", "M Auli", "D Grangier", "YN Dauphin"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02344", "author_id": ["jOwTwm4AAAAJ", "KMcwQtcAAAAJ", "CIQEGCYAAAAJ", "XSforroAAAAJ"], "url_scholarbib": "/scholar?q=info:WEYP0G71frUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BConvolutional%2BEncoder%2BModel%2Bfor%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WEYP0G71frUJ&ei=aBBkYqSnBYOEmgHx-5DADA&json=", "num_citations": 410, "citedby_url": "/scholar?cites=13078160224216368728&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WEYP0G71frUJ:scholar.google.com/&scioq=A+Convolutional+Encoder+Model+for+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02344.pdf?ref=https://githubhelp.com"}, "Calibrating Energy-based Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Calibrating energy-based generative adversarial networks", "author": ["Z Dai", "A Almahairi", "P Bachman", "E Hovy"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.01691", "author_id": ["", "WbYAa7IAAAAJ", "REMIv-kAAAAJ", "PUFxrroAAAAJ"], "url_scholarbib": "/scholar?q=info:qv0aKU45oG8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCalibrating%2BEnergy-based%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qv0aKU45oG8J&ei=bBBkYpW1IY6pywTd4KPADw&json=", "num_citations": 96, "citedby_url": "/scholar?cites=8043491942343572906&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qv0aKU45oG8J:scholar.google.com/&scioq=Calibrating+Energy-based+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.01691"}, "Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning": {"container_type": "Publication", "bib": {"title": "Representation stability as a regularizer for improved text analytics transfer learning", "author": ["M Riemer", "E Khabiri", "R Goodwin"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1704.03617", "abstract": "Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.03617", "author_id": ["PK7UzAwAAAAJ", "xp0Dx6oAAAAJ", ""], "url_scholarbib": "/scholar?q=info:OEviRXfooGAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRepresentation%2BStability%2Bas%2Ba%2BRegularizer%2Bfor%2BImproved%2BText%2BAnalytics%2BTransfer%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OEviRXfooGAJ&ei=bxBkYqynGY2ymgHg1rfQDQ&json=", "num_citations": 18, "citedby_url": "/scholar?cites=6962820622885997368&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OEviRXfooGAJ:scholar.google.com/&scioq=Representation+Stability+as+a+Regularizer+for+Improved+Text+Analytics+Transfer+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.03617"}, "Sentence Ordering using Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Sentence ordering using recurrent neural networks", "author": ["L Logeswaran", "H Lee", "D Radev"], "pub_year": "2016", "venue": "NA", "abstract": "Modeling the structure of coherent texts is a task of great importance in NLP. The task of organizing a given set of sentences into a coherent order has been commonly used to build and evaluate models that understand such structure. In this work we propose an end-to-end neural approach based on the recently proposed set to sequence mapping framework to address the sentence ordering problem. Our model achieves state-of-the-art performance in the order discrimination task on two datasets widely used in the literature. We also consider"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1AG8zYeg", "author_id": ["dcv4kpIAAAAJ", "fmSHtE8AAAAJ", "vIqWvgwAAAAJ"], "url_scholarbib": "/scholar?q=info:6QOouJXkyCwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSentence%2BOrdering%2Busing%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6QOouJXkyCwJ&ei=cxBkYrXkJo2ymgHg1rfQDQ&json=", "num_citations": 23, "citedby_url": "/scholar?cites=3227080464710239209&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6QOouJXkyCwJ:scholar.google.com/&scioq=Sentence+Ordering+using+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1AG8zYeg"}, "Learning similarity preserving representations with neural similarity and context encoders": {"container_type": "Publication", "bib": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "author": ["F Horn", "KR M\u00fcller"], "pub_year": "2016", "venue": "NA", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data. Furthermore, the idea"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkBsEQYll", "author_id": ["", "jplQac8AAAAJ"], "url_scholarbib": "/scholar?q=info:DFuGHHDlm20J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bsimilarity%2Bpreserving%2Brepresentations%2Bwith%2Bneural%2Bsimilarity%2Band%2Bcontext%2Bencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DFuGHHDlm20J&ei=eBBkYoOqAouKmgGY1YjABQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=7898158641201961740&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DFuGHHDlm20J:scholar.google.com/&scioq=Learning+similarity+preserving+representations+with+neural+similarity+and+context+encoders&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkBsEQYll"}, "Neural Functional Programming": {"container_type": "Publication", "bib": {"title": "Neural functional programming", "author": ["JK Feser", "M Brockschmidt", "AL Gaunt", "D Tarlow"], "pub_year": "2017", "venue": "NA", "abstract": "programming language suitable for learning programs from inputoutput examples. Taking  cues from programming  up to a simple differentiable functional programming language. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byp_ccVte", "author_id": ["ff2r-x0AAAAJ", "pF27eLMAAAAJ", "IzwvqPIAAAAJ", "oavgGaMAAAAJ"], "url_scholarbib": "/scholar?q=info:gWf7hebzH8gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BFunctional%2BProgramming%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gWf7hebzH8gJ&ei=exBkYvSSJ42ymgHg1rfQDQ&json=", "num_citations": 10, "citedby_url": "/scholar?cites=14420512703279490945&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gWf7hebzH8gJ:scholar.google.com/&scioq=Neural+Functional+Programming&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byp_ccVte"}, "Unsupervised Learning of State Representations for Multiple Tasks": {"container_type": "Publication", "bib": {"title": "Unsupervised learning of state representations for multiple tasks", "author": ["A Raffin", "S H\u00f6fer", "R Jonschkowski", "O Brock", "F Stulp"], "pub_year": "2016", "venue": "NA", "abstract": "We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1aGWUqgg", "author_id": ["kik4AwIAAAAJ", "srCW5e4AAAAJ", "5ErX8dMAAAAJ", "83OxU_4AAAAJ", "aHPX6PsAAAAJ"], "url_scholarbib": "/scholar?q=info:ZRyUhNUf30IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2Bof%2BState%2BRepresentations%2Bfor%2BMultiple%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZRyUhNUf30IJ&ei=fhBkYve7PJGJmwGY-qmYDQ&json=", "num_citations": 6, "citedby_url": "/scholar?cites=4818605128222514277&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZRyUhNUf30IJ:scholar.google.com/&scioq=Unsupervised+Learning+of+State+Representations+for+Multiple+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1aGWUqgg"}, "Adversarial Training Methods for Semi-Supervised Text Classification": {"container_type": "Publication", "bib": {"title": "Adversarial training methods for semi-supervised text classification", "author": ["T Miyato", "AM Dai", "I Goodfellow"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.07725", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.07725", "author_id": ["s2lG0X0AAAAJ", "2r2NuDAAAAAJ", "iYN86KEAAAAJ"], "url_scholarbib": "/scholar?q=info:iTbgKhGCg1sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BTraining%2BMethods%2Bfor%2BSemi-Supervised%2BText%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iTbgKhGCg1sJ&ei=ihBkYtPPDvmQ6rQP5OqKqAo&json=", "num_citations": 670, "citedby_url": "/scholar?cites=6594257289645930121&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iTbgKhGCg1sJ:scholar.google.com/&scioq=Adversarial+Training+Methods+for+Semi-Supervised+Text+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.07725"}, "Data Noising as Smoothing in Neural Network Language Models": {"container_type": "Publication", "bib": {"title": "Data noising as smoothing in neural network language models", "author": ["Z Xie", "SI Wang", "J Li", "D L\u00e9vy", "A Nie", "D Jurafsky"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $ n $-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.02573", "author_id": ["mT-seW0AAAAJ", "XUI4PMEAAAAJ", "PwU16JEAAAAJ", "xmOaZ8EAAAAJ", "r90OelAAAAAJ", "uZg9l58AAAAJ"], "url_scholarbib": "/scholar?q=info:2nUwmDBW7T8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DData%2BNoising%2Bas%2BSmoothing%2Bin%2BNeural%2BNetwork%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2nUwmDBW7T8J&ei=jxBkYoHbG5yO6rQP_qe3mAs&json=", "num_citations": 190, "citedby_url": "/scholar?cites=4606432760581617114&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2nUwmDBW7T8J:scholar.google.com/&scioq=Data+Noising+as+Smoothing+in+Neural+Network+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.02573"}, "Sample Importance in Training Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Sample importance in training deep neural networks", "author": ["T Gao", "V Jojic"], "pub_year": "2016", "venue": "NA", "abstract": "The contribution of each sample during model training varies across training iterations and the model's parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that\" easy\" samples--samples that are correctly and confidently classified at the end of the training--shape parameters closer to the output, while the\" hard\" samples impact parameters closer to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1IRctqxg", "author_id": ["TO7sPJQAAAAJ", "IzhtPq4AAAAJ"], "url_scholarbib": "/scholar?q=info:dIsUptKHEl0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSample%2BImportance%2Bin%2BTraining%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dIsUptKHEl0J&ei=kRBkYvzBNJGJmwGY-qmYDQ&json=", "num_citations": 10, "citedby_url": "/scholar?cites=6706572133907336052&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dIsUptKHEl0J:scholar.google.com/&scioq=Sample+Importance+in+Training+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1IRctqxg"}, "Options Discovery with Budgeted Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Options discovery with budgeted reinforcement learning", "author": ["A L\u00e9on", "L Denoyer"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.06824", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.06824", "author_id": ["", ""], "url_scholarbib": "/scholar?q=info:iXYIkvojaZsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptions%2BDiscovery%2Bwith%2BBudgeted%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iXYIkvojaZsJ&ei=lBBkYu_fKLKO6rQPy-CRsA8&json=", "num_citations": 5, "citedby_url": "/scholar?cites=11198521507532273289&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iXYIkvojaZsJ:scholar.google.com/&scioq=Options+Discovery+with+Budgeted+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.06824"}, "Nonparametrically Learning Activation Functions in Deep Neural Nets": {"container_type": "Publication", "bib": {"title": "Nonparametrically learning activation functions in deep neural nets", "author": ["C Eisenach", "Z Wang", "H Liu"], "pub_year": "2016", "venue": "NA", "abstract": "We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justification for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1wgawqxl", "author_id": ["Jx269u0AAAAJ", "HSx0BgQAAAAJ", "XaFT1o4AAAAJ"], "url_scholarbib": "/scholar?q=info:AR9TJoo0Ua0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNonparametrically%2BLearning%2BActivation%2BFunctions%2Bin%2BDeep%2BNeural%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AR9TJoo0Ua0J&ei=mBBkYoXrE-HDywSSipaYAg&json=", "num_citations": 11, "citedby_url": "/scholar?cites=12488821009628208897&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AR9TJoo0Ua0J:scholar.google.com/&scioq=Nonparametrically+Learning+Activation+Functions+in+Deep+Neural+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1wgawqxl"}, "Learning to Protect Communications with Adversarial Neural Cryptography": {"container_type": "Publication", "bib": {"title": "Learning to protect communications with adversarial neural cryptography", "author": ["M Abadi", "DG Andersen"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.06918", "abstract": "We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks;"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.06918", "author_id": ["vWTI60AAAAAJ", "wUArfPgAAAAJ"], "url_scholarbib": "/scholar?q=info:x5Ld2G_zs1MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BProtect%2BCommunications%2Bwith%2BAdversarial%2BNeural%2BCryptography%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x5Ld2G_zs1MJ&ei=nBBkYqu1FuiSy9YPp-OyiAE&json=", "num_citations": 203, "citedby_url": "/scholar?cites=6031431987684545223&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x5Ld2G_zs1MJ:scholar.google.com/&scioq=Learning+to+Protect+Communications+with+Adversarial+Neural+Cryptography&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.06918"}, "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement": {"container_type": "Publication", "bib": {"title": "Episodic exploration for deep deterministic policies for StarCraft micromanagement", "author": ["N Usunier", "G Synnaeve", "Z Lin", "S Chintala"], "pub_year": "2016", "venue": "NA", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state-action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1LXit5ee", "author_id": ["tYro5N8AAAAJ", "wN9rBkcAAAAJ", "ZDjmMuwAAAAJ", "36ofBJgAAAAJ"], "url_scholarbib": "/scholar?q=info:Ny7vp2D0UHUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEpisodic%2BExploration%2Bfor%2BDeep%2BDeterministic%2BPolicies%2Bfor%2BStarCraft%2BMicromanagement%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ny7vp2D0UHUJ&ei=nxBkYrmRG5GJmwGY-qmYDQ&json=", "num_citations": 8, "citedby_url": "/scholar?cites=8453525196545928759&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ny7vp2D0UHUJ:scholar.google.com/&scioq=Episodic+Exploration+for+Deep+Deterministic+Policies+for+StarCraft+Micromanagement&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1LXit5ee"}, "Exploring the Application of Deep Learning for Supervised Learning Problems": {"container_type": "Publication", "bib": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "author": ["J Rozanec", "G Katz", "ECR Shin", "D Song"], "pub_year": "2016", "venue": "NA", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byx5BTilg", "author_id": ["", "FTCVfdMAAAAJ", "xPnkc80AAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:TlbqEIVvp5wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploring%2Bthe%2BApplication%2Bof%2BDeep%2BLearning%2Bfor%2BSupervised%2BLearning%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TlbqEIVvp5wJ&ei=qhBkYt_eDYySyASZk6HgCA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:TlbqEIVvp5wJ:scholar.google.com/&scioq=Exploring+the+Application+of+Deep+Learning+for+Supervised+Learning+Problems&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byx5BTilg"}, "Compositional Kernel Machines": {"container_type": "Publication", "bib": {"title": "Compositional kernel machines", "author": ["R Gens", "P Domingos"], "pub_year": "2017", "venue": "NA", "abstract": "We propose the Compositional Kernel Machine (CKM), a kernel-based visual classifier that  has the symmetry and compositionality of convnets but with the training benefits of instance-"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1Bm3T_lg", "author_id": ["8cZgFK4AAAAJ", "KOrhfVMAAAAJ"], "url_scholarbib": "/scholar?q=info:JuhGq_JkdVwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompositional%2BKernel%2BMachines%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JuhGq_JkdVwJ&ei=shBkYqySIpGJmwGY-qmYDQ&json=", "num_citations": 6, "citedby_url": "/scholar?cites=6662342217182930982&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JuhGq_JkdVwJ:scholar.google.com/&scioq=Compositional+Kernel+Machines&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rk34W-DOl"}, "Bit-Pragmatic Deep Neural Network Computing": {"container_type": "Publication", "bib": {"title": "Bit-pragmatic deep neural network computing", "author": ["J Albericio", "A Delm\u00e1s", "P Judd", "S Sharify"], "pub_year": "2017", "venue": "Proceedings of the 50th \u2026", "abstract": "Deep Neural Networks expose a high degree of parallelism, making them amenable to highly  data parallel architectures. However, data-parallel architectures often accept inefficiency"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3123939.3123982?casa_token=HOD4jmxmzocAAAAA:d01J_mlXaCjxQ2KK4nkFXPL9X39BS0G_gGhLN-lOTUAW05nEusumKIT6g-jNyPIGWM8XtJ4rtH7RgQ", "author_id": ["C6GeyOwAAAAJ", "", "", "1QehFwQAAAAJ"], "url_scholarbib": "/scholar?q=info:cMBWPyznOa4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBit-Pragmatic%2BDeep%2BNeural%2BNetwork%2BComputing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cMBWPyznOa4J&ei=uBBkYouRDc6E6rQP5-KmKA&json=", "num_citations": 175, "citedby_url": "/scholar?cites=12554319613499195504&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cMBWPyznOa4J:scholar.google.com/&scioq=Bit-Pragmatic+Deep+Neural+Network+Computing&hl=en&as_sdt=0,33", "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3123939.3123982?casa_token=lYGStjRr_R8AAAAA:rVXyGQeHTJSf22xH2UsVxP9B0TRcJc3Go3wdV4YSTECISJ4RqaH5e5TepyqsI27SpFMSFoPPDb-xRQ"}, "Multi-view Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Multi-view generative adversarial networks", "author": ["M Chen", "L Denoyer"], "pub_year": "2017", "venue": "Joint European Conference on Machine Learning \u2026", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-319-71246-8_11", "author_id": ["QnRpMJAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:YdUxuBOjghAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-view%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YdUxuBOjghAJ&ei=vBBkYoyzM8LZmQHnraWYCA&json=", "num_citations": 34, "citedby_url": "/scholar?cites=1189692556669212001&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YdUxuBOjghAJ:scholar.google.com/&scioq=Multi-view+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02019.pdf?ref=https://githubhelp.com"}, "Dropout with Expectation-linear Regularization": {"container_type": "Publication", "bib": {"title": "Dropout with expectation-linear regularization", "author": ["X Ma", "Y Gao", "Z Hu", "Y Yu", "Y Deng", "E Hovy"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout's training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.08017", "author_id": ["6_MQLIcAAAAJ", "CRRSVpAAAAAJ", "N7_xhHoAAAAJ", "zbXIQMsAAAAJ", "tk0e5lYAAAAJ", "PUFxrroAAAAJ"], "url_scholarbib": "/scholar?q=info:uaAIqEwjCsIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDropout%2Bwith%2BExpectation-linear%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uaAIqEwjCsIJ&ei=wBBkYpnrEYOEmgHx-5DADA&json=", "num_citations": 31, "citedby_url": "/scholar?cites=13982026805268750521&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uaAIqEwjCsIJ:scholar.google.com/&scioq=Dropout+with+Expectation-linear+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.08017"}, "Enforcing constraints on outputs with unconstrained inference": {"container_type": "Publication", "bib": {"title": "Enforcing constraints on outputs with unconstrained inference", "author": ["JY Lee", "ML Wick", "JB Tristan"], "pub_year": "2016", "venue": "NA", "abstract": "Increasingly, practitioners apply neural networks to complex problems in natural language processing (NLP), such as syntactic parsing, that have rich output structures. Many such applications require deterministic constraints on the output values; for example, requiring that the sequential outputs encode a valid tree. While hidden units might capture such properties, the network is not always able to learn them from the training data alone, and practitioners must then resort to post-processing. In this paper, we present an inference"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1Jhfftgx", "author_id": ["_USiaqwAAAAJ", "D0DpJUwAAAAJ", "iMs3HJoAAAAJ"], "url_scholarbib": "/scholar?q=info:6Y23jmBQpb0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnforcing%2Bconstraints%2Bon%2Boutputs%2Bwith%2Bunconstrained%2Binference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6Y23jmBQpb0J&ei=xhBkYunxKIyuyAT-mrWwCA&json=", "num_citations": 5, "citedby_url": "/scholar?cites=13665417019967114729&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6Y23jmBQpb0J:scholar.google.com/&scioq=Enforcing+constraints+on+outputs+with+unconstrained+inference&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1Jhfftgx"}, "Deep unsupervised learning through spatial contrasting": {"container_type": "Publication", "bib": {"title": "Deep unsupervised learning through spatial contrasting", "author": ["E Hoffer", "I Hubara", "N Ailon"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.00243", "abstract": "Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.00243", "author_id": ["iEfTH7AAAAAJ", "dyYryZYAAAAJ", "MpckH9YAAAAJ"], "url_scholarbib": "/scholar?q=info:m5vw5e7_e1EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2Bunsupervised%2Blearning%2Bthrough%2Bspatial%2Bcontrasting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m5vw5e7_e1EJ&ei=zBBkYuf0LI2ymgHg1rfQDQ&json=", "num_citations": 22, "citedby_url": "/scholar?cites=5871567940732623771&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m5vw5e7_e1EJ:scholar.google.com/&scioq=Deep+unsupervised+learning+through+spatial+contrasting&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.00243"}, "Learning Graphical State Transitions": {"container_type": "Publication", "bib": {"title": "Learning graphical state transitions", "author": ["DD Johnson"], "pub_year": "2016", "venue": "NA", "abstract": "Note that it is natural to think of these transformations as operating on a single graphical  state, and each modifying the state in place. However, in the technical descriptions of these"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJ0NvFzxl", "author_id": ["44R4pgMAAAAJ"], "url_scholarbib": "/scholar?q=info:f_AEUL8UyqUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BGraphical%2BState%2BTransitions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=f_AEUL8UyqUJ&ei=1hBkYoztB8LZmQHnraWYCA&json=", "num_citations": 96, "citedby_url": "/scholar?cites=11946383773467471999&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:f_AEUL8UyqUJ:scholar.google.com/&scioq=Learning+Graphical+State+Transitions&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJ0NvFzxl"}, "Linear Time Complexity Deep Fourier Scattering Network and Extension to Nonlinear Invariants": {"container_type": "Publication", "bib": {"title": "Linear time complexity deep Fourier scattering network and extension to nonlinear invariants", "author": ["R Balestriero", "H Glotin"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.05841", "abstract": "In this paper we propose a scalable version of a state-of-the-art deterministic time-invariant feature extraction approach based on consecutive changes of basis and nonlinearities, namely, the scattering network. The first focus of the paper is to extend the scattering network to allow the use of higher order nonlinearities as well as extracting nonlinear and Fourier based statistics leading to the required invariants of any inherently structured input. In order to reach fast convolutions and to leverage the intrinsic structure of wavelets, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.05841", "author_id": ["S1x_xqcAAAAJ", "DqieizcAAAAJ"], "url_scholarbib": "/scholar?q=info:vz5TGQ0skgAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLinear%2BTime%2BComplexity%2BDeep%2BFourier%2BScattering%2BNetwork%2Band%2BExtension%2Bto%2BNonlinear%2BInvariants%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vz5TGQ0skgAJ&ei=2hBkYqDZM--Sy9YPs_mY8AM&json=", "num_citations": 7, "citedby_url": "/scholar?cites=41143781370838719&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vz5TGQ0skgAJ:scholar.google.com/&scioq=Linear+Time+Complexity+Deep+Fourier+Scattering+Network+and+Extension+to+Nonlinear+Invariants&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.05841"}, "Neural Architecture Search with Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Neural architecture search with reinforcement learning", "author": ["B Zoph", "QV Le"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01578", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01578", "author_id": ["H-BnRI0AAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:UhSMkIyvzDkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BArchitecture%2BSearch%2Bwith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UhSMkIyvzDkJ&ei=3RBkYtD0O7KO6rQPy-CRsA8&json=", "num_citations": 3885, "citedby_url": "/scholar?cites=4164896773666247762&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UhSMkIyvzDkJ:scholar.google.com/&scioq=Neural+Architecture+Search+with+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01578.pdf)"}, "Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization": {"container_type": "Publication", "bib": {"title": "Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization", "author": ["J Oh", "S Singh", "H Lee", "P Kohli"], "pub_year": "2016", "venue": "NA", "abstract": "The ability to generalize from past experience to solve previously unseen tasks is a key research challenge in reinforcement learning (RL). In this paper, we consider RL tasks defined as a sequence of high-level instructions described by natural language and study two types of generalization: to unseen and longer sequences of previously seen instructions, and to sequences where the instructions themselves were previously not seen. We present a novel hierarchical deep RL architecture that consists of two interacting neural controllers: a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJttqw5ge", "author_id": ["LNUeOu4AAAAJ", "8RgDBoEAAAAJ", "fmSHtE8AAAAJ", "3pyzQQ8AAAAJ"], "url_scholarbib": "/scholar?q=info:5nEo4Sbe1XkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCommunicating%2BHierarchical%2BNeural%2BControllers%2Bfor%2BLearning%2BZero-shot%2BTask%2BGeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5nEo4Sbe1XkJ&ei=4BBkYorMPIuKmgGY1YjABQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:5nEo4Sbe1XkJ:scholar.google.com/&scioq=Communicating+Hierarchical+Neural+Controllers+for+Learning+Zero-shot+Task+Generalization&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJttqw5ge"}, "Vocabulary Selection Strategies for Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Vocabulary selection strategies for neural machine translation", "author": ["G L'Hostis", "D Grangier", "M Auli"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.00072", "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.00072", "author_id": ["", "CIQEGCYAAAAJ", "KMcwQtcAAAAJ"], "url_scholarbib": "/scholar?q=info:8ttIdSZ64eUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVocabulary%2BSelection%2BStrategies%2Bfor%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8ttIdSZ64eUJ&ei=5BBkYq24HaKUy9YP_JONiAY&json=", "num_citations": 30, "citedby_url": "/scholar?cites=16564655210040450034&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8ttIdSZ64eUJ:scholar.google.com/&scioq=Vocabulary+Selection+Strategies+for+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.00072"}, "A Context-aware Attention Network for Interactive Question Answering": {"container_type": "Publication", "bib": {"title": "A context-aware attention network for interactive question answering", "author": ["H Li", "MR Min", "Y Ge", "A Kadav"], "pub_year": "2017", "venue": "Proceedings of the 23rd ACM SIGKDD \u2026", "abstract": "Neural network based sequence-to-sequence models in an encoder-decoder framework have been successfully applied to solve Question Answering (QA) problems, predicting answers from statements and questions. However, almost all previous models have failed to consider detailed context information and unknown states under which systems do not have enough information to answer given questions. These scenarios with incomplete or ambiguous information are very common in the setting of Interactive Question Answering"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3097983.3098115?casa_token=4aKX2UnPi28AAAAA:nUqvDRTSL0X_9sJkcnXn_RqcJq0tgzid1VnoWyEZbD-DEFF5TqQxGchGaG87ITrfWzSWg4YttKER", "author_id": ["n_L9Z64AAAAJ", "", "lU4gAFQAAAAJ", "IphEjqcAAAAJ"], "url_scholarbib": "/scholar?q=info:puMPfq27jCAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BContext-aware%2BAttention%2BNetwork%2Bfor%2BInteractive%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=puMPfq27jCAJ&ei=7xBkYuO6NY2ymgHg1rfQDQ&json=", "num_citations": 71, "citedby_url": "/scholar?cites=2345455859771892646&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:puMPfq27jCAJ:scholar.google.com/&scioq=A+Context-aware+Attention+Network+for+Interactive+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3097983.3098115?casa_token=L26h8RXVtM8AAAAA:j_j5f-TePTjSghYEkI6T7pnKVtweWXXy2AQaZfwUsqcD2HAF-8QMHqdrPbN6M9vtmCLwWwCAFm1J"}, "A Compositional Object-Based Approach to Learning Physical Dynamics": {"container_type": "Publication", "bib": {"title": "A compositional object-based approach to learning physical dynamics", "author": ["MB Chang", "T Ullman", "A Torralba"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present the Neural Physics Engine (NPE), a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations. We propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions. Like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions; realized as a neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.00341", "author_id": ["vgfGtykAAAAJ", "5SF-hRsAAAAJ", "8cxDHS4AAAAJ"], "url_scholarbib": "/scholar?q=info:XFwvNBoYtoYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCompositional%2BObject-Based%2BApproach%2Bto%2BLearning%2BPhysical%2BDynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XFwvNBoYtoYJ&ei=8xBkYva-HY2ymgHg1rfQDQ&json=", "num_citations": 357, "citedby_url": "/scholar?cites=9706972547667418204&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XFwvNBoYtoYJ:scholar.google.com/&scioq=A+Compositional+Object-Based+Approach+to+Learning+Physical+Dynamics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.00341.pdf?ref=https://githubhelp.com"}, "ParMAC: distributed optimisation of nested functions, with application to binary autoencoders": {"container_type": "Publication", "bib": {"title": "ParMAC: distributed optimisation of nested functions, with application to binary autoencoders", "author": ["MA Carreira-Perpinan", "M Alizadeh"], "pub_year": "2016", "venue": "NA", "abstract": "Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such\" nested\" functions is the\" method of auxiliary coordinates (MAC)\". MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SygvTcYee", "author_id": ["SYdYhxgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:Ei92wXyARrYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParMAC:%2Bdistributed%2Boptimisation%2Bof%2Bnested%2Bfunctions,%2Bwith%2Bapplication%2Bto%2Bbinary%2Bautoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ei92wXyARrYJ&ei=9xBkYrN4nI7qtA_-p7eYCw&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:Ei92wXyARrYJ:scholar.google.com/&scioq=ParMAC:+distributed+optimisation+of+nested+functions,+with+application+to+binary+autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SygvTcYee"}, "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "# exploration: A study of count-based exploration for deep reinforcement learning", "author": ["H Tang", "R Houthooft", "D Foote"], "pub_year": "2017", "venue": "Advances in neural \u2026", "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.neurips.cc/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html", "author_id": ["j_xavDQAAAAJ", "HBztuGIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:kf7PmRJuH2sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2523Exploration:%2BA%2BStudy%2Bof%2BCount-Based%2BExploration%2Bfor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kf7PmRJuH2sJ&ei=-hBkYuWaC7KO6rQPy-CRsA8&json=", "num_citations": 531, "citedby_url": "/scholar?cites=7719009312505331345&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kf7PmRJuH2sJ:scholar.google.com/&scioq=%23Exploration:+A+Study+of+Count-Based+Exploration+for+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://proceedings.neurips.cc/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf"}, "Regularizing Neural Networks by Penalizing Confident Output Distributions": {"container_type": "Publication", "bib": {"title": "Regularizing neural networks by penalizing confident output distributions", "author": ["G Pereyra", "G Tucker", "J Chorowski", "\u0141 Kaiser"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.06548", "author_id": ["b8ujmwgAAAAJ", "-gJkPHIAAAAJ", "Yc94070AAAAJ", "JWmiQR0AAAAJ"], "url_scholarbib": "/scholar?q=info:W6YUzxlPR-4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegularizing%2BNeural%2BNetworks%2Bby%2BPenalizing%2BConfident%2BOutput%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W6YUzxlPR-4J&ei=_RBkYu79IKKUy9YP_JONiAY&json=", "num_citations": 722, "citedby_url": "/scholar?cites=17169779076640319067&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W6YUzxlPR-4J:scholar.google.com/&scioq=Regularizing+Neural+Networks+by+Penalizing+Confident+Output+Distributions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.06548"}, "Knowledge Adaptation: Teaching to Adapt": {"container_type": "Publication", "bib": {"title": "Knowledge adaptation: Teaching to adapt", "author": ["S Ruder", "P Ghaffari", "JG Breslin"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.02052", "abstract": "Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, eg spam detection where attackers continuously change their tactics. To fill this gap, we propose Knowledge Adaptation, an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.02052", "author_id": ["8ONXPV8AAAAJ", "eQEHcQ0AAAAJ", "D8lvl64AAAAJ"], "url_scholarbib": "/scholar?q=info:1hSQgDmrmjIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKnowledge%2BAdaptation:%2BTeaching%2Bto%2BAdapt%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1hSQgDmrmjIJ&ei=ARFkYs3-A46pywTd4KPADw&json=", "num_citations": 33, "citedby_url": "/scholar?cites=3646415111768249558&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1hSQgDmrmjIJ:scholar.google.com/&scioq=Knowledge+Adaptation:+Teaching+to+Adapt&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.02052"}, "Classify or Select: Neural Architectures for Extractive Document Summarization": {"container_type": "Publication", "bib": {"title": "Classify or select: Neural architectures for extractive document summarization", "author": ["R Nallapati", "B Zhou", "M Ma"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.04244", "abstract": "In this work, we propose two neural architectures for extractive summarization. Our proposed  models under these architectures are not only very interpretable, but also achieve state-of-"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.04244", "author_id": ["mZcPPW4AAAAJ", "h3Nsz6YAAAAJ", "_vryxeMAAAAJ"], "url_scholarbib": "/scholar?q=info:px3notpFJOcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DClassify%2Bor%2BSelect:%2BNeural%2BArchitectures%2Bfor%2BExtractive%2BDocument%2BSummarization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=px3notpFJOcJ&ei=AxFkYrjTKYyuyAT-mrWwCA&json=", "num_citations": 71, "citedby_url": "/scholar?cites=16655514127261179303&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:px3notpFJOcJ:scholar.google.com/&scioq=Classify+or+Select:+Neural+Architectures+for+Extractive+Document+Summarization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.04244.pdf?ref=https://githubhelp.com"}, "Gradients of Counterfactuals": {"container_type": "Publication", "bib": {"title": "Gradients of counterfactuals", "author": ["M Sundararajan", "A Taly", "Q Yan"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02639", "abstract": "We propose to examine interior gradients, which are gradients of counterfactual inputs   how interior gradients better capture feature importance. Furthermore, interior gradients are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02639", "author_id": ["q39nzokAAAAJ", "EkmqsakAAAAJ", "Wn8xr_gAAAAJ"], "url_scholarbib": "/scholar?q=info:be6UYNY9kroJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradients%2Bof%2BCounterfactuals%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=be6UYNY9kroJ&ei=BxFkYri4B7KO6rQPy-CRsA8&json=", "num_citations": 73, "citedby_url": "/scholar?cites=13443875828607020653&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:be6UYNY9kroJ:scholar.google.com/&scioq=Gradients+of+Counterfactuals&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02639?ref=https://githubhelp.com"}, "Optimal Binary Autoencoding with Pairwise Correlations": {"container_type": "Publication", "bib": {"title": "Optimal Binary Autoencoding with Pairwise Correlations", "author": ["A Balsubramani"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02268", "abstract": "We formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits. Among all possible algorithms that use this information, ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss. The optimal decoder is a single layer of artificial neurons, emerging entirely from the minimax loss minimization, and with weights learned by convex optimization. All this is reflected in competitive experimental results, demonstrating"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02268", "author_id": ["Hd80zWMAAAAJ"], "url_scholarbib": "/scholar?q=info:OL5Qda4g4fYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BBinary%2BAutoencoding%2Bwith%2BPairwise%2BCorrelations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OL5Qda4g4fYJ&ei=ChFkYpPgDYOEmgHx-5DADA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:OL5Qda4g4fYJ:scholar.google.com/&scioq=Optimal+Binary+Autoencoding+with+Pairwise+Correlations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02268"}, "Shift Aggregate Extract Networks": {"container_type": "Publication", "bib": {"title": "Shift aggregate extract networks", "author": ["F Orsini", "D Baracchi", "P Frasconi"], "pub_year": "2018", "venue": "Frontiers in Robotics and AI", "abstract": "In the bottom-right part of the picture we show the shift step in which  shifted using the  Kronecker product in Equation 1. Then the shifted representation are summed in the aggregation"}, "filled": false, "gsrank": 1, "pub_url": "https://www.frontiersin.org/articles/10.3389/frobt.2018.00042/full", "author_id": ["jrrEHzQAAAAJ", "ARgZkiwAAAAJ", "s3l225EAAAAJ"], "url_scholarbib": "/scholar?q=info:x4EVHBMh7sgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DShift%2BAggregate%2BExtract%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x4EVHBMh7sgJ&ei=DhFkYrTcA4OEmgHx-5DADA&json=", "num_citations": 16, "citedby_url": "/scholar?cites=14478546218001990087&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x4EVHBMh7sgJ:scholar.google.com/&scioq=Shift+Aggregate+Extract+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://www.frontiersin.org/articles/10.3389/frobt.2018.00042/full"}, "RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "RL: Fast Reinforcement Learning via Slow Reinforcement Learning", "author": ["Y Duan", "J Schulman", "X Chen", "PL Bartlett"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a\" fast\" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL $^ 2$, the algorithm is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02779", "author_id": ["EMDboA4AAAAJ", "itSa94cAAAAJ", "5tVuggUAAAAJ", "yQNhFGUAAAAJ"], "url_scholarbib": "/scholar?q=info:0ma30PR40b4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRL%255E2:%2BFast%2BReinforcement%2BLearning%2Bvia%2BSlow%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0ma30PR40b4J&ei=ExFkYof5OYuKmgGY1YjABQ&json=", "num_citations": 676, "citedby_url": "/scholar?cites=13749904130207868626&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0ma30PR40b4J:scholar.google.com/&scioq=RL%5E2:+Fast+Reinforcement+Learning+via+Slow+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02779.pdf?ref=https://githubhelp.com"}, "Pointer Sentinel Mixture Models": {"container_type": "Publication", "bib": {"title": "Pointer sentinel mixture models", "author": ["S Merity", "C Xiong", "J Bradbury", "R Socher"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.07843", "abstract": "We introduce the pointer sentinel mixture architecture for neural sequence models which  has the ability to either  Our pointer sentinelLSTM model achieves state of the art language"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.07843", "author_id": ["AolIi4QAAAAJ", "vaSdahkAAAAJ", "GprA5UsAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:ylU5cq3kM_cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPointer%2BSentinel%2BMixture%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ylU5cq3kM_cJ&ei=HxFkYu2aJoySyASZk6HgCA&json=", "num_citations": 1029, "citedby_url": "/scholar?cites=17812832384777278922&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ylU5cq3kM_cJ:scholar.google.com/&scioq=Pointer+Sentinel+Mixture+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.07843"}, "Incremental Sequence Learning": {"container_type": "Publication", "bib": {"title": "Incremental sequence learning", "author": ["ED de Jong"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.03068", "abstract": "the learning problem over time, increasingly complex learning  incremental learning in the  context of sequence learning,  of incremental or curriculum learning to enhance learning is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.03068", "author_id": ["l9w80gcAAAAJ"], "url_scholarbib": "/scholar?q=info:UXBn8Cipkl4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIncremental%2BSequence%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UXBn8Cipkl4J&ei=JBFkYqCOH5qSy9YP8pKNsAE&json=", "num_citations": 16, "citedby_url": "/scholar?cites=6814695179462078545&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UXBn8Cipkl4J:scholar.google.com/&scioq=Incremental+Sequence+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.03068"}, "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning": {"container_type": "Publication", "bib": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "author": ["J Daily", "A Vishnu", "C Siegel"], "pub_year": "2016", "venue": "NA", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkFd2P5gl", "author_id": ["aETUJXQAAAAJ", "", "2DBCINAAAAAJ"], "url_scholarbib": "/scholar?q=info:2gEj1iFL8EsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLeveraging%2BAsynchronicity%2Bin%2BGradient%2BDescent%2Bfor%2BScalable%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2gEj1iFL8EsJ&ei=JxFkYtrUIPmQ6rQP5OqKqAo&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:2gEj1iFL8EsJ:scholar.google.com/&scioq=Leveraging+Asynchronicity+in+Gradient+Descent+for+Scalable+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkFd2P5gl"}, "Neural Machine Translation with Latent Semantic of Image and Text": {"container_type": "Publication", "bib": {"title": "Neural machine translation with latent semantic of image and text", "author": ["J Toyama", "M Misono", "M Suzuki", "K Nakayama"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.08459", "author_id": ["", "rE5nD7cAAAAJ", "r2nt5kUAAAAJ", "MT4jukMAAAAJ"], "url_scholarbib": "/scholar?q=info:XwNnBxnvm2kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BMachine%2BTranslation%2Bwith%2BLatent%2BSemantic%2Bof%2BImage%2Band%2BText%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XwNnBxnvm2kJ&ei=ag9kYtODG-iSy9YPp-OyiAE&json=", "num_citations": 19, "citedby_url": "/scholar?cites=7609938886149997407&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XwNnBxnvm2kJ:scholar.google.com/&scioq=Neural+Machine+Translation+with+Latent+Semantic+of+Image+and+Text&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.08459"}, "Investigating Recurrence and Eligibility Traces in Deep Q-Networks": {"container_type": "Publication", "bib": {"title": "Investigating recurrence and eligibility traces in deep Q-networks", "author": ["J Harb", "D Precup"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1704.05495", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.05495", "author_id": ["5Qp_0TUAAAAJ", "j54VcVEAAAAJ"], "url_scholarbib": "/scholar?q=info:fiJ4XusRksQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInvestigating%2BRecurrence%2Band%2BEligibility%2BTraces%2Bin%2BDeep%2BQ-Networks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fiJ4XusRksQJ&ei=bg9kYp2oFM6E6rQP5-KmKA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=14164403480633549438&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fiJ4XusRksQJ:scholar.google.com/&scioq=Investigating+Recurrence+and+Eligibility+Traces+in+Deep+Q-Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.05495"}, "Towards a Neural Statistician": {"container_type": "Publication", "bib": {"title": "Towards a neural statistician", "author": ["H Edwards", "A Storkey"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1606.02185", "abstract": "neural networks to learn statistics and we refer to our approach as a neural statistician.  In  our first experiment we wanted to know if the neural statistician will learn to cluster synthetic 1-"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.02185", "author_id": ["0o470HsAAAAJ", "3Rlc8EAAAAAJ"], "url_scholarbib": "/scholar?q=info:0HJRHgWL6rwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2Ba%2BNeural%2BStatistician%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0HJRHgWL6rwJ&ei=cQ9kYvCYBvmQ6rQP5OqKqAo&json=", "num_citations": 350, "citedby_url": "/scholar?cites=13612845677780497104&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0HJRHgWL6rwJ:scholar.google.com/&scioq=Towards+a+Neural+Statistician&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.02185.pdf?ref=https://githubhelp.com"}, "Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning": {"container_type": "Publication", "bib": {"title": "Central moment discrepancy (cmd) for domain-invariant representation learning", "author": ["W Zellinger", "T Grubinger", "E Lughofer"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the discrepancy between domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, eg Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.08811", "author_id": ["dqPaRCEAAAAJ", "emxAoI0AAAAJ", "G6sMKIkAAAAJ"], "url_scholarbib": "/scholar?q=info:Lu8bDC_9x5oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCentral%2BMoment%2BDiscrepancy%2B(CMD)%2Bfor%2BDomain-Invariant%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Lu8bDC_9x5oJ&ei=dA9kYpiCF-HDywSSipaYAg&json=", "num_citations": 356, "citedby_url": "/scholar?cites=11153161380714770222&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Lu8bDC_9x5oJ:scholar.google.com/&scioq=Central+Moment+Discrepancy+(CMD)+for+Domain-Invariant+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.08811"}, "Semantic Noise Modeling for Better Representation Learning": {"container_type": "Publication", "bib": {"title": "Semantic Noise Modeling for Better Representation Learning", "author": ["HE Kim", "S Hwang", "K Cho"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01268", "abstract": "Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation which is obtained from an appropriate training scenario with a task-specific objective on a designed network model. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01268", "author_id": ["BKMKQt8AAAAJ", "QtI8XmgAAAAJ", "0RAmmIAAAAAJ"], "url_scholarbib": "/scholar?q=info:xCKU4F3RnRwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemantic%2BNoise%2BModeling%2Bfor%2BBetter%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xCKU4F3RnRwJ&ei=dw9kYpvyD4ySyASZk6HgCA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:xCKU4F3RnRwJ:scholar.google.com/&scioq=Semantic+Noise+Modeling+for+Better+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01268"}, "Neuro-Symbolic Program Synthesis": {"container_type": "Publication", "bib": {"title": "Neuro-symbolic program synthesis", "author": ["E Parisotto", "A Mohamed", "R Singh", "L Li", "D Zhou"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Motivated by the need for model interpretability and scalability to multiple tasks, we  address the problem of Program Synthesis. Program Synthesis, the problem of automatically"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01855", "author_id": ["-GduGkcAAAAJ", "tJ_PrzgAAAAJ", "5kVcNS4AAAAJ", "Rqy5KDEAAAAJ", "UwLsYw8AAAAJ"], "url_scholarbib": "/scholar?q=info:LMUr8r8_dRsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeuro-Symbolic%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LMUr8r8_dRsJ&ei=eg9kYqucJZLeyQTE46-QAg&json=", "num_citations": 265, "citedby_url": "/scholar?cites=1978557704933459244&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LMUr8r8_dRsJ:scholar.google.com/&scioq=Neuro-Symbolic+Program+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01855.pdf?ref=https://githubhelp.com"}, "Coarse Pruning of Convolutional Neural Networks with Random Masks": {"container_type": "Publication", "bib": {"title": "Coarse pruning of convolutional neural networks with random masks", "author": ["S Anwar", "W Sung"], "pub_year": "2016", "venue": "NA", "abstract": "The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns further increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning changes the dense connectivity pattern into a sparse"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkvS3Mqxe", "author_id": ["", "1IfNFz4AAAAJ"], "url_scholarbib": "/scholar?q=info:JQ01RmpiUG8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCoarse%2BPruning%2Bof%2BConvolutional%2BNeural%2BNetworks%2Bwith%2BRandom%2BMasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JQ01RmpiUG8J&ei=gQ9kYry_Ao6pywTd4KPADw&json=", "num_citations": 14, "citedby_url": "/scholar?cites=8021019144930790693&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JQ01RmpiUG8J:scholar.google.com/&scioq=Coarse+Pruning+of+Convolutional+Neural+Networks+with+Random+Masks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkvS3Mqxe"}, "Submodular Sum-product Networks for Scene Understanding": {"container_type": "Publication", "bib": {"title": "Submodular Sum-product Networks for Scene Understanding", "author": ["AL Friesen", "P Domingos"], "pub_year": "2016", "venue": "NA", "abstract": "Sum-product networks (SPNs) are an expressive class of deep probabilistic models in which inference takes time linear in their size, enabling them to be learned effectively. However, for certain challenging problems, such as scene understanding, the corresponding SPN has exponential size and is thus intractable. In this work, we introduce submodular sum-product networks (SSPNs), an extension of SPNs in which sum-node weights are defined by a submodular energy function. SSPNs combine the expressivity and depth of SPNs with the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryEGFD9gl", "author_id": ["sfvCNiEAAAAJ", "KOrhfVMAAAAJ"], "url_scholarbib": "/scholar?q=info:5KmMbPPnRCQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSubmodular%2BSum-product%2BNetworks%2Bfor%2BScene%2BUnderstanding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5KmMbPPnRCQJ&ei=gw9kYsaBJYySyASZk6HgCA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:5KmMbPPnRCQJ:scholar.google.com/&scioq=Submodular+Sum-product+Networks+for+Scene+Understanding&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryEGFD9gl"}, "Multi-Agent Cooperation and the Emergence of (Natural) Language": {"container_type": "Publication", "bib": {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["A Lazaridou", "A Peysakhovich", "M Baroni"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.07182", "abstract": "The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.07182", "author_id": ["BMgUIC0AAAAJ", "zwLePrsAAAAJ", "l-xu2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:bklSsZKKzBoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-Agent%2BCooperation%2Band%2Bthe%2BEmergence%2Bof%2B(Natural)%2BLanguage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bklSsZKKzBoJ&ei=hw9kYuHWN7KO6rQPy-CRsA8&json=", "num_citations": 312, "citedby_url": "/scholar?cites=1931070702879918446&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bklSsZKKzBoJ:scholar.google.com/&scioq=Multi-Agent+Cooperation+and+the+Emergence+of+(Natural)+Language&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.07182.pdf?ref=https://githubhelp.com"}, "Hadamard Product for Low-rank Bilinear Pooling": {"container_type": "Publication", "bib": {"title": "Hadamard product for low-rank bilinear pooling", "author": ["JH Kim", "KW On", "W Lim", "J Kim", "JW Ha"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.04325", "author_id": ["3f2wPekAAAAJ", "feMRfHUAAAAJ", "ori6200AAAAJ", "yzV_-ZIAAAAJ", "eGj3ay4AAAAJ"], "url_scholarbib": "/scholar?q=info:mxEHacBGF5wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHadamard%2BProduct%2Bfor%2BLow-rank%2BBilinear%2BPooling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mxEHacBGF5wJ&ei=iw9kYue6BJqSy9YP8pKNsAE&json=", "num_citations": 558, "citedby_url": "/scholar?cites=11247536386590839195&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mxEHacBGF5wJ:scholar.google.com/&scioq=Hadamard+Product+for+Low-rank+Bilinear+Pooling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.04325"}, "Efficient Vector Representation for Documents through Corruption": {"container_type": "Publication", "bib": {"title": "Efficient vector representation for documents through corruption", "author": ["M Chen"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.02377", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.02377", "author_id": ["kR7DersAAAAJ"], "url_scholarbib": "/scholar?q=info:ggPNq9frXAoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BVector%2BRepresentation%2Bfor%2BDocuments%2Bthrough%2BCorruption%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ggPNq9frXAoJ&ei=kA9kYuXvBfmQ6rQP5OqKqAo&json=", "num_citations": 120, "citedby_url": "/scholar?cites=746730949769495426&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ggPNq9frXAoJ:scholar.google.com/&scioq=Efficient+Vector+Representation+for+Documents+through+Corruption&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.02377"}, "Mode Regularized Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Mode regularized generative adversarial networks", "author": ["T Che", "Y Li", "AP Jacob", "Y Bengio", "W Li"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.02136", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.02136", "author_id": ["", "", "XT3E7RoAAAAJ", "kukA0LcAAAAJ", "Rx5swD4AAAAJ"], "url_scholarbib": "/scholar?q=info:8FNlDYriSXIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMode%2BRegularized%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8FNlDYriSXIJ&ei=kg9kYq7FM_mQ6rQP5OqKqAo&json=", "num_citations": 491, "citedby_url": "/scholar?cites=8235362476181771248&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8FNlDYriSXIJ:scholar.google.com/&scioq=Mode+Regularized+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.02136?ref=https://githubhelp.com"}, "Learning a Static Analyzer: A Case Study on a Toy Language": {"container_type": "Publication", "bib": {"title": "Learning a static analyzer: A case study on a toy language", "author": ["M Zaheer", "JB Tristan", "ML Wick", "GL Steele Jr"], "pub_year": "2016", "venue": "NA", "abstract": "Static analyzers are meta-programs that analyze programs to detect potential errors or collect information. For example, they are used as security tools to detect potential buffer overflows. Also, they are used by compilers to verify that a program is well-formed and collect information to generate better code. In this paper, we address the following question: can a static analyzer be learned from data? More specifically, can we use deep learning to learn a static analyzer without the need for complicated feature engineering? We show that"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ry54RWtxx", "author_id": ["A33FhJMAAAAJ", "iMs3HJoAAAAJ", "D0DpJUwAAAAJ", "Ur3sFscAAAAJ"], "url_scholarbib": "/scholar?q=info:d4Hir3_0g4IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ba%2BStatic%2BAnalyzer:%2BA%2BCase%2BStudy%2Bon%2Ba%2BToy%2BLanguage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d4Hir3_0g4IJ&ei=lg9kYuOSJoySyASZk6HgCA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=9404629276128608631&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:d4Hir3_0g4IJ:scholar.google.com/&scioq=Learning+a+Static+Analyzer:+A+Case+Study+on+a+Toy+Language&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ry54RWtxx"}, "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes": {"container_type": "Publication", "bib": {"title": "Dynamic neural turing machine with continuous and discrete addressing schemes", "author": ["C Gulcehre", "S Chandar", "K Cho", "Y Bengio"], "pub_year": "2018", "venue": "Neural computation", "abstract": "We extend the neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing trainable address vectors. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies, including both linear and nonlinear ones. We implement the D-NTM with both continuous and discrete read and write mechanisms. We investigate the mechanisms and effects of learning to read and write into a"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8321025/", "author_id": ["7hwJ2ckAAAAJ", "yxWtZLAAAAAJ", "0RAmmIAAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:644qd96UtQQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BNeural%2BTuring%2BMachine%2Bwith%2BContinuous%2Band%2BDiscrete%2BAddressing%2BSchemes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=644qd96UtQQJ&ei=nA9kYtz2AeHDywSSipaYAg&json=", "num_citations": 50, "citedby_url": "/scholar?cites=339341030139268843&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:644qd96UtQQJ:scholar.google.com/&scioq=Dynamic+Neural+Turing+Machine+with+Continuous+and+Discrete+Addressing+Schemes&hl=en&as_sdt=0,33"}, "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data": {"container_type": "Publication", "bib": {"title": "Deep variational bayes filters: Unsupervised learning of state space models from raw data", "author": ["M Karl", "M Soelch", "J Bayer", "P Van der Smagt"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.06432", "author_id": ["noekAeoAAAAJ", "MtTyY5IAAAAJ", "kczEEFAAAAAJ", "5ybzvbsAAAAJ"], "url_scholarbib": "/scholar?q=info:H7WfDWBERVwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BVariational%2BBayes%2BFilters:%2BUnsupervised%2BLearning%2Bof%2BState%2BSpace%2BModels%2Bfrom%2BRaw%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H7WfDWBERVwJ&ei=nw9kYujPAs6E6rQP5-KmKA&json=", "num_citations": 274, "citedby_url": "/scholar?cites=6648795604218524959&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:H7WfDWBERVwJ:scholar.google.com/&scioq=Deep+Variational+Bayes+Filters:+Unsupervised+Learning+of+State+Space+Models+from+Raw+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.06432"}, "Recurrent Neural Networks for Multivariate Time Series with Missing Values": {"container_type": "Publication", "bib": {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Z Che", "S Purushotham", "K Cho", "D Sontag", "Y Liu"], "pub_year": "2018", "venue": "Scientific reports", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, aka, informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one"}, "filled": false, "gsrank": 1, "pub_url": "https://www.nature.com/articles/s41598-018-24271-9", "author_id": ["f6uvd6kAAAAJ", "Q0iwucYAAAAJ", "0RAmmIAAAAAJ", "LfcroyAAAAAJ", "UUKLPMYAAAAJ"], "url_scholarbib": "/scholar?q=info:l07c-PZb3OgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BNeural%2BNetworks%2Bfor%2BMultivariate%2BTime%2BSeries%2Bwith%2BMissing%2BValues%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=l07c-PZb3OgJ&ei=og9kYpfDIYuKmgGY1YjABQ&json=", "num_citations": 1214, "citedby_url": "/scholar?cites=16779387427970895511&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:l07c-PZb3OgJ:scholar.google.com/&scioq=Recurrent+Neural+Networks+for+Multivariate+Time+Series+with+Missing+Values&hl=en&as_sdt=0,33", "eprint_url": "https://www.nature.com/articles/s41598-018-24271-9"}, "Efficient Calculation of Polynomial Features on Sparse Matrices": {"container_type": "Publication", "bib": {"title": "Efficient Calculation of Polynomial Features on Sparse Matrices", "author": ["A Nystrom", "J Hughes"], "pub_year": "2016", "venue": "NA", "abstract": "We provide an algorithm for polynomial feature expansion that both operates on and produces a compressed sparse row matrix without any densification. For a vector of dimension D, density d, and degree k the algorithm has time complexity O (d^ k* D^ k) where k is the polynomial-feature order; this is an improvement by a factor d^ k over the standard method."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1j4RqYxg", "author_id": ["s8PV87cAAAAJ", "EEG6tPMAAAAJ"], "url_scholarbib": "/scholar?q=info:_XeTPsz-mXQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BCalculation%2Bof%2BPolynomial%2BFeatures%2Bon%2BSparse%2BMatrices%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_XeTPsz-mXQJ&ei=pg9kYtjUB5qSy9YP8pKNsAE&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:_XeTPsz-mXQJ:scholar.google.com/&scioq=Efficient+Calculation+of+Polynomial+Features+on+Sparse+Matrices&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1j4RqYxg"}, "Deep Character-Level Neural Machine Translation By Learning Morphology": {"container_type": "Publication", "bib": {"title": "Deep character-level neural machine translation by learning morphology", "author": ["S Zhao", "Z Zhang"], "pub_year": "2016", "venue": "NA", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJq_YBqxx", "author_id": ["", "M5YT8IoAAAAJ"], "url_scholarbib": "/scholar?q=info:d9xXgGOGndUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BCharacter-Level%2BNeural%2BMachine%2BTranslation%2BBy%2BLearning%2BMorphology%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d9xXgGOGndUJ&ei=qA9kYpCWKJqSy9YP8pKNsAE&json=", "num_citations": 2, "citedby_url": "/scholar?cites=15392606863335349367&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:d9xXgGOGndUJ:scholar.google.com/&scioq=Deep+Character-Level+Neural+Machine+Translation+By+Learning+Morphology&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJq_YBqxx"}, "Hierarchical compositional feature learning": {"container_type": "Publication", "bib": {"title": "Hierarchical compositional feature learning", "author": ["M L\u00e1zaro-Gredilla", "Y Liu", "DS Phoenix"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce the hierarchical compositional network (HCN), a directed generative model able to discover and disentangle, without supervision, the building blocks of a set of binary images. The building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below, arranged in a particular manner. At a high level, HCN is similar to a sigmoid belief network with pooling. Inference and learning in HCN are very challenging and existing variational approximations do not work satisfactorily"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02252", "author_id": ["SFjDQk8AAAAJ", "-qFe-7wAAAAJ", ""], "url_scholarbib": "/scholar?q=info:XGyePAoPCJUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2Bcompositional%2Bfeature%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XGyePAoPCJUJ&ei=tA9kYsquC5LeyQTE46-QAg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=10738849848106052700&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XGyePAoPCJUJ:scholar.google.com/&scioq=Hierarchical+compositional+feature+learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02252"}, "Learning to Query, Reason, and Answer Questions On Ambiguous Texts": {"container_type": "Publication", "bib": {"title": "Learning to query, reason, and answer questions on ambiguous texts", "author": ["X Guo", "T Klinger", "C Rosenbaum", "JP Bigus", "M Campbell"], "pub_year": "2016", "venue": "NA", "abstract": "A key goal of research in conversational systems is to train an interactive agent to help a user with a task. Human conversation, however, is notoriously incomplete, ambiguous, and full of extraneous detail. To operate effectively, the agent must not only understand what was explicitly conveyed but also be able to reason in the presence of missing or unclear information. When unable to resolve ambiguities on its own, the agent must be able to ask the user for the necessary clarifications and incorporate the response in its reasoning"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJ0-tY5xe", "author_id": ["DIBOO50AAAAJ", "dd8awr4AAAAJ", "pP7H0fkAAAAJ", "", "8rykXfcAAAAJ"], "url_scholarbib": "/scholar?q=info:NxstnKZy1KgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BQuery,%2BReason,%2Band%2BAnswer%2BQuestions%2BOn%2BAmbiguous%2BTexts%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NxstnKZy1KgJ&ei=tw9kYp7oC-HDywSSipaYAg&json=", "num_citations": 19, "citedby_url": "/scholar?cites=12165474553344891703&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NxstnKZy1KgJ:scholar.google.com/&scioq=Learning+to+Query,+Reason,+and+Answer+Questions+On+Ambiguous+Texts&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJ0-tY5xe"}, "Learning to Optimize": {"container_type": "Publication", "bib": {"title": "Learning to optimize", "author": ["K Li", "J Malik"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1606.01885", "abstract": "learning from the algorithm that is learned, we will henceforth refer to the former as the \u201clearning   We use an off-the-shelf reinforcement learning algorithm known as guided policy search"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.01885", "author_id": ["nx3Alr4AAAAJ", "oY9R5YQAAAAJ"], "url_scholarbib": "/scholar?q=info:rwJWbY5oBOAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BOptimize%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rwJWbY5oBOAJ&ei=vA9kYvOxJPmQ6rQP5OqKqAo&json=", "num_citations": 258, "citedby_url": "/scholar?cites=16142141925331698351&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rwJWbY5oBOAJ:scholar.google.com/&scioq=Learning+to+Optimize&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.01885.pdf?ref=https://githubhelp.com"}, "Rectified Factor Networks for Biclustering": {"container_type": "Publication", "bib": {"title": "Rectified Factor Networks for Biclustering", "author": ["DA Clevert", "T Unterthiner", "S Hochreiter"], "pub_year": "2016", "venue": "NA", "abstract": "Biclustering is evolving into one of the major tools for analyzing large datasets given as matrix of samples times features. Biclustering has several noteworthy applications and has been successfully applied in life sciences and e-commerce for drug design and recommender systems, respectively. FABIA is one of the most successful biclustering methods and is used by companies like Bayer, Janssen, or Zalando. FABIA is a generative model that represents each bicluster by two sparse membership vectors: one for the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryb-q1Olg", "author_id": ["id2clmMAAAAJ", "QCARd5gAAAAJ", "tvUH3WMAAAAJ"], "url_scholarbib": "/scholar?q=info:aXlO1Y5cWKgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRectified%2BFactor%2BNetworks%2Bfor%2BBiclustering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aXlO1Y5cWKgJ&ei=wA9kYsWdEouKmgGY1YjABQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:aXlO1Y5cWKgJ:scholar.google.com/&scioq=Rectified+Factor+Networks+for+Biclustering&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryb-q1Olg"}, "Machine Comprehension Using Match-LSTM and Answer Pointer": {"container_type": "Publication", "bib": {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["S Wang", "J Jiang"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1608.07905", "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.07905", "author_id": ["mN-IO6wAAAAJ", "hVTK2YwAAAAJ"], "url_scholarbib": "/scholar?q=info:FT5awWhid3YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMachine%2BComprehension%2BUsing%2BMatch-LSTM%2Band%2BAnswer%2BPointer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FT5awWhid3YJ&ei=xQ9kYqeLMo6pywTd4KPADw&json=", "num_citations": 538, "citedby_url": "/scholar?cites=8536399820764102165&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FT5awWhid3YJ:scholar.google.com/&scioq=Machine+Comprehension+Using+Match-LSTM+and+Answer+Pointer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.07905"}, "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty": {"container_type": "Publication", "bib": {"title": "Training compressed fully-connected networks with a density-diversity penalty", "author": ["S Wang", "H Cai", "J Bilmes", "W Noble"], "pub_year": "2016", "venue": "NA", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How-ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (ie"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hku9NK5lx", "author_id": ["hLxJ02wAAAAJ", "", "L9QufAsAAAAJ", "plt2_DsAAAAJ"], "url_scholarbib": "/scholar?q=info:csa6FND9xfsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BCompressed%2BFully-Connected%2BNetworks%2Bwith%2Ba%2BDensity-Diversity%2BPenalty%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=csa6FND9xfsJ&ei=yA9kYpL8DYuKmgGY1YjABQ&json=", "num_citations": 7, "citedby_url": "/scholar?cites=18142185744074720882&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:csa6FND9xfsJ:scholar.google.com/&scioq=Training+Compressed+Fully-Connected+Networks+with+a+Density-Diversity+Penalty&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hku9NK5lx"}, "Variable Computation in Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Variable computation in recurrent neural networks", "author": ["Y Jernite", "E Grave", "A Joulin", "T Mikolov"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.06188", "abstract": "Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.06188", "author_id": ["AK_7EBgAAAAJ", "7UV4ET4AAAAJ", "kRJkDakAAAAJ", "oBu8kMMAAAAJ"], "url_scholarbib": "/scholar?q=info:W4WyEv0K--oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariable%2BComputation%2Bin%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W4WyEv0K--oJ&ei=yw9kYs6EA8LZmQHnraWYCA&json=", "num_citations": 53, "citedby_url": "/scholar?cites=16932139306086204763&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W4WyEv0K--oJ:scholar.google.com/&scioq=Variable+Computation+in+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.06188"}, "Lifelong Perceptual Programming By Example": {"container_type": "Publication", "bib": {"title": "Lifelong perceptual programming by example", "author": ["AL Gaunt", "M Brockschmidt", "N Kushman", "D Tarlow"], "pub_year": "2017", "venue": "NA", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJNulIVtx", "author_id": ["IzwvqPIAAAAJ", "pF27eLMAAAAJ", "I_YIc0YAAAAJ", "oavgGaMAAAAJ"], "url_scholarbib": "/scholar?q=info:eu3jmxk30bYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLifelong%2BPerceptual%2BProgramming%2BBy%2BExample%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eu3jmxk30bYJ&ei=0g9kYqqXGYOEmgHx-5DADA&json=", "num_citations": 5, "citedby_url": "/scholar?cites=13173370968164527482&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eu3jmxk30bYJ:scholar.google.com/&scioq=Lifelong+Perceptual+Programming+By+Example&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJNulIVtx"}, "Inference and Introspection in Deep Generative Models of Sparse Data": {"container_type": "Publication", "bib": {"title": "Inference and introspection in deep generative models of sparse data", "author": ["RG Krishnan", "M Hoffman"], "pub_year": "2016", "venue": "NA", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1Chut9xl", "author_id": ["ilJgXHkAAAAJ", "IeHKeGYAAAAJ"], "url_scholarbib": "/scholar?q=info:8Jd5hMzf680J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInference%2Band%2BIntrospection%2Bin%2BDeep%2BGenerative%2BModels%2Bof%2BSparse%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8Jd5hMzf680J&ei=1A9kYo6yO5GJmwGY-qmYDQ&json=", "num_citations": 3, "citedby_url": "/scholar?cites=14838199466791114736&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8Jd5hMzf680J:scholar.google.com/&scioq=Inference+and+Introspection+in+Deep+Generative+Models+of+Sparse+Data&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1Chut9xl"}, "Joint Multimodal Learning with Deep Generative Models": {"container_type": "Publication", "bib": {"title": "Joint multimodal learning with deep generative models", "author": ["M Suzuki", "K Nakayama", "Y Matsuo"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01891", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, eg, generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, ie, we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01891", "author_id": ["r2nt5kUAAAAJ", "MT4jukMAAAAJ", ""], "url_scholarbib": "/scholar?q=info:RoMGw3Rz26sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DJoint%2BMultimodal%2BLearning%2Bwith%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RoMGw3Rz26sJ&ei=2g9kYrzoEoOEmgHx-5DADA&json=", "num_citations": 143, "citedby_url": "/scholar?cites=12383618545710695238&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RoMGw3Rz26sJ:scholar.google.com/&scioq=Joint+Multimodal+Learning+with+Deep+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01891"}, "Learning to Perform Physics Experiments via Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning to perform physics experiments via deep reinforcement learning", "author": ["M Denil", "P Agrawal", "TD Kulkarni", "T Erez"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01843", "author_id": ["XrKLUO0AAAAJ", "UpZmJI0AAAAJ", "rrPyvsgAAAAJ", "gVFnjOcAAAAJ"], "url_scholarbib": "/scholar?q=info:ygpjG2m_Y7YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BPerform%2BPhysics%2BExperiments%2Bvia%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ygpjG2m_Y7YJ&ei=4A9kYvejI4OEmgHx-5DADA&json=", "num_citations": 74, "citedby_url": "/scholar?cites=13142558595749186250&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ygpjG2m_Y7YJ:scholar.google.com/&scioq=Learning+to+Perform+Physics+Experiments+via+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01843"}, "Epitomic Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "Epitomic Variational Autoencoders", "author": ["S Yeung", "A Kannan", "Y Dauphin", "L Fei-Fei"], "pub_year": "2016", "venue": "NA", "abstract": "sion of variational autoencoders called epitomic variational autoencoder (Epitomic VAE, or   Epitomic VAE exploits sparsity using an additional categorical latent variable in the encoder-"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bk3F5Y9lx", "author_id": ["Tw2m5kUAAAAJ", "eoBHpj4AAAAJ", "XSforroAAAAJ", "rDfyQnIAAAAJ"], "url_scholarbib": "/scholar?q=info:F60r6FL7BQQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEpitomic%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F60r6FL7BQQJ&ei=4w9kYrKXLJLeyQTE46-QAg&json=", "num_citations": 7, "citedby_url": "/scholar?cites=289914084536331543&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F60r6FL7BQQJ:scholar.google.com/&scioq=Epitomic+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bk3F5Y9lx"}, "Towards Understanding the Invertibility of Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Towards understanding the invertibility of convolutional neural networks", "author": ["AC Gilbert", "Y Zhang", "K Lee", "Y Zhang", "H Lee"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.08664", "author_id": ["l7ShEBkAAAAJ", "lc6CVqEAAAAJ", "6wwWRdEAAAAJ", "9UfZJskAAAAJ", "fmSHtE8AAAAJ"], "url_scholarbib": "/scholar?q=info:1Jq-Mr4c8GUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BUnderstanding%2Bthe%2BInvertibility%2Bof%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1Jq-Mr4c8GUJ&ei=5g9kYoqBEZqSy9YP8pKNsAE&json=", "num_citations": 60, "citedby_url": "/scholar?cites=7345402595461995220&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1Jq-Mr4c8GUJ:scholar.google.com/&scioq=Towards+Understanding+the+Invertibility+of+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.08664"}, "Trained Ternary Quantization": {"container_type": "Publication", "bib": {"title": "Trained ternary quantization", "author": ["C Zhu", "S Han", "H Mao", "WJ Dally"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.01064", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.01064", "author_id": ["nPrC7jkAAAAJ", "E0iCaa4AAAAJ", "r5WezOYAAAAJ", "YZHj-Y4AAAAJ"], "url_scholarbib": "/scholar?q=info:VSPYVG2Sz7MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTrained%2BTernary%2BQuantization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VSPYVG2Sz7MJ&ei=6Q9kYuGfH46pywTd4KPADw&json=", "num_citations": 942, "citedby_url": "/scholar?cites=12956735651240747861&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VSPYVG2Sz7MJ:scholar.google.com/&scioq=Trained+Ternary+Quantization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.01064?ref=https://githubhelp.com"}, "Classless Association using Neural Networks": {"container_type": "Publication", "bib": {"title": "Classless association using neural networks", "author": ["F Raue", "S Palacio", "A Dengel", "M Liwicki"], "pub_year": "2017", "venue": "International Conference on \u2026", "abstract": "The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class. This task is inspired by the Symbol Grounding Problem and the association learning between modalities in infants. We propose a novel model called Classless Association that has two parallel Multilayer Perceptrons (MLPs) with a EM-training rule. Moreover, the training relies on matching the output vectors of the MLPs against a statistical distribution as alternative loss function because of the unlabeled data. In"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-319-68612-7_19", "author_id": ["Jx9-sU0AAAAJ", "Vxp_FgUAAAAJ", "p3YP0DMAAAAJ", "n1Y4zq4AAAAJ"], "url_scholarbib": "/scholar?q=info:HHrFwVNN8moJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DClassless%2BAssociation%2Busing%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HHrFwVNN8moJ&ei=7g9kYvHfDs6E6rQP5-KmKA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=7706306934512908828&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HHrFwVNN8moJ:scholar.google.com/&scioq=Classless+Association+using+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkB_5hEKe"}, "PixelVAE: A Latent Variable Model for Natural Images": {"container_type": "Publication", "bib": {"title": "Pixelvae: A latent variable model for natural images", "author": ["I Gulrajani", "K Kumar", "F Ahmed", "AA Taiga"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.05013", "author_id": ["E2SLBwIAAAAJ", "Sh5PsUUAAAAJ", "eo9JtywAAAAJ", "rRbCqtoAAAAJ"], "url_scholarbib": "/scholar?q=info:JQkfFA6r9UQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPixelVAE:%2BA%2BLatent%2BVariable%2BModel%2Bfor%2BNatural%2BImages%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JQkfFA6r9UQJ&ei=8Q9kYvfTEpyO6rQP_qe3mAs&json=", "num_citations": 293, "citedby_url": "/scholar?cites=4969065840828680485&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JQkfFA6r9UQJ:scholar.google.com/&scioq=PixelVAE:+A+Latent+Variable+Model+for+Natural+Images&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.05013"}, "Unsupervised Perceptual Rewards for Imitation Learning": {"container_type": "Publication", "bib": {"title": "Unsupervised perceptual rewards for imitation learning", "author": ["P Sermanet", "K Xu", "S Levine"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.06699", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.06699", "author_id": ["0nPi5YYAAAAJ", "GyoKzFwAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:D8Xps3bYkCoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BPerceptual%2BRewards%2Bfor%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D8Xps3bYkCoJ&ei=_A9kYqybIM6E6rQP5-KmKA&json=", "num_citations": 122, "citedby_url": "/scholar?cites=3067189350575490319&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D8Xps3bYkCoJ:scholar.google.com/&scioq=Unsupervised+Perceptual+Rewards+for+Imitation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.06699"}, "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications": {"container_type": "Publication", "bib": {"title": "Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications", "author": ["T Salimans", "A Karpathy", "X Chen", "DP Kingma"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github. com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.05517", "author_id": ["w68-7AYAAAAJ", "", "5tVuggUAAAAJ", "yyIoQu4AAAAJ"], "url_scholarbib": "/scholar?q=info:r5hlMVXePzQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPixelCNN%252B%252B:%2BImproving%2Bthe%2BPixelCNN%2Bwith%2BDiscretized%2BLogistic%2BMixture%2BLikelihood%2Band%2BOther%2BModifications%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r5hlMVXePzQJ&ei=_w9kYpXoIMLZmQHnraWYCA&json=", "num_citations": 667, "citedby_url": "/scholar?cites=3764972270987352239&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r5hlMVXePzQJ:scholar.google.com/&scioq=PixelCNN%2B%2B:+Improving+the+PixelCNN+with+Discretized+Logistic+Mixture+Likelihood+and+Other+Modifications&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.05517.pdf?source=post_page---------------------------"}, "Variational Lossy Autoencoder": {"container_type": "Publication", "bib": {"title": "Variational lossy autoencoder", "author": ["X Chen", "DP Kingma", "T Salimans", "Y Duan"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Representation learning seeks to expose certain aspects of observed data in a learned  representation that's amenable to downstream tasks like classification. For instance, a good"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02731", "author_id": ["5tVuggUAAAAJ", "yyIoQu4AAAAJ", "w68-7AYAAAAJ", "EMDboA4AAAAJ"], "url_scholarbib": "/scholar?q=info:9v8gP9uFN6QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BLossy%2BAutoencoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9v8gP9uFN6QJ&ei=AxBkYtDfA5qSy9YP8pKNsAE&json=", "num_citations": 568, "citedby_url": "/scholar?cites=11833073722642726902&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9v8gP9uFN6QJ:scholar.google.com/&scioq=Variational+Lossy+Autoencoder&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02731.pdf,"}, "Towards Principled Methods for Training Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Towards principled methods for training generative adversarial networks", "author": ["M Arjovsky", "L Bottou"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.04862", "abstract": "the training dynamics of generative adversarial networks.  training generative adversarial  networks. The third section examines a practical and theoretically grounded direction towards"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.04862", "author_id": ["A6qfFPkAAAAJ", "kbN88gsAAAAJ"], "url_scholarbib": "/scholar?q=info:EPJDFKVbQcQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BPrincipled%2BMethods%2Bfor%2BTraining%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EPJDFKVbQcQJ&ei=CBBkYqbVEsLZmQHnraWYCA&json=", "num_citations": 1653, "citedby_url": "/scholar?cites=14141685069487796752&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EPJDFKVbQcQJ:scholar.google.com/&scioq=Towards+Principled+Methods+for+Training+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.04862.pdf?ref=https://githubhelp.com"}, "Deep Variational Canonical Correlation Analysis": {"container_type": "Publication", "bib": {"title": "Deep variational canonical correlation analysis", "author": ["W Wang", "X Yan", "H Lee", "K Livescu"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.03454", "abstract": "canonical correlation objective, the authors found that in practice, the canonical correlation   of this paper is a new deep multiview learning model, deep variational CCA (VCCA), which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.03454", "author_id": ["O9djN1AAAAAJ", "xfskSZEAAAAJ", "fmSHtE8AAAAJ", "kCYbVq0AAAAJ"], "url_scholarbib": "/scholar?q=info:0901-2NZ_SYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BVariational%2BCanonical%2BCorrelation%2BAnalysis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0901-2NZ_SYJ&ei=DRBkYrDqDJGJmwGY-qmYDQ&json=", "num_citations": 106, "citedby_url": "/scholar?cites=2809500028500303315&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0901-2NZ_SYJ:scholar.google.com/&scioq=Deep+Variational+Canonical+Correlation+Analysis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.03454"}, "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification": {"container_type": "Publication", "bib": {"title": "Learning to understand: Incorporating local contexts with global attention for sentiment classification", "author": ["Z Yuan", "Y Hu", "Y Huang"], "pub_year": "2016", "venue": "NA", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJlgm-B9lx", "author_id": ["", "2BmHVnAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:jiVyVclXe9QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BUnderstand:%2BIncorporating%2BLocal%2BContexts%2Bwith%2BGlobal%2BAttention%2Bfor%2BSentiment%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jiVyVclXe9QJ&ei=DxBkYrzYKO-Sy9YPs_mY8AM&json=", "num_citations": 2, "citedby_url": "/scholar?cites=15310927880409720206&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jiVyVclXe9QJ:scholar.google.com/&scioq=Learning+to+Understand:+Incorporating+Local+Contexts+with+Global+Attention+for+Sentiment+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJlgm-B9lx"}, "Improved Architectures for Computer Go": {"container_type": "Publication", "bib": {"title": "Improved Architectures for Computer Go", "author": ["T Cazenave"], "pub_year": "2016", "venue": "NA", "abstract": "AlphaGo trains policy networks with both supervised and reinforcement learning and makes different policy networks play millions of games so as to train a value network. The reinforcement learning part requires massive ammount of computation. We propose to train networks for computer Go so that given accuracy is reached with much less examples. We modify the architecture of the networks in order to train them faster and to have better accuracy in the end."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bk67W4Yxl", "author_id": ["FLamzMUAAAAJ"], "url_scholarbib": "/scholar?q=info:Rp82y_zyD3cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproved%2BArchitectures%2Bfor%2BComputer%2BGo%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Rp82y_zyD3cJ&ei=GRBkYoH2Os6E6rQP5-KmKA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:Rp82y_zyD3cJ:scholar.google.com/&scioq=Improved+Architectures+for+Computer+Go&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bk67W4Yxl"}, "DSD: Dense-Sparse-Dense Training for Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Dsd: Dense-sparse-dense training for deep neural networks", "author": ["S Han", "J Pool", "S Narang", "H Mao", "E Gong"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Modern deep neural networks have a large number of parameters, making them very hard to train. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks and achieving better optimization performance. In the first D (Dense) step, we train a dense network to learn connection weights and importance. In the S (Sparse) step, we regularize the network by pruning the unimportant connections with small weights and retraining the network given the sparsity constraint. In the final D (re-Dense) step, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1607.04381", "author_id": ["E0iCaa4AAAAJ", "DagH37xI9soC", "CWOixywAAAAJ", "r5WezOYAAAAJ", "V4qgr8kAAAAJ"], "url_scholarbib": "/scholar?q=info:9TJrVoUXdp4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDSD:%2BDense-Sparse-Dense%2BTraining%2Bfor%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9TJrVoUXdp4J&ei=HhBkYoKNG5qSy9YP8pKNsAE&json=", "num_citations": 159, "citedby_url": "/scholar?cites=11418339766692426485&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9TJrVoUXdp4J:scholar.google.com/&scioq=DSD:+Dense-Sparse-Dense+Training+for+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1607.04381"}, "Autoencoding Variational Inference For Topic Models": {"container_type": "Publication", "bib": {"title": "Autoencoding variational inference for topic models", "author": ["A Srivastava", "C Sutton"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.01488", "abstract": "Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi-cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.01488", "author_id": ["2h6SZeEAAAAJ", "hYtGXD0AAAAJ"], "url_scholarbib": "/scholar?q=info:KduMljNL31MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutoencoding%2BVariational%2BInference%2BFor%2BTopic%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KduMljNL31MJ&ei=JhBkYquRN-iSy9YPp-OyiAE&json=", "num_citations": 339, "citedby_url": "/scholar?cites=6043631909895723817&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KduMljNL31MJ:scholar.google.com/&scioq=Autoencoding+Variational+Inference+For+Topic+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.01488"}, "Pruning Filters for Efficient ConvNets": {"container_type": "Publication", "bib": {"title": "Pruning filters for efficient convnets", "author": ["H Li", "A Kadav", "I Durdanovic", "H Samet"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "the pruned networks. We present an acceleration method for CNNs, where we prune filters  from  By removing whole filters in the network together with their connecting feature maps, the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.08710", "author_id": ["BNEeEosAAAAJ", "IphEjqcAAAAJ", "", "ATOfqHEAAAAJ"], "url_scholarbib": "/scholar?q=info:XhhjKcjfRZUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPruning%2BFilters%2Bfor%2BEfficient%2BConvNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XhhjKcjfRZUJ&ei=KRBkYrbUO86E6rQP5-KmKA&json=", "num_citations": 2538, "citedby_url": "/scholar?cites=10756249335825111134&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XhhjKcjfRZUJ:scholar.google.com/&scioq=Pruning+Filters+for+Efficient+ConvNets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.08710"}, "Batch Policy Gradient  Methods for  Improving Neural Conversation Models": {"container_type": "Publication", "bib": {"title": "Batch policy gradient methods for improving neural conversation models", "author": ["K Kandasamy", "Y Bachrach", "R Tomioka"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.03334", "author_id": ["kohOJPcAAAAJ", "0W63ivcAAAAJ", "TxdeO-UAAAAJ"], "url_scholarbib": "/scholar?q=info:5BVCoy9SHgYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBatch%2BPolicy%2BGradient%2B%2BMethods%2Bfor%2B%2BImproving%2BNeural%2BConversation%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5BVCoy9SHgYJ&ei=LRBkYrXrCoyuyAT-mrWwCA&json=", "num_citations": 27, "citedby_url": "/scholar?cites=440880178084845028&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5BVCoy9SHgYJ:scholar.google.com/&scioq=Batch+Policy+Gradient++Methods+for++Improving+Neural+Conversation+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.03334"}, "Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization": {"container_type": "Publication", "bib": {"title": "Hyperband: Bandit-based configuration evaluation for hyperparameter optimization", "author": ["L Li", "KG Jamieson", "G DeSalvo", "A Rostamizadeh"], "pub_year": "2017", "venue": "ICLR (Poster)", "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation. We present Hyperband, a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound. Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, eg, iterations, data samples or number of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=f2c-2al5FIa", "author_id": ["xPSkgtIAAAAJ", "dq3yXjkAAAAJ", "xp9yor0AAAAJ", "EPl3EkgAAAAJ"], "url_scholarbib": "/scholar?q=info:JV9BqWF2_ZIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHyperband:%2BBandit-Based%2BConfiguration%2BEvaluation%2Bfor%2BHyperparameter%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JV9BqWF2_ZIJ&ei=LxBkYpv1Ms6E6rQP5-KmKA&json=", "num_citations": 95, "citedby_url": "/scholar?cites=10591752060468813605&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JV9BqWF2_ZIJ:scholar.google.com/&scioq=Hyperband:+Bandit-Based+Configuration+Evaluation+for+Hyperparameter+Optimization&hl=en&as_sdt=0,33"}, "An Actor-Critic Algorithm for Sequence Prediction": {"container_type": "Publication", "bib": {"title": "An actor-critic algorithm for sequence prediction", "author": ["D Bahdanau", "P Brakel", "K Xu", "A Goyal", "R Lowe"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic  methods from reinforcement learning (RL). Current log-likelihood training methods are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1607.07086", "author_id": ["Nq0dVMcAAAAJ", "Q6UMpRYAAAAJ", "GyoKzFwAAAAJ", "krrh6OUAAAAJ", "iRgYMuEAAAAJ"], "url_scholarbib": "/scholar?q=info:Fc757LBQjkgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BActor-Critic%2BAlgorithm%2Bfor%2BSequence%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Fc757LBQjkgJ&ei=QBBkYu-xEZyO6rQP_qe3mAs&json=", "num_citations": 508, "citedby_url": "/scholar?cites=5228204938243984917&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Fc757LBQjkgJ:scholar.google.com/&scioq=An+Actor-Critic+Algorithm+for+Sequence+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1607.07086.pdf?ref=https://githubhelp.com"}, "Unsupervised Pretraining for Sequence to Sequence Learning": {"container_type": "Publication", "bib": {"title": "Unsupervised pretraining for sequence to sequence learning", "author": ["P Ramachandran", "PJ Liu", "QV Le"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02683", "abstract": "This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02683", "author_id": ["ktKXDuMAAAAJ", "1EPxhywAAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:nlPmZHG86FsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BPretraining%2Bfor%2BSequence%2Bto%2BSequence%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nlPmZHG86FsJ&ei=RBBkYp-9JOiSy9YPp-OyiAE&json=", "num_citations": 257, "citedby_url": "/scholar?cites=6622750447258456990&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nlPmZHG86FsJ:scholar.google.com/&scioq=Unsupervised+Pretraining+for+Sequence+to+Sequence+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02683"}, "Understanding trained CNNs by indexing neuron selectivity": {"container_type": "Publication", "bib": {"title": "Understanding trained CNNs by indexing neuron selectivity", "author": ["I Rafegas", "M Vanrell", "LA Alexandre", "G Arias"], "pub_year": "2020", "venue": "Pattern Recognition Letters", "abstract": "The impressive performance of Convolutional Neural Networks (CNNs) when solving different vision problems is shadowed by their black-box nature and our consequent lack of understanding of the representations they build and how these representations are organized. To help understanding these issues, we propose to describe the activity of individual neurons by their Neuron Feature visualization and quantify their inherent selectivity with two specific properties. We explore selectivity indexes for: an image feature"}, "filled": false, "gsrank": 1, "pub_url": "https://www.sciencedirect.com/science/article/pii/S0167865519302909", "author_id": ["vshA7F4AAAAJ", "HGyvslEAAAAJ", "lG8XbuEAAAAJ", "8AMhUYQAAAAJ"], "url_scholarbib": "/scholar?q=info:aGOa4a6E_z4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2Btrained%2BCNNs%2Bby%2Bindexing%2Bneuron%2Bselectivity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aGOa4a6E_z4J&ei=SRBkYsv_Fo2ymgHg1rfQDQ&json=", "num_citations": 29, "citedby_url": "/scholar?cites=4539492836056916840&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aGOa4a6E_z4J:scholar.google.com/&scioq=Understanding+trained+CNNs+by+indexing+neuron+selectivity&hl=en&as_sdt=0,33", "eprint_url": "https://www.sciencedirect.com/science/article/am/pii/S0167865519302909"}, "Highway and Residual Networks learn Unrolled Iterative Estimation": {"container_type": "Publication", "bib": {"title": "Highway and residual networks learn unrolled iterative estimation", "author": ["K Greff", "RK Srivastava", "J Schmidhuber"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.07771", "abstract": "The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer. In this report, we argue that this view is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.07771", "author_id": ["OcownLgAAAAJ", "vTWuk1gAAAAJ", "gLnCTgIAAAAJ"], "url_scholarbib": "/scholar?q=info:_lfNg70JosgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHighway%2Band%2BResidual%2BNetworks%2Blearn%2BUnrolled%2BIterative%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_lfNg70JosgJ&ei=TRBkYsK_NIyuyAT-mrWwCA&json=", "num_citations": 201, "citedby_url": "/scholar?cites=14457128463377455102&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_lfNg70JosgJ:scholar.google.com/&scioq=Highway+and+Residual+Networks+learn+Unrolled+Iterative+Estimation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.07771"}, "Deep Information Propagation": {"container_type": "Publication", "bib": {"title": "Deep information propagation", "author": ["SS Schoenholz", "J Gilmer", "S Ganguli"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "limit the maximum depth of signal propagation through these random networks.  information  can travel through them. Thus, the depth scales that we identify provide bounds on how deep"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01232", "author_id": ["mk-zQBsAAAAJ", "Ml_vQ8MAAAAJ", "rF2VvOgAAAAJ"], "url_scholarbib": "/scholar?q=info:AphjvB77_zIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BInformation%2BPropagation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AphjvB77_zIJ&ei=UhBkYofsG5LeyQTE46-QAg&json=", "num_citations": 241, "citedby_url": "/scholar?cites=3674931930385848322&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AphjvB77_zIJ:scholar.google.com/&scioq=Deep+Information+Propagation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01232"}, "Deep Neural Networks and the Tree of Life": {"container_type": "Publication", "bib": {"title": "Deep Neural Networks and the Tree of Life", "author": ["Y Wang", "K He", "JE Hopcroft", "Y Sun"], "pub_year": "2016", "venue": "NA", "abstract": "In Evolutionary Biology, species close in the tree of evolution are identified by similar visual features. In computer vision, deep neural networks perform image classification by learning to identify similar visual features. This leads to an interesting question: is it possible to leverage the advantage of deep networks to construct a tree of life? In this paper, we make the first attempt at building the phylogenetic tree diagram by leveraging the high-level features learned by deep neural networks. Our method is based on the intuition that if two"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r17RD2oxe", "author_id": ["", "", "4Z6vo5QAAAAJ", ""], "url_scholarbib": "/scholar?q=info:8M4poyHDyA4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BNeural%2BNetworks%2Band%2Bthe%2BTree%2Bof%2BLife%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8M4poyHDyA4J&ei=VhBkYoKVPOiSy9YPp-OyiAE&json=", "num_citations": 1, "citedby_url": "/scholar?cites=1065315861111885552&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8M4poyHDyA4J:scholar.google.com/&scioq=Deep+Neural+Networks+and+the+Tree+of+Life&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r17RD2oxe"}, "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights": {"container_type": "Publication", "bib": {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["A Zhou", "A Yao", "Y Guo", "L Xu", "Y Chen"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.03044", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.03044", "author_id": ["cC8lXi8AAAAJ", "b9hCmPYAAAAJ", "oi_lEwYAAAAJ", "PA70p0cAAAAJ", "MKRyHXsAAAAJ"], "url_scholarbib": "/scholar?q=info:nBFPYxCacJIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIncremental%2BNetwork%2BQuantization:%2BTowards%2BLossless%2BCNNs%2Bwith%2BLow-precision%2BWeights%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nBFPYxCacJIJ&ei=WxBkYqiwCfmQ6rQP5OqKqAo&json=", "num_citations": 859, "citedby_url": "/scholar?cites=10552103322105352604&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nBFPYxCacJIJ:scholar.google.com/&scioq=Incremental+Network+Quantization:+Towards+Lossless+CNNs+with+Low-precision+Weights&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.03044.pdf?ref=https://githubhelp.com"}, "Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU": {"container_type": "Publication", "bib": {"title": "Reinforcement learning through asynchronous advantage actor-critic on a gpu", "author": ["M Babaeizadeh", "I Frosio", "S Tyree", "J Clemons"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.06256", "author_id": ["3Y4egcYAAAAJ", "PCJJ8LkAAAAJ", "PGZLZFUAAAAJ", "J_1GGJsAAAAJ"], "url_scholarbib": "/scholar?q=info:EwdMik6Gh3kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforcement%2BLearning%2Bthrough%2BAsynchronous%2BAdvantage%2BActor-Critic%2Bon%2Ba%2BGPU%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EwdMik6Gh3kJ&ei=XhBkYtuJAfmQ6rQP5OqKqAo&json=", "num_citations": 182, "citedby_url": "/scholar?cites=8757115672331028243&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EwdMik6Gh3kJ:scholar.google.com/&scioq=Reinforcement+Learning+through+Asynchronous+Advantage+Actor-Critic+on+a+GPU&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.06256"}, "TreNet: Hybrid Neural Networks for Learning the Local Trend in Time Series": {"container_type": "Publication", "bib": {"title": "TreNet: Hybrid Neural Networks for Learning the Local Trend in Time Series", "author": ["T Lin", "T Guo", "K Aberer"], "pub_year": "2016", "venue": "NA", "abstract": "Local trends of time series characterize the intermediate upward and downward patterns of time series. Learning and forecasting the local trend in time series data play an important role in many real applications, ranging from investing in the stock market, resource allocation in data centers and load schedule in smart grid. Inspired by the recent successes of neural networks, in this paper we propose TreNet, a novel end-to-end hybrid neural network that predicts the local trend of time series based on local and global contextual"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryCcJaqgl", "author_id": ["", "I4mkY_4AAAAJ", "ifU81ikAAAAJ"], "url_scholarbib": "/scholar?q=info:w4Whgj5JiqIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTreNet:%2BHybrid%2BNeural%2BNetworks%2Bfor%2BLearning%2Bthe%2BLocal%2BTrend%2Bin%2BTime%2BSeries%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w4Whgj5JiqIJ&ei=YRBkYpLAEvmQ6rQP5OqKqAo&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:w4Whgj5JiqIJ:scholar.google.com/&scioq=TreNet:+Hybrid+Neural+Networks+for+Learning+the+Local+Trend+in+Time+Series&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryCcJaqgl"}, "Structured Sequence Modeling with Graph Convolutional Recurrent Networks": {"container_type": "Publication", "bib": {"title": "Structured sequence modeling with graph convolutional recurrent networks", "author": ["Y Seo", "M Defferrard", "P Vandergheynst"], "pub_year": "2018", "venue": "\u2026 Conference on Neural \u2026", "abstract": "This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep learning model able to predict structured sequences of data. Precisely, GCRN is a generalization of classical recurrent neural networks (RNN) to data structured by an arbitrary graph. The structured sequences can represent series of frames in videos, spatio-temporal measurements on a network of sensors, or random walks on a vocabulary graph for natural language modeling. The proposed model combines convolutional neural networks (CNN)"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-04167-0_33", "author_id": ["pCqnDsEAAAAJ", "Ztj2-gUAAAAJ", "1p9NOFEAAAAJ"], "url_scholarbib": "/scholar?q=info:EN51tH00EfQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructured%2BSequence%2BModeling%2Bwith%2BGraph%2BConvolutional%2BRecurrent%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EN51tH00EfQJ&ei=YxBkYvDZN4OEmgHx-5DADA&json=", "num_citations": 344, "citedby_url": "/scholar?cites=17586895734361677328&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EN51tH00EfQJ:scholar.google.com/&scioq=Structured+Sequence+Modeling+with+Graph+Convolutional+Recurrent+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.07659.pdf?ref=https://githubhelp.com"}, "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy": {"container_type": "Publication", "bib": {"title": "Generative models and model criticism via optimized maximum mean discrepancy", "author": ["DJ Sutherland", "HY Tung", "H Strathmann", "S De"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We will begin by reviewing the maximum mean discrepancy and its use in two-sample tests.  Let k be the kernel of a reproducing kernel Hilbert space (RKHS) Hk of functions on a set X."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.04488", "author_id": ["uO_NqicAAAAJ", "SMAmWOQAAAAJ", "QFseZ2gAAAAJ", "njZTrWYAAAAJ"], "url_scholarbib": "/scholar?q=info:YVunElCD1cIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BModels%2Band%2BModel%2BCriticism%2Bvia%2BOptimized%2BMaximum%2BMean%2BDiscrepancy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YVunElCD1cIJ&ei=bhBkYvrFHYOEmgHx-5DADA&json=", "num_citations": 140, "citedby_url": "/scholar?cites=14039271793330969441&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YVunElCD1cIJ:scholar.google.com/&scioq=Generative+Models+and+Model+Criticism+via+Optimized+Maximum+Mean+Discrepancy&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.04488.pdf?ref=https://githubhelp.com"}, "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving": {"container_type": "Publication", "bib": {"title": "Holstep: A machine learning dataset for higher-order logic theorem proving", "author": ["C Kaliszyk", "F Chollet", "C Szegedy"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.00426", "abstract": "Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.00426", "author_id": ["bp68Q1kAAAAJ", "VfYhf2wAAAAJ", "3QeF7mAAAAAJ"], "url_scholarbib": "/scholar?q=info:-Mr1SvbV58gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHolStep:%2BA%2BMachine%2BLearning%2BDataset%2Bfor%2BHigher-order%2BLogic%2BTheorem%2BProving%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-Mr1SvbV58gJ&ei=cRBkYpLELY2ymgHg1rfQDQ&json=", "num_citations": 71, "citedby_url": "/scholar?cites=14476774781002042104&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-Mr1SvbV58gJ:scholar.google.com/&scioq=HolStep:+A+Machine+Learning+Dataset+for+Higher-order+Logic+Theorem+Proving&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.00426"}, "Modularized Morphing of Neural Networks": {"container_type": "Publication", "bib": {"title": "Modularized morphing of neural networks", "author": ["T Wei", "C Wang", "CW Chen"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.03281", "abstract": "In this work we study the problem of network morphism, an effective learning scheme to morph a well-trained neural network to a new one with the network function completely preserved. Different from existing work where basic morphing types on the layer level were addressed, we target at the central problem of network morphism at a higher level, ie, how a convolutional layer can be morphed into an arbitrary module of a neural network. To simplify the representation of a network, we abstract a module as a graph with blobs as vertices and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.03281", "author_id": ["", "DsVZkjAAAAAJ", "w2HXPUUAAAAJ"], "url_scholarbib": "/scholar?q=info:CxQ0z70zOK8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModularized%2BMorphing%2Bof%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CxQ0z70zOK8J&ei=hhBkYqnAOYuKmgGY1YjABQ&json=", "num_citations": 17, "citedby_url": "/scholar?cites=12625898445651317771&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CxQ0z70zOK8J:scholar.google.com/&scioq=Modularized+Morphing+of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.03281"}, "Unsupervised Cross-Domain Image Generation": {"container_type": "Publication", "bib": {"title": "Unsupervised cross-domain image generation", "author": ["Y Taigman", "A Polyak", "L Wolf"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02200", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02200", "author_id": ["mbB3MRIAAAAJ", "CP62OTMAAAAJ", "UbFrXTsAAAAJ"], "url_scholarbib": "/scholar?q=info:DGS8ljSdgA4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BCross-Domain%2BImage%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DGS8ljSdgA4J&ei=khBkYvDGDZLeyQTE46-QAg&json=", "num_citations": 927, "citedby_url": "/scholar?cites=1045007962742744076&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DGS8ljSdgA4J:scholar.google.com/&scioq=Unsupervised+Cross-Domain+Image+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02200.pdf%C2%A0"}, "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning": {"container_type": "Publication", "bib": {"title": "Deepdsl: A compilation-based domain-specific language for deep learning", "author": ["T Zhao", "X Huang", "Y Cao"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.02284", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications. In this paper, we present"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.02284", "author_id": ["65p2anoAAAAJ", "", "97RDUygAAAAJ"], "url_scholarbib": "/scholar?q=info:Jd983Hg1QPkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeepDSL:%2BA%2BCompilation-based%2BDomain-Specific%2BLanguage%2Bfor%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Jd983Hg1QPkJ&ei=lRBkYobTJ_mQ6rQP5OqKqAo&json=", "num_citations": 9, "citedby_url": "/scholar?cites=17960414107165056805&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Jd983Hg1QPkJ:scholar.google.com/&scioq=DeepDSL:+A+Compilation-based+Domain-Specific+Language+for+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.02284"}, "Gated Multimodal Units for Information Fusion": {"container_type": "Publication", "bib": {"title": "Gated multimodal units for information fusion", "author": ["J Arevalo", "T Solorio", "M Montes-y-G\u00f3mez"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.01992", "author_id": ["OTOILtIAAAAJ", "Gmjwy-IAAAAJ", "DKpXXTgAAAAJ"], "url_scholarbib": "/scholar?q=info:YBZPXszHqPwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGated%2BMultimodal%2BUnits%2Bfor%2BInformation%2BFusion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YBZPXszHqPwJ&ei=mBBkYsmJJOiSy9YPp-OyiAE&json=", "num_citations": 149, "citedby_url": "/scholar?cites=18206021174214727264&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YBZPXszHqPwJ:scholar.google.com/&scioq=Gated+Multimodal+Units+for+Information+Fusion&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.01992"}, "Support Regularized Sparse Coding and Its Fast Encoder": {"container_type": "Publication", "bib": {"title": "Support regularized sparse coding and its fast encoder", "author": ["Y Yang", "J Yu", "P Kohli", "J Yang", "TS Huang"], "pub_year": "2016", "venue": "NA", "abstract": "Sparse coding represents a signal by a linear combination of only a few atoms of a learned over-complete dictionary. While sparse coding exhibits compelling performance for various machine learning tasks, the process of obtaining sparse code with fixed dictionary is independent for each data point without considering the geometric information and manifold structure of the entire data. We propose Support Regularized Sparse Coding (SRSC) which produces sparse codes that account for the manifold structure of the data by encouraging"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkljfjFee", "author_id": ["36vh2hgAAAAJ", "", "3pyzQQ8AAAAJ", "HWFvq_wAAAAJ", "rGF6-WkAAAAJ"], "url_scholarbib": "/scholar?q=info:NLn8vAD5UQwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSupport%2BRegularized%2BSparse%2BCoding%2Band%2BIts%2BFast%2BEncoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NLn8vAD5UQwJ&ei=nRBkYqazM86E6rQP5-KmKA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=887764383134693684&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NLn8vAD5UQwJ:scholar.google.com/&scioq=Support+Regularized+Sparse+Coding+and+Its+Fast+Encoder&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkljfjFee"}, "Revisiting Classifier Two-Sample Tests": {"container_type": "Publication", "bib": {"title": "Revisiting classifier two-sample tests", "author": ["D Lopez-Paz", "M Oquab"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.06545", "abstract": "Q\u201d is true, then the classification accuracy of a binary classifier on a held-out subset  Classifier  Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly, return test"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.06545", "author_id": ["SiCHxTkAAAAJ", "5vteYV8AAAAJ"], "url_scholarbib": "/scholar?q=info:eGHvB16b-68J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRevisiting%2BClassifier%2BTwo-Sample%2BTests%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eGHvB16b-68J&ei=oBBkYtXpJIOEmgHx-5DADA&json=", "num_citations": 236, "citedby_url": "/scholar?cites=12680900003954123128&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eGHvB16b-68J:scholar.google.com/&scioq=Revisiting+Classifier+Two-Sample+Tests&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.06545"}, "Density estimation using Real NVP": {"container_type": "Publication", "bib": {"title": "Density estimation using real nvp", "author": ["L Dinh", "J Sohl-Dickstein", "S Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.08803", "abstract": "We extend the space of such models using real-valued non-volume preserving (real NVP)  transformations, a set of powerful, stably invertible, and learnable transformations, resulting in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.08803", "author_id": ["h7OHSkoAAAAJ", "-3zYIjQAAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:Uo6_uawta18J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDensity%2Bestimation%2Busing%2BReal%2BNVP%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Uo6_uawta18J&ei=oxBkYpCDBpyO6rQP_qe3mAs&json=", "num_citations": 1796, "citedby_url": "/scholar?cites=6875639475985157714&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Uo6_uawta18J:scholar.google.com/&scioq=Density+estimation+using+Real+NVP&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.08803"}, "Dialogue Learning With Human-in-the-Loop": {"container_type": "Publication", "bib": {"title": "Dialogue learning with human-in-the-loop", "author": ["J Li", "AH Miller", "S Chopra", "MA Ranzato"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09823", "author_id": ["PwU16JEAAAAJ", "3b0l5LEAAAAJ", "ETU-ePAAAAAJ", "NbXF7T8AAAAJ"], "url_scholarbib": "/scholar?q=info:uK1VDelitMoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDialogue%2BLearning%2BWith%2BHuman-in-the-Loop%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uK1VDelitMoJ&ei=pRBkYrj-GY6pywTd4KPADw&json=", "num_citations": 102, "citedby_url": "/scholar?cites=14606408244559982008&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uK1VDelitMoJ:scholar.google.com/&scioq=Dialogue+Learning+With+Human-in-the-Loop&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09823"}, "On the Expressive Power of Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "On the expressive power of deep neural networks", "author": ["M Raghu", "B Poole", "J Kleinberg"], "pub_year": "2017", "venue": "international \u2026", "abstract": "We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that:(1) The complexity of the computed function grows exponentially with depth (2) All weights are not"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v70/raghu17a.html", "author_id": ["xdwK2NsAAAAJ", "i5FMLA4AAAAJ", "VX7d5EQAAAAJ"], "url_scholarbib": "/scholar?q=info:Djagjjc9hlUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BExpressive%2BPower%2Bof%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Djagjjc9hlUJ&ei=qBBkYqCfGIyuyAT-mrWwCA&json=", "num_citations": 579, "citedby_url": "/scholar?cites=6162680448928462350&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Djagjjc9hlUJ:scholar.google.com/&scioq=On+the+Expressive+Power+of+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf"}, "Opening the vocabulary of  neural language models with character-level word representations": {"container_type": "Publication", "bib": {"title": "Opening the vocabulary of neural language models with character-level word representations", "author": ["M Labeau", "A Allauzen"], "pub_year": "2016", "venue": "NA", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SygGlIBcel", "author_id": ["", "B2-gXkkAAAAJ"], "url_scholarbib": "/scholar?q=info:YYhvL95fgocJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOpening%2Bthe%2Bvocabulary%2Bof%2B%2Bneural%2Blanguage%2Bmodels%2Bwith%2Bcharacter-level%2Bword%2Brepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YYhvL95fgocJ&ei=qhBkYuv6Oc6E6rQP5-KmKA&json=", "num_citations": 1, "citedby_url": "/scholar?cites=9764472349975873633&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YYhvL95fgocJ:scholar.google.com/&scioq=Opening+the+vocabulary+of++neural+language+models+with+character-level+word+representations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SygGlIBcel"}, "Neural Causal Regularization under the Independence of Mechanisms Assumption": {"container_type": "Publication", "bib": {"title": "Neural Causal Regularization under the Independence of Mechanisms Assumption", "author": ["MT Bahadori", "K Chalupka", "E Choi", "R Chen", "WF Stewart"], "pub_year": "2016", "venue": "NA", "abstract": "Neural networks provide a powerful framework for learning the association between input and response variables and making accurate predictions. However, in many applications such as healthcare, it is important to identify causal relationships between the inputs and the response variables to be able to change the response variables by intervention on the inputs. In pursuit of models whose predictive power comes maximally from causal variables, we propose a novel causal regularizer based on the independence of mechanisms"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByW2Avqgg", "author_id": ["tlZvhyoAAAAJ", "ieTR4uEAAAAJ", "GUlGIPkAAAAJ", "", "flGRoHEAAAAJ"], "url_scholarbib": "/scholar?q=info:FjbIdnzRJJAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BCausal%2BRegularization%2Bunder%2Bthe%2BIndependence%2Bof%2BMechanisms%2BAssumption%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FjbIdnzRJJAJ&ei=sxBkYouMEfmQ6rQP5OqKqAo&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:FjbIdnzRJJAJ:scholar.google.com/&scioq=Neural+Causal+Regularization+under+the+Independence+of+Mechanisms+Assumption&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByW2Avqgg"}, "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices": {"container_type": "Publication", "bib": {"title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "author": ["D Li", "X Wang", "D Kong", "MC Chuah"], "pub_year": "2016", "venue": "NA", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (ie, SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkwSJ99ex", "author_id": ["", "", "wCkI3_AAAAAJ", "SZBKvksAAAAJ"], "url_scholarbib": "/scholar?q=info:uOYN-YnYexIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeepRebirth:%2BA%2BGeneral%2BApproach%2Bfor%2BAccelerating%2BDeep%2BNeural%2BNetwork%2BExecution%2Bon%2BMobile%2BDevices%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uOYN-YnYexIJ&ei=vxBkYsfiHYOEmgHx-5DADA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:uOYN-YnYexIJ:scholar.google.com/&scioq=DeepRebirth:+A+General+Approach+for+Accelerating+Deep+Neural+Network+Execution+on+Mobile+Devices&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkwSJ99ex"}, "Sequence to Sequence Transduction with Hard Monotonic Attention": {"container_type": "Publication", "bib": {"title": "Sequence to sequence transduction with hard monotonic attention", "author": ["R Aharoni", "Y Goldberg"], "pub_year": "2016", "venue": "NA", "abstract": "We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkyYqU9lx", "author_id": ["wV0mHWgAAAAJ", "0rskDKgAAAAJ"], "url_scholarbib": "/scholar?q=info:TB4JfphlXEgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSequence%2Bto%2BSequence%2BTransduction%2Bwith%2BHard%2BMonotonic%2BAttention%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TB4JfphlXEgJ&ei=xBBkYvWjGoOEmgHx-5DADA&json=", "num_citations": 10, "citedby_url": "/scholar?cites=5214154174212152908&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TB4JfphlXEgJ:scholar.google.com/&scioq=Sequence+to+Sequence+Transduction+with+Hard+Monotonic+Attention&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkyYqU9lx"}, "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Parametric exponential linear unit for deep convolutional neural networks", "author": ["L Trottier", "P Giguere"], "pub_year": "2017", "venue": "2017 16th IEEE \u2026", "abstract": "Object recognition is an important task for improving the ability of visual systems to perform complex scene understanding. Recently, the Exponential Linear Unit (ELU) has been proposed as a key component for managing bias shift in Convolutional Neural Networks (CNNs), but defines a parameter that must be set by hand. In this paper, we propose learning a parameterization of ELU in order to learn the proper activation shape at each layer in the CNNs. Our results on the MNIST, CIFAR-10/100 and ImageNet datasets using"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8260635/", "author_id": ["Jurm2FgAAAAJ", "tgZPkzkAAAAJ"], "url_scholarbib": "/scholar?q=info:yv6qGSj9eNIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParametric%2BExponential%2BLinear%2BUnit%2Bfor%2BDeep%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yv6qGSj9eNIJ&ei=3RBkYvr4A6KUy9YP_JONiAY&json=", "num_citations": 174, "citedby_url": "/scholar?cites=15166150093841301194&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yv6qGSj9eNIJ:scholar.google.com/&scioq=Parametric+Exponential+Linear+Unit+for+Deep+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.09332"}, "Diet Networks: Thin Parameters for Fat Genomics": {"container_type": "Publication", "bib": {"title": "Diet networks: thin parameters for fat genomics", "author": ["A Romero", "PL Carrier", "A Erraqabi", "T Sylvain"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09340", "author_id": ["Sm15FXIAAAAJ", "GS_Rl4kAAAAJ", "FfN2oo4AAAAJ", "Dg5qUb0AAAAJ"], "url_scholarbib": "/scholar?q=info:osm61ux0ouQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiet%2BNetworks:%2BThin%2BParameters%2Bfor%2BFat%2BGenomics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=osm61ux0ouQJ&ei=4BBkYoTGBIOEmgHx-5DADA&json=", "num_citations": 53, "citedby_url": "/scholar?cites=16474858947438365090&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:osm61ux0ouQJ:scholar.google.com/&scioq=Diet+Networks:+Thin+Parameters+for+Fat+Genomics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09340"}, "Making Neural Programming Architectures Generalize via Recursion": {"container_type": "Publication", "bib": {"title": "Making neural programming architectures generalize via recursion", "author": ["J Cai", "R Shin", "D Song"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1704.06611", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.06611", "author_id": ["6pkJu4gAAAAJ", "xPnkc80AAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:2DyoHPBGNQ4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaking%2BNeural%2BProgramming%2BArchitectures%2BGeneralize%2Bvia%2BRecursion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2DyoHPBGNQ4J&ei=5RBkYumWELKO6rQPy-CRsA8&json=", "num_citations": 112, "citedby_url": "/scholar?cites=1023802487383538904&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2DyoHPBGNQ4J:scholar.google.com/&scioq=Making+Neural+Programming+Architectures+Generalize+via+Recursion&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.06611?ref=https://githubhelp.com"}, "Decomposing Motion and Content for Natural Video Sequence Prediction": {"container_type": "Publication", "bib": {"title": "Decomposing motion and content for natural video sequence prediction", "author": ["R Villegas", "J Yang", "S Hong", "X Lin", "H Lee"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.08033", "abstract": "We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.08033", "author_id": ["uGDQoU0AAAAJ", "GwKF9rMAAAAJ", "hvr3ALkAAAAJ", "", "fmSHtE8AAAAJ"], "url_scholarbib": "/scholar?q=info:L9iuyxNqKZcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecomposing%2BMotion%2Band%2BContent%2Bfor%2BNatural%2BVideo%2BSequence%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=L9iuyxNqKZcJ&ei=6RBkYpfOIoyuyAT-mrWwCA&json=", "num_citations": 455, "citedby_url": "/scholar?cites=10892353807026411567&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:L9iuyxNqKZcJ:scholar.google.com/&scioq=Decomposing+Motion+and+Content+for+Natural+Video+Sequence+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.08033"}, "Deep Learning with Dynamic Computation Graphs": {"container_type": "Publication", "bib": {"title": "Deep learning with dynamic computation graphs", "author": ["M Looks", "M Herreshoff", "DL Hutchins"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.02181", "author_id": ["", "", "C6CJkqcAAAAJ"], "url_scholarbib": "/scholar?q=info:CyR9jh5xO_QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bwith%2BDynamic%2BComputation%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CyR9jh5xO_QJ&ei=7RBkYobOG5qSy9YP8pKNsAE&json=", "num_citations": 125, "citedby_url": "/scholar?cites=17598784344933868555&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CyR9jh5xO_QJ:scholar.google.com/&scioq=Deep+Learning+with+Dynamic+Computation+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.02181.pdf?source=post_page---------------------------"}, "Recurrent Mixture Density Network for Spatiotemporal Visual Attention": {"container_type": "Publication", "bib": {"title": "Recurrent mixture density network for spatiotemporal visual attention", "author": ["L Bazzani", "H Larochelle", "L Torresani"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1603.08199", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, eg, by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1603.08199", "author_id": ["1cdNGL4AAAAJ", "U89FHq4AAAAJ", "ss8KR5gAAAAJ"], "url_scholarbib": "/scholar?q=info:AFt1MwGntx0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BMixture%2BDensity%2BNetwork%2Bfor%2BSpatiotemporal%2BVisual%2BAttention%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AFt1MwGntx0J&ei=8BBkYre3F4yuyAT-mrWwCA&json=", "num_citations": 125, "citedby_url": "/scholar?cites=2141363771438095104&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AFt1MwGntx0J:scholar.google.com/&scioq=Recurrent+Mixture+Density+Network+for+Spatiotemporal+Visual+Attention&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1603.08199"}, "Delving into Transferable Adversarial Examples and Black-box Attacks": {"container_type": "Publication", "bib": {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Y Liu", "X Chen", "C Liu", "D Song"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02770", "abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02770", "author_id": ["", "d4W1UT0AAAAJ", "Zrbs8hIAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:FggEjpnxZqUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDelving%2Binto%2BTransferable%2BAdversarial%2BExamples%2Band%2BBlack-box%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FggEjpnxZqUJ&ei=9hBkYtnKLLKO6rQPy-CRsA8&json=", "num_citations": 1171, "citedby_url": "/scholar?cites=11918479105697515542&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FggEjpnxZqUJ:scholar.google.com/&scioq=Delving+into+Transferable+Adversarial+Examples+and+Black-box+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02770)%EF%BC%8C%E5%A6%82%E4%BD%95%E8%AE%A9%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E9%AA%97%E8%BF%87%E5%BA%94%E7%94%A8%E5%9C%A8%E7%8E%B0%E5%AE%9E%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B"}, "Learning Word-Like Units from Joint Audio-Visual Analylsis": {"container_type": "Publication", "bib": {"title": "Learning word-like units from joint audio-visual analysis", "author": ["D Harwath", "JR Glass"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.07481", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word'lighthouse'within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.07481", "author_id": ["C0kDOzcAAAAJ", "pfGI-KcAAAAJ"], "url_scholarbib": "/scholar?q=info:VqMdjtgUUQcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BWord-Like%2BUnits%2Bfrom%2BJoint%2BAudio-Visual%2BAnalylsis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VqMdjtgUUQcJ&ei=-RBkYpuJM42ymgHg1rfQDQ&json=", "num_citations": 101, "citedby_url": "/scholar?cites=527225551708857174&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VqMdjtgUUQcJ:scholar.google.com/&scioq=Learning+Word-Like+Units+from+Joint+Audio-Visual+Analylsis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.07481"}, "Generalizing Skills with Semi-Supervised Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Generalizing skills with semi-supervised reinforcement learning", "author": ["C Finn", "T Yu", "J Fu", "P Abbeel", "S Levine"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.00429", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while deployed. However, this learning requires"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.00429", "author_id": ["vfPE6hgAAAAJ", "5VaXUQsAAAAJ", "T9To2C0AAAAJ", "vtwH6GkAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:bFp954oJFSwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizing%2BSkills%2Bwith%2BSemi-Supervised%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bFp954oJFSwJ&ei=_RBkYqbfMO-Sy9YPs_mY8AM&json=", "num_citations": 57, "citedby_url": "/scholar?cites=3176455604373641836&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bFp954oJFSwJ:scholar.google.com/&scioq=Generalizing+Skills+with+Semi-Supervised+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.00429.pdf?ref=https://githubhelp.com"}, "Modular Multitask Reinforcement Learning with Policy Sketches": {"container_type": "Publication", "bib": {"title": "Modular multitask reinforcement learning with policy sketches", "author": ["J Andreas", "D Klein", "S Levine"], "pub_year": "2017", "venue": "International Conference on \u2026", "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them\u2014specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (eg intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/andreas17a.html", "author_id": ["dnZ8udEAAAAJ", "", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:1MksK2Bhc0sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModular%2BMultitask%2BReinforcement%2BLearning%2Bwith%2BPolicy%2BSketches%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1MksK2Bhc0sJ&ei=AxFkYqLZIuHDywSSipaYAg&json=", "num_citations": 333, "citedby_url": "/scholar?cites=5436796240835430868&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1MksK2Bhc0sJ:scholar.google.com/&scioq=Modular+Multitask+Reinforcement+Learning+with+Policy+Sketches&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/andreas17a/andreas17a.pdf"}, "A Simple but Tough-to-Beat Baseline for Sentence Embeddings": {"container_type": "Publication", "bib": {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["S Arora", "Y Liang", "T Ma"], "pub_year": "2017", "venue": "International conference on learning \u2026", "abstract": "The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as"}, "filled": false, "gsrank": 1, "pub_url": "https://oar.princeton.edu/handle/88435/pr1rk2k", "author_id": ["RUP4S68AAAAJ", "_RVvnS4AAAAJ", "i38QlUwAAAAJ"], "url_scholarbib": "/scholar?q=info:XHz21aRyb6UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BSimple%2Bbut%2BTough-to-Beat%2BBaseline%2Bfor%2BSentence%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XHz21aRyb6UJ&ei=BxFkYozTNrKO6rQPy-CRsA8&json=", "num_citations": 1120, "citedby_url": "/scholar?cites=11920872790962895964&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XHz21aRyb6UJ:scholar.google.com/&scioq=A+Simple+but+Tough-to-Beat+Baseline+for+Sentence+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://oar.princeton.edu/bitstream/88435/pr1rk2k/1/BaselineSentenceEmbedding.pdf"}, "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent": {"container_type": "Publication", "bib": {"title": "Training long short-term memory with sparsified stochastic gradient descent", "author": ["M Zhu", "M Rhu", "J Clemons", "SW Keckler", "Y Xie"], "pub_year": "2016", "venue": "NA", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs). However, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJWzXsKxx", "author_id": ["NRbwpx8AAAAJ", "u_3ShigAAAAJ", "J_1GGJsAAAAJ", "PpjjRvoAAAAJ", "dK2ZuDcAAAAJ"], "url_scholarbib": "/scholar?q=info:2hAEn2jLRIoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BLong%2BShort-Term%2BMemory%2BWith%2BSparsified%2BStochastic%2BGradient%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2hAEn2jLRIoJ&ei=CxFkYsONI7KO6rQPy-CRsA8&json=", "num_citations": 4, "citedby_url": "/scholar?cites=9963311925855260890&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2hAEn2jLRIoJ:scholar.google.com/&scioq=Training+Long+Short-Term+Memory+With+Sparsified+Stochastic+Gradient+Descent&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJWzXsKxx"}, "Generative Adversarial Parallelization": {"container_type": "Publication", "bib": {"title": "Generative adversarial parallelization", "author": ["DJ Im", "H Ma", "CD Kim", "G Taylor"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.04021", "abstract": "We observe that more parallelization leads to less of a spread between the train and  ,  we show individual learning curves of the parallelized GANs (see Figure 13 in Appendix A.3)."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.04021", "author_id": ["bzmGSYIAAAAJ", "D7ypICQAAAAJ", "SzLOAE8AAAAJ", "PUeKU8kAAAAJ"], "url_scholarbib": "/scholar?q=info:C3_TffneEkAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BAdversarial%2BParallelization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=C3_TffneEkAJ&ei=DxFkYqnDOu-Sy9YPs_mY8AM&json=", "num_citations": 33, "citedby_url": "/scholar?cites=4616997731147415307&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:C3_TffneEkAJ:scholar.google.com/&scioq=Generative+Adversarial+Parallelization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.04021.pdf?ref=https://githubhelp.com"}, "Why Deep Neural Networks for Function Approximation?": {"container_type": "Publication", "bib": {"title": "Why deep neural networks for function approximation?", "author": ["S Liang", "R Srikant"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.04161", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.04161", "author_id": ["L8r9ox4AAAAJ", "tDWt_MQAAAAJ"], "url_scholarbib": "/scholar?q=info:yVfHkc1oCHoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhy%2BDeep%2BNeural%2BNetworks%2Bfor%2BFunction%2BApproximation%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yVfHkc1oCHoJ&ei=ExFkYqOWK-iSy9YPp-OyiAE&json=", "num_citations": 266, "citedby_url": "/scholar?cites=8793393504564238281&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yVfHkc1oCHoJ:scholar.google.com/&scioq=Why+Deep+Neural+Networks+for+Function+Approximation%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.04161"}, "Generative Multi-Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Generative multi-adversarial networks", "author": ["I Durugkar", "I Gemp", "S Mahadevan"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01673", "abstract": "Generative adversarial networks (GANs) are a framework for  we propose the Generative  Multi-Adversarial Network (GMAN) of GAM, the generative multi-adversarial metric (GMAM), that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01673", "author_id": ["eb81CnYAAAAJ", "5vo3MeEAAAAJ", "xTj2eQwAAAAJ"], "url_scholarbib": "/scholar?q=info:I--xSyuLNaQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BMulti-Adversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=I--xSyuLNaQJ&ei=FhFkYqreFIuKmgGY1YjABQ&json=", "num_citations": 313, "citedby_url": "/scholar?cites=11832516614055653155&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:I--xSyuLNaQJ:scholar.google.com/&scioq=Generative+Multi-Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01673.pdf?ref=https://githubhelp.com"}, "L-SR1: A Second Order Optimization Method for Deep Learning": {"container_type": "Publication", "bib": {"title": "L-SR1: A Second Order Optimization Method for Deep Learning", "author": ["V Ramamurthy", "N Duffy"], "pub_year": "2016", "venue": "NA", "abstract": "We describe L-SR1, a new second order method to train deep neural networks. Second order methods hold great promise for distributed training of deep networks. Unfortunately, they have not proven practical. Two significant barriers to their success are inappropriate handling of saddle points, and poor conditioning of the Hessian. L-SR1 is a practical second order method that addresses these concerns. We provide experimental results showing that L-SR1 performs at least as well as Nesterov's Accelerated Gradient Descent, on the MNIST"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=By1snw5gl", "author_id": ["", "fiVcAx0AAAAJ"], "url_scholarbib": "/scholar?q=info:UpX4WtAM-ZUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DL-SR1:%2BA%2BSecond%2BOrder%2BOptimization%2BMethod%2Bfor%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UpX4WtAM-ZUJ&ei=GBFkYt_oMc6E6rQP5-KmKA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=10806682869871187282&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UpX4WtAM-ZUJ:scholar.google.com/&scioq=L-SR1:+A+Second+Order+Optimization+Method+for+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=By1snw5gl"}, "Deep Variational Information Bottleneck": {"container_type": "Publication", "bib": {"title": "Deep variational information bottleneck", "author": ["AA Alemi", "I Fischer", "JV Dillon", "K Murphy"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.00410", "abstract": "variational approximation to the information bottleneck of Tishby et al. (1999). This variational   Here we demonstrate that appealing to the information bottleneck objective gives a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.00410", "author_id": ["68hTs9wAAAAJ", "Z63Zf_0AAAAJ", "g8vrSV8AAAAJ", "MxxZkEcAAAAJ"], "url_scholarbib": "/scholar?q=info:xVU-kbAeDWcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BVariational%2BInformation%2BBottleneck%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xVU-kbAeDWcJ&ei=HRFkYqm2Me-Sy9YPs_mY8AM&json=", "num_citations": 887, "citedby_url": "/scholar?cites=7425625104303674821&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xVU-kbAeDWcJ:scholar.google.com/&scioq=Deep+Variational+Information+Bottleneck&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.00410"}, "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition": {"container_type": "Publication", "bib": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "author": ["T Bluche", "C Kermorvant", "C Touzet"], "pub_year": "2017", "venue": "2017 14th IAPR \u2026", "abstract": "Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing open-bigrams, ie couple of letters that are not necessarily next to each other. In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8269952/", "author_id": ["cdyHKj4AAAAJ", "u5SbAJAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:t116KRu2Pf4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCortical-Inspired%2BOpen-Bigram%2BRepresentation%2Bfor%2BHandwritten%2BWord%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=t116KRu2Pf4J&ei=IhFkYubbO6KUy9YP_JONiAY&json=", "num_citations": 5, "citedby_url": "/scholar?cites=18319999086989303223&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:t116KRu2Pf4J:scholar.google.com/&scioq=Cortical-Inspired+Open-Bigram+Representation+for+Handwritten+Word+Recognition&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkXMikqxx"}, "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models": {"container_type": "Publication", "bib": {"title": "Evaluation of defensive methods for dnns against multiple adversarial evasion models", "author": ["X Chen", "B Li", "Y Vorobeychik"], "pub_year": "2016", "venue": "NA", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications. However, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly. Even worse, these adversarial examples have the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByToKu9ll", "author_id": ["d4W1UT0AAAAJ", "K8vJkTcAAAAJ", "ptI-HHkAAAAJ"], "url_scholarbib": "/scholar?q=info:Ys1T_d0YNa4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvaluation%2Bof%2BDefensive%2BMethods%2Bfor%2BDNNs%2Bagainst%2BMultiple%2BAdversarial%2BEvasion%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ys1T_d0YNa4J&ei=JhFkYo8-jbKaAeDWt9AN&json=", "num_citations": 6, "citedby_url": "/scholar?cites=12552966878082092386&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ys1T_d0YNa4J:scholar.google.com/&scioq=Evaluation+of+Defensive+Methods+for+DNNs+against+Multiple+Adversarial+Evasion+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByToKu9ll"}, "Towards Information-Seeking Agents": {"container_type": "Publication", "bib": {"title": "Towards information-seeking agents", "author": ["P Bachman", "A Sordoni", "A Trischler"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.02605", "abstract": "information-seeking model should be able to ask questions such as \u201cwhat would happen  if...?\u201d. Second, we develop agents  devised to realize information-seeking behavior. Section 4"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.02605", "author_id": ["REMIv-kAAAAJ", "DJon7w4AAAAJ", "EvUM6UUAAAAJ"], "url_scholarbib": "/scholar?q=info:xQLRzO2PyQsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BInformation-Seeking%2BAgents%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xQLRzO2PyQsJ&ei=KRFkYuy_Es6E6rQP5-KmKA&json=", "num_citations": 9, "citedby_url": "/scholar?cites=849368256242320069&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xQLRzO2PyQsJ:scholar.google.com/&scioq=Towards+Information-Seeking+Agents&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.02605"}, "Combining policy gradient and Q-learning": {"container_type": "Publication", "bib": {"title": "Combining policy gradient and Q-learning", "author": ["B O'Donoghue", "R Munos", "K Kavukcuoglu"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "-policy only and not able to take advantage of off-policy data. In this paper we describe a new  technique that combines policy gradient with off-policy Q-learning policy gradient algorithm"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01626", "author_id": ["0Pzjj-cAAAAJ", "OvKEnVwAAAAJ", "sGFyDIUAAAAJ"], "url_scholarbib": "/scholar?q=info:zM4C4y81ncUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCombining%2Bpolicy%2Bgradient%2Band%2BQ-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zM4C4y81ncUJ&ei=LRFkYoGvEu-Sy9YPs_mY8AM&json=", "num_citations": 145, "citedby_url": "/scholar?cites=14239596076603723468&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zM4C4y81ncUJ:scholar.google.com/&scioq=Combining+policy+gradient+and+Q-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01626.pdf?ref=https://githubhelp.com"}, "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening": {"container_type": "Publication", "bib": {"title": "Learning to play in a day: Faster deep reinforcement learning by optimality tightening", "author": ["FS He", "Y Liu", "AG Schwing", "J Peng"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01606", "abstract": "We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01606", "author_id": ["wfN6SU4AAAAJ", "", "3B2c31wAAAAJ", "H2JX-RQAAAAJ"], "url_scholarbib": "/scholar?q=info:coTxd9J5M5AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BPlay%2Bin%2Ba%2BDay:%2BFaster%2BDeep%2BReinforcement%2BLearning%2Bby%2BOptimality%2BTightening%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=coTxd9J5M5AJ&ei=MRFkYrD2H--Sy9YPs_mY8AM&json=", "num_citations": 73, "citedby_url": "/scholar?cites=10390782710136276082&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:coTxd9J5M5AJ:scholar.google.com/&scioq=Learning+to+Play+in+a+Day:+Faster+Deep+Reinforcement+Learning+by+Optimality+Tightening&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01606.pdf?ref=https://githubhelp.com"}, "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "author": ["L Shi", "S Feng", "Z Zhu"], "pub_year": "2016", "venue": "NA", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hyanrrqlg", "author_id": ["s3dUf_EAAAAJ", "u9CYmnAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:8kN2elYGuWYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHFH:%2BHomologically%2BFunctional%2BHashing%2Bfor%2BCompressing%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8kN2elYGuWYJ&ei=NRFkYu-CH-HDywSSipaYAg&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:8kN2elYGuWYJ:scholar.google.com/&scioq=HFH:+Homologically+Functional+Hashing+for+Compressing+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hyanrrqlg"}, "What does it take to generate natural textures?": {"container_type": "Publication", "bib": {"title": "What does it take to generate natural textures?", "author": ["I Ustyuzhaninov", "W Brendel", "L Gatys", "M Bethge"], "pub_year": "2016", "venue": "NA", "abstract": "Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, eg for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJhZeLsxx", "author_id": ["YGEMpYUAAAAJ", "v-JL-hsAAAAJ", "ADMVEmsAAAAJ", "0z0fNxUAAAAJ"], "url_scholarbib": "/scholar?q=info:v0IckMhEwtkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhat%2Bdoes%2Bit%2Btake%2Bto%2Bgenerate%2Bnatural%2Btextures%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v0IckMhEwtkJ&ei=OBFkYq37DoySyASZk6HgCA&json=", "num_citations": 20, "citedby_url": "/scholar?cites=15691179679914148543&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v0IckMhEwtkJ:scholar.google.com/&scioq=What+does+it+take+to+generate+natural+textures%3F&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJhZeLsxx"}, "Inefficiency of stochastic gradient descent with larger mini-batches (and more learners)": {"container_type": "Publication", "bib": {"title": "Inefficiency of stochastic gradient descent with larger mini-batches (and more learners)", "author": ["O Bhardwaj", "G Cong"], "pub_year": "2016", "venue": "NA", "abstract": "Stochastic Gradient Descent (SGD) and its variants are the most important optimization algorithms used in large scale machine learning. Mini-batch version of stochastic gradient is often used in practice for taking advantage of hardware parallelism. In this work, we analyze the effect of mini-batch size over SGD convergence for the case of general non-convex objective functions. Building on the past analyses, we justify mathematically that there can often be a large difference between the convergence guarantees provided by small and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bk_zTU5eg", "author_id": ["lYXf3HwAAAAJ", "KDkk0csAAAAJ"], "url_scholarbib": "/scholar?q=info:WV6mRp6VtoIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInefficiency%2Bof%2Bstochastic%2Bgradient%2Bdescent%2Bwith%2Blarger%2Bmini-batches%2B(and%2Bmore%2Blearners)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WV6mRp6VtoIJ&ei=PRFkYtyZAZyO6rQP_qe3mAs&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:WV6mRp6VtoIJ:scholar.google.com/&scioq=Inefficiency+of+stochastic+gradient+descent+with+larger+mini-batches+(and+more+learners)&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bk_zTU5eg"}, "Learning to Remember Rare Events": {"container_type": "Publication", "bib": {"title": "Learning to remember rare events", "author": ["\u0141 Kaiser", "O Nachum", "A Roy", "S Bengio"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.03129", "abstract": "rare words or rare events of other kinds. This illustrates a general problem with current deep  learning  and re-train them to handle such rare or new events. Humans, on the other hand,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.03129", "author_id": ["JWmiQR0AAAAJ", "C-ZlBWMAAAAJ", "mCmda68AAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:phyiBJZ5yAkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BRemember%2BRare%2BEvents%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=phyiBJZ5yAkJ&ei=QRFkYqf2KZyO6rQP_qe3mAs&json=", "num_citations": 283, "citedby_url": "/scholar?cites=704947026913270950&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:phyiBJZ5yAkJ:scholar.google.com/&scioq=Learning+to+Remember+Rare+Events&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.03129.pdf?ref=https://githubhelp.com"}, "A hybrid network: Scattering and Convnet": {"container_type": "Publication", "bib": {"title": "A hybrid network: scattering and convnet", "author": ["E Oyallon"], "pub_year": "2016", "venue": "NA", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryPx38qge", "author_id": ["Y8XGVkYAAAAJ"], "url_scholarbib": "/scholar?q=info:GTz9tEPbQgQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2Bhybrid%2Bnetwork:%2BScattering%2Band%2BConvnet%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GTz9tEPbQgQJ&ei=RBFkYu7eJY6pywTd4KPADw&json=", "num_citations": 6, "citedby_url": "/scholar?cites=307048808460401689&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GTz9tEPbQgQJ:scholar.google.com/&scioq=A+hybrid+network:+Scattering+and+Convnet&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryPx38qge"}, "Learning in Implicit Generative Models": {"container_type": "Publication", "bib": {"title": "Learning in implicit generative models", "author": ["S Mohamed", "B Lakshminarayanan"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.03483", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.03483", "author_id": ["cIlDEugAAAAJ", "QYn8RbgAAAAJ"], "url_scholarbib": "/scholar?q=info:BTxvmqAT6NYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bin%2BImplicit%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BTxvmqAT6NYJ&ei=RxFkYr69OI6pywTd4KPADw&json=", "num_citations": 324, "citedby_url": "/scholar?cites=15485648899220126725&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BTxvmqAT6NYJ:scholar.google.com/&scioq=Learning+in+Implicit+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.03483"}, "Towards an automatic Turing test: Learning to evaluate dialogue responses": {"container_type": "Publication", "bib": {"title": "Towards an automatic turing test: Learning to evaluate dialogue responses", "author": ["R Lowe", "M Noseworthy", "IV Serban"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.07149", "author_id": ["iRgYMuEAAAAJ", "Ybj21gEAAAAJ", "0g31OfAAAAAJ"], "url_scholarbib": "/scholar?q=info:3RDiOBncsNYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2Ban%2Bautomatic%2BTuring%2Btest:%2BLearning%2Bto%2Bevaluate%2Bdialogue%2Bresponses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3RDiOBncsNYJ&ei=pA9kYrrZHIySyASZk6HgCA&json=", "num_citations": 301, "citedby_url": "/scholar?cites=15470106720904286429&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3RDiOBncsNYJ:scholar.google.com/&scioq=Towards+an+automatic+Turing+test:+Learning+to+evaluate+dialogue+responses&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.07149"}, "Metacontrol for Adaptive Imagination-Based Optimization": {"container_type": "Publication", "bib": {"title": "Metacontrol for adaptive imagination-based optimization", "author": ["JB Hamrick", "AJ Ballard", "R Pascanu", "O Vinyals"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this\" one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.02670", "author_id": ["2ylcZSsAAAAJ", "syjQhAMAAAAJ", "eSPY8LwAAAAJ", "NkzyCvUAAAAJ"], "url_scholarbib": "/scholar?q=info:ymG8HvB6J-gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMetacontrol%2Bfor%2BAdaptive%2BImagination-Based%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ymG8HvB6J-gJ&ei=qA9kYr2gFrKO6rQPy-CRsA8&json=", "num_citations": 67, "citedby_url": "/scholar?cites=16728474512617398730&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ymG8HvB6J-gJ:scholar.google.com/&scioq=Metacontrol+for+Adaptive+Imagination-Based+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.02670.pdf?ref=https://githubhelp.com"}, "The Variational Walkback Algorithm": {"container_type": "Publication", "bib": {"title": "The variational walkback algorithm", "author": ["A Goyal", "NR Ke", "A Lamb", "Y Bengio"], "pub_year": "2016", "venue": "NA", "abstract": "of walk-back and a new algorithm for learning transition operators or undirected graphical  models. Our algorithm  Variational walk-back increases the temperature along the chain as it is"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkpdnIqlx", "author_id": ["krrh6OUAAAAJ", "dxwPYhQAAAAJ", "", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:y-nhHuwMBosJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BVariational%2BWalkback%2BAlgorithm%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y-nhHuwMBosJ&ei=qg9kYsqMO7KO6rQPy-CRsA8&json=", "num_citations": 3, "citedby_url": "/scholar?cites=10017708629402184139&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y-nhHuwMBosJ:scholar.google.com/&scioq=The+Variational+Walkback+Algorithm&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkpdnIqlx"}, "Inductive Bias of Deep Convolutional Networks through Pooling Geometry": {"container_type": "Publication", "bib": {"title": "Inductive bias of deep convolutional networks through pooling geometry", "author": ["N Cohen", "A Shashua"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.06743", "abstract": "Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional networks to model correlations among regions of their input. We theoretically analyze convolutional arithmetic circuits, and empirically validate our findings on other types of convolutional networks as well"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.06743", "author_id": ["DmzoCRMAAAAJ", "dwi5wvYAAAAJ"], "url_scholarbib": "/scholar?q=info:kgnzltpkPWUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInductive%2BBias%2Bof%2BDeep%2BConvolutional%2BNetworks%2Bthrough%2BPooling%2BGeometry%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kgnzltpkPWUJ&ei=rQ9kYviyOuHDywSSipaYAg&json=", "num_citations": 71, "citedby_url": "/scholar?cites=7295097861408229778&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kgnzltpkPWUJ:scholar.google.com/&scioq=Inductive+Bias+of+Deep+Convolutional+Networks+through+Pooling+Geometry&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.06743"}, "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework": {"container_type": "Publication", "bib": {"title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "author": ["I Higgins", "L Matthey", "A Pal", "C Burgess", "X Glorot"], "pub_year": "2016", "venue": "NA", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE)"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Sy2fzU9gl", "author_id": ["YWVuCKUAAAAJ", "f520HmwAAAAJ", "", "coNoFVAAAAAJ", "_WnkXlkAAAAJ"], "url_scholarbib": "/scholar?q=info:vT8Zkz5uX4kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3Dbeta-VAE:%2BLearning%2BBasic%2BVisual%2BConcepts%2Bwith%2Ba%2BConstrained%2BVariational%2BFramework%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vT8Zkz5uX4kJ&ei=sg9kYr6rFaKUy9YP_JONiAY&json=", "num_citations": 2552, "citedby_url": "/scholar?cites=9898751721018572733&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vT8Zkz5uX4kJ:scholar.google.com/&scioq=beta-VAE:+Learning+Basic+Visual+Concepts+with+a+Constrained+Variational+Framework&hl=en&as_sdt=0,33"}, "The Power of Sparsity in Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "The power of sparsity in convolutional neural networks", "author": ["S Changpinyo", "M Sandler", "A Zhmoginov"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.06257", "abstract": "Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.06257", "author_id": ["2TWx9x0AAAAJ", "IcPc-OUAAAAJ", "jj6IfzEAAAAJ"], "url_scholarbib": "/scholar?q=info:IAUTkKF6ko8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BPower%2Bof%2BSparsity%2Bin%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IAUTkKF6ko8J&ei=tA9kYobgJ7KO6rQPy-CRsA8&json=", "num_citations": 120, "citedby_url": "/scholar?cites=10345466128348939552&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IAUTkKF6ko8J:scholar.google.com/&scioq=The+Power+of+Sparsity+in+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.06257?ref=https://githubhelp.com"}, "Hierarchical Memory Networks": {"container_type": "Publication", "bib": {"title": "Hierarchical memory networks", "author": ["S Chandar", "S Ahn", "H Larochelle", "P Vincent"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "of hierarchical memory network, which can be considered as a hybrid between hard and soft  attention memory networks.  regular memory networks in two of its components: the memory"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.07427", "author_id": ["yxWtZLAAAAAJ", "nfHyDeUAAAAJ", "U89FHq4AAAAJ", "WBCKQMsAAAAJ"], "url_scholarbib": "/scholar?q=info:vqVo-U7ipkQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BMemory%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vqVo-U7ipkQJ&ei=wA9kYsGIG42ymgHg1rfQDQ&json=", "num_citations": 85, "citedby_url": "/scholar?cites=4946890069532779966&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vqVo-U7ipkQJ:scholar.google.com/&scioq=Hierarchical+Memory+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.07427"}, "Loss-aware Binarization of Deep Networks": {"container_type": "Publication", "bib": {"title": "Loss-aware binarization of deep networks", "author": ["L Hou", "Q Yao", "JT Kwok"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01600", "abstract": "and recurrent networks show that the proposed loss-aware binarization algorithm outperforms  existing binarization schemes, and is also more robust for wide and deep networks."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01600", "author_id": ["rnjoL5cAAAAJ", "6PEbAiYAAAAJ", "-oTraZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:sMxmVSuNzdkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLoss-aware%2BBinarization%2Bof%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sMxmVSuNzdkJ&ei=yQ9kYpe4BpGJmwGY-qmYDQ&json=", "num_citations": 175, "citedby_url": "/scholar?cites=15694355493711957168&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sMxmVSuNzdkJ:scholar.google.com/&scioq=Loss-aware+Binarization+of+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01600"}, "Geometry of Polysemy": {"container_type": "Publication", "bib": {"title": "Geometry of polysemy", "author": ["J Mu", "S Bhat", "P Viswanath"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.07569", "abstract": "In this paper, we study the geometry of contexts and polysemy and propose a three-fold  approach (entitled K-Grassmeans) to model target polysemous words in an unsupervised"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.07569", "author_id": ["7koUBUIAAAAJ", "F6x3R1oAAAAJ", "lPycXNcAAAAJ"], "url_scholarbib": "/scholar?q=info:sn6qlYX92WsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeometry%2Bof%2BPolysemy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sn6qlYX92WsJ&ei=zw9kYvLeFs6E6rQP5-KmKA&json=", "num_citations": 17, "citedby_url": "/scholar?cites=7771521382187957938&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sn6qlYX92WsJ:scholar.google.com/&scioq=Geometry+of+Polysemy&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.07569"}, "Significance of Softmax-Based Features over Metric Learning-Based Features": {"container_type": "Publication", "bib": {"title": "Significance of softmax-based features over metric learning-based features", "author": ["S Horiguchi", "D Ikami", "K Aizawa"], "pub_year": "2016", "venue": "NA", "abstract": "The extraction of useful deep features is important for many computer vision tasks. Deep features extracted from classification networks have proved to perform well in those tasks. To obtain features of greater usefulness, end-to-end distance metric learning (DML) has been applied to train the feature extractor directly. End-to-end DML approaches such as Magnet Loss and lifted structured feature embedding show state-of-the-art performance in several image recognition tasks. However, in these DML studies, there were no equitable"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyQWFOVge", "author_id": ["9U5YK3wAAAAJ", "", "CJRhhi0AAAAJ"], "url_scholarbib": "/scholar?q=info:VoJidWF0Yz0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSignificance%2Bof%2BSoftmax-Based%2BFeatures%2Bover%2BMetric%2BLearning-Based%2BFeatures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VoJidWF0Yz0J&ei=0g9kYrTYLJqSy9YP8pKNsAE&json=", "num_citations": 10, "citedby_url": "/scholar?cites=4423507220937998934&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VoJidWF0Yz0J:scholar.google.com/&scioq=Significance+of+Softmax-Based+Features+over+Metric+Learning-Based+Features&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyQWFOVge"}, "LipNet: End-to-End Sentence-level Lipreading": {"container_type": "Publication", "bib": {"title": "Lipnet: End-to-end sentence-level lipreading", "author": ["YM Assael", "B Shillingford", "S Whiteson"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level  lipreading  On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01599", "author_id": ["DwHtHE8AAAAJ", "0tPZW4kAAAAJ", "9zeEI-cAAAAJ"], "url_scholarbib": "/scholar?q=info:vCgJLWXo8nAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLipNet:%2BEnd-to-End%2BSentence-level%2BLipreading%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vCgJLWXo8nAJ&ei=1g9kYuXpE8LZmQHnraWYCA&json=", "num_citations": 250, "citedby_url": "/scholar?cites=8138822997856823484&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vCgJLWXo8nAJ:scholar.google.com/&scioq=LipNet:+End-to-End+Sentence-level+Lipreading&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01599"}, "SGDR: Stochastic Gradient Descent with Warm Restarts": {"container_type": "Publication", "bib": {"title": "Sgdr: Stochastic gradient descent with warm restarts", "author": ["I Loshchilov", "F Hutter"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1608.03983", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.03983", "author_id": ["GladWQwAAAAJ", "YUrxwrkAAAAJ"], "url_scholarbib": "/scholar?q=info:rxYcO-LPyYMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSGDR:%2BStochastic%2BGradient%2BDescent%2Bwith%2BWarm%2BRestarts%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rxYcO-LPyYMJ&ei=2Q9kYsGQO4yuyAT-mrWwCA&json=", "num_citations": 2954, "citedby_url": "/scholar?cites=9496349859848656559&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rxYcO-LPyYMJ:scholar.google.com/&scioq=SGDR:+Stochastic+Gradient+Descent+with+Warm+Restarts&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.03983.pdf?ref=https://githubhelp.com"}, "Reasoning with Memory Augmented Neural Networks for Language Comprehension": {"container_type": "Publication", "bib": {"title": "Reasoning with memory augmented neural networks for language comprehension", "author": ["T Munkhdalai", "H Yu"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.06454", "abstract": "Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.06454", "author_id": ["-fHyrYQAAAAJ", "TyXe64wAAAAJ"], "url_scholarbib": "/scholar?q=info:sF2FBs_zElMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReasoning%2Bwith%2BMemory%2BAugmented%2BNeural%2BNetworks%2Bfor%2BLanguage%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sF2FBs_zElMJ&ei=3Q9kYsPxJ8LZmQHnraWYCA&json=", "num_citations": 25, "citedby_url": "/scholar?cites=5986114925221993904&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sF2FBs_zElMJ:scholar.google.com/&scioq=Reasoning+with+Memory+Augmented+Neural+Networks+for+Language+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.06454"}, "Deep Error-Correcting Output Codes": {"container_type": "Publication", "bib": {"title": "Deep Error-Correcting Output Codes", "author": ["G Zhong", "Y Zheng", "P Zhang", "M Li", "J Dong"], "pub_year": "2016", "venue": "NA", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training+ fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hkz6aNqle", "author_id": ["HqKD-fwAAAAJ", "yyyNMRcAAAAJ", "", "", "iPYdUpAAAAAJ"], "url_scholarbib": "/scholar?q=info:WANqefp7IW4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BError-Correcting%2BOutput%2BCodes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WANqefp7IW4J&ei=4A9kYu-sLsLZmQHnraWYCA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=7935760334112555864&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WANqefp7IW4J:scholar.google.com/&scioq=Deep+Error-Correcting+Output+Codes&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hkz6aNqle"}, "Learning Continuous Semantic Representations of Symbolic Expressions": {"container_type": "Publication", "bib": {"title": "Learning continuous semantic representations of symbolic expressions", "author": ["M Allamanis", "P Chanthirasegaran"], "pub_year": "2017", "venue": "International \u2026", "abstract": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence network, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/allamanis17a.html", "author_id": ["rYsjwZgAAAAJ", "Si7mxAsAAAAJ"], "url_scholarbib": "/scholar?q=info:9-LHvRXsNpsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BContinuous%2BSemantic%2BRepresentations%2Bof%2BSymbolic%2BExpressions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9-LHvRXsNpsJ&ei=5g9kYtClIcLZmQHnraWYCA&json=", "num_citations": 75, "citedby_url": "/scholar?cites=11184386302743667447&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9-LHvRXsNpsJ:scholar.google.com/&scioq=Learning+Continuous+Semantic+Representations+of+Symbolic+Expressions&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/allamanis17a/allamanis17a.pdf"}, "Stochastic Neural Networks for Hierarchical Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["C Florensa", "Y Duan", "P Abbeel"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1704.03012", "abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.03012", "author_id": ["7_7op_IAAAAJ", "EMDboA4AAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:cLCsdpbOXNkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BNeural%2BNetworks%2Bfor%2BHierarchical%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cLCsdpbOXNkJ&ei=6g9kYr_tC8LZmQHnraWYCA&json=", "num_citations": 300, "citedby_url": "/scholar?cites=15662620749719187568&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cLCsdpbOXNkJ:scholar.google.com/&scioq=Stochastic+Neural+Networks+for+Hierarchical+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.03012"}, "Third Person Imitation Learning": {"container_type": "Publication", "bib": {"title": "Third-person imitation learning", "author": ["BC Stadie", "P Abbeel", "I Sutskever"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.01703", "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.01703", "author_id": ["lEV5F5kAAAAJ", "vtwH6GkAAAAJ", "x04W_mMAAAAJ"], "url_scholarbib": "/scholar?q=info:axFetpWiSekJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThird%2BPerson%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=axFetpWiSekJ&ei=7Q9kYsbDDo2ymgHg1rfQDQ&json=", "num_citations": 198, "citedby_url": "/scholar?cites=16810145848030531947&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:axFetpWiSekJ:scholar.google.com/&scioq=Third+Person+Imitation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.01703"}, "Gated-Attention Readers for Text Comprehension": {"container_type": "Publication", "bib": {"title": "Gated-attention readers for text comprehension", "author": ["B Dhingra", "H Liu", "Z Yang", "WW Cohen"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.01549", "author_id": ["SLISZFAAAAAJ", "IMkVH_8AAAAJ", "7qXxyJkAAAAJ", "8ys-38kAAAAJ"], "url_scholarbib": "/scholar?q=info:o7OujSrcqS0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGated-Attention%2BReaders%2Bfor%2BText%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o7OujSrcqS0J&ei=7w9kYv6hKpqSy9YP8pKNsAE&json=", "num_citations": 395, "citedby_url": "/scholar?cites=3290403078094631843&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o7OujSrcqS0J:scholar.google.com/&scioq=Gated-Attention+Readers+for+Text+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.01549"}, "Lossy Image Compression with Compressive Autoencoders": {"container_type": "Publication", "bib": {"title": "Lossy image compression with compressive autoencoders", "author": ["L Theis", "W Shi", "A Cunningham", "F Husz\u00e1r"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.00395", "abstract": "Figure 2: Illustration of the compressive autoencoder architecture used in this paper. Inspired  by the work of Shi et al. (2016), most convolutions are performed in a downsampled space"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.00395", "author_id": ["dgVYYngAAAAJ", "cQYo4SkAAAAJ", "", "koQCVT4AAAAJ"], "url_scholarbib": "/scholar?q=info:F21vMJfujbcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLossy%2BImage%2BCompression%2Bwith%2BCompressive%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F21vMJfujbcJ&ei=8w9kYuPwG--Sy9YPs_mY8AM&json=", "num_citations": 729, "citedby_url": "/scholar?cites=13226490013777095959&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F21vMJfujbcJ:scholar.google.com/&scioq=Lossy+Image+Compression+with+Compressive+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.00395"}, "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization": {"container_type": "Publication", "bib": {"title": "Adjusting for dropout variance in batch normalization and weight initialization", "author": ["D Hendrycks", "K Gimpel"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1607.02488", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1607.02488", "author_id": ["czyretsAAAAJ", "kDHs7DYAAAAJ"], "url_scholarbib": "/scholar?q=info:pk1rv2lWP28J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdjusting%2Bfor%2BDropout%2BVariance%2Bin%2BBatch%2BNormalization%2Band%2BWeight%2BInitialization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pk1rv2lWP28J&ei=_g9kYoCHDoyuyAT-mrWwCA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=8016220873925807526&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pk1rv2lWP28J:scholar.google.com/&scioq=Adjusting+for+Dropout+Variance+in+Batch+Normalization+and+Weight+Initialization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1607.02488"}, "Prototypical Networks for Few-shot Learning": {"container_type": "Publication", "bib": {"title": "Prototypical networks for few-shot learning", "author": ["J Snell", "K Swersky", "R Zemel"], "pub_year": "2017", "venue": "Advances in neural \u2026", "abstract": "We propose Prototypical Networks for the problem of few-shot classification, where a  training  set, given only a small number of examples of each new class. Prototypical Networks learn"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.neurips.cc/paper/6996-prototypical-networks-for-few-shot-learning", "author_id": ["MbXKAK8AAAAJ", "IrixA8MAAAAJ", "iBeDoRAAAAAJ"], "url_scholarbib": "/scholar?q=info:nis2f0vbCXkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPrototypical%2BNetworks%2Bfor%2BFew-shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nis2f0vbCXkJ&ei=CRBkYu7rEouKmgGY1YjABQ&json=", "num_citations": 3992, "citedby_url": "/scholar?cites=8721743270682962846&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nis2f0vbCXkJ:scholar.google.com/&scioq=Prototypical+Networks+for+Few-shot+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://proceedings.neurips.cc/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf"}, "Improving Policy Gradient by Exploring Under-appreciated Rewards": {"container_type": "Publication", "bib": {"title": "Improving policy gradient by exploring under-appreciated rewards", "author": ["O Nachum", "M Norouzi", "D Schuurmans"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.09321", "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09321", "author_id": ["C-ZlBWMAAAAJ", "Lncr-VoAAAAJ", "xaQuPloAAAAJ"], "url_scholarbib": "/scholar?q=info:WMqCBsu377sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BPolicy%2BGradient%2Bby%2BExploring%2BUnder-appreciated%2BRewards%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WMqCBsu377sJ&ei=DBBkYsCPBJGJmwGY-qmYDQ&json=", "num_citations": 33, "citedby_url": "/scholar?cites=13542244687141849688&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WMqCBsu377sJ:scholar.google.com/&scioq=Improving+Policy+Gradient+by+Exploring+Under-appreciated+Rewards&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09321"}, "Sequence generation with a physiologically plausible model of handwriting and Recurrent Mixture Density Networks": {"container_type": "Publication", "bib": {"title": "Sequence generation with a physiologically plausible model of handwriting and Recurrent Mixture Density Networks", "author": ["D Berio", "M Akten", "FF Leymarie", "M Grierson"], "pub_year": "2016", "venue": "NA", "abstract": "The purpose of this study is to explore the feasibility and potential benefits of using a physiological plausible model of handwriting as a feature representation for sequence generation with recurrent mixture density networks. We build on recent results in handwriting prediction developed by Graves (2013), and we focus on generating sequences that possess the statistical and dynamic qualities of handwriting and calligraphic art forms. Rather than model raw sequence data, we first preprocess and reconstruct the input training"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1S083cgx", "author_id": ["th_S5xEAAAAJ", "giZdm0sAAAAJ", "Lahlh-cAAAAJ", ""], "url_scholarbib": "/scholar?q=info:E-tPRcvBXfQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSequence%2Bgeneration%2Bwith%2Ba%2Bphysiologically%2Bplausible%2Bmodel%2Bof%2Bhandwriting%2Band%2BRecurrent%2BMixture%2BDensity%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=E-tPRcvBXfQJ&ei=EBBkYvPjCpLeyQTE46-QAg&json=", "num_citations": 3, "citedby_url": "/scholar?cites=17608443196873894675&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:E-tPRcvBXfQJ:scholar.google.com/&scioq=Sequence+generation+with+a+physiologically+plausible+model+of+handwriting+and+Recurrent+Mixture+Density+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1S083cgx"}, "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network": {"container_type": "Publication", "bib": {"title": "Ternary weight decomposition and binary activation encoding for fast and compact neural network", "author": ["M Ambai", "T Matsumoto", "T Yamashita", "H Fujiyoshi"], "pub_year": "2016", "venue": "NA", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values,{-1, 0,+ 1}, it only consumes 2 bits per element. At test-time, an activation vector that"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByOK0rwlx", "author_id": ["", "", "hkguTPgAAAAJ", "CIHKZpEAAAAJ"], "url_scholarbib": "/scholar?q=info:m18NwPj9dkcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTernary%2BWeight%2BDecomposition%2Band%2BBinary%2BActivation%2BEncoding%2Bfor%2BFast%2Band%2BCompact%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m18NwPj9dkcJ&ei=ExBkYuLtGoySyASZk6HgCA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=5149582468760559515&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m18NwPj9dkcJ:scholar.google.com/&scioq=Ternary+Weight+Decomposition+and+Binary+Activation+Encoding+for+Fast+and+Compact+Neural+Network&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByOK0rwlx"}, "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks": {"container_type": "Publication", "bib": {"title": "Softtarget regularization: An effective technique to reduce over-fitting in neural networks", "author": ["A Aghajanyan"], "pub_year": "2017", "venue": "2017 3rd IEEE International Conference on \u2026", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014),(Wan et al., 2013),(Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/7985811/", "author_id": ["KxQfzRcAAAAJ"], "url_scholarbib": "/scholar?q=info:bH7EasJHEEgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSoftTarget%2BRegularization:%2BAn%2BEffective%2BTechnique%2Bto%2BReduce%2BOver-Fitting%2Bin%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bH7EasJHEEgJ&ei=FxBkYvrYDpLeyQTE46-QAg&json=", "num_citations": 14, "citedby_url": "/scholar?cites=5192729270698671724&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bH7EasJHEEgJ:scholar.google.com/&scioq=SoftTarget+Regularization:+An+Effective+Technique+to+Reduce+Over-Fitting+in+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.06693"}, "Low-rank passthrough neural networks": {"container_type": "Publication", "bib": {"title": "Low-rank passthrough neural networks", "author": ["AVM Barone"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1603.03116", "abstract": "Various common deep learning architectures, such as LSTMs, GRUs, Resnets and Highway Networks, employ state passthrough connections that support training with high feed-forward depth or recurrence over many time steps. These\" Passthrough Networks\" architectures also enable the decoupling of the network state size from the number of parameters of the network, a possibility has been studied by\\newcite {Sak2014} with their low-rank parametrization of the LSTM. In this work we extend this line of research, proposing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1603.03116", "author_id": ["VXdL2a0AAAAJ"], "url_scholarbib": "/scholar?q=info:DPian58lm3wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLow-rank%2Bpassthrough%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DPian58lm3wJ&ei=GxBkYvPAHcLZmQHnraWYCA&json=", "num_citations": 12, "citedby_url": "/scholar?cites=8978811649600976908&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DPian58lm3wJ:scholar.google.com/&scioq=Low-rank+passthrough+neural+networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1603.03116"}, "The Preimage of Rectifier Network Activities": {"container_type": "Publication", "bib": {"title": "The preimage of rectifier network activities", "author": ["S Carlsson", "H Azizpour", "A Razavian"], "pub_year": "2016", "venue": "NA", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJcLcw9xg", "author_id": ["pSQ6NH4AAAAJ", "t6CRgJsAAAAJ", "E3fqfDIAAAAJ"], "url_scholarbib": "/scholar?q=info:zDtWypW8acgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BPreimage%2Bof%2BRectifier%2BNetwork%2BActivities%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zDtWypW8acgJ&ei=HhBkYobvHY2ymgHg1rfQDQ&json=", "num_citations": 8, "citedby_url": "/scholar?cites=14441281031671004108&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zDtWypW8acgJ:scholar.google.com/&scioq=The+Preimage+of+Rectifier+Network+Activities&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJcLcw9xg"}, "Programming With a Differentiable Forth Interpreter": {"container_type": "Publication", "bib": {"title": "Programming with a differentiable forth interpreter", "author": ["M Bo\u0161njak", "T Rockt\u00e4schel"], "pub_year": "2017", "venue": "\u2026 on machine learning", "abstract": "Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/bosnjak17a.html?ref=https://githubhelp.com", "author_id": ["JDaHecMAAAAJ", "mWBY8aIAAAAJ"], "url_scholarbib": "/scholar?q=info:lGno1OTzvgUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgramming%2BWith%2Ba%2BDifferentiable%2BForth%2BInterpreter%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lGno1OTzvgUJ&ei=IRBkYpqLJIuKmgGY1YjABQ&json=", "num_citations": 78, "citedby_url": "/scholar?cites=414036379914758548&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lGno1OTzvgUJ:scholar.google.com/&scioq=Programming+With+a+Differentiable+Forth+Interpreter&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/bosnjak17a/bosnjak17a.pdf"}, "Hierarchical Multiscale Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J Chung", "S Ahn", "Y Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.01704", "abstract": "Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.01704", "author_id": ["2HE7cTEAAAAJ", "nfHyDeUAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:XMJ3XbRYZTIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BMultiscale%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XMJ3XbRYZTIJ&ei=LhBkYvHoEuiSy9YPp-OyiAE&json=", "num_citations": 496, "citedby_url": "/scholar?cites=3631406206229660252&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XMJ3XbRYZTIJ:scholar.google.com/&scioq=Hierarchical+Multiscale+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.01704.pdf%20http://arxiv.org/abs/1609.01704"}, "Inverse Problems in Computer Vision using  Adversarial  Imagination Priors": {"container_type": "Publication", "bib": {"title": "Inverse problems in computer vision using adversarial imagination priors", "author": ["HYF Tung", "K Fragkiadaki"], "pub_year": "2016", "venue": "NA", "abstract": "Given an image, humans effortlessly run the image formation process backwards in their minds: they can tell albedo from shading, foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H13F3Pqll", "author_id": ["SMAmWOQAAAAJ", "FWp7728AAAAJ"], "url_scholarbib": "/scholar?q=info:xJvJWmpSM9sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInverse%2BProblems%2Bin%2BComputer%2BVision%2Busing%2B%2BAdversarial%2B%2BImagination%2BPriors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xJvJWmpSM9sJ&ei=NRBkYrWHJOHDywSSipaYAg&json=", "num_citations": 1, "citedby_url": "/scholar?cites=15795058934861634500&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xJvJWmpSM9sJ:scholar.google.com/&scioq=Inverse+Problems+in+Computer+Vision+using++Adversarial++Imagination+Priors&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H13F3Pqll"}, "Neural Data Filter for Bootstrapping Stochastic Gradient Descent": {"container_type": "Publication", "bib": {"title": "Neural data filter for bootstrapping stochastic gradient descent", "author": ["Y Fan", "F Tian", "T Qin", "TY Liu"], "pub_year": "2016", "venue": "NA", "abstract": "Mini-batch based Stochastic Gradient Descent (SGD) has been widely used to train deep neural networks efficiently. In this paper, we design a general framework to automatically and adaptively select training data for SGD. The framework is based on neural networks and we call it\\emph {\\textbf {N} eural\\textbf {D} ata\\textbf {F} ilter}(\\textbf {NDF}). In Neural Data Filter, the whole training process of the original neural network is monitored and supervised by a deep reinforcement network, which controls whether to filter some data in sequentially"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SyJNmVqgg", "author_id": ["ZSevpaAAAAAJ", "SZbCPDEAAAAJ", "Bl4SRU0AAAAJ", "Nh832fgAAAAJ"], "url_scholarbib": "/scholar?q=info:d5tVq3hamCEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BData%2BFilter%2Bfor%2BBootstrapping%2BStochastic%2BGradient%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d5tVq3hamCEJ&ei=OhBkYoLPHo2ymgHg1rfQDQ&json=", "num_citations": 6, "citedby_url": "/scholar?cites=2420784274028731255&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:d5tVq3hamCEJ:scholar.google.com/&scioq=Neural+Data+Filter+for+Bootstrapping+Stochastic+Gradient+Descent&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SyJNmVqgg"}, "Automatic Rule Extraction from Long Short Term Memory Networks": {"container_type": "Publication", "bib": {"title": "Automatic rule extraction from long short term memory networks", "author": ["WJ Murdoch", "A Szlam"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.02540", "abstract": "Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.02540", "author_id": ["3JNHcMwAAAAJ", "u3-FxUgAAAAJ"], "url_scholarbib": "/scholar?q=info:OCtAMbYXT7IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomatic%2BRule%2BExtraction%2Bfrom%2BLong%2BShort%2BTerm%2BMemory%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OCtAMbYXT7IJ&ei=RRBkYq_mJIySyASZk6HgCA&json=", "num_citations": 94, "citedby_url": "/scholar?cites=12848514333189090104&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OCtAMbYXT7IJ:scholar.google.com/&scioq=Automatic+Rule+Extraction+from+Long+Short+Term+Memory+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.02540"}, "Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear": {"container_type": "Publication", "bib": {"title": "Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear", "author": ["ZC Lipton", "J Gao", "L Li", "J Chen", "L Deng"], "pub_year": "2016", "venue": "NA", "abstract": "To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically relive catastrophic mistakes. Many real-world environments where people might"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1tHvHKge", "author_id": ["MN9Kfg8AAAAJ", "CQ1cqKkAAAAJ", "Rqy5KDEAAAAJ", "jQeFWdoAAAAJ", "GQWTo4MAAAAJ"], "url_scholarbib": "/scholar?q=info:AdSC9lTKlrQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCombating%2BDeep%2BReinforcement%2BLearning%2527s%2BSisyphean%2BCurse%2Bwith%2BIntrinsic%2BFear%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AdSC9lTKlrQJ&ei=SRBkYpLUIMLZmQHnraWYCA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=13012810639595459585&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AdSC9lTKlrQJ:scholar.google.com/&scioq=Combating+Deep+Reinforcement+Learning%27s+Sisyphean+Curse+with+Intrinsic+Fear&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1tHvHKge"}, "Adversarial Feature Learning": {"container_type": "Publication", "bib": {"title": "Adversarial feature learning", "author": ["J Donahue", "P Kr\u00e4henb\u00fchl", "T Darrell"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.09782", "abstract": "We evaluate the feature learning capabilities of BiGANs by first training them unsupervised   encoder\u2019s learned feature representations for use in auxiliary supervised learning tasks. To"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.09782", "author_id": ["UfbuDH8AAAAJ", "dzOd2hgAAAAJ", "bh-uRFMAAAAJ"], "url_scholarbib": "/scholar?q=info:kV2pvi7P9ZMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BFeature%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kV2pvi7P9ZMJ&ei=SxBkYsnNM5qSy9YP8pKNsAE&json=", "num_citations": 1565, "citedby_url": "/scholar?cites=10661655492543733137&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kV2pvi7P9ZMJ:scholar.google.com/&scioq=Adversarial+Feature+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.09782"}, "Soft Weight-Sharing for Neural Network Compression": {"container_type": "Publication", "bib": {"title": "Soft weight-sharing for neural network compression", "author": ["K Ullrich", "E Meeds", "M Welling"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.04008", "abstract": "The success of deep learning in numerous application domains created the de-sire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al.(2015a) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of soft weight"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.04008", "author_id": ["TMIPmNAAAAAJ", "oxrYi1cAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:xzEUKcpsRj8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSoft%2BWeight-Sharing%2Bfor%2BNeural%2BNetwork%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xzEUKcpsRj8J&ei=WBBkYqydG-iSy9YPp-OyiAE&json=", "num_citations": 334, "citedby_url": "/scholar?cites=4559451288287588807&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xzEUKcpsRj8J:scholar.google.com/&scioq=Soft+Weight-Sharing+for+Neural+Network+Compression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.04008.pdf?ref=https://githubhelp.com"}, "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning", "author": ["S Sharma", "A Srinivas", "B Ravindran"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.06054", "abstract": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, ie, select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.06054", "author_id": ["P6l5tBcAAAAJ", "GhrKC1gAAAAJ", "nGUcGrYAAAAJ"], "url_scholarbib": "/scholar?q=info:llIixxrMQuQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BRepeat:%2BFine%2BGrained%2BAction%2BRepetition%2Bfor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=llIixxrMQuQJ&ei=XBBkYsDYFvmQ6rQP5OqKqAo&json=", "num_citations": 63, "citedby_url": "/scholar?cites=16447933204492604054&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:llIixxrMQuQJ:scholar.google.com/&scioq=Learning+to+Repeat:+Fine+Grained+Action+Repetition+for+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.06054"}, "Deep Multi-task Representation Learning: A Tensor Factorisation Approach": {"container_type": "Publication", "bib": {"title": "Deep multi-task representation learning: A tensor factorisation approach", "author": ["Y Yang", "T Hospedales"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.06391", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.06391", "author_id": ["F7PtrL8AAAAJ", "nHhtvqkAAAAJ"], "url_scholarbib": "/scholar?q=info:2lCLaq8wv9UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BMulti-task%2BRepresentation%2BLearning:%2BA%2BTensor%2BFactorisation%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2lCLaq8wv9UJ&ei=XxBkYt6sK42ymgHg1rfQDQ&json=", "num_citations": 213, "citedby_url": "/scholar?cites=15402082780595310810&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2lCLaq8wv9UJ:scholar.google.com/&scioq=Deep+Multi-task+Representation+Learning:+A+Tensor+Factorisation+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.06391"}, "A Neural Knowledge Language Model": {"container_type": "Publication", "bib": {"title": "A neural knowledge language model", "author": ["S Ahn", "H Choi", "T P\u00e4rnamaa", "Y Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1608.00318", "abstract": "In this paper, we presented a novel Neural Knowledge Language Model (NKLM) that  brings the symbolic knowledge from a knowledge graph into the expressive power of RNN"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.00318", "author_id": ["nfHyDeUAAAAJ", "j3ck5XUAAAAJ", "dZzMiwgAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:VH3b8aC_wVYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BNeural%2BKnowledge%2BLanguage%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VH3b8aC_wVYJ&ei=ZRBkYqD1BpGJmwGY-qmYDQ&json=", "num_citations": 125, "citedby_url": "/scholar?cites=6251488455740325204&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VH3b8aC_wVYJ:scholar.google.com/&scioq=A+Neural+Knowledge+Language+Model&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.00318?ref=https://githubhelp.com"}, "Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Semi-supervised learning with context-conditional generative adversarial networks", "author": ["E Denton", "S Gross", "R Fergus"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.06430", "abstract": "We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.06430", "author_id": ["pcFsc-AAAAAJ", "8qNEbiUAAAAJ", "GgQ9GEkAAAAJ"], "url_scholarbib": "/scholar?q=info:UI3NFjHXkX4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-Supervised%2BLearning%2Bwith%2BContext-Conditional%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UI3NFjHXkX4J&ei=aBBkYsz1EPmQ6rQP5OqKqAo&json=", "num_citations": 137, "citedby_url": "/scholar?cites=9120307326237904208&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UI3NFjHXkX4J:scholar.google.com/&scioq=Semi-Supervised+Learning+with+Context-Conditional+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.06430"}, "Surprisal-Driven Feedback in Recurrent Networks": {"container_type": "Publication", "bib": {"title": "Surprisal-driven feedback in recurrent networks", "author": ["KM Rocki"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1608.06027", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.06027", "author_id": ["CIs8LAEAAAAJ"], "url_scholarbib": "/scholar?q=info:x5hVc1H_RToJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSurprisal-Driven%2BFeedback%2Bin%2BRecurrent%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x5hVc1H_RToJ&ei=axBkYrXGB5LeyQTE46-QAg&json=", "num_citations": 10, "citedby_url": "/scholar?cites=4199042952885278919&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x5hVc1H_RToJ:scholar.google.com/&scioq=Surprisal-Driven+Feedback+in+Recurrent+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.06027"}, "Adversarial examples in the physical world": {"container_type": "Publication", "bib": {"title": "Adversarial examples in the physical world", "author": ["A Kurakin", "I Goodfellow", "S Bengio"], "pub_year": "2016", "venue": "NA", "abstract": "Images,\u201d we review different methods which we used to generate adversarial examples. This  is followed in section \u201cPhotos of Adversarial Examples\u201d by details about our \u201cphysical world"}, "filled": false, "gsrank": 1, "pub_url": "https://books.google.com/books?hl=en&lr=&id=ekkPEAAAQBAJ&oi=fnd&pg=PA99&dq=Adversarial+examples+in+the+physical+world&ots=vRBvhNbt4q&sig=2CsjnpW1SMvFUFCz-AoBLSOI3lc", "author_id": ["nCh4qyMAAAAJ", "iYN86KEAAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:RSFPERpAqAMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2Bexamples%2Bin%2Bthe%2Bphysical%2Bworld%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RSFPERpAqAMJ&ei=dRBkYpuZMeHDywSSipaYAg&json=", "num_citations": 3655, "citedby_url": "/scholar?cites=263531058904899909&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RSFPERpAqAMJ:scholar.google.com/&scioq=Adversarial+examples+in+the+physical+world&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1607.02533?ref=https://githubhelp.com"}, "A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games": {"container_type": "Publication", "bib": {"title": "A deep learning approach for joint video frame and reward prediction in atari games", "author": ["F Leibfried", "N Kushman", "K Hofmann"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.07078", "abstract": "Reinforcement learning is concerned with identifying reward-maximizing behaviour policies in environments that are initially unknown. State-of-the-art reinforcement learning approaches, such as deep Q-networks, are model-free and learn to act effectively across a wide range of environments such as Atari games, but require huge amounts of data. Model-based techniques are more data-efficient, but need to acquire explicit knowledge about the environment. In this paper, we take a step towards using model-based techniques in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.07078", "author_id": ["-sxdNd0AAAAJ", "I_YIc0YAAAAJ", "bHsjbLwAAAAJ"], "url_scholarbib": "/scholar?q=info:J22VmXUG5IcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDeep%2BLearning%2BApproach%2Bfor%2BJoint%2BVideo%2BFrame%2Band%2BReward%2BPrediction%2Bin%2BAtari%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J22VmXUG5IcJ&ei=eBBkYoO8KOHDywSSipaYAg&json=", "num_citations": 51, "citedby_url": "/scholar?cites=9791958591967948071&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:J22VmXUG5IcJ:scholar.google.com/&scioq=A+Deep+Learning+Approach+for+Joint+Video+Frame+and+Reward+Prediction+in+Atari+Games&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.07078"}, "On the Quantitative Analysis of Decoder-Based Generative Models": {"container_type": "Publication", "bib": {"title": "On the quantitative analysis of decoder-based generative models", "author": ["Y Wu", "Y Burda", "R Salakhutdinov", "R Grosse"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.04273", "author_id": ["bOQGfFIAAAAJ", "Amky96kAAAAJ", "ITZ1e7MAAAAJ", "xgQd1qgAAAAJ"], "url_scholarbib": "/scholar?q=info:S_-Leb1Z0UgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BQuantitative%2BAnalysis%2Bof%2BDecoder-Based%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S_-Leb1Z0UgJ&ei=exBkYrmzEpLeyQTE46-QAg&json=", "num_citations": 216, "citedby_url": "/scholar?cites=5247073711186247499&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:S_-Leb1Z0UgJ:scholar.google.com/&scioq=On+the+Quantitative+Analysis+of+Decoder-Based+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.04273.pdf%5D"}, "Learning to superoptimize programs": {"container_type": "Publication", "bib": {"title": "Learning to superoptimize programs", "author": ["R Bunel", "A Desmaison", "MP Kumar", "PHS Torr"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "to learn a prior distribution (unconditioned on the input program)  contains automatically  generated programs that introduce  can learn a conditional distribution given the initial program"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01787", "author_id": ["7cqQFSoAAAAJ", "_cHRq1kAAAAJ", "BfmcfEAAAAAJ", "kPxa2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:CSQt46T0qpMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2Bsuperoptimize%2Bprograms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CSQt46T0qpMJ&ei=hhBkYpLZG4OEmgHx-5DADA&json=", "num_citations": 30, "citedby_url": "/scholar?cites=10640586058639418377&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CSQt46T0qpMJ:scholar.google.com/&scioq=Learning+to+superoptimize+programs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01787"}, "Understanding deep learning requires rethinking generalization": {"container_type": "Publication", "bib": {"title": "Understanding deep learning (still) requires rethinking generalization", "author": ["C Zhang", "S Bengio", "M Hardt", "B Recht"], "pub_year": "2021", "venue": "Communications of the \u2026", "abstract": "size, successful deep artificial neural networks can exhibit a  Conventional wisdom attributes  small generalization error  to explain why large neural networks generalize well in practice."}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3446776", "author_id": ["l_G2vr0AAAAJ", "Vs-MdPcAAAAJ", "adnTgaAAAAAJ", "a_dbdxAAAAAJ"], "url_scholarbib": "/scholar?q=info:HTqel5TgRqoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2Bdeep%2Blearning%2Brequires%2Brethinking%2Bgeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HTqel5TgRqoJ&ei=lRBkYrXXA5GJmwGY-qmYDQ&json=", "num_citations": 3868, "citedby_url": "/scholar?cites=12269741163621005853&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HTqel5TgRqoJ:scholar.google.com/&scioq=Understanding+deep+learning+requires+rethinking+generalization&hl=en&as_sdt=0,33", "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3446776"}, "Modelling Relational Time Series using Gaussian Embeddings": {"container_type": "Publication", "bib": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "author": ["L Dos Santos", "A Ziat", "L Denoyer", "B Piwowarski"], "pub_year": "2016", "venue": "NA", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc. We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJ7O61Yxe", "author_id": ["TNPp0cwAAAAJ", "", "", "RP4lPE8AAAAJ"], "url_scholarbib": "/scholar?q=info:aj4deJrGnb0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModelling%2BRelational%2BTime%2BSeries%2Busing%2BGaussian%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aj4deJrGnb0J&ei=mRBkYuvALOiSy9YPp-OyiAE&json=", "num_citations": 1, "citedby_url": "/scholar?cites=13663295211254398570&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aj4deJrGnb0J:scholar.google.com/&scioq=Modelling+Relational+Time+Series+using+Gaussian+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJ7O61Yxe"}, "Generating Long and Diverse Responses with Neural Conversation Models": {"container_type": "Publication", "bib": {"title": "Generating long and diverse responses with neural conversation models", "author": ["L Shao", "S Gouws", "D Britz", "A Goldie", "B Strope"], "pub_year": "2016", "venue": "NA", "abstract": "Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models--purely data-driven systems trained end-to-end on dialogue corpora--have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJDdiT9gl", "author_id": ["gKtR2X4AAAAJ", "lLTdYUYAAAAJ", "zZNKHdEAAAAJ", "uyFDSDwAAAAJ", "GwSCM2AAAAAJ"], "url_scholarbib": "/scholar?q=info:TZQyLaK0XW0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BLong%2Band%2BDiverse%2BResponses%2Bwith%2BNeural%2BConversation%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TZQyLaK0XW0J&ei=nhBkYpGbJYuKmgGY1YjABQ&json=", "num_citations": 60, "citedby_url": "/scholar?cites=7880653531604227149&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TZQyLaK0XW0J:scholar.google.com/&scioq=Generating+Long+and+Diverse+Responses+with+Neural+Conversation+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJDdiT9gl"}, "Neural Program Lattices": {"container_type": "Publication", "bib": {"title": "Neural program lattices", "author": ["C Li", "D Tarlow", "AL Gaunt", "M Brockschmidt", "N Kushman"], "pub_year": "2016", "venue": "NA", "abstract": "Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by  composing low-level programs to  the neural lattice structure after which Neural Program Lattices"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJjiFK5gx", "author_id": ["", "oavgGaMAAAAJ", "IzwvqPIAAAAJ", "pF27eLMAAAAJ", "I_YIc0YAAAAJ"], "url_scholarbib": "/scholar?q=info:CRh38o7XyGYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BProgram%2BLattices%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CRh38o7XyGYJ&ei=phBkYs6PMoySyASZk6HgCA&json=", "num_citations": 28, "citedby_url": "/scholar?cites=7406406596163999753&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CRh38o7XyGYJ:scholar.google.com/&scioq=Neural+Program+Lattices&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJjiFK5gx"}, "Collaborative Deep Embedding via Dual Networks": {"container_type": "Publication", "bib": {"title": "Collaborative Deep Embedding via Dual Networks", "author": ["Y Xiong", "D Lin", "H Niu", "JI Cheng", "Z Li"], "pub_year": "2016", "venue": "NA", "abstract": "Despite the long history of research on recommender systems, current approaches still face a number of challenges in practice, eg the difficulties in handling new items, the high diversity of user interests, and the noisiness and sparsity of observations. Many of such difficulties stem from the lack of expressive power to capture the complex relations between items and users. This paper presents a new method to tackle this problem, called Collaborative Deep Embedding. In this method, a pair of dual networks, one for encoding"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1w7Jdqxl", "author_id": ["", "GMzzRRUAAAAJ", "", "GKHIanEAAAAJ", "XboZC1AAAAAJ"], "url_scholarbib": "/scholar?q=info:o2wwM5PWjKsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCollaborative%2BDeep%2BEmbedding%2Bvia%2BDual%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o2wwM5PWjKsJ&ei=qhBkYq2bBs6E6rQP5-KmKA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:o2wwM5PWjKsJ:scholar.google.com/&scioq=Collaborative+Deep+Embedding+via+Dual+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1w7Jdqxl"}, "Revisiting Denoising Auto-Encoders": {"container_type": "Publication", "bib": {"title": "Revisiting denoising auto-encoders", "author": ["LGS Giraldo"], "pub_year": "2016", "venue": "NA", "abstract": "auto-encoders, one uses the logsig units g(x)=1/(1 + exp(\u2212x)), and the other a rectified linear  units (ReLU) g(x) = max{0,x}. Figure 1 shows the outputs of the three auto-encoders on the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1ZXuTolx", "author_id": [""], "url_scholarbib": "/scholar?q=info:1z9AxlbCwwIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRevisiting%2BDenoising%2BAuto-Encoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1z9AxlbCwwIJ&ei=tRBkYuy9HZLeyQTE46-QAg&json=", "num_citations": 2, "citedby_url": "/scholar?cites=199216486483509207&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1z9AxlbCwwIJ:scholar.google.com/&scioq=Revisiting+Denoising+Auto-Encoders&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1ZXuTolx"}, "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling": {"container_type": "Publication", "bib": {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["H Inan", "K Khosravi", "R Socher"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01462", "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01462", "author_id": ["326a3s0AAAAJ", "HOzdZp0AAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:jN6P0T9zy8IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTying%2BWord%2BVectors%2Band%2BWord%2BClassifiers:%2BA%2BLoss%2BFramework%2Bfor%2BLanguage%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jN6P0T9zy8IJ&ei=uBBkYsyxEI2ymgHg1rfQDQ&json=", "num_citations": 347, "citedby_url": "/scholar?cites=14036439381566283404&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jN6P0T9zy8IJ:scholar.google.com/&scioq=Tying+Word+Vectors+and+Word+Classifiers:+A+Loss+Framework+for+Language+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01462"}, "Topology and Geometry of Half-Rectified Network Optimization": {"container_type": "Publication", "bib": {"title": "Topology and geometry of half-rectified network optimization", "author": ["CD Freeman", "J Bruna"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01540", "abstract": "The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01540", "author_id": ["t5Xsx0IAAAAJ", "L4bNmsMAAAAJ"], "url_scholarbib": "/scholar?q=info:ImgIaKKxPk8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTopology%2Band%2BGeometry%2Bof%2BHalf-Rectified%2BNetwork%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ImgIaKKxPk8J&ei=uxBkYpB5opTL1g_8k42IBg&json=", "num_citations": 167, "citedby_url": "/scholar?cites=5710196688640567330&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ImgIaKKxPk8J:scholar.google.com/&scioq=Topology+and+Geometry+of+Half-Rectified+Network+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01540.pdf,"}, "Dataset Augmentation in Feature Space": {"container_type": "Publication", "bib": {"title": "Dataset augmentation in feature space", "author": ["T DeVries", "GW Taylor"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.05538", "abstract": "Dataset augmentation, the practice of applying a wide array  -agnostic approach to dataset  augmentation. We start with  not in input space, but in a learned feature space. A re-kindling of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.05538", "author_id": ["VFPOOsoAAAAJ", "PUeKU8kAAAAJ"], "url_scholarbib": "/scholar?q=info:EICwXvnowEMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDataset%2BAugmentation%2Bin%2BFeature%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EICwXvnowEMJ&ei=vhBkYp_8CIySyASZk6HgCA&json=", "num_citations": 290, "citedby_url": "/scholar?cites=4882158153802743824&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EICwXvnowEMJ:scholar.google.com/&scioq=Dataset+Augmentation+in+Feature+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.05538.pdf&usg=ALkJrhjjYSurI0w0fjJ6JZVJk4_YzWwB0w"}, "Discovering objects and their relations from entangled scene representations": {"container_type": "Publication", "bib": {"title": "Discovering objects and their relations from entangled scene representations", "author": ["D Raposo", "A Santoro", "D Barrett", "R Pascanu"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.05068", "author_id": ["4iINIzkAAAAJ", "evIkDWoAAAAJ", "Whh_d2EAAAAJ", "eSPY8LwAAAAJ"], "url_scholarbib": "/scholar?q=info:IBmIlirA-ewJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscovering%2Bobjects%2Band%2Btheir%2Brelations%2Bfrom%2Bentangled%2Bscene%2Brepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IBmIlirA-ewJ&ei=wRBkYsrSIZyO6rQP_qe3mAs&json=", "num_citations": 94, "citedby_url": "/scholar?cites=17075890751298607392&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IBmIlirA-ewJ:scholar.google.com/&scioq=Discovering+objects+and+their+relations+from+entangled+scene+representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.05068.pdf?ref=https://githubhelp.com"}, "Unsupervised Learning Using Generative Adversarial Training And Clustering": {"container_type": "Publication", "bib": {"title": "Unsupervised learning using generative adversarial training and clustering", "author": ["V Premachandran", "AL Yuille"], "pub_year": "2016", "venue": "NA", "abstract": "In this paper, we propose an unsupervised learning approach that makes use of two components; a deep hierarchical feature extractor, and a more traditional clustering algorithm. We train the feature extractor in a purely unsupervised manner using generative adversarial training and, in the process, study the strengths of learning using a generative model as an adversary. We also show that adversarial training as done in Generative Adversarial Networks (GANs) is not sufficient to automatically group data into categorical"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJ8BZTjeg", "author_id": ["LYlzbLgAAAAJ", "FJ-huxgAAAAJ"], "url_scholarbib": "/scholar?q=info:CF6q-w4N_vgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2BUsing%2BGenerative%2BAdversarial%2BTraining%2BAnd%2BClustering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CF6q-w4N_vgJ&ei=yhBkYqyIBbKO6rQPy-CRsA8&json=", "num_citations": 18, "citedby_url": "/scholar?cites=17941792323493584392&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CF6q-w4N_vgJ:scholar.google.com/&scioq=Unsupervised+Learning+Using+Generative+Adversarial+Training+And+Clustering&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJ8BZTjeg"}, "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain": {"container_type": "Publication", "bib": {"title": "Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain", "author": ["J Rajendran", "A Srinivas", "MM Khapra"], "pub_year": "2015", "venue": "arXiv preprint arXiv \u2026", "abstract": "Transferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1510.02879", "author_id": ["-novlhIAAAAJ", "GhrKC1gAAAAJ", "DV8z8DYAAAAJ"], "url_scholarbib": "/scholar?q=info:nai0dYxVw4MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttend,%2BAdapt%2Band%2BTransfer:%2BAttentive%2BDeep%2BArchitecture%2Bfor%2BAdaptive%2BTransfer%2Bfrom%2Bmultiple%2Bsources%2Bin%2Bthe%2Bsame%2Bdomain%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nai0dYxVw4MJ&ei=0RBkYq6iBZyO6rQP_qe3mAs&json=", "num_citations": 36, "citedby_url": "/scholar?cites=9494526501185693853&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nai0dYxVw4MJ:scholar.google.com/&scioq=Attend,+Adapt+and+Transfer:+Attentive+Deep+Architecture+for+Adaptive+Transfer+from+multiple+sources+in+the+same+domain&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1510.02879"}, "Incorporating long-range consistency in CNN-based texture generation": {"container_type": "Publication", "bib": {"title": "Incorporating long-range consistency in CNN-based texture generation", "author": ["G Berger", "R Memisevic"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1606.01286", "abstract": "Gatys et al.(2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.01286", "author_id": ["OY4_O9UAAAAJ", "8oJl5P0AAAAJ"], "url_scholarbib": "/scholar?q=info:1R4uNfhmuEgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIncorporating%2Blong-range%2Bconsistency%2Bin%2BCNN-based%2Btexture%2Bgeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1R4uNfhmuEgJ&ei=1RBkYobyB4ySyASZk6HgCA&json=", "num_citations": 41, "citedby_url": "/scholar?cites=5240051382675709653&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1R4uNfhmuEgJ:scholar.google.com/&scioq=Incorporating+long-range+consistency+in+CNN-based+texture+generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.01286"}, "b-GAN: Unified Framework of Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "b-gan: Unified framework of generative adversarial networks", "author": ["M Uehara", "I Sato", "M Suzuki", "K Nakayama", "Y Matsuo"], "pub_year": "2016", "venue": "NA", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1JG13oee", "author_id": ["", "", "r2nt5kUAAAAJ", "MT4jukMAAAAJ", ""], "url_scholarbib": "/scholar?q=info:zq6nMLnAAGAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3Db-GAN:%2BUnified%2BFramework%2Bof%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zq6nMLnAAGAJ&ei=1xBkYrPzJYOEmgHx-5DADA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=6917740929258860238&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zq6nMLnAAGAJ:scholar.google.com/&scioq=b-GAN:+Unified+Framework+of+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1JG13oee"}, "Learning to Act by Predicting the Future": {"container_type": "Publication", "bib": {"title": "Learning to act by predicting the future", "author": ["A Dosovitskiy", "V Koltun"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01779", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01779", "author_id": ["FXNJRDoAAAAJ", "kg4bCpgAAAAJ"], "url_scholarbib": "/scholar?q=info:4EgjFoxDqhUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BAct%2Bby%2BPredicting%2Bthe%2BFuture%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4EgjFoxDqhUJ&ei=2hBkYoGYOJLeyQTE46-QAg&json=", "num_citations": 287, "citedby_url": "/scholar?cites=1561134489783191776&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4EgjFoxDqhUJ:scholar.google.com/&scioq=Learning+to+Act+by+Predicting+the+Future&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01779.pdf?ref=https://githubhelp.com"}, "Deep Biaffine Attention for Neural Dependency Parsing": {"container_type": "Publication", "bib": {"title": "Deep biaffine attention for neural dependency parsing", "author": ["T Dozat", "CD Manning"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01734", "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01734", "author_id": ["ucA5EPEAAAAJ", "1zmDOdwAAAAJ"], "url_scholarbib": "/scholar?q=info:kVnfel2y0R4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BBiaffine%2BAttention%2Bfor%2BNeural%2BDependency%2BParsing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kVnfel2y0R4J&ei=3hBkYtqLEJqSy9YP8pKNsAE&json=", "num_citations": 816, "citedby_url": "/scholar?cites=2220752205833525649&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kVnfel2y0R4J:scholar.google.com/&scioq=Deep+Biaffine+Attention+for+Neural+Dependency+Parsing&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01734"}, "Group Sparse CNNs for Question Sentence Classification with Answer Sets": {"container_type": "Publication", "bib": {"title": "Group Sparse CNNs for Question Sentence Classification with Answer Sets", "author": ["M Ma", "L Huang", "B Xiang", "B Zhou"], "pub_year": "2016", "venue": "NA", "abstract": "Classifying question sentences into their corresponding categories is an important task with wide applications, for example in many websites' FAQ sections. However, traditional question classification techniques do not fully utilize the well-prepared answer data which has great potential for improving question sentence representations which could lead to better classification performance. In order to encode answer information into question representation, we first introduce novel group sparse autoencoders which could utilize the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJFG8Yqxl", "author_id": ["_vryxeMAAAAJ", "", "A6yjdJAAAAAJ", "h3Nsz6YAAAAJ"], "url_scholarbib": "/scholar?q=info:eaNKcawIBbUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGroup%2BSparse%2BCNNs%2Bfor%2BQuestion%2BSentence%2BClassification%2Bwith%2BAnswer%2BSets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eaNKcawIBbUJ&ei=4hBkYpHrE5GJmwGY-qmYDQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:eaNKcawIBbUJ:scholar.google.com/&scioq=Group+Sparse+CNNs+for+Question+Sentence+Classification+with+Answer+Sets&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJFG8Yqxl"}, "Filling in the details: Perceiving from low fidelity visual input": {"container_type": "Publication", "bib": {"title": "Filling in the details: Perceiving from low fidelity visual input", "author": ["FA Wick", "ML Wick", "M Pomplun"], "pub_year": "2016", "venue": "NA", "abstract": "Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (eg, dichromatic) input by the retina. In contrast, most deep learning architectures deploy computational resources homogeneously to every part of the visual input. Is such a prodigal deployment of resources necessary? In this paper, we present a framework for investigating the extent to which connectionist architectures can perceive an image in full detail even when presented with low acuity, distorted input. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Sk36NgFeg", "author_id": ["cEUDlgYAAAAJ", "D0DpJUwAAAAJ", "k7cIJjUAAAAJ"], "url_scholarbib": "/scholar?q=info:mwtKqPFVF-IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFilling%2Bin%2Bthe%2Bdetails:%2BPerceiving%2Bfrom%2Blow%2Bfidelity%2Bvisual%2Binput%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mwtKqPFVF-IJ&ei=5xBkYuDZJpyO6rQP_qe3mAs&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:mwtKqPFVF-IJ:scholar.google.com/&scioq=Filling+in+the+details:+Perceiving+from+low+fidelity+visual+input&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Sk36NgFeg"}, "Predicting Medications from Diagnostic Codes with Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Predicting medications from diagnostic codes with recurrent neural networks", "author": ["JM Bajor", "TA Lasko"], "pub_year": "2016", "venue": "NA", "abstract": "It is a surprising fact that electronic medical records are failing at one of their primary purposes, that of tracking the set of medications that the patient is actively taking. Studies estimate that up to 50% of such lists omit active drugs, and that up to 25% of all active medications do not appear on the appropriate patient list. Manual efforts to maintain these lists involve a great deal of tedious human labor, which could be reduced by computational tools to suggest likely missing or incorrect medications on a patient's list. We report here an"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJEgeXFex", "author_id": ["", "Ibzfs-kAAAAJ"], "url_scholarbib": "/scholar?q=info:cSrSKBarQnAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPredicting%2BMedications%2Bfrom%2BDiagnostic%2BCodes%2Bwith%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cSrSKBarQnAJ&ei=6hBkYvDcJIuKmgGY1YjABQ&json=", "num_citations": 37, "citedby_url": "/scholar?cites=8089215992373324401&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cSrSKBarQnAJ:scholar.google.com/&scioq=Predicting+Medications+from+Diagnostic+Codes+with+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJEgeXFex"}, "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data": {"container_type": "Publication", "bib": {"title": "Semi-supervised knowledge transfer for deep learning from private training data", "author": ["N Papernot", "M Abadi", "U Erlingsson"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Our results are encouraging, and highlight the benefits of combining a learning strategy  based on semi-supervised knowledge transfer with a precise, data-dependent privacy analysis."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.05755", "author_id": ["cGxq0cMAAAAJ", "vWTI60AAAAAJ", "cX2HlhQAAAAJ"], "url_scholarbib": "/scholar?q=info:hy1IFhndbmcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-supervised%2BKnowledge%2BTransfer%2Bfor%2BDeep%2BLearning%2Bfrom%2BPrivate%2BTraining%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hy1IFhndbmcJ&ei=9RBkYvWDOJqSy9YP8pKNsAE&json=", "num_citations": 641, "citedby_url": "/scholar?cites=7453137533162499463&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hy1IFhndbmcJ:scholar.google.com/&scioq=Semi-supervised+Knowledge+Transfer+for+Deep+Learning+from+Private+Training+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.05755.pdf,"}, "Adding Gradient Noise Improves Learning for Very Deep Networks": {"container_type": "Publication", "bib": {"title": "Adding gradient noise improves learning for very deep networks", "author": ["A Neelakantan", "L Vilnis", "QV Le", "I Sutskever"], "pub_year": "2015", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1511.06807", "author_id": ["ygTCc6cAAAAJ", "xWrOthYAAAAJ", "vfT6-XIAAAAJ", "x04W_mMAAAAJ"], "url_scholarbib": "/scholar?q=info:qDlEHBov1x4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdding%2BGradient%2BNoise%2BImproves%2BLearning%2Bfor%2BVery%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qDlEHBov1x4J&ei=-BBkYqujL_mQ6rQP5OqKqAo&json=", "num_citations": 420, "citedby_url": "/scholar?cites=2222296730320517544&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qDlEHBov1x4J:scholar.google.com/&scioq=Adding+Gradient+Noise+Improves+Learning+for+Very+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1511.06807"}, "Progressive Attention Networks for Visual Attribute Prediction": {"container_type": "Publication", "bib": {"title": "Progressive attention networks for visual attribute prediction", "author": ["PH Seo", "Z Lin", "S Cohen", "X Shen", "B Han"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1606.02393", "abstract": "We propose a novel attention model that can accurately attends to target objects of various scales and shapes in images. The model is trained to gradually suppress irrelevant regions in an input image via a progressive attentive process over multiple layers of a convolutional neural network. The attentive process in each layer determines whether to pass or block features at certain spatial locations for use in the subsequent layers. The proposed progressive attention mechanism works well especially when combined with hard attention"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.02393", "author_id": ["Tp7U8_UAAAAJ", "R0bnqaAAAAAJ", "Br80hVMAAAAJ", "pViZYwIAAAAJ", "9aaeCToAAAAJ"], "url_scholarbib": "/scholar?q=info:fVRC2fboh7UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgressive%2BAttention%2BNetworks%2Bfor%2BVisual%2BAttribute%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fVRC2fboh7UJ&ei=_BBkYpDHOcLZmQHnraWYCA&json=", "num_citations": 35, "citedby_url": "/scholar?cites=13080679789625496701&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fVRC2fboh7UJ:scholar.google.com/&scioq=Progressive+Attention+Networks+for+Visual+Attribute+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.02393"}, "Learning Curve Prediction with Bayesian Neural Networks": {"container_type": "Publication", "bib": {"title": "Learning curve prediction with Bayesian neural networks", "author": ["A Klein", "S Falkner", "JT Springenberg", "F Hutter"], "pub_year": "2016", "venue": "NA", "abstract": "Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time. Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. Exploiting the same information in automatic Bayesian hyperparameter optimization requires a probabilistic model of learning curves across hyperparameter settings. Here, we study the use of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S11KBYclx", "author_id": ["usl__skAAAAJ", "r7FWJEkAAAAJ", "MGXJkIAAAAAJ", "YUrxwrkAAAAJ"], "url_scholarbib": "/scholar?q=info:wAse4_y68yUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BCurve%2BPrediction%2Bwith%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wAse4_y68yUJ&ei=ChFkYqn-JeiSy9YPp-OyiAE&json=", "num_citations": 151, "citedby_url": "/scholar?cites=2734734994048945088&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wAse4_y68yUJ:scholar.google.com/&scioq=Learning+Curve+Prediction+with+Bayesian+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S11KBYclx"}, "DyVEDeep: Dynamic Variable Effort Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "DyVEDeep Dynamic Variable Effort Deep Neural Networks", "author": ["S Ganapathy", "S Venkataramani", "G Sriraman"], "pub_year": "2020", "venue": "ACM Transactions on \u2026", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. To address this challenge, one promising approach is to exploit the error resilient nature of DNNs by skipping or approximating computations that have negligible"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3372882", "author_id": ["Q1OGU54AAAAJ", "l2RQ_S8AAAAJ", "w0QC16wAAAAJ"], "url_scholarbib": "/scholar?q=info:Z8s2YCKCnMIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDyVEDeep:%2BDynamic%2BVariable%2BEffort%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z8s2YCKCnMIJ&ei=DxFkYpPjBJyO6rQP_qe3mAs&json=", "num_citations": 3, "citedby_url": "/scholar?cites=14023226423879584615&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Z8s2YCKCnMIJ:scholar.google.com/&scioq=DyVEDeep:+Dynamic+Variable+Effort+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.01137"}, "Neural Graph Machines: Learning Neural Networks Using Graphs": {"container_type": "Publication", "bib": {"title": "Neural graph learning: Training neural networks using graphs", "author": ["TD Bui", "S Ravi", "V Ramavajjala"], "pub_year": "2018", "venue": "\u2026 Conference on Web Search and Data \u2026", "abstract": "objective function for training neural network architectures using both labeled and  using  this objective Neural Graph Machines (NGM), and schematically illustrate the concept in figure 1"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3159652.3159731", "author_id": ["SVq3y1sAAAAJ", "sM-dHicAAAAJ", "6vGZe6cAAAAJ"], "url_scholarbib": "/scholar?q=info:9XVQLn1B_OoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BGraph%2BMachines:%2BLearning%2BNeural%2BNetworks%2BUsing%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9XVQLn1B_OoJ&ei=FBFkYqmxG4yuyAT-mrWwCA&json=", "num_citations": 94, "citedby_url": "/scholar?cites=16932480704909964789&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9XVQLn1B_OoJ:scholar.google.com/&scioq=Neural+Graph+Machines:+Learning+Neural+Networks+Using+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3159652.3159731"}, "Latent Sequence Decompositions": {"container_type": "Publication", "bib": {"title": "Latent sequence decompositions", "author": ["W Chan", "Y Zhang", "Q Le", "N Jaitly"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.03035", "abstract": "decomposition and output sequence, by using beam search to find the most likely output  sequence  It tells us to sample some latent sequence decomposition z \u223c p(z|y, x; \u03b8) under our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.03035", "author_id": ["Nla9qfUAAAAJ", "EilVnKwAAAAJ", "vfT6-XIAAAAJ", "kjMNMLkAAAAJ"], "url_scholarbib": "/scholar?q=info:qEidnnkD6ZMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BSequence%2BDecompositions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qEidnnkD6ZMJ&ei=JxFkYo6ME4uKmgGY1YjABQ&json=", "num_citations": 55, "citedby_url": "/scholar?cites=10658053814036023464&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qEidnnkD6ZMJ:scholar.google.com/&scioq=Latent+Sequence+Decompositions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.03035.pdf?ref=https://githubhelp.com"}, "Deep Learning with Sets and Point Clouds": {"container_type": "Publication", "bib": {"title": "Deep learning with sets and point clouds", "author": ["S Ravanbakhsh", "J Schneider", "B Poczos"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.04500", "abstract": "We introduce a simple permutation equivariant layer for deep learning with set structure. This type of layer, obtained by parameter-sharing, has a simple implementation and linear-time complexity in the size of each set. We use deep permutation-invariant networks to perform point-could classification and MNIST-digit summation, where in both cases the output is invariant to permutations of the input. In a semi-supervised setting, where the goal is make predictions for each instance within a set, we demonstrate the usefulness of this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.04500", "author_id": ["VciG3C8AAAAJ", "3bSbb20AAAAJ", "sUriZlUAAAAJ"], "url_scholarbib": "/scholar?q=info:6Bq9faSfg3gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bwith%2BSets%2Band%2BPoint%2BClouds%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6Bq9faSfg3gJ&ei=MhFkYsLgA--Sy9YPs_mY8AM&json=", "num_citations": 160, "citedby_url": "/scholar?cites=8683960035333446376&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6Bq9faSfg3gJ:scholar.google.com/&scioq=Deep+Learning+with+Sets+and+Point+Clouds&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.04500"}, "Out-of-class novelty generation: an experimental foundation": {"container_type": "Publication", "bib": {"title": "Out-of-class novelty generation: an experimental foundation", "author": ["M Cherti", "B K\u00e9gl", "A Kazak\u00e7\u0131"], "pub_year": "2017", "venue": "2017 IEEE 29th International \u2026", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8372100/", "author_id": ["JgOyYi8AAAAJ", "s0njcGgAAAAJ", "w7veSWIAAAAJ"], "url_scholarbib": "/scholar?q=info:sy8pAjY8GngJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOut-of-class%2Bnovelty%2Bgeneration:%2Ban%2Bexperimental%2Bfoundation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sy8pAjY8GngJ&ei=NBFkYsKfOu-Sy9YPs_mY8AM&json=", "num_citations": 11, "citedby_url": "/scholar?cites=8654295836607983539&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sy8pAjY8GngJ:scholar.google.com/&scioq=Out-of-class+novelty+generation:+an+experimental+foundation&hl=en&as_sdt=0,33", "eprint_url": "https://hal.archives-ouvertes.fr/hal-01773776/document"}, "RenderGAN: Generating Realistic Labeled Data": {"container_type": "Publication", "bib": {"title": "Rendergan: Generating realistic labeled data", "author": ["L Sixt", "B Wild", "T Landgraf"], "pub_year": "2018", "venue": "Frontiers in Robotics and AI", "abstract": "data such that the generated images are strikingly realistic while  We apply the RenderGAN  framework to generate images of  Training a DCNN on data generated by the RenderGAN"}, "filled": false, "gsrank": 1, "pub_url": "https://www.frontiersin.org/articles/10.3389/frobt.2018.00066/full", "author_id": ["XtejLN8AAAAJ", "_aIyvUkAAAAJ", "ChX0opIAAAAJ"], "url_scholarbib": "/scholar?q=info:W6u4vKCACd0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRenderGAN:%2BGenerating%2BRealistic%2BLabeled%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W6u4vKCACd0J&ei=NxFkYv2eM4OEmgHx-5DADA&json=", "num_citations": 153, "citedby_url": "/scholar?cites=15927402985021811547&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W6u4vKCACd0J:scholar.google.com/&scioq=RenderGAN:+Generating+Realistic+Labeled+Data&hl=en&as_sdt=0,33", "eprint_url": "https://www.frontiersin.org/articles/10.3389/frobt.2018.00066/full"}, "Variational Recurrent Adversarial Deep Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Variational recurrent adversarial deep domain adaptation", "author": ["S Purushotham", "W Carvalho", "T Nilanon", "Y Liu"], "pub_year": "2016", "venue": "NA", "abstract": "We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rk9eAFcxg", "author_id": ["Q0iwucYAAAAJ", "tvJTXwoAAAAJ", "MF3l3foAAAAJ", "UUKLPMYAAAAJ"], "url_scholarbib": "/scholar?q=info:0jTsPawHr5sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BRecurrent%2BAdversarial%2BDeep%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0jTsPawHr5sJ&ei=RBFkYo-bDM6E6rQP5-KmKA&json=", "num_citations": 79, "citedby_url": "/scholar?cites=11218193633157854418&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0jTsPawHr5sJ:scholar.google.com/&scioq=Variational+Recurrent+Adversarial+Deep+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rk9eAFcxg"}, "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering": {"container_type": "Publication", "bib": {"title": "Gaussian attention model and its application to knowledge base embedding and question answering", "author": ["L Zhang", "J Winn", "R Tomioka"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02266", "abstract": "We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02266", "author_id": ["aP_nXu4AAAAJ", "GYksTEEAAAAJ", "TxdeO-UAAAAJ"], "url_scholarbib": "/scholar?q=info:py68fwpQyooJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGaussian%2BAttention%2BModel%2Band%2BIts%2BApplication%2Bto%2BKnowledge%2BBase%2BEmbedding%2Band%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=py68fwpQyooJ&ei=SBFkYvC4NpqSy9YP8pKNsAE&json=", "num_citations": 12, "citedby_url": "/scholar?cites=10000893928552541863&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:py68fwpQyooJ:scholar.google.com/&scioq=Gaussian+Attention+Model+and+Its+Application+to+Knowledge+Base+Embedding+and+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02266"}, "Generalizable Features From Unsupervised Learning": {"container_type": "Publication", "bib": {"title": "Generalizable features from unsupervised learning", "author": ["M Mirza", "A Courville", "Y Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.03809", "abstract": "Humans learn a predictive model of the world and use this model to reason about future events and the consequences of actions. In contrast to most machine predictors, we exhibit an impressive ability to generalize to unseen scenarios and reason intelligently in these settings. One important aspect of this ability is physical intuition (Lake et al., 2016). In this work, we explore the potential of unsupervised learning to find features that promote better generalization to settings outside the supervised training distribution. Our task is predicting"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.03809", "author_id": ["c646VbAAAAAJ", "km6CP8cAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:tSODhCMrNM8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizable%2BFeatures%2BFrom%2BUnsupervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tSODhCMrNM8J&ei=SxFkYvrDEYuKmgGY1YjABQ&json=", "num_citations": 13, "citedby_url": "/scholar?cites=14930606096187073461&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tSODhCMrNM8J:scholar.google.com/&scioq=Generalizable+Features+From+Unsupervised+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.03809"}, "Unsupervised Deep Learning of State Representation Using Robotic Priors ": {"container_type": "Publication", "bib": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors", "author": ["T LESORT", "D FILLIAT"], "pub_year": "2016", "venue": "NA", "abstract": "Our understanding of the world depends highly on how we represent it. Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bkp_y7qxe", "author_id": ["5NttkuoAAAAJ", "Wzq_c20AAAAJ"], "url_scholarbib": "/scholar?q=info:NplnZRtyhLEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BDeep%2BLearning%2Bof%2BState%2BRepresentation%2BUsing%2BRobotic%2BPriors%2B%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NplnZRtyhLEJ&ei=TxFkYpfjApqSy9YP8pKNsAE&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:NplnZRtyhLEJ:scholar.google.com/&scioq=Unsupervised+Deep+Learning+of+State+Representation+Using+Robotic+Priors+&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bkp_y7qxe"}, "Faster CNNs with Direct Sparse Convolutions and Guided Pruning": {"container_type": "Publication", "bib": {"title": "Faster cnns with direct sparse convolutions and guided pruning", "author": ["J Park", "S Li", "W Wen", "PTP Tang", "H Li", "Y Chen"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers. The number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. Nevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.01409", "author_id": ["dlX-GboAAAAJ", "KSvkraAAAAAJ", "JYD36ocAAAAJ", "", "E6Tpfq8AAAAJ", "3G-nnjMAAAAJ"], "url_scholarbib": "/scholar?q=info:gh792xF89gwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFaster%2BCNNs%2Bwith%2BDirect%2BSparse%2BConvolutions%2Band%2BGuided%2BPruning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gh792xF89gwJ&ei=UhFkYrLSJIyuyAT-mrWwCA&json=", "num_citations": 169, "citedby_url": "/scholar?cites=934070388873043586&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gh792xF89gwJ:scholar.google.com/&scioq=Faster+CNNs+with+Direct+Sparse+Convolutions+and+Guided+Pruning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.01409.pdf?ref=https://githubhelp.com"}, "Fast Adaptation in Generative Models with Generative Matching Networks": {"container_type": "Publication", "bib": {"title": "Fast adaptation in generative models with generative matching networks", "author": ["S Bartunov", "DP Vetrov"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.02192", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.02192", "author_id": ["VDmoGxwAAAAJ", "7HU0UoUAAAAJ"], "url_scholarbib": "/scholar?q=info:N5KhQwKzhXYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFast%2BAdaptation%2Bin%2BGenerative%2BModels%2Bwith%2BGenerative%2BMatching%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N5KhQwKzhXYJ&ei=VRFkYveLK-iSy9YPp-OyiAE&json=", "num_citations": 23, "citedby_url": "/scholar?cites=8540429090683982391&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:N5KhQwKzhXYJ:scholar.google.com/&scioq=Fast+Adaptation+in+Generative+Models+with+Generative+Matching+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.02192"}, "Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM": {"container_type": "Publication", "bib": {"title": "Annealing Gaussian into ReLU: a new sampling strategy for leaky-ReLU RBM", "author": ["CL Li", "S Ravanbakhsh", "B Poczos"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.03879", "abstract": "Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood, RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units--called leaky RBM. We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.03879", "author_id": ["vqHIt_sAAAAJ", "VciG3C8AAAAJ", "sUriZlUAAAAJ"], "url_scholarbib": "/scholar?q=info:tUGaS7z1jhgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnnealing%2BGaussian%2Binto%2BReLU:%2Ba%2BNew%2BSampling%2BStrategy%2Bfor%2BLeaky-ReLU%2BRBM%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tUGaS7z1jhgJ&ei=WBFkYs72BI6pywTd4KPADw&json=", "num_citations": 7, "citedby_url": "/scholar?cites=1769621892674240949&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tUGaS7z1jhgJ:scholar.google.com/&scioq=Annealing+Gaussian+into+ReLU:+a+New+Sampling+Strategy+for+Leaky-ReLU+RBM&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.03879"}, "On orthogonality and learning recurrent networks with long term dependencies": {"container_type": "Publication", "bib": {"title": "On orthogonality and learning recurrent networks with long term dependencies", "author": ["E Vorontsov", "C Trabelsi"], "pub_year": "2017", "venue": "\u2026 on Machine Learning", "abstract": "It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/vorontsov17a.html", "author_id": ["5o1gS_sAAAAJ", "M0bhIh4AAAAJ"], "url_scholarbib": "/scholar?q=info:lV3wMkbK37AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Borthogonality%2Band%2Blearning%2Brecurrent%2Bnetworks%2Bwith%2Blong%2Bterm%2Bdependencies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lV3wMkbK37AJ&ei=WhFkYoK9MYuKmgGY1YjABQ&json=", "num_citations": 175, "citedby_url": "/scholar?cites=12745127873332927893&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lV3wMkbK37AJ:scholar.google.com/&scioq=On+orthogonality+and+learning+recurrent+networks+with+long+term+dependencies&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/vorontsov17a/vorontsov17a.pdf"}, "Investigating Different Context Types and Representations for Learning Word Embeddings": {"container_type": "Publication", "bib": {"title": "Investigating Different Context Types and Representations for Learning Word Embeddings", "author": ["B Li", "T Liu", "Z Zhao", "B Tang", "X Du"], "pub_year": "2016", "venue": "NA", "abstract": "The number of word embedding models is growing every year. Most of them learn word embeddings based on the co-occurrence information of words and their context. However, it's still an open question what is the best definition of context. We provide the first systematical investigation of different context types and representations for learning word embeddings. We conduct comprehensive experiments to evaluate their effectiveness under 4 tasks (21 datasets), which give us some insights about context selection. We hope that this"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bkfwyw5xg", "author_id": ["dhYK5c8AAAAJ", "CU5OeFIAAAAJ", "Xh7oU4kAAAAJ", "5oGXsxUAAAAJ", ""], "url_scholarbib": "/scholar?q=info:_blL6IjYDYMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInvestigating%2BDifferent%2BContext%2BTypes%2Band%2BRepresentations%2Bfor%2BLearning%2BWord%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_blL6IjYDYMJ&ei=XhFkYt_CEO-Sy9YPs_mY8AM&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:_blL6IjYDYMJ:scholar.google.com/&scioq=Investigating+Different+Context+Types+and+Representations+for+Learning+Word+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bkfwyw5xg"}, "Transformational Sparse Coding": {"container_type": "Publication", "bib": {"title": "Learning transformational invariants from natural movies", "author": ["C Cadieu", "B Olshausen"], "pub_year": "2008", "venue": "Advances in neural information \u2026", "abstract": "The sparse coding model imposes a kurtotic, independent  We propose here a generalization  of the sparse coding model to  , we have modified the sparse coding model by utilizing a"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.neurips.cc/paper/2008/hash/2e2c080d5490760af59d0baf5acbb84e-Abstract.html", "author_id": ["ebaAp_MAAAAJ", "4aqK_74AAAAJ"], "url_scholarbib": "/scholar?q=info:YDCgUwBsza8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransformational%2BSparse%2BCoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YDCgUwBsza8J&ei=YRFkYqnOH4yuyAT-mrWwCA&json=", "num_citations": 103, "citedby_url": "/scholar?cites=12667900075521880160&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YDCgUwBsza8J:scholar.google.com/&scioq=Transformational+Sparse+Coding&hl=en&as_sdt=0,33", "eprint_url": "https://proceedings.neurips.cc/paper/2008/file/2e2c080d5490760af59d0baf5acbb84e-Paper.pdf"}, "Intelligible Language Modeling with Input Switched Affine Networks": {"container_type": "Publication", "bib": {"title": "Intelligible language modeling with input switched affine networks", "author": ["J Foerster", "J Gilmer", "J Chorowski", "J Sohl-Dickstein"], "pub_year": "2016", "venue": "NA", "abstract": "The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1MjAnqxg", "author_id": ["6z4lQzMAAAAJ", "Ml_vQ8MAAAAJ", "Yc94070AAAAJ", "-3zYIjQAAAAJ"], "url_scholarbib": "/scholar?q=info:wL0fgi9vENAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntelligible%2BLanguage%2BModeling%2Bwith%2BInput%2BSwitched%2BAffine%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wL0fgi9vENAJ&ei=ZBFkYq7KM86E6rQP5-KmKA&json=", "num_citations": 7, "citedby_url": "/scholar?cites=14992605409353645504&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wL0fgi9vENAJ:scholar.google.com/&scioq=Intelligible+Language+Modeling+with+Input+Switched+Affine+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1MjAnqxg"}, "Energy-Based Spherical Sparse Coding": {"container_type": "Publication", "bib": {"title": "Energy-based spherical sparse coding", "author": ["B Kong", "CC Fowlkes"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.01820", "abstract": "(b) Our spherical sparse coding layer has a similar structure but with an  energy-based  model uses (c) energy-based spherical sparse coding (EB-SSC) blocks that produces sparse"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.01820", "author_id": ["hK6YyRwAAAAJ", "yLQF4mkAAAAJ"], "url_scholarbib": "/scholar?q=info:n0jVD1bDphIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnergy-Based%2BSpherical%2BSparse%2BCoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n0jVD1bDphIJ&ei=ZxFkYuLyNYuKmgGY1YjABQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=1343976313216911519&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n0jVD1bDphIJ:scholar.google.com/&scioq=Energy-Based+Spherical+Sparse+Coding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.01820"}, "Frustratingly Short Attention Spans in Neural Language Modeling": {"container_type": "Publication", "bib": {"title": "Frustratingly short attention spans in neural language modeling", "author": ["M Daniluk", "T Rockt\u00e4schel", "J Welbl", "S Riedel"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid-and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.04521", "author_id": ["", "mWBY8aIAAAAJ", "SaHRjy4AAAAJ", "AcCtcrsAAAAJ"], "url_scholarbib": "/scholar?q=info:1XQ7z9MAUwUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrustratingly%2BShort%2BAttention%2BSpans%2Bin%2BNeural%2BLanguage%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1XQ7z9MAUwUJ&ei=bBFkYoiTI8LZmQHnraWYCA&json=", "num_citations": 100, "citedby_url": "/scholar?cites=383651302971503829&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1XQ7z9MAUwUJ:scholar.google.com/&scioq=Frustratingly+Short+Attention+Spans+in+Neural+Language+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.04521.pdf?fbclid=IwAR3hAvPgZtClwiFhR4TtS8h1bbXqj74qeeItSVNnSFN0w87l_eJx7Pia2sQ"}, "Reference-Aware Language Models": {"container_type": "Publication", "bib": {"title": "Reference-aware language models", "author": ["Z Yang", "P Blunsom", "C Dyer", "W Ling"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01628", "abstract": "Here we propose a language modeling framework that explicitly incorporates reference  decisions. In part, this is based on the principle of pointer networks in which copies are made"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01628", "author_id": ["siCYLcUAAAAJ", "eJwbbXEAAAAJ", "W2DsnAkAAAAJ", "gl0PhvEAAAAJ"], "url_scholarbib": "/scholar?q=info:UpRI-z0c4i0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReference-Aware%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UpRI-z0c4i0J&ei=cBFkYrlinI7qtA_-p7eYCw&json=", "num_citations": 85, "citedby_url": "/scholar?cites=3306236128977785938&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UpRI-z0c4i0J:scholar.google.com/&scioq=Reference-Aware+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01628"}, "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks": {"container_type": "Publication", "bib": {"title": "Dragnn: A transition-based framework for dynamically connected neural networks", "author": ["L Kong", "C Alberti", "D Andor", "I Bogatyy"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work, we present a compact, modular framework for constructing novel recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and re-cursive tree"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.04474", "author_id": ["f1hBi5wAAAAJ", "SA1BuxoAAAAJ", "v1DZ5jQAAAAJ", "3umCgZYAAAAJ"], "url_scholarbib": "/scholar?q=info:vikSTSm8_kgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDRAGNN:%2BA%2BTransition-Based%2BFramework%2Bfor%2BDynamically%2BConnected%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vikSTSm8_kgJ&ei=cxFkYp-vNOiSy9YPp-OyiAE&json=", "num_citations": 33, "citedby_url": "/scholar?cites=5259848300388035006&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vikSTSm8_kgJ:scholar.google.com/&scioq=DRAGNN:+A+Transition-Based+Framework+for+Dynamically+Connected+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.04474"}, "Cooperative Training of Descriptor and Generator Networks": {"container_type": "Publication", "bib": {"title": "Cooperative training of descriptor and generator networks", "author": ["J Xie", "Y Lu", "R Gao", "SC Zhu"], "pub_year": "2018", "venue": "IEEE transactions on \u2026", "abstract": "This paper studies the cooperative training of two generative models for image modeling and synthesis. Both models are parametrized by convolutional neural networks (ConvNets). The first model is a deep energy-based model, whose energy function is defined by a bottom-up ConvNet, which maps the observed image to the energy. We call it the descriptor network. The second model is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8519332/", "author_id": ["O3p4CIQAAAAJ", "UuYkhSkAAAAJ", "VdlgOXoAAAAJ", "Al8dyb4AAAAJ"], "url_scholarbib": "/scholar?q=info:a9qIpDRenfwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCooperative%2BTraining%2Bof%2BDescriptor%2Band%2BGenerator%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a9qIpDRenfwJ&ei=dhFkYtylOouKmgGY1YjABQ&json=", "num_citations": 103, "citedby_url": "/scholar?cites=18202808849093155435&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a9qIpDRenfwJ:scholar.google.com/&scioq=Cooperative+Training+of+Descriptor+and+Generator+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://ieeexplore.ieee.org/ielaam/34/8922815/8519332-aam.pdf"}, "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation": {"container_type": "Publication", "bib": {"title": "Lr-gan: Layered recursive generative adversarial networks for image generation", "author": ["J Yang", "A Kannan", "D Batra", "D Parikh"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.01560", "abstract": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.01560", "author_id": ["Cl9byD8AAAAJ", "eoBHpj4AAAAJ", "_bs7PqgAAAAJ", "ijpYJQwAAAAJ"], "url_scholarbib": "/scholar?q=info:yqI49fmDHlwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLR-GAN:%2BLayered%2BRecursive%2BGenerative%2BAdversarial%2BNetworks%2Bfor%2BImage%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yqI49fmDHlwJ&ei=gRFkYtbtHY6pywTd4KPADw&json=", "num_citations": 201, "citedby_url": "/scholar?cites=6637888010374914762&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yqI49fmDHlwJ:scholar.google.com/&scioq=LR-GAN:+Layered+Recursive+Generative+Adversarial+Networks+for+Image+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.01560.pdf?ref=https://githubhelp.com"}, "Differentiable Canonical Correlation Analysis": {"container_type": "Publication", "bib": {"title": "Differentiable Canonical Correlation Analysis", "author": ["M Dorfer", "J Schl\u00fcter", "G Widmer"], "pub_year": "2016", "venue": "NA", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated linear projections of two modalities. We propose Differentiable CCA, a formulation of CCA that can be cast as a layer within a multi-view neural network. Unlike Deep CCA, an earlier extension of CCA to nonlinear projections, our formulation enables gradient flow through the computation of the CCA projection matrices, and free choice of the final optimization target. We show the effectiveness of this approach in cross-modality retrieval experiments on two public image-to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rywUcQogx", "author_id": ["", "AjmDxgoAAAAJ", "dyGS5YYAAAAJ"], "url_scholarbib": "/scholar?q=info:9XSgix0GwcAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BCanonical%2BCorrelation%2BAnalysis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9XSgix0GwcAJ&ei=hBFkYrnlKI2ymgHg1rfQDQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:9XSgix0GwcAJ:scholar.google.com/&scioq=Differentiable+Canonical+Correlation+Analysis&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rywUcQogx"}, "The loss surface of residual networks: Ensembles and the role of batch normalization": {"container_type": "Publication", "bib": {"title": "The loss surface of residual networks: Ensembles and the role of batch normalization", "author": ["E Littwin", "L Wolf"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02525", "abstract": "Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensembles are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network's depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, eg"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02525", "author_id": ["NOVS7vwAAAAJ", "UbFrXTsAAAAJ"], "url_scholarbib": "/scholar?q=info:heneduSAO-0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2Bloss%2Bsurface%2Bof%2Bresidual%2Bnetworks:%2BEnsembles%2Band%2Bthe%2Brole%2Bof%2Bbatch%2Bnormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=heneduSAO-0J&ei=iBFkYuX-Ku-Sy9YPs_mY8AM&json=", "num_citations": 13, "citedby_url": "/scholar?cites=17094398529350068613&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:heneduSAO-0J:scholar.google.com/&scioq=The+loss+surface+of+residual+networks:+Ensembles+and+the+role+of+batch+normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02525"}, "Iterative Refinement for Machine Translation": {"container_type": "Publication", "bib": {"title": "Iterative refinement for machine translation", "author": ["R Novak", "M Auli", "D Grangier"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.06602", "abstract": "Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.06602", "author_id": ["syG6krEAAAAJ", "KMcwQtcAAAAJ", "CIQEGCYAAAAJ"], "url_scholarbib": "/scholar?q=info:cOOlucmtihkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIterative%2BRefinement%2Bfor%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cOOlucmtihkJ&ei=jBFkYrDBFeiSy9YPp-OyiAE&json=", "num_citations": 24, "citedby_url": "/scholar?cites=1840474479648957296&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cOOlucmtihkJ:scholar.google.com/&scioq=Iterative+Refinement+for+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.06602"}, "Emergent Predication Structure in Vector Representations of Neural Readers": {"container_type": "Publication", "bib": {"title": "Emergent predication structure in vector representations of neural readers", "author": ["H Wang", "T Onishi", "K Gimpel", "D McAllester"], "pub_year": "2016", "venue": "NA", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryWKREqxx", "author_id": ["", "BAWpl8MAAAAJ", "kDHs7DYAAAAJ", "nbpafUkAAAAJ"], "url_scholarbib": "/scholar?q=info:a18JetRzYVwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BPredication%2BStructure%2Bin%2BVector%2BRepresentations%2Bof%2BNeural%2BReaders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a18JetRzYVwJ&ei=kBFkYuf-FOiSy9YPp-OyiAE&json=", "num_citations": 1, "citedby_url": "/scholar?cites=6656729080647999339&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a18JetRzYVwJ:scholar.google.com/&scioq=Emergent+Predication+Structure+in+Vector+Representations+of+Neural+Readers&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryWKREqxx"}, "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic": {"container_type": "Publication", "bib": {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["S Gu", "T Lillicrap", "Z Ghahramani", "RE Turner"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02247", "author_id": ["B8wslVsAAAAJ", "htPVdRMAAAAJ", "0uTu7fYAAAAJ", "DgLEyZgAAAAJ"], "url_scholarbib": "/scholar?q=info:4BrKWf3m_YYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQ-Prop:%2BSample-Efficient%2BPolicy%2BGradient%2Bwith%2BAn%2BOff-Policy%2BCritic%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4BrKWf3m_YYJ&ei=kxFkYqHYJoOEmgHx-5DADA&json=", "num_citations": 302, "citedby_url": "/scholar?cites=9727184745997671136&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4BrKWf3m_YYJ:scholar.google.com/&scioq=Q-Prop:+Sample-Efficient+Policy+Gradient+with+An+Off-Policy+Critic&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02247"}, "Introducing Active Learning for CNN under the light of Variational Inference": {"container_type": "Publication", "bib": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "author": ["M Ducoffe", "F Precioso"], "pub_year": "2016", "venue": "NA", "abstract": "One main concern of the deep learning community is to increase the capacity of representation of deep networks by increasing their depth. This requires to scale up the size of the training database accordingly. Indeed a major intuition lies in the fact that the depth of the network and the size of the training set are strongly correlated. However recent works tend to show that deep learning may be handled with smaller dataset as long as the training samples are carefully selected (let us mention for instance curriculum learning). In this"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryaFG5ige", "author_id": ["jqG6J-AAAAAJ", "-0cKTucAAAAJ"], "url_scholarbib": "/scholar?q=info:12K_8vIqMw8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntroducing%2BActive%2BLearning%2Bfor%2BCNN%2Bunder%2Bthe%2Blight%2Bof%2BVariational%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=12K_8vIqMw8J&ei=lxFkYq3HFYuKmgGY1YjABQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:12K_8vIqMw8J:scholar.google.com/&scioq=Introducing+Active+Learning+for+CNN+under+the+light+of+Variational+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryaFG5ige"}, "Distributed Second-Order Optimization using Kronecker-Factored Approximations": {"container_type": "Publication", "bib": {"title": "Distributed second-order optimization using Kronecker-factored approximations", "author": ["J Ba", "R Grosse", "J Martens"], "pub_year": "2016", "venue": "NA", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase. Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkkTMpjex", "author_id": ["ymzxRhAAAAAJ", "xgQd1qgAAAAJ", "LlK_saMAAAAJ"], "url_scholarbib": "/scholar?q=info:XYatsNIcjRQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BSecond-Order%2BOptimization%2Busing%2BKronecker-Factored%2BApproximations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XYatsNIcjRQJ&ei=mhFkYrC_KOHDywSSipaYAg&json=", "num_citations": 68, "citedby_url": "/scholar?cites=1480871543707633245&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XYatsNIcjRQJ:scholar.google.com/&scioq=Distributed+Second-Order+Optimization+using+Kronecker-Factored+Approximations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkkTMpjex"}, "Neural Photo Editing with Introspective Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Neural photo editing with introspective adversarial networks", "author": ["A Brock", "T Lim", "JM Ritchie", "N Weston"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.07093", "abstract": "The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.07093", "author_id": ["NIxD36wAAAAJ", "qpfMj-8AAAAJ", "", "PZSrJvIAAAAJ"], "url_scholarbib": "/scholar?q=info:M_Vf0ZzbcbsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BPhoto%2BEditing%2Bwith%2BIntrospective%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M_Vf0ZzbcbsJ&ei=nRFkYqjED4uKmgGY1YjABQ&json=", "num_citations": 386, "citedby_url": "/scholar?cites=13506818224034936115&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:M_Vf0ZzbcbsJ:scholar.google.com/&scioq=Neural+Photo+Editing+with+Introspective+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.07093.pdf?source=post_page---------------------------"}, "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond": {"container_type": "Publication", "bib": {"title": "Eigenvalues of the hessian in deep learning: Singularity and beyond", "author": ["L Sagun", "L Bottou", "Y LeCun"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.07476", "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.07476", "author_id": ["-iPZaBcAAAAJ", "kbN88gsAAAAJ", ""], "url_scholarbib": "/scholar?q=info:acsevRQsqi0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEigenvalues%2Bof%2Bthe%2BHessian%2Bin%2BDeep%2BLearning:%2BSingularity%2Band%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=acsevRQsqi0J&ei=oRFkYvz-G4ySyASZk6HgCA&json=", "num_citations": 88, "citedby_url": "/scholar?cites=3290490945331448681&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:acsevRQsqi0J:scholar.google.com/&scioq=Eigenvalues+of+the+Hessian+in+Deep+Learning:+Singularity+and+Beyond&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.07476"}, "Layer Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Layer recurrent neural networks", "author": ["W Xie", "A Noble", "A Zisserman"], "pub_year": "2016", "venue": "NA", "abstract": "adaptively using within-layer recurrence. Our contributions are three-fold: (i) we propose  a hybrid neural network architecture that interleaves traditional convolutional layers with L-RNN"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJJRDvcex", "author_id": ["Vtrqj4gAAAAJ", "b0tmmYMAAAAJ", "UZ5wscMAAAAJ"], "url_scholarbib": "/scholar?q=info:0KF_zrQjEPAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLayer%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0KF_zrQjEPAJ&ei=pBFkYrm6Oo6pywTd4KPADw&json=", "num_citations": 4, "citedby_url": "/scholar?cites=17298365428195631568&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0KF_zrQjEPAJ:scholar.google.com/&scioq=Layer+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJJRDvcex"}, "Multi-label learning with the RNNs for Fashion Search": {"container_type": "Publication", "bib": {"title": "Multi-label learning with the RNNs for Fashion Search", "author": ["T Kim"], "pub_year": "2016", "venue": "NA", "abstract": "We build a large-scale visual search system which finds similar product images given a fashion item. Defining similarity among arbitrary fashion-products is still remains a challenging problem, even there is no exact ground-truth. To resolve this problem, we define more than 90 fashion-related attributes, and combination of these attributes can represent thousands of unique fashion-styles. We then introduce to use the recurrent neural networks (RNNs) recognising multi fashion-attributes with the end-to-end manner. To build our system"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyWDCXjgx", "author_id": [""], "url_scholarbib": "/scholar?q=info:rKUjrqRBfGsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-label%2Blearning%2Bwith%2Bthe%2BRNNs%2Bfor%2BFashion%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rKUjrqRBfGsJ&ei=pw9kYumUM4yuyAT-mrWwCA&json=", "num_citations": 1, "citedby_url": "/scholar?cites=7745137634722424236&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rKUjrqRBfGsJ:scholar.google.com/&scioq=Multi-label+learning+with+the+RNNs+for+Fashion+Search&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyWDCXjgx"}, "Tracking the World State with Recurrent Entity Networks": {"container_type": "Publication", "bib": {"title": "Tracking the world state with recurrent entity networks", "author": ["M Henaff", "J Weston", "A Szlam", "A Bordes"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.03969", "author_id": ["bX__wkYAAAAJ", "lMkTx0EAAAAJ", "u3-FxUgAAAAJ", "jWWx33IAAAAJ"], "url_scholarbib": "/scholar?q=info:l5gBVLWzad0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTracking%2Bthe%2BWorld%2BState%2Bwith%2BRecurrent%2BEntity%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=l5gBVLWzad0J&ei=rA9kYtPUDO-Sy9YPs_mY8AM&json=", "num_citations": 220, "citedby_url": "/scholar?cites=15954480746316535959&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:l5gBVLWzad0J:scholar.google.com/&scioq=Tracking+the+World+State+with+Recurrent+Entity+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.03969?ref=https://githubhelp.com"}, "Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding": {"container_type": "Publication", "bib": {"title": "Spatio-temporal abstractions in reinforcement learning through neural encoding", "author": ["N Baram", "T Zahavy", "S Mannor"], "pub_year": "2016", "venue": "NA", "abstract": "Recent progress in the field of Reinforcement Learning (RL) has enabled to tackle bigger and more challenging tasks. However, the increasing complexity of the problems, as well as the use of more sophisticated models such as Deep Neural Networks (DNN), impedes the understanding of artificial agents behavior. In this work, we present the Semi-Aggregated Markov Decision Process (SAMDP) model. The purpose of the SAMDP modeling is to describe and allow a better understanding of complex behaviors by identifying temporal and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1yjkAtxe", "author_id": ["feU9M4QAAAAJ", "9dXN6cMAAAAJ", "q1HlbIUAAAAJ"], "url_scholarbib": "/scholar?q=info:7GTDWeg0NQQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpatio-Temporal%2BAbstractions%2Bin%2BReinforcement%2BLearning%2BThrough%2BNeural%2BEncoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7GTDWeg0NQQJ&ei=rg9kYpLEL42ymgHg1rfQDQ&json=", "num_citations": 2, "citedby_url": "/scholar?cites=303206722460411116&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7GTDWeg0NQQJ:scholar.google.com/&scioq=Spatio-Temporal+Abstractions+in+Reinforcement+Learning+Through+Neural+Encoding&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1yjkAtxe"}, "Recurrent Hidden Semi-Markov Model": {"container_type": "Publication", "bib": {"title": "Recurrent hidden semi-markov model", "author": ["H Dai", "B Dai", "YM Zhang", "S Li", "L Song"], "pub_year": "2016", "venue": "NA", "abstract": "modeling nonlinear and complicated dependencies (Sutskever et al., 2014; Du et al., 2016),  we introduce the recurrent neural emission model  flexibility of recurrent neural model comes"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJGODLqgx", "author_id": ["obpl7GQAAAAJ", "TIKl_foAAAAJ", "uzuzYhgAAAAJ", "HxCZsCUAAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:rx65_MUGYn8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BHidden%2BSemi-Markov%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rx65_MUGYn8J&ei=sg9kYuCaH4uKmgGY1YjABQ&json=", "num_citations": 28, "citedby_url": "/scholar?cites=9178906437952806575&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rx65_MUGYn8J:scholar.google.com/&scioq=Recurrent+Hidden+Semi-Markov+Model&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJGODLqgx"}, "Mollifying Networks": {"container_type": "Publication", "bib": {"title": "Mollifying networks", "author": ["C Gulcehre", "M Moczulski", "F Visin", "Y Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We compare an MLP with residual connections using batch normalization and a mollified  network with sigmoid activation function. As can be seen in Figure 4, the mollified network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1608.04980", "author_id": ["7hwJ2ckAAAAJ", "zeX8KGAAAAAJ", "kaAnZw0AAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:_pyDC6fxtVwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMollifying%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_pyDC6fxtVwJ&ei=tQ9kYunjILKO6rQPy-CRsA8&json=", "num_citations": 52, "citedby_url": "/scholar?cites=6680511322029006078&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_pyDC6fxtVwJ:scholar.google.com/&scioq=Mollifying+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1608.04980"}, "Deep Probabilistic Programming": {"container_type": "Publication", "bib": {"title": "Deep probabilistic programming", "author": ["D Tran", "MD Hoffman", "RA Saurous", "E Brevdo"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward  that  probabilistic programming can be as flexible and computationally efficient as traditional deep"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.03757", "author_id": ["wVazIm8AAAAJ", "IeHKeGYAAAAJ", "QNnjg7YAAAAJ", "NvMCACEAAAAJ"], "url_scholarbib": "/scholar?q=info:MxKHpKFtdDIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BProbabilistic%2BProgramming%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MxKHpKFtdDIJ&ei=ug9kYsmVFsLZmQHnraWYCA&json=", "num_citations": 204, "citedby_url": "/scholar?cites=3635651340212310579&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MxKHpKFtdDIJ:scholar.google.com/&scioq=Deep+Probabilistic+Programming&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.03757.pdf?source=post_page---------------------------"}, "Character-aware Attention Residual Network for Sentence Representation": {"container_type": "Publication", "bib": {"title": "Character-aware Attention Residual Network for Sentence Representation", "author": ["X Zheng", "Z Wu"], "pub_year": "2016", "venue": "NA", "abstract": "Text classification in general is a well studied area. However, classifying short and noisy text remains challenging. Feature sparsity is a major issue. The quality of document representation here has a great impact on the classification accuracy. Existing methods represent text using bag-of-word model, with TFIDF or other weighting schemes. Recently word embedding and even document embedding are proposed to represent text. The purpose is to capture features at both word level and sentence level. However, the character"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1Go7Koex", "author_id": ["", "KxqIRhcAAAAJ"], "url_scholarbib": "/scholar?q=info:XVi7WTc2dpAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCharacter-aware%2BAttention%2BResidual%2BNetwork%2Bfor%2BSentence%2BRepresentation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XVi7WTc2dpAJ&ei=vg9kYsaXAoyuyAT-mrWwCA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:XVi7WTc2dpAJ:scholar.google.com/&scioq=Character-aware+Attention+Residual+Network+for+Sentence+Representation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1Go7Koex"}, "Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory": {"container_type": "Publication", "bib": {"title": "Implicit reasonet: Modeling large-scale structured relationships with shared memory", "author": ["Y Shen", "PS Huang", "MW Chang", "J Gao"], "pub_year": "2016", "venue": "NA", "abstract": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1PRvK9el", "author_id": ["S6OFEFEAAAAJ", "4oJB32YAAAAJ", "GiCqMFkAAAAJ", "CQ1cqKkAAAAJ"], "url_scholarbib": "/scholar?q=info:0xHTLf1tnn0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImplicit%2BReasoNet:%2BModeling%2BLarge-Scale%2BStructured%2BRelationships%2Bwith%2BShared%2BMemory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0xHTLf1tnn0J&ei=yQ9kYoiPBMLZmQHnraWYCA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=9051793235224236499&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0xHTLf1tnn0J:scholar.google.com/&scioq=Implicit+ReasoNet:+Modeling+Large-Scale+Structured+Relationships+with+Shared+Memory&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1PRvK9el"}, "Multi-modal Variational Encoder-Decoders": {"container_type": "Publication", "bib": {"title": "Multi-modal variational encoder-decoders", "author": ["IV Serban", "AG Ororbia II", "J Pineau", "A Courville"], "pub_year": "2016", "venue": "NA", "abstract": "Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, uni-modal priors\u2014such as the multivariate Gaussian distribution\u2014yet many real-world data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJ9fZNqle", "author_id": ["0g31OfAAAAAJ", "IAFscrMAAAAJ", "CEt6_mMAAAAJ", "km6CP8cAAAAJ"], "url_scholarbib": "/scholar?q=info:1AWeEn-KU3MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-modal%2BVariational%2BEncoder-Decoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1AWeEn-KU3MJ&ei=yw9kYpSHNpyO6rQP_qe3mAs&json=", "num_citations": 18, "citedby_url": "/scholar?cites=8310138015806522836&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1AWeEn-KU3MJ:scholar.google.com/&scioq=Multi-modal+Variational+Encoder-Decoders&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJ9fZNqle"}, "Learning Disentangled Representations in Deep Generative Models": {"container_type": "Publication", "bib": {"title": "Learning disentangled representations in deep generative models", "author": ["N Siddharth", "B Paige", "A Desmaison", "JW van de Meent"], "pub_year": "2016", "venue": "NA", "abstract": "Deep generative models provide a powerful and flexible means to learn complex distributions over data by incorporating neural networks into latent-variable models. Variational approaches to training such models introduce a probabilistic encoder that casts data, typically unsupervised, into an entangled and unstructured representation space. While unsupervised learning is often desirable, sometimes even necessary, when we lack prior knowledge about what to represent, being able to incorporate domain knowledge in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJtN5K9gx", "author_id": ["V7D7hxMAAAAJ", "JrFJmx0AAAAJ", "_cHRq1kAAAAJ", "aCGsfUAAAAAJ"], "url_scholarbib": "/scholar?q=info:hs5m7VDylFoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDisentangled%2BRepresentations%2Bin%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hs5m7VDylFoJ&ei=zw9kYuT2A_mQ6rQP5OqKqAo&json=", "num_citations": 3, "citedby_url": "/scholar?cites=6527108189360934534&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hs5m7VDylFoJ:scholar.google.com/&scioq=Learning+Disentangled+Representations+in+Deep+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJtN5K9gx"}, "Demystifying ResNet": {"container_type": "Publication", "bib": {"title": "Demystifying resnet", "author": ["S Li", "J Jiao", "Y Han", "T Weissman"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01186", "abstract": "Now we present a simplified ResNet structure with shortcut of length 2, and prove that as   have vanishing norm, which is observed in ResNet as we mentioned. This argument is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01186", "author_id": ["ZNquLr8AAAAJ", "", "hdTDzlQAAAAJ", "nTiSnwUAAAAJ"], "url_scholarbib": "/scholar?q=info:YcjT1kLyIsIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDemystifying%2BResNet%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YcjT1kLyIsIJ&ei=1Q9kYrSUFZGJmwGY-qmYDQ&json=", "num_citations": 37, "citedby_url": "/scholar?cites=13989009761452148833&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YcjT1kLyIsIJ:scholar.google.com/&scioq=Demystifying+ResNet&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01186.pdf?source=post_page---------------------------"}, "Sampling Generative Networks": {"container_type": "Publication", "bib": {"title": "Sampling generative networks", "author": ["T White"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.04468", "abstract": "several techniques for sampling and visualizing the latent spaces of generative models.   from a model\u2019s prior distribution and produces sharper samples. J-Diagrams and MINE grids are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.04468", "author_id": [""], "url_scholarbib": "/scholar?q=info:1DW7ne6IRA0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSampling%2BGenerative%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1DW7ne6IRA0J&ei=2A9kYuS2J5qSy9YP8pKNsAE&json=", "num_citations": 188, "citedby_url": "/scholar?cites=956039579339273684&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1DW7ne6IRA0J:scholar.google.com/&scioq=Sampling+Generative+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.04468"}, "Learning Recurrent Span Representations for Extractive Question Answering": {"container_type": "Publication", "bib": {"title": "Learning recurrent span representations for extractive question answering", "author": ["K Lee", "S Salant", "T Kwiatkowski", "A Parikh", "D Das"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al.(2016) recently released the SQuAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01436", "author_id": ["qXwJkr8AAAAJ", "cSLTQNAAAAAJ", "MpZ6dTEAAAAJ", "bRpjhycAAAAJ", "Xlv5PDYAAAAJ"], "url_scholarbib": "/scholar?q=info:3964ddafPTMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRecurrent%2BSpan%2BRepresentations%2Bfor%2BExtractive%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3964ddafPTMJ&ei=3g9kYr2cFM6E6rQP5-KmKA&json=", "num_citations": 132, "citedby_url": "/scholar?cites=3692283012960542431&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3964ddafPTMJ:scholar.google.com/&scioq=Learning+Recurrent+Span+Representations+for+Extractive+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01436"}, "Efficient Communications in Training Large Scale Neural Networks": {"container_type": "Publication", "bib": {"title": "Efficient communications in training large scale neural networks", "author": ["L Wang", "W Wu", "G Bosilca", "R Vuduc", "Z Xu"], "pub_year": "2016", "venue": "NA", "abstract": "We consider the problem of how to reduce the cost of communication that is re-quired for the parallel training of a neural network. The state-of-the-art method, Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD), requires a many collective communication operations, like broadcasts of parameters or reduc-tions for sub-gradient aggregations, which for large messages quickly dominates overall execution time and limits parallel scalability. To address this problem, we develop a new technique for collective operations"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkzuKpLgg", "author_id": ["k1cGv3MAAAAJ", "PCCajlEAAAAJ", "8Rls4ewAAAAJ", "CCGI7x4AAAAJ", "gF0H9nEAAAAJ"], "url_scholarbib": "/scholar?q=info:u3vWqu-lmNUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BCommunications%2Bin%2BTraining%2BLarge%2BScale%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=u3vWqu-lmNUJ&ei=4g9kYrX3JZGJmwGY-qmYDQ&json=", "num_citations": 8, "citedby_url": "/scholar?cites=15391234175320619963&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:u3vWqu-lmNUJ:scholar.google.com/&scioq=Efficient+Communications+in+Training+Large+Scale+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkzuKpLgg"}, "Improving Invariance and Equivariance Properties of Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Improving invariance and equivariance properties of convolutional neural networks", "author": ["C Tensmeyer", "T Martinez"], "pub_year": "2016", "venue": "NA", "abstract": "Convolutional Neural Networks (CNNs) learn highly discriminative representations from data, but how robust and structured are these representations? How does the data shape the internal network representation? We shed light on these questions by empirically measuring the invariance and equivariance properties of a large number of CNNs trained with various types of input transformations. We find that CNNs learn invariance wrt all 9 tested transformation types and that invariance extends to transformations outside the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Syfkm6cgx", "author_id": ["RjfiYWkAAAAJ", "R1JyT9AAAAAJ"], "url_scholarbib": "/scholar?q=info:q19IMCemKs8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BInvariance%2Band%2BEquivariance%2BProperties%2Bof%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=q19IMCemKs8J&ei=5Q9kYu3JNY6pywTd4KPADw&json=", "num_citations": 5, "citedby_url": "/scholar?cites=14927926602116915115&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:q19IMCemKs8J:scholar.google.com/&scioq=Improving+Invariance+and+Equivariance+Properties+of+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Syfkm6cgx"}, "Representing inferential uncertainty in deep neural networks through sampling": {"container_type": "Publication", "bib": {"title": "Representing inferential uncertainty in deep neural networks through sampling", "author": ["P McClure", "N Kriegeskorte"], "pub_year": "2016", "venue": "NA", "abstract": "As deep neural networks (DNNs) are applied to increasingly challenging problems, they will need to be able to represent their own uncertainty. Modelling uncertainty is one of the key features of Bayesian methods. Bayesian DNNs that use dropout-based variational distributions and scale to complex tasks have recently been proposed. We evaluate Bayesian DNNs trained with Bernoulli or Gaussian multiplicative masking of either the units (dropout) or the weights (dropconnect). We compare these Bayesian DNNs ability to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJ1JBJ5gl", "author_id": ["8TKyUl4AAAAJ", "w6M4YN4AAAAJ"], "url_scholarbib": "/scholar?q=info:zOIEvwmqDuUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRepresenting%2Binferential%2Buncertainty%2Bin%2Bdeep%2Bneural%2Bnetworks%2Bthrough%2Bsampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zOIEvwmqDuUJ&ei=6Q9kYrXaBLKO6rQPy-CRsA8&json=", "num_citations": 6, "citedby_url": "/scholar?cites=16505316643195642572&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zOIEvwmqDuUJ:scholar.google.com/&scioq=Representing+inferential+uncertainty+in+deep+neural+networks+through+sampling&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJ1JBJ5gl"}, "Transfer of View-manifold Learning to Similarity Perception of Novel Objects": {"container_type": "Publication", "bib": {"title": "Transfer of view-manifold learning to similarity perception of novel objects", "author": ["X Lin", "H Wang", "Z Li", "Y Zhang", "A Yuille"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.00033", "author_id": ["hankD2kAAAAJ", "", "", "43HsZ4oAAAAJ", "FJ-huxgAAAAJ"], "url_scholarbib": "/scholar?q=info:WXo3xIEMaB8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransfer%2Bof%2BView-manifold%2BLearning%2Bto%2BSimilarity%2BPerception%2Bof%2BNovel%2BObjects%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WXo3xIEMaB8J&ei=7A9kYsDcIuiSy9YPp-OyiAE&json=", "num_citations": 10, "citedby_url": "/scholar?cites=2263072564235958873&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WXo3xIEMaB8J:scholar.google.com/&scioq=Transfer+of+View-manifold+Learning+to+Similarity+Perception+of+Novel+Objects&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.00033.pdf?ref=https://githubhelp.com"}, "Song From PI: A Musically Plausible Network for Pop Music Generation": {"container_type": "Publication", "bib": {"title": "Song from PI: A musically plausible network for pop music generation", "author": ["H Chu", "R Urtasun", "S Fidler"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.03477", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.03477", "author_id": ["awvsNQYAAAAJ", "jyxO2akAAAAJ", "CUlqK5EAAAAJ"], "url_scholarbib": "/scholar?q=info:58qQQlk0bqYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSong%2BFrom%2BPI:%2BA%2BMusically%2BPlausible%2BNetwork%2Bfor%2BPop%2BMusic%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=58qQQlk0bqYJ&ei=7w9kYu-5EeHDywSSipaYAg&json=", "num_citations": 115, "citedby_url": "/scholar?cites=11992580415707728615&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:58qQQlk0bqYJ:scholar.google.com/&scioq=Song+From+PI:+A+Musically+Plausible+Network+for+Pop+Music+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.03477"}, "Semi-Supervised Classification with Graph Convolutional Networks": {"container_type": "Publication", "bib": {"title": "Semi-supervised classification with graph convolutional networks", "author": ["TN Kipf", "M Welling"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.02907", "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.02907", "author_id": ["83HL5FwAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:BY9LF23IgoYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-Supervised%2BClassification%2Bwith%2BGraph%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BY9LF23IgoYJ&ei=8g9kYum4JZLeyQTE46-QAg&json=", "num_citations": 14375, "citedby_url": "/scholar?cites=9692529718922546949&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BY9LF23IgoYJ:scholar.google.com/&scioq=Semi-Supervised+Classification+with+Graph+Convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.02907.pdf%EF%BC%89"}, "Discrete Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "Discrete variational autoencoders", "author": ["JT Rolfe"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.02200", "abstract": "In the following sections, we present the discrete variational autoencoder (discrete  variational  autoencoder formalism, as in Equation 3, including backpropagation through its discrete"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.02200", "author_id": ["dh03btIAAAAJ"], "url_scholarbib": "/scholar?q=info:grCdU3qDnsEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscrete%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=grCdU3qDnsEJ&ei=_g9kYsOKEoyuyAT-mrWwCA&json=", "num_citations": 148, "citedby_url": "/scholar?cites=13951733257052467330&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:grCdU3qDnsEJ:scholar.google.com/&scioq=Discrete+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.02200"}, "Categorical Reparameterization with Gumbel-Softmax": {"container_type": "Publication", "bib": {"title": "Categorical reparameterization with gumbel-softmax", "author": ["E Jang", "S Gu", "B Poole"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01144", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01144", "author_id": ["", "B8wslVsAAAAJ", "i5FMLA4AAAAJ"], "url_scholarbib": "/scholar?q=info:yCvxFXWy13YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCategorical%2BReparameterization%2Bwith%2BGumbel-Softmax%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yCvxFXWy13YJ&ei=ARBkYtz0JO-Sy9YPs_mY8AM&json=", "num_citations": 2880, "citedby_url": "/scholar?cites=8563509432417332168&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yCvxFXWy13YJ:scholar.google.com/&scioq=Categorical+Reparameterization+with+Gumbel-Softmax&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01144.pdf%20http://arxiv.org/abs/1611.01144"}, "Efficient Summarization with Read-Again and Copy Mechanism": {"container_type": "Publication", "bib": {"title": "Efficient summarization with read-again and copy mechanism", "author": ["W Zeng", "W Luo", "S Fidler", "R Urtasun"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.03382", "abstract": "introduce a simple mechanism that first reads the input sequence before committing to a  representation of each word. Furthermore, we propose a simple copy mechanism that is able to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.03382", "author_id": ["qSo45J0AAAAJ", "os-DVLkAAAAJ", "CUlqK5EAAAAJ", "jyxO2akAAAAJ"], "url_scholarbib": "/scholar?q=info:hbZvmpIhUt4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BSummarization%2Bwith%2BRead-Again%2Band%2BCopy%2BMechanism%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hbZvmpIhUt4J&ei=BhBkYuXkH6KUy9YP_JONiAY&json=", "num_citations": 98, "citedby_url": "/scholar?cites=16019903738050229893&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hbZvmpIhUt4J:scholar.google.com/&scioq=Efficient+Summarization+with+Read-Again+and+Copy+Mechanism&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.03382"}, "Short and Deep: Sketching and Neural Networks": {"container_type": "Publication", "bib": {"title": "Short and deep: Sketching and neural networks", "author": ["A Daniely", "N Lazic", "Y Singer", "K Talwar"], "pub_year": "2017", "venue": "NA", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years. These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data. For example, preserving linear separability requires $\\Omega (1/\\gamma^ 2) $ dimensions, where $\\gamma $ is the margin, and in the case of polynomial functions, the number of required dimensions has an"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1br_2Kge", "author_id": ["jUtYwE0AAAAJ", "TXHN2DMAAAAJ", "", "XD_01h8AAAAJ"], "url_scholarbib": "/scholar?q=info:xzWAAq9_z4gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DShort%2Band%2BDeep:%2BSketching%2Band%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xzWAAq9_z4gJ&ei=DRBkYpEP6JLL1g-n47KIAQ&json=", "num_citations": 6, "citedby_url": "/scholar?cites=9858238498975266247&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xzWAAq9_z4gJ:scholar.google.com/&scioq=Short+and+Deep:+Sketching+and+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1hsDCNFx"}, "Learning to Discover Sparse Graphical Models": {"container_type": "Publication", "bib": {"title": "Learning to discover sparse graphical models", "author": ["E Belilovsky", "K Kastner", "G Varoquaux"], "pub_year": "2017", "venue": "International \u2026", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. Popular methods rely on estimating a penalized maximum likelihood of the precision matrix. However, in these approaches structure recovery is an indirect consequence of the data-fit term, the penalty can be difficult to adapt for domain-specific knowledge, and the inference is computationally demanding. By"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v70/belilovsky17a.html", "author_id": ["CffJDoEAAAAJ", "0XtGoMUAAAAJ", "OGGu384AAAAJ"], "url_scholarbib": "/scholar?q=info:TAKbgrRgpXgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BDiscover%2BSparse%2BGraphical%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TAKbgrRgpXgJ&ei=FBBkYqedKY2ymgHg1rfQDQ&json=", "num_citations": 16, "citedby_url": "/scholar?cites=8693460984110187084&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TAKbgrRgpXgJ:scholar.google.com/&scioq=Learning+to+Discover+Sparse+Graphical+Models&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/belilovsky17a/belilovsky17a.pdf"}, "Beyond Fine Tuning: A Modular Approach to Learning on Small Data": {"container_type": "Publication", "bib": {"title": "Beyond fine tuning: A modular approach to learning on small data", "author": ["A Anderson", "K Shaffer", "A Yankov", "CD Corley"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01714", "author_id": ["", "0FhPWtUAAAAJ", "", "98JOm8wAAAAJ"], "url_scholarbib": "/scholar?q=info:3gDTYJu8QwEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BFine%2BTuning:%2BA%2BModular%2BApproach%2Bto%2BLearning%2Bon%2BSmall%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3gDTYJu8QwEJ&ei=GRBkYsb0GO-Sy9YPs_mY8AM&json=", "num_citations": 18, "citedby_url": "/scholar?cites=91123793007935710&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3gDTYJu8QwEJ:scholar.google.com/&scioq=Beyond+Fine+Tuning:+A+Modular+Approach+to+Learning+on+Small+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01714"}, "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model": {"container_type": "Publication", "bib": {"title": "SampleRNN: An unconditional end-to-end neural audio generation model", "author": ["S Mehri", "K Kumar", "I Gulrajani", "R Kumar", "S Jain"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.07837", "author_id": ["", "Sh5PsUUAAAAJ", "E2SLBwIAAAAJ", "hJjeVsQAAAAJ", "FORjyOAAAAAJ"], "url_scholarbib": "/scholar?q=info:4SGJ2QYl6f0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSampleRNN:%2BAn%2BUnconditional%2BEnd-to-End%2BNeural%2BAudio%2BGeneration%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4SGJ2QYl6f0J&ei=HBBkYovjGeiSy9YPp-OyiAE&json=", "num_citations": 491, "citedby_url": "/scholar?cites=18296195672519025121&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4SGJ2QYl6f0J:scholar.google.com/&scioq=SampleRNN:+An+Unconditional+End-to-End+Neural+Audio+Generation+Model&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.07837.pdf?ref=https://githubhelp.com"}, "Recurrent Inference Machines for Solving Inverse Problems": {"container_type": "Publication", "bib": {"title": "Recurrent inference machines for solving inverse problems", "author": ["P Putzky", "M Welling"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.04008", "abstract": "definition of Recurrent Inference Machines (RIM) we aim to establish a framework  of inverse  problem. In the following, we will motivate the RIM framework from gradient-based inference"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.04008", "author_id": ["9-7SpW8AAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:NLZsZ4DAvCMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BInference%2BMachines%2Bfor%2BSolving%2BInverse%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NLZsZ4DAvCMJ&ei=IBBkYpLWEu-Sy9YPs_mY8AM&json=", "num_citations": 85, "citedby_url": "/scholar?cites=2575144744672605748&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NLZsZ4DAvCMJ:scholar.google.com/&scioq=Recurrent+Inference+Machines+for+Solving+Inverse+Problems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.04008"}, "LSTM-Based System-Call Language Modeling and Ensemble Method for Host-Based Intrusion Detection": {"container_type": "Publication", "bib": {"title": "LSTM-based system-call language modeling and ensemble method for host-based intrusion detection", "author": ["G Kim", "H Yi", "J Lee", "Y Paek", "S Yoon"], "pub_year": "2016", "venue": "NA", "abstract": "In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulatehighly normal'sequences. The proposed system-call"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rksfwnFxl", "author_id": ["LAl0EukAAAAJ", "QANTjTEAAAAJ", "hnqxuIcAAAAJ", "lQ6E5F0AAAAJ", "Bphl_fIAAAAJ"], "url_scholarbib": "/scholar?q=info:B9b_6xCkot8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLSTM-Based%2BSystem-Call%2BLanguage%2BModeling%2Band%2BEnsemble%2BMethod%2Bfor%2BHost-Based%2BIntrusion%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=B9b_6xCkot8J&ei=KRBkYufkI5qSy9YP8pKNsAE&json=", "num_citations": 1, "citedby_url": "/scholar?cites=16114622809270900231&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:B9b_6xCkot8J:scholar.google.com/&scioq=LSTM-Based+System-Call+Language+Modeling+and+Ensemble+Method+for+Host-Based+Intrusion+Detection&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rksfwnFxl"}, "GRAM: Graph-based Attention Model for Healthcare Representation Learning": {"container_type": "Publication", "bib": {"title": "GRAM: graph-based attention model for healthcare representation learning", "author": ["E Choi", "MT Bahadori", "L Song", "WF Stewart"], "pub_year": "2017", "venue": "Proceedings of the 23rd \u2026", "abstract": "\u2022 Interpretation: The representations learned by deep learning methods should align with  medical knowledge. To address these challenges, we propose GRaph-based Attention  Model (GRAM) that supplements electronic health records (EHR) with hierarchical  information inherent to medical ontologies. Based on the data volume and the ontology  structure,  We proposed GRAM, a graph-based attention model using both a knowledge  DAG and EHR to learn an accurate and interpretable representations for medical"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3097983.3098126", "author_id": ["GUlGIPkAAAAJ", "tlZvhyoAAAAJ", "Xl4E0CsAAAAJ", "flGRoHEAAAAJ"], "url_scholarbib": "/scholar?q=info:3Uj4lMW8MIAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGRAM:%2BGraph-based%2BAttention%2BModel%2Bfor%2BHealthcare%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3Uj4lMW8MIAJ&ei=LRBkYrexCeiSy9YPp-OyiAE&json=", "num_citations": 419, "citedby_url": "/scholar?cites=9237090392530766045&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3Uj4lMW8MIAJ:scholar.google.com/&scioq=GRAM:+Graph-based+Attention+Model+for+Healthcare+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3097983.3098126"}, "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks": {"container_type": "Publication", "bib": {"title": "Learning and policy search in stochastic dynamical systems with bayesian neural networks", "author": ["S Depeweg", "JM Hern\u00e1ndez-Lobato"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing $\\alpha $-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, eg multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.07127", "author_id": ["qvgzd6gAAAAJ", "BEBccCQAAAAJ"], "url_scholarbib": "/scholar?q=info:nf0xMHlCLcoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Band%2BPolicy%2BSearch%2Bin%2BStochastic%2BDynamical%2BSystems%2Bwith%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nf0xMHlCLcoJ&ei=LxBkYrXOO6KUy9YP_JONiAY&json=", "num_citations": 140, "citedby_url": "/scholar?cites=14568373457880481181&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nf0xMHlCLcoJ:scholar.google.com/&scioq=Learning+and+Policy+Search+in+Stochastic+Dynamical+Systems+with+Bayesian+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.07127"}, "An Analysis of Feature Regularization for Low-shot Learning": {"container_type": "Publication", "bib": {"title": "An analysis of feature regularization for low-shot learning", "author": ["Z Chen", "H Zhao", "X Liu", "W Xu"], "pub_year": "2016", "venue": "NA", "abstract": "Low-shot visual learning, the ability to recognize novel object categories from very few, or even one example, is a hallmark of human visual intelligence. Though successful on many tasks, deep learning approaches tends to be notoriously data-hungry. Recently, feature penalty regularization has been proved effective on capturing new concepts. In this work, we provide both empirical evidence and theoretical analysis on how and why these methods work. We also propose a better design of cost function with improved performance. Close"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkgSXUKxx", "author_id": ["CWCGa-IAAAAJ", "", "", ""], "url_scholarbib": "/scholar?q=info:V6rgfJDb5lwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BAnalysis%2Bof%2BFeature%2BRegularization%2Bfor%2BLow-shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=V6rgfJDb5lwJ&ei=NRBkYt_mMIyuyAT-mrWwCA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=6694279309749693015&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:V6rgfJDb5lwJ:scholar.google.com/&scioq=An+Analysis+of+Feature+Regularization+for+Low-shot+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkgSXUKxx"}, "ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation": {"container_type": "Publication", "bib": {"title": "Enet: A deep neural network architecture for real-time semantic segmentation", "author": ["A Paszke", "A Chaurasia", "S Kim", "E Culurciello"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18$\\times $ faster, requires 75$\\times $ less FLOPs, has"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.02147", "author_id": ["LkVtZkQAAAAJ", "r8tMFqMAAAAJ", "ZbM6S_AAAAAJ", "SeGmqkIAAAAJ"], "url_scholarbib": "/scholar?q=info:6W65FEGvrIsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DENet:%2BA%2BDeep%2BNeural%2BNetwork%2BArchitecture%2Bfor%2BReal-Time%2BSemantic%2BSegmentation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6W65FEGvrIsJ&ei=QBBkYtGgN-iSy9YPp-OyiAE&json=", "num_citations": 1495, "citedby_url": "/scholar?cites=10064611961321647849&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6W65FEGvrIsJ:scholar.google.com/&scioq=ENet:+A+Deep+Neural+Network+Architecture+for+Real-Time+Semantic+Segmentation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.02147.pdf)%C3%AC%20%E2%82%AC"}, "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks": {"container_type": "Publication", "bib": {"title": "Neural taylor approximations: Convergence and exploration in rectifier networks", "author": ["D Balduzzi", "B McWilliams"], "pub_year": "2017", "venue": "\u2026 on Machine Learning", "abstract": "Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex; standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets, which furthermore matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation\u2013a"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v70/balduzzi17c.html", "author_id": ["xA3Jd5gAAAAJ", "IS4VSXAAAAAJ"], "url_scholarbib": "/scholar?q=info:OKVFFSrxM9kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BTaylor%2BApproximations:%2BConvergence%2Band%2BExploration%2Bin%2BRectifier%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OKVFFSrxM9kJ&ei=RRBkYq67D_mQ6rQP5OqKqAo&json=", "num_citations": 30, "citedby_url": "/scholar?cites=15651118293090411832&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OKVFFSrxM9kJ:scholar.google.com/&scioq=Neural+Taylor+Approximations:+Convergence+and+Exploration+in+Rectifier+Networks&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/balduzzi17c/balduzzi17c.pdf"}, "A Learned Representation For Artistic Style": {"container_type": "Publication", "bib": {"title": "A learned representation for artistic style", "author": ["V Dumoulin", "J Shlens", "M Kudlur"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.07629", "abstract": "across a diversity of artistic styles by reducing a painting to a  painting styles by arbitrarily  combining the styles learned from  to the structure of the learned representation of artistic style."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.07629", "author_id": ["mZfgLA4AAAAJ", "sm1q2bYAAAAJ", "Jjp8eYUAAAAJ"], "url_scholarbib": "/scholar?q=info:ByWJUYWS1mIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BLearned%2BRepresentation%2BFor%2BArtistic%2BStyle%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ByWJUYWS1mIJ&ei=SRBkYtr-BJGJmwGY-qmYDQ&json=", "num_citations": 825, "citedby_url": "/scholar?cites=7122040962029266183&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ByWJUYWS1mIJ:scholar.google.com/&scioq=A+Learned+Representation+For+Artistic+Style&hl=en&as_sdt=0,33"}, "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency": {"container_type": "Publication", "bib": {"title": "Topicrnn: A recurrent neural network with long-range semantic dependency", "author": ["AB Dieng", "C Wang", "J Gao", "J Paisley"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01702", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence-both semantic and syntactic-but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01702", "author_id": ["ZCniP_MAAAAJ", "vRI2blsAAAAJ", "CQ1cqKkAAAAJ", "r31_fYQAAAAJ"], "url_scholarbib": "/scholar?q=info:YLPXyH8zzpMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTopicRNN:%2BA%2BRecurrent%2BNeural%2BNetwork%2Bwith%2BLong-Range%2BSemantic%2BDependency%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YLPXyH8zzpMJ&ei=TBBkYsWWFpLeyQTE46-QAg&json=", "num_citations": 214, "citedby_url": "/scholar?cites=10650506792701244256&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YLPXyH8zzpMJ:scholar.google.com/&scioq=TopicRNN:+A+Recurrent+Neural+Network+with+Long-Range+Semantic+Dependency&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01702.pdf)."}, "Skip-graph: Learning graph embeddings with an encoder-decoder model": {"container_type": "Publication", "bib": {"title": "Skip-graph: Learning graph embeddings with an encoder-decoder model", "author": ["JB Lee", "X Kong"], "pub_year": "2016", "venue": "NA", "abstract": "In this work, we study the problem of feature representation learning for graph-structured data. Many of the existing work in the area are task-specific and based on supervised techniques. We study a method for obtaining a generic feature representation for a graph using an unsupervised approach. The neural encoder-decoder model is a method that has been used in the natural language processing domain to learn feature representations of sentences. In our proposed approach, we train the encoder-decoder model to predict the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkSqjHqxg", "author_id": ["", "Lh7VfoMAAAAJ"], "url_scholarbib": "/scholar?q=info:UuCZ1NMHzc4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkip-graph:%2BLearning%2Bgraph%2Bembeddings%2Bwith%2Ban%2Bencoder-decoder%2Bmodel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UuCZ1NMHzc4J&ei=YRBkYszWOY6pywTd4KPADw&json=", "num_citations": 6, "citedby_url": "/scholar?cites=14901575348425187410&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UuCZ1NMHzc4J:scholar.google.com/&scioq=Skip-graph:+Learning+graph+embeddings+with+an+encoder-decoder+model&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkSqjHqxg"}, "Joint Training of Ratings and Reviews with Recurrent Recommender Networks": {"container_type": "Publication", "bib": {"title": "Joint training of ratings and reviews with recurrent recommender networks", "author": ["CY Wu", "A Ahmed", "A Beutel", "AJ Smola"], "pub_year": "2017", "venue": "NA", "abstract": "Accurate modeling of ratings and text reviews is at the core of successful recommender systems. While neural networks have been remarkably successful in modeling images and natural language, they have been largely unexplored in recommender system research. In this paper, we provide a neural network model that combines ratings, reviews, and temporal patterns to learn highly accurate recommendations. We co-train for prediction on both numerical ratings and natural language reviews, as well as using a recurrent architecture to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bkv9FyHYx", "author_id": ["QiJPlOIAAAAJ", "ivUi2T0AAAAJ", "zEYMVR0AAAAJ", "Tb0ZrYwAAAAJ"], "url_scholarbib": "/scholar?q=info:5VcstdQLRC0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DJoint%2BTraining%2Bof%2BRatings%2Band%2BReviews%2Bwith%2BRecurrent%2BRecommender%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5VcstdQLRC0J&ei=ZBBkYsydHJGJmwGY-qmYDQ&json=", "num_citations": 28, "citedby_url": "/scholar?cites=3261745038323636197&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5VcstdQLRC0J:scholar.google.com/&scioq=Joint+Training+of+Ratings+and+Reviews+with+Recurrent+Recommender+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bkv9FyHYx"}, "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System": {"container_type": "Publication", "bib": {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["R Collobert", "C Puhrsch", "G Synnaeve"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.03193", "abstract": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.03193", "author_id": ["32w7x1cAAAAJ", "YfompPIAAAAJ", "wN9rBkcAAAAJ"], "url_scholarbib": "/scholar?q=info:UT2YYRYdjmEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWav2Letter:%2Ban%2BEnd-to-End%2BConvNet-based%2BSpeech%2BRecognition%2BSystem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UT2YYRYdjmEJ&ei=ZxBkYoi4HO-Sy9YPs_mY8AM&json=", "num_citations": 248, "citedby_url": "/scholar?cites=7029588050335776081&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UT2YYRYdjmEJ:scholar.google.com/&scioq=Wav2Letter:+an+End-to-End+ConvNet-based+Speech+Recognition+System&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.03193"}, "Recurrent Batch Normalization": {"container_type": "Publication", "bib": {"title": "Recurrent batch normalization", "author": ["T Cooijmans", "N Ballas", "C Laurent", "\u00c7 G\u00fcl\u00e7ehre"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "batch normalization to recurrent neural networks. Whereas previous works only apply  batch normalization  that it is both possible and beneficial to batch-normalize the hidden-to-hidden"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1603.09025", "author_id": ["Ec6vKzwAAAAJ", "euUV4iUAAAAJ", "Ev9dOhQAAAAJ", "7hwJ2ckAAAAJ"], "url_scholarbib": "/scholar?q=info:8gbfRkSJHeQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8gbfRkSJHeQJ&ei=ahBkYpy9PJLeyQTE46-QAg&json=", "num_citations": 404, "citedby_url": "/scholar?cites=16437445141311981298&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8gbfRkSJHeQJ:scholar.google.com/&scioq=Recurrent+Batch+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1603.09025"}, "Counterpoint by Convolution": {"container_type": "Publication", "bib": {"title": "Counterpoint by convolution", "author": ["CZA Huang", "T Cooijmans", "A Roberts"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Moreover, convolutional neural networks have shown to be  We introduce COCONET, a deep  convolutional model trained  complex domain of musical counterpoint. Instead, we propose"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.07227", "author_id": ["NRz_EVgAAAAJ", "Ec6vKzwAAAAJ", "U5UpKq8AAAAJ"], "url_scholarbib": "/scholar?q=info:iDPEn6Wtz-0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCounterpoint%2Bby%2BConvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iDPEn6Wtz-0J&ei=fhBkYpi5BY6pywTd4KPADw&json=", "num_citations": 119, "citedby_url": "/scholar?cites=17136106034029671304&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iDPEn6Wtz-0J:scholar.google.com/&scioq=Counterpoint+by+Convolution&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.07227"}, "A recurrent neural network without chaos": {"container_type": "Publication", "bib": {"title": "A recurrent neural network without chaos", "author": ["T Laurent", "J von Brecht"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.06212", "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.06212", "author_id": ["_Ag_9uAAAAAJ", "pgcetcYAAAAJ"], "url_scholarbib": "/scholar?q=info:vu8U7_sHx6UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2Brecurrent%2Bneural%2Bnetwork%2Bwithout%2Bchaos%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vu8U7_sHx6UJ&ei=hhBkYo9Q6JLL1g-n47KIAQ&json=", "num_citations": 56, "citedby_url": "/scholar?cites=11945525315252842430&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vu8U7_sHx6UJ:scholar.google.com/&scioq=A+recurrent+neural+network+without+chaos&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.06212"}, "Sigma Delta Quantized Networks": {"container_type": "Publication", "bib": {"title": "Sigma delta quantized networks", "author": ["P O'Connor", "M Welling"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.02024", "abstract": ", we introduce SigmaDelta networks. With each new input, each layer in this network sends  a  Thus the amount of computation that the network does scales with the amount of change in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02024", "author_id": ["", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:CyV9yIysfgwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSigma%2BDelta%2BQuantized%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CyV9yIysfgwJ&ei=ihBkYq6yN5GJmwGY-qmYDQ&json=", "num_citations": 28, "citedby_url": "/scholar?cites=900346696179721483&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CyV9yIysfgwJ:scholar.google.com/&scioq=Sigma+Delta+Quantized+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02024"}, "Parallel Stochastic Gradient Descent with Sound Combiners": {"container_type": "Publication", "bib": {"title": "Parallel stochastic gradient descent with sound combiners", "author": ["S Maleki", "M Musuvathi", "T Mytkowicz"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.08030", "abstract": "Stochastic gradient descent (SGD) is a well known method for regression and classification tasks. However, it is an inherently sequential algorithm at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing linear learners using SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependencies across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SYMSGD, a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.08030", "author_id": ["ndFBNlYAAAAJ", "6ENuGyoAAAAJ", "Z4y_Z3sAAAAJ"], "url_scholarbib": "/scholar?q=info:XNOB1fk1xyMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParallel%2BStochastic%2BGradient%2BDescent%2Bwith%2BSound%2BCombiners%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XNOB1fk1xyMJ&ei=jRBkYvabKaKUy9YP_JONiAY&json=", "num_citations": 5, "citedby_url": "/scholar?cites=2578088658838082396&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XNOB1fk1xyMJ:scholar.google.com/&scioq=Parallel+Stochastic+Gradient+Descent+with+Sound+Combiners&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.08030"}, "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks": {"container_type": "Publication", "bib": {"title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks", "author": ["D Hendrycks", "K Gimpel"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.02136", "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.02136", "author_id": ["czyretsAAAAJ", "kDHs7DYAAAAJ"], "url_scholarbib": "/scholar?q=info:VB-8elP7TMkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BBaseline%2Bfor%2BDetecting%2BMisclassified%2Band%2BOut-of-Distribution%2BExamples%2Bin%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VB-8elP7TMkJ&ei=kxBkYsqDGoySyASZk6HgCA&json=", "num_citations": 1311, "citedby_url": "/scholar?cites=14505244835813531476&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VB-8elP7TMkJ:scholar.google.com/&scioq=A+Baseline+for+Detecting+Misclassified+and+Out-of-Distribution+Examples+in+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.02136.pdf?ref=https://githubhelp.com"}, "Pruning Convolutional Neural Networks for Resource Efficient Inference": {"container_type": "Publication", "bib": {"title": "Pruning convolutional neural networks for resource efficient inference", "author": ["P Molchanov", "S Tyree", "T Karras", "T Aila"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation-a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.06440", "author_id": ["J9PoyoIAAAAJ", "PGZLZFUAAAAJ", "-50qJW8AAAAJ", "e7abmgkAAAAJ"], "url_scholarbib": "/scholar?q=info:SvebtpGhtL4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPruning%2BConvolutional%2BNeural%2BNetworks%2Bfor%2BResource%2BEfficient%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SvebtpGhtL4J&ei=lhBkYqH0F_mQ6rQP5OqKqAo&json=", "num_citations": 1242, "citedby_url": "/scholar?cites=13741786010220230474&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SvebtpGhtL4J:scholar.google.com/&scioq=Pruning+Convolutional+Neural+Networks+for+Resource+Efficient+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.06440.pdf?ref=https://githubhelp.com"}, "Nonparametric Neural Networks": {"container_type": "Publication", "bib": {"title": "Nonparametric neural networks", "author": ["G Philipp", "JG Carbonell"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.05440", "abstract": "We define a nonparametric neural network in the same way, except that the dimensionality  of the weight matrices is undetermined. Hence, the optimization problem becomes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.05440", "author_id": ["IMi6NZMAAAAJ", "wlqqttEAAAAJ"], "url_scholarbib": "/scholar?q=info:jiGskpHx870J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNonparametric%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jiGskpHx870J&ei=oRBkYqG5D4OEmgHx-5DADA&json=", "num_citations": 22, "citedby_url": "/scholar?cites=13687549300042375566&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jiGskpHx870J:scholar.google.com/&scioq=Nonparametric+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.05440"}, "Generative Paragraph Vector": {"container_type": "Publication", "bib": {"title": "Generative Paragraph Vector", "author": ["R Zhang", "J Guo", "Y Lan", "J Xu", "X Cheng"], "pub_year": "2018", "venue": "China Conference on Information \u2026", "abstract": "generative process, we are able to employ the corpus-wide constraint (ie, the prior distribution  over paragraph vectors)  Meanwhile, by modeling the generation of the paragraph vectors,"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-01012-6_9", "author_id": ["qwdqaO4AAAAJ", "nD0I3PUAAAAJ", "mNfCP3gAAAAJ", "su14mcEAAAAJ", "hY8aLqAAAAAJ"], "url_scholarbib": "/scholar?q=info:r0vaBl1Fp78J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BParagraph%2BVector%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r0vaBl1Fp78J&ei=oxBkYvLnOvmQ6rQP5OqKqAo&json=", "num_citations": 1, "citedby_url": "/scholar?cites=13810083048204159919&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r0vaBl1Fp78J:scholar.google.com/&scioq=Generative+Paragraph+Vector&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryT9R3Yxe"}, "On Robust Concepts and Small Neural Nets": {"container_type": "Publication", "bib": {"title": "Knowledge transfer in deep convolutional neural nets", "author": ["S Gutstein", "O Fuentes", "E Freudenthal"], "pub_year": "2008", "venue": "International Journal on \u2026", "abstract": "complex concepts when given only small training sets. In this paper, we apply knowledge  transfer to deep convolutional neural nets,  When such a net is forced to learn a robust internal"}, "filled": false, "gsrank": 1, "pub_url": "https://www.worldscientific.com/doi/abs/10.1142/S0218213008004059", "author_id": ["yy4_mswAAAAJ", "NdcgT00AAAAJ", "P7TJnpQAAAAJ"], "url_scholarbib": "/scholar?q=info:DnqQSO1HijgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BRobust%2BConcepts%2Band%2BSmall%2BNeural%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DnqQSO1HijgJ&ei=phBkYqrlBs6E6rQP5-KmKA&json=", "num_citations": 51, "citedby_url": "/scholar?cites=4074147897360284174&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DnqQSO1HijgJ:scholar.google.com/&scioq=On+Robust+Concepts+and+Small+Neural+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://www.aaai.org/Papers/FLAIRS/2007/Flairs07-020.pdf"}, "Training Group Orthogonal Neural Networks with Privileged Information": {"container_type": "Publication", "bib": {"title": "Training group orthogonal neural networks with privileged information", "author": ["Y Chen", "X Jin", "J Feng", "S Yan"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.06772", "abstract": "Learning rich and diverse representations is critical for the performance of deep convolutional neural networks (CNNs). In this paper, we consider how to use privileged information to promote inherent diversity of a single CNN model such that the model can learn better representations and offer stronger generalization ability. To this end, we propose a novel group orthogonal convolutional neural network (GoCNN) that learns untangled representations within each layer by exploiting provided privileged information"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.06772", "author_id": ["-JiwekUAAAAJ", "OEZ816YAAAAJ", "Q8iay0gAAAAJ", "DNuiPHwAAAAJ"], "url_scholarbib": "/scholar?q=info:KAXWkzRvrvAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BGroup%2BOrthogonal%2BNeural%2BNetworks%2Bwith%2BPrivileged%2BInformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KAXWkzRvrvAJ&ei=qRBkYpKKF5GJmwGY-qmYDQ&json=", "num_citations": 39, "citedby_url": "/scholar?cites=17342921486659618088&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KAXWkzRvrvAJ:scholar.google.com/&scioq=Training+Group+Orthogonal+Neural+Networks+with+Privileged+Information&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.06772"}, "Adaptive Feature Abstraction for Translating Video to Language": {"container_type": "Publication", "bib": {"title": "Adaptive feature abstraction for translating video to language", "author": ["Y Pu", "MR Min", "Z Gan", "L Carin"], "pub_year": "2017", "venue": "NA", "abstract": "A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. A novel attention mechanism with spatiotemporal alignment is employed to adaptively and sequentially focus on different layers of CNN features (levels of feature\" abstraction\"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on the YouTube2Text"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJ0K82lKg", "author_id": ["ftW7RoAAAAAJ", "", "E64XWyMAAAAJ", "ZhGL6WcAAAAJ"], "url_scholarbib": "/scholar?q=info:9kBzKfxO-FMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BFeature%2BAbstraction%2Bfor%2BTranslating%2BVideo%2Bto%2BLanguage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9kBzKfxO-FMJ&ei=rBBkYo7-IPmQ6rQP5OqKqAo&json=", "num_citations": 13, "citedby_url": "/scholar?cites=6050672944306405622&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9kBzKfxO-FMJ:scholar.google.com/&scioq=Adaptive+Feature+Abstraction+for+Translating+Video+to+Language&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJ0K82lKg"}, "Development of JavaScript-based deep learning platform and application to distributed training": {"container_type": "Publication", "bib": {"title": "Development of JavaScript-based deep learning platform and application to distributed training", "author": ["M Hidaka", "K Miura", "T Harada"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.01846", "abstract": "Deep learning is increasingly attracting attention for processing big data. Existing frameworks for deep learning must be set up to specialized computer systems. Gaining sufficient computing resources therefore entails high costs of deployment and maintenance. In this work, we implement a matrix library and deep learning framework that uses JavaScript. It can run on web browsers operating on ordinary personal computers and smartphones. Using JavaScript, deep learning can be accomplished in widely diverse"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.01846", "author_id": ["sRJNi0wAAAAJ", "M7MPeusAAAAJ", "k8rlJ8AAAAAJ"], "url_scholarbib": "/scholar?q=info:6qI37prgpnMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDevelopment%2Bof%2BJavaScript-based%2Bdeep%2Blearning%2Bplatform%2Band%2Bapplication%2Bto%2Bdistributed%2Btraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6qI37prgpnMJ&ei=thBkYp2CD7KO6rQPy-CRsA8&json=", "num_citations": 2, "citedby_url": "/scholar?cites=8333595116521890538&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6qI37prgpnMJ:scholar.google.com/&scioq=Development+of+JavaScript-based+deep+learning+platform+and+application+to+distributed+training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.01846"}, "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks": {"container_type": "Publication", "bib": {"title": "A joint many-task model: Growing a neural network for multiple nlp tasks", "author": ["K Hashimoto", "C Xiong", "Y Tsuruoka"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01587", "author_id": ["gVi99BIAAAAJ", "vaSdahkAAAAJ", "J2CkFngAAAAJ"], "url_scholarbib": "/scholar?q=info:9CN4qGaetC8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BJoint%2BMany-Task%2BModel:%2BGrowing%2Ba%2BNeural%2BNetwork%2Bfor%2BMultiple%2BNLP%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9CN4qGaetC8J&ei=uhBkYo7vBOHDywSSipaYAg&json=", "num_citations": 514, "citedby_url": "/scholar?cites=3437546579340829684&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9CN4qGaetC8J:scholar.google.com/&scioq=A+Joint+Many-Task+Model:+Growing+a+Neural+Network+for+Multiple+NLP+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01587.pdf%5D%5BCode%5D%5Bhttps:/github.com/rajarsheem/joint-many-task-model"}, "Rule Mining in Feature Space": {"container_type": "Publication", "bib": {"title": "Reducing the Feature Space Using Constraint-Governed Association Rule Mining", "author": ["MU Salma"], "pub_year": "2017", "venue": "Journal of Intelligent Systems", "abstract": "develop a feature selection process, where a data set subjected to constraint-governed  association rule mining and interestingness measures results in a small feature subset capable"}, "filled": false, "gsrank": 1, "pub_url": "https://www.degruyter.com/document/doi/10.1515/jisys-2015-0059/html", "author_id": ["fBneG-IAAAAJ"], "url_scholarbib": "/scholar?q=info:sgZuYL1bctcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRule%2BMining%2Bin%2BFeature%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sgZuYL1bctcJ&ei=vxBkYsTVFeHDywSSipaYAg&json=", "num_citations": 2, "citedby_url": "/scholar?cites=15524571734424290994&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sgZuYL1bctcJ:scholar.google.com/&scioq=Rule+Mining+in+Feature+Space&hl=en&as_sdt=0,33", "eprint_url": "https://www.degruyter.com/document/doi/10.1515/jisys-2015-0059/html"}, "Cat2Vec: Learning Distributed Representation of Multi-field Categorical Data": {"container_type": "Publication", "bib": {"title": "Cat2Vec: Learning Distributed Representation of Multi-field Categorical Data", "author": ["Y Wen", "J Wang", "T Chen", "W Zhang"], "pub_year": "2016", "venue": "NA", "abstract": "This paper presents a method of learning distributed representation for multi-field categorical data, which is a common data format with various applications such as recommender systems, social link prediction, and computational advertising. The success of non-linear models, eg, factorisation machines, boosted trees, has proved the potential of exploring the interactions among inter-field categories. Inspired by Word2Vec, the distributed representation for natural language, we propose Cat2Vec (categories to vectors)"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyNxRZ9xg", "author_id": ["_A1CxG8AAAAJ", "wIE1tY4AAAAJ", "", "Qzss0GEAAAAJ"], "url_scholarbib": "/scholar?q=info:rBG4qckEVhcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCat2Vec:%2BLearning%2BDistributed%2BRepresentation%2Bof%2BMulti-field%2BCategorical%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rBG4qckEVhcJ&ei=xxBkYrOOHY6pywTd4KPADw&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:rBG4qckEVhcJ:scholar.google.com/&scioq=Cat2Vec:+Learning+Distributed+Representation+of+Multi-field+Categorical+Data&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyNxRZ9xg"}, "Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music": {"container_type": "Publication", "bib": {"title": "Towards deep interpretability (MUS-ROVER II): Learning hierarchical representations of tonal music", "author": ["H Yu", "LR Varshney"], "pub_year": "2016", "venue": "NA", "abstract": "Music theory studies the regularity of patterns in music to capture concepts underlying music styles and composers' decisions. This paper continues the study of building\\emph {automatic theorists}(rovers) to learn and represent music concepts that lead to human interpretable knowledge and further lead to materials for educating people. Our previous work took a first step in algorithmic concept learning of tonal music, studying high-level representations (concepts) of symbolic music (scores) and extracting interpretable rules for"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryhqQFKgl", "author_id": ["", "JIJGu30AAAAJ"], "url_scholarbib": "/scholar?q=info:q40Iuq1AoggJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BDeep%2BInterpretability%2B(MUS-ROVER%2BII):%2BLearning%2BHierarchical%2BRepresentations%2Bof%2BTonal%2BMusic%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=q40Iuq1AoggJ&ei=yhBkYpHcNLKO6rQPy-CRsA8&json=", "num_citations": 9, "citedby_url": "/scholar?cites=622130813425192363&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:q40Iuq1AoggJ:scholar.google.com/&scioq=Towards+Deep+Interpretability+(MUS-ROVER+II):+Learning+Hierarchical+Representations+of+Tonal+Music&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryhqQFKgl"}, "Rethinking Numerical Representations for Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Rethinking numerical representations for deep neural networks", "author": ["P Hill", "B Zamirai", "S Lu", "YW Chao", "M Laurenzano"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms. We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.02513", "author_id": ["GqzwV5EAAAAJ", "24pExC0AAAAJ", "", "48Y9F-YAAAAJ", "w1Z0CGIAAAAJ"], "url_scholarbib": "/scholar?q=info:zR-z_wtNL58J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2BNumerical%2BRepresentations%2Bfor%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zR-z_wtNL58J&ei=zhBkYvCnLOiSy9YPp-OyiAE&json=", "num_citations": 7, "citedby_url": "/scholar?cites=11470471489865850829&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zR-z_wtNL58J:scholar.google.com/&scioq=Rethinking+Numerical+Representations+for+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.02513"}, "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity": {"container_type": "Publication", "bib": {"title": "Symmetry-breaking convergence analysis of certain two-layered neural networks with ReLU nonlinearity", "author": ["Y Tian"], "pub_year": "2017", "venue": "NA", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $ g (\\vx;\\vw)=\\sum_ {j= 1}^ K\\sigma (\\vw_j\\trans\\vx) $, where $\\sigma (\\cdot) $ is ReLU nonlinearity. We assume that the input $\\vx $ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $\\vw\\opt $ using $ l_2 $ loss. We first show that when $ K= 1$, the nonlinear dynamics can be written in close form, and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hk85q85ee", "author_id": ["0mgEF28AAAAJ"], "url_scholarbib": "/scholar?q=info:kgkxDHW7RisJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSymmetry-Breaking%2BConvergence%2BAnalysis%2Bof%2BCertain%2BTwo-layered%2BNeural%2BNetworks%2Bwith%2BReLU%2Bnonlinearity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kgkxDHW7RisJ&ei=1xBkYtC1BIyuyAT-mrWwCA&json=", "num_citations": 29, "citedby_url": "/scholar?cites=3118385903390755218&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kgkxDHW7RisJ:scholar.google.com/&scioq=Symmetry-Breaking+Convergence+Analysis+of+Certain+Two-layered+Neural+Networks+with+ReLU+nonlinearity&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1lVgRNtx"}, "Recurrent Coevolutionary Feature Embedding Processes for Recommendation": {"container_type": "Publication", "bib": {"title": "Recurrent coevolutionary latent feature processes for continuous-time recommendation", "author": ["H Dai", "Y Wang", "R Trivedi", "L Song"], "pub_year": "2016", "venue": "Proceedings of the 1st workshop on \u2026", "abstract": "feature process for each user and item, and the co-evolution of these latent feature processes  is  of these users\u2019 features is also modeled as a nonlinear embedding. Besides the two sets"}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/2988450.2988451?casa_token=jjoAVqqaxNsAAAAA:ikeBoL9Ore05WSNlO5csaUrFQYqusxBjvxBydEONxIF_zRveGj9bwe4lBQuWQfqqMzVfRpqYEW28", "author_id": ["obpl7GQAAAAJ", "dduhwvYAAAAJ", "Jq1MCAYAAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:473O_o1QmtgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BCoevolutionary%2BFeature%2BEmbedding%2BProcesses%2Bfor%2BRecommendation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=473O_o1QmtgJ&ei=4xBkYv_ECYuKmgGY1YjABQ&json=", "num_citations": 49, "citedby_url": "/scholar?cites=15607876029401447907&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:473O_o1QmtgJ:scholar.google.com/&scioq=Recurrent+Coevolutionary+Feature+Embedding+Processes+for+Recommendation&hl=en&as_sdt=0,33", "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/2988450.2988451?casa_token=u6N9ymZwzkoAAAAA:PaLhIBQyyUcNalBtyd-SWwOddonSc20WE5sTpyj7vsfvN90oK9rOINIOsoPE6IjW3EY5lm070ECq"}, "HyperNetworks": {"container_type": "Publication", "bib": {"title": "Hypernetworks", "author": ["D Ha", "A Dai", "QV Le"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1609.09106", "abstract": "reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to- hypernetworks  useful for deep convolutional networks and long recurrent networks, where hypernetworks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1609.09106", "author_id": ["J1j92GsxVUMC", "2r2NuDAAAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:PTIClj-d6DIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHyperNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PTIClj-d6DIJ&ei=5hBkYqHtO6KUy9YP_JONiAY&json=", "num_citations": 832, "citedby_url": "/scholar?cites=3668354792918495805&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PTIClj-d6DIJ:scholar.google.com/&scioq=HyperNetworks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1609.09106"}, "Stick-Breaking Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "Stick-breaking variational autoencoders", "author": ["E Nalisnick", "P Smyth"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1605.06197", "abstract": ", using Stochastic Gradient Variational Bayes for the weights of a stick-breaking process.   We focus on performing inference for just the series of stick-breaking weights, which we will"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1605.06197", "author_id": ["-hM1oqwAAAAJ", "OsoQ-dcAAAAJ"], "url_scholarbib": "/scholar?q=info:4I_HAKWjbDcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStick-Breaking%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4I_HAKWjbDcJ&ei=-RBkYp_sPOiSy9YPp-OyiAE&json=", "num_citations": 133, "citedby_url": "/scholar?cites=3993746898648797152&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4I_HAKWjbDcJ:scholar.google.com/&scioq=Stick-Breaking+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1605.06197.pdf?ref=https://githubhelp.com"}, "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer": {"container_type": "Publication", "bib": {"title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer", "author": ["S Zagoruyko", "N Komodakis"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.03928", "abstract": "Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.03928", "author_id": ["EqJw1-4AAAAJ", "xCPoT4EAAAAJ"], "url_scholarbib": "/scholar?q=info:cUx-QaEHH3UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPaying%2BMore%2BAttention%2Bto%2BAttention:%2BImproving%2Bthe%2BPerformance%2Bof%2BConvolutional%2BNeural%2BNetworks%2Bvia%2BAttention%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cUx-QaEHH3UJ&ei=_RBkYo78A4yuyAT-mrWwCA&json=", "num_citations": 1259, "citedby_url": "/scholar?cites=8439472615885524081&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cUx-QaEHH3UJ:scholar.google.com/&scioq=Paying+More+Attention+to+Attention:+Improving+the+Performance+of+Convolutional+Neural+Networks+via+Attention+Transfer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.03928"}, "Boosted Residual Networks": {"container_type": "Publication", "bib": {"title": "Boosting the accuracy of multispectral image pansharpening by learning a deep residual network", "author": ["Y Wei", "Q Yuan", "H Shen", "L Zhang"], "pub_year": "2017", "venue": "IEEE Geoscience and \u2026", "abstract": "vision, we propose the deep residual pan-sharpening neural network (DRPNN) to overcome   The prototype of the DRPNN is introduced from a deep residual network for image super-"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8012503/", "author_id": ["YKWwyKwAAAAJ", "aItnA-sAAAAJ", "ore_9NIAAAAJ", "vzj2hcYAAAAJ"], "url_scholarbib": "/scholar?q=info:-iBzC0VfZssJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosted%2BResidual%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-iBzC0VfZssJ&ei=CBFkYqqfO42ymgHg1rfQDQ&json=", "num_citations": 278, "citedby_url": "/scholar?cites=14656506787473334522&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-iBzC0VfZssJ:scholar.google.com/&scioq=Boosted+Residual+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07556"}, "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce": {"container_type": "Publication", "bib": {"title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "author": ["T Zahavy", "A Magnani", "A Krishnan", "S Mannor"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.09534", "author_id": ["9dXN6cMAAAAJ", "FV76LCYAAAAJ", "hp3IUe4AAAAJ", "q1HlbIUAAAAJ"], "url_scholarbib": "/scholar?q=info:83lOyKhZIU4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIs%2Ba%2Bpicture%2Bworth%2Ba%2Bthousand%2Bwords%253F%2BA%2BDeep%2BMulti-Modal%2BFusion%2BArchitecture%2Bfor%2BProduct%2BClassification%2Bin%2Be-commerce%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=83lOyKhZIU4J&ei=DRFkYp2wCZqSy9YP8pKNsAE&json=", "num_citations": 49, "citedby_url": "/scholar?cites=5629879590639794675&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:83lOyKhZIU4J:scholar.google.com/&scioq=Is+a+picture+worth+a+thousand+words%3F+A+Deep+Multi-Modal+Fusion+Architecture+for+Product+Classification+in+e-commerce&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.09534"}, "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units": {"container_type": "Publication", "bib": {"title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units", "author": ["D Hendrycks", "K Gimpel"], "pub_year": "2016", "venue": "NA", "abstract": "the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function.  The GELU nonlinearity is the expected transformation of a stochastic regularizer which"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bk0MRI5lg", "author_id": ["czyretsAAAAJ", "kDHs7DYAAAAJ"], "url_scholarbib": "/scholar?q=info:jmEKF6eQsWQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBridging%2BNonlinearities%2Band%2BStochastic%2BRegularizers%2Bwith%2BGaussian%2BError%2BLinear%2BUnits%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jmEKF6eQsWQJ&ei=ERFkYqboNOiSy9YPp-OyiAE&json=", "num_citations": 267, "citedby_url": "/scholar?cites=7255739521991074190&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jmEKF6eQsWQJ:scholar.google.com/&scioq=Bridging+Nonlinearities+and+Stochastic+Regularizers+with+Gaussian+Error+Linear+Units&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bk0MRI5lg"}, "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications": {"container_type": "Publication", "bib": {"title": "Marginal deep architectures: Deep learning for small and middle scale applications", "author": ["Y Zheng", "G Zhong", "J Dong"], "pub_year": "2016", "venue": "NA", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we propose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d(MDA). In the implementation of MDA, the weight matrices of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BysZhEqee", "author_id": ["yyyNMRcAAAAJ", "HqKD-fwAAAAJ", "iPYdUpAAAAAJ"], "url_scholarbib": "/scholar?q=info:88ArQIep2LkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMarginal%2BDeep%2BArchitectures:%2BDeep%2Blearning%2Bfor%2BSmall%2Band%2BMiddle%2BScale%2BApplications%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=88ArQIep2LkJ&ei=FBFkYv3gHZqSy9YP8pKNsAE&json=", "num_citations": 1, "citedby_url": "/scholar?cites=13391639890348458227&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:88ArQIep2LkJ:scholar.google.com/&scioq=Marginal+Deep+Architectures:+Deep+learning+for+Small+and+Middle+Scale+Applications&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BysZhEqee"}, "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure": {"container_type": "Publication", "bib": {"title": "Convolutional neural networks generalization utilizing the data graph structure", "author": ["Y Hechtlinger", "P Chakravarti", "J Qin"], "pub_year": "2016", "venue": "NA", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low-dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkIqod5ll", "author_id": ["", "", ""], "url_scholarbib": "/scholar?q=info:GXmTvSW_C04J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConvolutional%2BNeural%2BNetworks%2BGeneralization%2BUtilizing%2Bthe%2BData%2BGraph%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GXmTvSW_C04J&ei=FxFkYrfIOaKUy9YP_JONiAY&json=", "num_citations": 2, "citedby_url": "/scholar?cites=5623798728517450009&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GXmTvSW_C04J:scholar.google.com/&scioq=Convolutional+Neural+Networks+Generalization+Utilizing+the+Data+Graph+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkIqod5ll"}, "Maximum Entropy Flow Networks": {"container_type": "Publication", "bib": {"title": "Maximum entropy flow networks", "author": ["G Loaiza-Ganem", "Y Gao", "JP Cunningham"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "maximum entropy distribution. Doing so is nontrivial in that the objective being maximized  (entropy)  developments in normalizing flow networks, we cast the maximum entropy problem"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.03504", "author_id": ["lUAFQsgAAAAJ", "jfdR-6kAAAAJ", "88cU_4UAAAAJ"], "url_scholarbib": "/scholar?q=info:7dRMGNBLKPEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaximum%2BEntropy%2BFlow%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7dRMGNBLKPEJ&ei=GxFkYu7HBOHDywSSipaYAg&json=", "num_citations": 23, "citedby_url": "/scholar?cites=17377222519342028013&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7dRMGNBLKPEJ:scholar.google.com/&scioq=Maximum+Entropy+Flow+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.03504"}, "Making Stochastic Neural Networks from Deterministic Ones": {"container_type": "Publication", "bib": {"title": "Making Stochastic Neural Networks from Deterministic Ones", "author": ["K Lee", "J Kim", "S Chong", "J Shin"], "pub_year": "2016", "venue": "NA", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1akgy9xx", "author_id": ["92M8xv4AAAAJ", "", "m2Lm6f4AAAAJ", "m3eDp7kAAAAJ"], "url_scholarbib": "/scholar?q=info:_Q-vARgePIAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaking%2BStochastic%2BNeural%2BNetworks%2Bfrom%2BDeterministic%2BOnes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_Q-vARgePIAJ&ei=HxFkYrDBF5LeyQTE46-QAg&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:_Q-vARgePIAJ:scholar.google.com/&scioq=Making+Stochastic+Neural+Networks+from+Deterministic+Ones&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1akgy9xx"}, "Introspection:Accelerating Neural Network Training By Learning Weight Evolution": {"container_type": "Publication", "bib": {"title": "Introspection: Accelerating neural network training by learning weight evolution", "author": ["A Sinha", "M Sarkar", "A Mukherjee"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.04959", "author_id": ["2llY8lwAAAAJ", "N6J7J4IAAAAJ", ""], "url_scholarbib": "/scholar?q=info:QNl87F6XZZoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntrospection:Accelerating%2BNeural%2BNetwork%2BTraining%2BBy%2BLearning%2BWeight%2BEvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QNl87F6XZZoJ&ei=JBFkYteNN86E6rQP5-KmKA&json=", "num_citations": 19, "citedby_url": "/scholar?cites=11125464888439003456&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QNl87F6XZZoJ:scholar.google.com/&scioq=Introspection:Accelerating+Neural+Network+Training+By+Learning+Weight+Evolution&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.04959"}, "Deep Generalized Canonical Correlation Analysis": {"container_type": "Publication", "bib": {"title": "Deep generalized canonical correlation analysis", "author": ["A Benton", "H Khayrallah", "B Gujral", "DA Reisinger"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA)--a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA,(Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.02519", "author_id": ["k9g68LwAAAAJ", "muJh2XQAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:C9FaGxiT1MUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BGeneralized%2BCanonical%2BCorrelation%2BAnalysis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=C9FaGxiT1MUJ&ei=KhFkYs7-HM6E6rQP5-KmKA&json=", "num_citations": 97, "citedby_url": "/scholar?cites=14255180452281897227&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:C9FaGxiT1MUJ:scholar.google.com/&scioq=Deep+Generalized+Canonical+Correlation+Analysis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.02519"}, " A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples": {"container_type": "Publication", "bib": {"title": "A theoretical framework for robustness of (deep) classifiers against adversarial examples", "author": ["B Wang", "J Gao", "Y Qi"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1612.00334", "abstract": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($ f_1"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.00334", "author_id": ["PoHiv9wAAAAJ", "zcNplSgAAAAJ", "eXKdSu0AAAAJ"], "url_scholarbib": "/scholar?q=info:rdcGZ0iKi-gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BA%2BTheoretical%2BFramework%2Bfor%2BRobustness%2Bof%2B(Deep)%2BClassifiers%2Bagainst%2BAdversarial%2BSamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rdcGZ0iKi-gJ&ei=LRFkYrPgIYuKmgGY1YjABQ&json=", "num_citations": 27, "citedby_url": "/scholar?cites=16756638882132842413&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rdcGZ0iKi-gJ:scholar.google.com/&scioq=+A+Theoretical+Framework+for+Robustness+of+(Deep)+Classifiers+against+Adversarial+Samples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.00334"}, "Sample Efficient Actor-Critic with  Experience Replay": {"container_type": "Publication", "bib": {"title": "Sample efficient actor-critic with experience replay", "author": ["Z Wang", "V Bapst", "N Heess", "V Mnih", "R Munos"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01224", "author_id": ["Rne0FzEAAAAJ", "95mnc80AAAAJ", "79k7bGEAAAAJ", "rLdfJ1gAAAAJ", "OvKEnVwAAAAJ"], "url_scholarbib": "/scholar?q=info:PG6dibJzJXQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSample%2BEfficient%2BActor-Critic%2Bwith%2B%2BExperience%2BReplay%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PG6dibJzJXQJ&ei=MBFkYv7hFJqSy9YP8pKNsAE&json=", "num_citations": 664, "citedby_url": "/scholar?cites=8369222693188103740&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PG6dibJzJXQJ:scholar.google.com/&scioq=Sample+Efficient+Actor-Critic+with++Experience+Replay&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01224.pdf?ref=https://githubhelp.com"}, "Tree-Structured Variational Autoencoder": {"container_type": "Publication", "bib": {"title": "Tree-structured variational autoencoder", "author": ["R Shin", "AA Alemi", "G Irving", "O Vinyals"], "pub_year": "2016", "venue": "NA", "abstract": "In this paper, we explore how we can adapt the variational autoencoder to modeling tree-structured  data. In general, it is possible to treat the tree as a sequence and then use a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hy0L4t5el", "author_id": ["xPnkc80AAAAJ", "68hTs9wAAAAJ", "TrdtzgwAAAAJ", "NkzyCvUAAAAJ"], "url_scholarbib": "/scholar?q=info:d7P-uYNFFEQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTree-Structured%2BVariational%2BAutoencoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d7P-uYNFFEQJ&ei=OxFkYuj8DOiSy9YPp-OyiAE&json=", "num_citations": 3, "citedby_url": "/scholar?cites=4905622326176822135&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:d7P-uYNFFEQJ:scholar.google.com/&scioq=Tree-Structured+Variational+Autoencoder&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hy0L4t5el"}, "The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning": {"container_type": "Publication", "bib": {"title": "The incredible shrinking neural network: New perspectives on learning representations through the lens of pruning", "author": ["N Wolfe", "A Sharma", "L Drude", "B Raj"], "pub_year": "2016", "venue": "NA", "abstract": "necessarily forces us to confront our assumptions about how neural networks actually learn   about neural network learning representations and numerical approaches to pruning. To"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkV4VS9ll", "author_id": ["75Qg5eQAAAAJ", "PIDrCp8AAAAJ", "-SENxxIAAAAJ", "IWcGY98AAAAJ"], "url_scholarbib": "/scholar?q=info:zL7qo8JXdIoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BIncredible%2BShrinking%2BNeural%2BNetwork:%2BNew%2BPerspectives%2Bon%2BLearning%2BRepresentations%2BThrough%2BThe%2BLens%2Bof%2BPruning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zL7qo8JXdIoJ&ei=PhFkYqTbCOiSy9YPp-OyiAE&json=", "num_citations": 13, "citedby_url": "/scholar?cites=9976695568017833676&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zL7qo8JXdIoJ:scholar.google.com/&scioq=The+Incredible+Shrinking+Neural+Network:+New+Perspectives+on+Learning+Representations+Through+The+Lens+of+Pruning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkV4VS9ll"}, "Understanding intermediate layers using linear classifier probes": {"container_type": "Publication", "bib": {"title": "Understanding intermediate layers using linear classifier probes", "author": ["G Alain", "Y Bengio"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1610.01644", "abstract": "Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as\" probes\", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1610.01644", "author_id": ["j40pfugAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:6RURCcGQbsYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2Bintermediate%2Blayers%2Busing%2Blinear%2Bclassifier%2Bprobes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6RURCcGQbsYJ&ei=QhFkYoueOYuKmgGY1YjABQ&json=", "num_citations": 371, "citedby_url": "/scholar?cites=14298525025703106025&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6RURCcGQbsYJ:scholar.google.com/&scioq=Understanding+intermediate+layers+using+linear+classifier+probes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1610.01644"}, "Regularizing CNNs with Locally Constrained Decorrelations": {"container_type": "Publication", "bib": {"title": "Regularizing cnns with locally constrained decorrelations", "author": ["P Rodr\u00edguez", "J Gonzalez", "G Cucurull"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we show that regularizing negatively correlated  decorrelation bounds, and  reducing the overfitting more effectively. In particular, we show that the models regularized with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01967", "author_id": ["IwBx73wAAAAJ", "Lphp7WUAAAAJ", "dEtv5r4AAAAJ"], "url_scholarbib": "/scholar?q=info:FZtGc2FSTDUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegularizing%2BCNNs%2Bwith%2BLocally%2BConstrained%2BDecorrelations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FZtGc2FSTDUJ&ei=ShFkYpWfMOHDywSSipaYAg&json=", "num_citations": 111, "citedby_url": "/scholar?cites=3840535160739502869&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FZtGc2FSTDUJ:scholar.google.com/&scioq=Regularizing+CNNs+with+Locally+Constrained+Decorrelations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01967"}, "Perception Updating Networks: On architectural constraints for interpretable video generative models": {"container_type": "Publication", "bib": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "author": ["E Santana", "JC Principe"], "venue": "NA", "pub_year": "NA", "abstract": ""}, "filled": false, "gsrank": 1, "author_id": ["VSGnfXAAAAAJ", "GkpvilQAAAAJ"], "url_scholarbib": "/scholar?q=info:qxaf9VYdPc8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPerception%2BUpdating%2BNetworks:%2BOn%2Barchitectural%2Bconstraints%2Bfor%2Binterpretable%2Bvideo%2Bgenerative%2Bmodels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qxaf9VYdPc8J&ei=ThFkYs3cD4OEmgHx-5DADA&json=", "num_citations": 0}, "Adversarially Learned Inference": {"container_type": "Publication", "bib": {"title": "Adversarially learned inference", "author": ["V Dumoulin", "I Belghazi", "B Poole", "O Mastropietro"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "adversarial training benefits from learning an inference mechanism jointly with the decoder.  Furthermore, it shows that our proposed approach for learning inference in an adversarial"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1606.00704", "author_id": ["mZfgLA4AAAAJ", "DFv8bqAAAAAJ", "i5FMLA4AAAAJ", ""], "url_scholarbib": "/scholar?q=info:8_ZABCCJC5EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarially%2BLearned%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8_ZABCCJC5EJ&ei=URFkYoGwKYuKmgGY1YjABQ&json=", "num_citations": 1272, "citedby_url": "/scholar?cites=10451598130846693107&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8_ZABCCJC5EJ:scholar.google.com/&scioq=Adversarially+Learned+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1606.00704"}, "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters": {"container_type": "Publication", "bib": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "author": ["J Serra", "A Karatzoglou"], "pub_year": "2017", "venue": "NA", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rySCp-1Yg", "author_id": ["sZLj96sAAAAJ", "j5u5iiYAAAAJ"], "url_scholarbib": "/scholar?q=info:rUIp-Bju2gcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompact%2BEmbedding%2Bof%2BBinary-coded%2BInputs%2Band%2BOutputs%2Busing%2BBloom%2BFilters%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rUIp-Bju2gcJ&ei=VBFkYqDbNu-Sy9YPs_mY8AM&json=", "num_citations": 1, "citedby_url": "/scholar?cites=566026494198497965&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rUIp-Bju2gcJ:scholar.google.com/&scioq=Compact+Embedding+of+Binary-coded+Inputs+and+Outputs+using+Bloom+Filters&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rySCp-1Yg"}, "Multi-view Recurrent Neural Acoustic Word Embeddings": {"container_type": "Publication", "bib": {"title": "Multi-view recurrent neural acoustic word embeddings", "author": ["W He", "W Wang", "K Livescu"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.04496", "abstract": "Recent work has begun exploring neural acoustic word embeddings---fixed-dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.04496", "author_id": ["", "O9djN1AAAAAJ", "kCYbVq0AAAAJ"], "url_scholarbib": "/scholar?q=info:wgeCyWau9_oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-view%2BRecurrent%2BNeural%2BAcoustic%2BWord%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wgeCyWau9_oJ&ei=WBFkYpS-F5qSy9YP8pKNsAE&json=", "num_citations": 75, "citedby_url": "/scholar?cites=18084114585220155330&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wgeCyWau9_oJ:scholar.google.com/&scioq=Multi-view+Recurrent+Neural+Acoustic+Word+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.04496"}, "An Empirical Analysis of Deep Network Loss Surfaces": {"container_type": "Publication", "bib": {"title": "An empirical analysis of deep network loss surfaces", "author": ["DJ Im", "M Tao", "K Branson"], "pub_year": "2016", "venue": "NA", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkuDV6iex", "author_id": ["bzmGSYIAAAAJ", "Q1yFGOUAAAAJ", "g558OVoAAAAJ"], "url_scholarbib": "/scholar?q=info:gJJtTO4fNJgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BEmpirical%2BAnalysis%2Bof%2BDeep%2BNetwork%2BLoss%2BSurfaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gJJtTO4fNJgJ&ei=WxFkYurbOIOEmgHx-5DADA&json=", "num_citations": 36, "citedby_url": "/scholar?cites=10967426100898927232&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gJJtTO4fNJgJ:scholar.google.com/&scioq=An+Empirical+Analysis+of+Deep+Network+Loss+Surfaces&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkuDV6iex"}, "A Neural Stochastic Volatility Model": {"container_type": "Publication", "bib": {"title": "A neural stochastic volatility model", "author": ["R Luo", "W Zhang", "X Xu", "J Wang"], "pub_year": "2018", "venue": "\u2026 of the AAAI Conference on Artificial \u2026", "abstract": "In this paper, we show that the recent integration of statistical models with deep recurrent neural networks provides a new way of formulating volatility (the degree of variation of time series) models that have been widely used in time series analysis and prediction in finance. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the"}, "filled": false, "gsrank": 1, "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/12124", "author_id": ["5AnhSDQAAAAJ", "Qzss0GEAAAAJ", "rdMZZQwAAAAJ", "wIE1tY4AAAAJ"], "url_scholarbib": "/scholar?q=info:Sa1CwLDojCEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BNeural%2BStochastic%2BVolatility%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Sa1CwLDojCEJ&ei=XhFkYtqyLeiSy9YPp-OyiAE&json=", "num_citations": 44, "citedby_url": "/scholar?cites=2417562945828597065&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Sa1CwLDojCEJ:scholar.google.com/&scioq=A+Neural+Stochastic+Volatility+Model&hl=en&as_sdt=0,33", "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/download/12124/11983"}, "Revisiting Batch Normalization For Practical Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Revisiting batch normalization for practical domain adaptation", "author": ["Y Li", "N Wang", "J Shi", "J Liu", "X Hou"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1603.04779", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1603.04779", "author_id": ["-VgS8AIAAAAJ", "yAWtq6QAAAAJ", "mwsxrm4AAAAJ", "-OcSne0AAAAJ", "5m8wv2EAAAAJ"], "url_scholarbib": "/scholar?q=info:fjtSViQJFaIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRevisiting%2BBatch%2BNormalization%2BFor%2BPractical%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fjtSViQJFaIJ&ei=YhFkYoLpCM6E6rQP5-KmKA&json=", "num_citations": 406, "citedby_url": "/scholar?cites=11679251260326951806&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fjtSViQJFaIJ:scholar.google.com/&scioq=Revisiting+Batch+Normalization+For+Practical+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1603.04779.pdf?ref=https://githubhelp.com"}, "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution": {"container_type": "Publication", "bib": {"title": "Recursive regression with neural networks: Approximating the HJI PDE solution", "author": ["VR Royo", "C Tomlin"], "pub_year": "2016", "venue": "NA", "abstract": "Most machine learning applications using neural networks seek to approximate some function g (x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y= g (x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJTXaw9gx", "author_id": ["S5c-A-oAAAAJ", "-5_ksIkAAAAJ"], "url_scholarbib": "/scholar?q=info:ECx3BIBT--YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecursive%2BRegression%2Bwith%2BNeural%2BNetworks:%2BApproximating%2Bthe%2BHJI%2BPDE%2BSolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ECx3BIBT--YJ&ei=ZhFkYorGC4yuyAT-mrWwCA&json=", "num_citations": 8, "citedby_url": "/scholar?cites=16643988657173638160&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ECx3BIBT--YJ:scholar.google.com/&scioq=Recursive+Regression+with+Neural+Networks:+Approximating+the+HJI+PDE+Solution&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJTXaw9gx"}, "Deep Convolutional Neural Network Design Patterns": {"container_type": "Publication", "bib": {"title": "Deep convolutional neural network design patterns", "author": ["LN Smith", "N Topin"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.00847", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (ie, Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.00847", "author_id": ["pwh7Pw4AAAAJ", "IiSg8R0AAAAJ"], "url_scholarbib": "/scholar?q=info:kQHXHNyZfbcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BConvolutional%2BNeural%2BNetwork%2BDesign%2BPatterns%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kQHXHNyZfbcJ&ei=aRFkYv-XIpqSy9YP8pKNsAE&json=", "num_citations": 63, "citedby_url": "/scholar?cites=13221893251685351825&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kQHXHNyZfbcJ:scholar.google.com/&scioq=Deep+Convolutional+Neural+Network+Design+Patterns&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.00847"}, "Recurrent Environment Simulators": {"container_type": "Publication", "bib": {"title": "Recurrent environment simulators", "author": ["S Chiappa", "S Racaniere", "D Wierstra"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We also introduce a simulator that does not need to predict visual inputs after every action,   We test our simulators on three diverse and challenging families of environments, namely"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.02254", "author_id": ["GAvF3gUAAAAJ", "o-h0vrQAAAAJ", "aDbsf28AAAAJ"], "url_scholarbib": "/scholar?q=info:nwlhmKGeQ1sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BEnvironment%2BSimulators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nwlhmKGeQ1sJ&ei=axFkYuiUMZGJmwGY-qmYDQ&json=", "num_citations": 169, "citedby_url": "/scholar?cites=6576274297774475679&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nwlhmKGeQ1sJ:scholar.google.com/&scioq=Recurrent+Environment+Simulators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.02254"}, "Warped Convolutions: Efficient Invariance to Spatial Transformations": {"container_type": "Publication", "bib": {"title": "Warped convolutions: Efficient invariance to spatial transformations", "author": ["JF Henriques", "A Vedaldi"], "pub_year": "2017", "venue": "International Conference on \u2026", "abstract": "Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v70/henriques17a.html", "author_id": ["aCQjyp0AAAAJ", "bRT7t28AAAAJ"], "url_scholarbib": "/scholar?q=info:D0JpXiITOJcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWarped%2BConvolutions:%2BEfficient%2BInvariance%2Bto%2BSpatial%2BTransformations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D0JpXiITOJcJ&ei=bxFkYsuwIM6E6rQP5-KmKA&json=", "num_citations": 96, "citedby_url": "/scholar?cites=10896480336756687375&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D0JpXiITOJcJ:scholar.google.com/&scioq=Warped+Convolutions:+Efficient+Invariance+to+Spatial+Transformations&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v70/henriques17a/henriques17a.pdf"}, "Optimization as a Model for Few-Shot Learning": {"container_type": "Publication", "bib": {"title": "Optimization as a model for few-shot learning", "author": ["S Ravi", "H Larochelle"], "pub_year": "2016", "venue": "NA", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJY0-Kcll", "author_id": ["cr53lHIAAAAJ", "U89FHq4AAAAJ"], "url_scholarbib": "/scholar?q=info:seXVxZ654yYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimization%2Bas%2Ba%2BModel%2Bfor%2BFew-Shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=seXVxZ654yYJ&ei=cxFkYrmUHY6pywTd4KPADw&json=", "num_citations": 2131, "citedby_url": "/scholar?cites=2802287484729681329&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:seXVxZ654yYJ:scholar.google.com/&scioq=Optimization+as+a+Model+for+Few-Shot+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJY0-Kcll"}, "Memory-augmented Attention Modelling for Videos": {"container_type": "Publication", "bib": {"title": "Memory-augmented attention modelling for videos", "author": ["R Fakoor", "A Mohamed", "M Mitchell", "SB Kang"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a method to improve video description generation by modeling higher-order interactions between video frames and described concepts. By storing past visual attention in the video associated to previously generated words, the system is able to decide what to look at and describe in light of what it has already looked at and described. This enables not only more effective local attention, but tractable consideration of the video sequence while generating each word. Evaluation on the challenging and popular MSVD and Charades"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.02261", "author_id": ["nVsOPtQAAAAJ", "tJ_PrzgAAAAJ", "5na92fcAAAAJ", "2rzyuRQAAAAJ"], "url_scholarbib": "/scholar?q=info:EcX7rEMsE14J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemory-augmented%2BAttention%2BModelling%2Bfor%2BVideos%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EcX7rEMsE14J&ei=dxFkYoOuBYOEmgHx-5DADA&json=", "num_citations": 29, "citedby_url": "/scholar?cites=6778810533299340561&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EcX7rEMsE14J:scholar.google.com/&scioq=Memory-augmented+Attention+Modelling+for+Videos&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.02261"}, "Generative Adversarial Networks for Image Steganography": {"container_type": "Publication", "bib": {"title": "Generative adversarial networks for image steganography", "author": ["D Volkhonskiy", "B Borisenko", "E Burnaev"], "pub_year": "2016", "venue": "NA", "abstract": "Steganography is collection of methods to hide secret information (\" payload\") within non-secret information (\" container\"). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1hoFU9xe", "author_id": ["3c97Xy4AAAAJ", "", "pCRdcOwAAAAJ"], "url_scholarbib": "/scholar?q=info:cwB_U6WyLvIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BAdversarial%2BNetworks%2Bfor%2BImage%2BSteganography%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cwB_U6WyLvIJ&ei=ehFkYpH5EpLeyQTE46-QAg&json=", "num_citations": 24, "citedby_url": "/scholar?cites=17451082029247430771&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cwB_U6WyLvIJ:scholar.google.com/&scioq=Generative+Adversarial+Networks+for+Image+Steganography&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1hoFU9xe"}}