{"GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks": {"container_type": "Publication", "bib": {"title": "GeoSeq2Seq: Information geometric sequence-to-sequence networks", "author": ["A Bay", "B Sengupta"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.09363", "abstract": "The Fisher information metric is an important foundation of information geometry, wherein it allows us to approximate the local geometry of a probability distribution. Recurrent neural networks such as the Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield state-of-the-art performance on speech translation or image captioning have so far ignored the geometry of the latent embedding, that they iteratively learn. We propose the information geometric Seq2Seq (GeoSeq2Seq) network which abridges the gap between"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.09363", "author_id": ["QzLCq1wAAAAJ", "YZHhV9kAAAAJ"], "url_scholarbib": "/scholar?q=info:JJXiH6j1Wv4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeoSeq2Seq:%2BInformation%2BGeometric%2BSequence-to-Sequence%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JJXiH6j1Wv4J&ei=3hVkYrTBM86E6rQP5-KmKA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=18328231735975908644&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JJXiH6j1Wv4J:scholar.google.com/&scioq=GeoSeq2Seq:+Information+Geometric+Sequence-to-Sequence+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.09363"}, "Adversarial Policy Gradient for Alternating Markov Games": {"container_type": "Publication", "bib": {"title": "Adversarial policy gradient for alternating markov games", "author": ["C Gao", "M Mueller", "R Hayward"], "pub_year": "2018", "venue": "NA", "abstract": "Policy gradient reinforcement learning has been applied to two-player alternate-turn zero-sum games, eg, in AlphaGo, self-play REINFORCE was used to improve the neural net model after supervised learning. In this paper, we emphasize that two-player zero-sum games with alternating turns, which have been previously formulated as Alternating Markov Games (AMGs), are different from standard MDP because of their two-agent nature. We exploit the difference in associated Bellman equations, which leads to different policy"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJk51gJRb", "author_id": ["N2H5y_MAAAAJ", "J60BcHkAAAAJ", "h-VFYuUAAAAJ"], "url_scholarbib": "/scholar?q=info:aVanj3PLoT0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BPolicy%2BGradient%2Bfor%2BAlternating%2BMarkov%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aVanj3PLoT0J&ei=5RVkYp_aD5qSy9YP8pKNsAE&json=", "num_citations": 8, "citedby_url": "/scholar?cites=4441054404755805801&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aVanj3PLoT0J:scholar.google.com/&scioq=Adversarial+Policy+Gradient+for+Alternating+Markov+Games&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJk51gJRb"}, "PDE-Net: Learning PDEs from Data": {"container_type": "Publication", "bib": {"title": "Pde-net: Learning pdes from data", "author": ["Z Long", "Y Lu", "X Ma", "B Dong"], "pub_year": "2018", "venue": "\u2026 on Machine Learning", "abstract": "Such vast quantity of data offers new opportunities for data-driven discovery of physical laws  designs in deep learning, we propose a new feed-forward deep network, called PDENet, to"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v80/long18a.html?ref=https://githubhelp.com", "author_id": ["0KXcwnkAAAAJ", "NmhvVBgAAAAJ", "zKKdOt0AAAAJ", "zLXcC90AAAAJ"], "url_scholarbib": "/scholar?q=info:KW7G5ZqtD6gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPDE-Net:%2BLearning%2BPDEs%2Bfrom%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KW7G5ZqtD6gJ&ei=6RVkYravN4yuyAT-mrWwCA&json=", "num_citations": 446, "citedby_url": "/scholar?cites=12110088803814108713&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KW7G5ZqtD6gJ:scholar.google.com/&scioq=PDE-Net:+Learning+PDEs+from+Data&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/long18a/long18a.pdf"}, "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Espresso: Efficient forward propagation for binary deep neural networks", "author": ["F Pedersoli", "G Tzanetakis"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "There are many applications scenarios for which the computational performance and memory footprint of the prediction phase of Deep Neural Networks (DNNs) need to be optimized. Binary Deep Neural Networks (BDNNs) have been shown to be an effective way of achieving this objective. In this paper, we show how Convolutional Neural Networks (CNNs) can be implemented using binary representations. Espresso is a compact, yet powerful library written in C/CUDA that features all the functionalities required for the forward"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Sk6fD5yCb&noteId=Sk6fD5yCb&ref=https://githubhelp.com", "author_id": ["a8Eqtv0AAAAJ", "yPgxxpwAAAAJ"], "url_scholarbib": "/scholar?q=info:fsbB183g2hYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEspresso:%2BEfficient%2BForward%2BPropagation%2Bfor%2BBinary%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fsbB183g2hYJ&ei=7RVkYq70BeiSy9YPp-OyiAE&json=", "num_citations": 9, "citedby_url": "/scholar?cites=1646875788450055806&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fsbB183g2hYJ:scholar.google.com/&scioq=Espresso:+Efficient+Forward+Propagation+for+Binary+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Sk6fD5yCb"}, "Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control": {"container_type": "Publication", "bib": {"title": "Progressive reinforcement learning with distillation for multi-skilled motion control", "author": ["G Berseth", "C Xie", "P Cernek"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems, including agents that can move with skill and agility through their environment. An open problem in this setting is that of developing good strategies for integrating or merging policies for multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. We extend policy distillation methods to the continuous action setting and leverage this technique to combine expert policies, as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.04765", "author_id": ["-WZcuuwAAAAJ", "04dL0akAAAAJ", ""], "url_scholarbib": "/scholar?q=info:CX3rookSF5cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgressive%2BReinforcement%2BLearning%2Bwith%2BDistillation%2Bfor%2BMulti-Skilled%2BMotion%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CX3rookSF5cJ&ei=8RVkYqHcEeHDywSSipaYAg&json=", "num_citations": 44, "citedby_url": "/scholar?cites=10887191006544624905&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CX3rookSF5cJ:scholar.google.com/&scioq=Progressive+Reinforcement+Learning+with+Distillation+for+Multi-Skilled+Motion+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.04765"}, "A Bayesian Perspective on Generalization and Stochastic Gradient Descent": {"container_type": "Publication", "bib": {"title": "A bayesian perspective on generalization and stochastic gradient descent", "author": ["SL Smith", "QV Le"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.06451", "abstract": "We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to Zhang et al.(2016), who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.06451", "author_id": ["fyEqU5oAAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:89z_JWix87oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BBayesian%2BPerspective%2Bon%2BGeneralization%2Band%2BStochastic%2BGradient%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=89z_JWix87oJ&ei=-hVkYrbePPmQ6rQP5OqKqAo&json=", "num_citations": 250, "citedby_url": "/scholar?cites=13471305971267525875&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:89z_JWix87oJ:scholar.google.com/&scioq=A+Bayesian+Perspective+on+Generalization+and+Stochastic+Gradient+Descent&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.06451"}, "COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS": {"container_type": "Publication", "bib": {"title": "Cold fusion: Training seq2seq models together with language models", "author": ["A Sriram", "H Jun", "S Satheesh", "A Coates"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.06426", "abstract": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.06426", "author_id": ["D4uRc_UAAAAJ", "", "VUi7eM8AAAAJ", "bLUllHEAAAAJ"], "url_scholarbib": "/scholar?q=info:hnj8zzMK2kgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCOLD%2BFUSION:%2BTRAINING%2BSEQ2SEQ%2BMODELS%2BTOGETHER%2BWITH%2BLANGUAGE%2BMODELS%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hnj8zzMK2kgJ&ei=_hVkYsLVCuHDywSSipaYAg&json=", "num_citations": 207, "citedby_url": "/scholar?cites=5249519533302773894&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hnj8zzMK2kgJ:scholar.google.com/&scioq=COLD+FUSION:+TRAINING+SEQ2SEQ+MODELS+TOGETHER+WITH+LANGUAGE+MODELS&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.06426"}, "AUTOMATED DESIGN USING NEURAL NETWORKS AND GRADIENT DESCENT": {"container_type": "Publication", "bib": {"title": "Automated design using neural networks and gradient descent", "author": ["O Hennigh"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.10352", "abstract": "We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks. Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness. We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10352", "author_id": [""], "url_scholarbib": "/scholar?q=info:ayr-7jd-MjsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAUTOMATED%2BDESIGN%2BUSING%2BNEURAL%2BNETWORKS%2BAND%2BGRADIENT%2BDESCENT%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ayr-7jd-MjsJ&ei=ABZkYp2LPOiSy9YPp-OyiAE&json=", "num_citations": 7, "citedby_url": "/scholar?cites=4265610575771216491&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ayr-7jd-MjsJ:scholar.google.com/&scioq=AUTOMATED+DESIGN+USING+NEURAL+NETWORKS+AND+GRADIENT+DESCENT&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10352"}, "Implicit Causal Models for Genome-wide Association Studies": {"container_type": "Publication", "bib": {"title": "Implicit causal models for genome-wide association studies", "author": ["D Tran", "DM Blei"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.10742", "abstract": "Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10742", "author_id": ["wVazIm8AAAAJ", "8OYE6iEAAAAJ"], "url_scholarbib": "/scholar?q=info:3vqvCdN3qeYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImplicit%2BCausal%2BModels%2Bfor%2BGenome-wide%2BAssociation%2BStudies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3vqvCdN3qeYJ&ei=AxZkYqCzM86E6rQP5-KmKA&json=", "num_citations": 36, "citedby_url": "/scholar?cites=16620947648071858910&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3vqvCdN3qeYJ:scholar.google.com/&scioq=Implicit+Causal+Models+for+Genome-wide+Association+Studies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10742"}, "TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN": {"container_type": "Publication", "bib": {"title": "Training generative adversarial networks via primal-dual subgradient methods: a lagrangian perspective on gan", "author": ["X Chen", "J Wang", "H Ge"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.01765", "abstract": "We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.01765", "author_id": ["vFaPyjAAAAAJ", "UyLVh08AAAAJ", "e4nHo2MAAAAJ"], "url_scholarbib": "/scholar?q=info:YxEkVc8DMY8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTRAINING%2BGENERATIVE%2BADVERSARIAL%2BNETWORKS%2BVIA%2BPRIMAL-DUAL%2BSUBGRADIENT%2BMETHODS:%2BA%2BLAGRANGIAN%2BPERSPECTIVE%2BON%2BGAN%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YxEkVc8DMY8J&ei=BxZkYqfMDbKO6rQPy-CRsA8&json=", "num_citations": 12, "citedby_url": "/scholar?cites=10318032410304057699&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YxEkVc8DMY8J:scholar.google.com/&scioq=TRAINING+GENERATIVE+ADVERSARIAL+NETWORKS+VIA+PRIMAL-DUAL+SUBGRADIENT+METHODS:+A+LAGRANGIAN+PERSPECTIVE+ON+GAN&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.01765"}, "Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio": {"container_type": "Publication", "bib": {"title": "Viterbi-based pruning for sparse matrix with fixed and high index compression ratio", "author": ["D Lee", "D Ahn", "T Kim", "PI Chuang"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy. Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources. Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists. In this paper, a new"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1D8MPxA-", "author_id": ["ALiieEkAAAAJ", "a4e-yE4AAAAJ", "zzII2gsAAAAJ", "DIRkcngAAAAJ"], "url_scholarbib": "/scholar?q=info:n8-4UpxIynsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DViterbi-based%2BPruning%2Bfor%2BSparse%2BMatrix%2Bwith%2BFixed%2Band%2BHigh%2BIndex%2BCompression%2BRatio%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n8-4UpxIynsJ&ei=CxZkYvX9A--Sy9YPs_mY8AM&json=", "num_citations": 16, "citedby_url": "/scholar?cites=8920021848200630175&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n8-4UpxIynsJ:scholar.google.com/&scioq=Viterbi-based+Pruning+for+Sparse+Matrix+with+Fixed+and+High+Index+Compression+Ratio&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1D8MPxA-"}, "Large scale distributed neural network training through online distillation": {"container_type": "Publication", "bib": {"title": "Large scale distributed neural network training through online distillation", "author": ["R Anil", "G Pereyra", "A Passos", "R Ormandi"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.03235", "author_id": ["Mv71-IcAAAAJ", "b8ujmwgAAAAJ", "P3ER6nYAAAAJ", "-0g4be8AAAAJ"], "url_scholarbib": "/scholar?q=info:XIGZa2I8kxcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2Bscale%2Bdistributed%2Bneural%2Bnetwork%2Btraining%2Bthrough%2Bonline%2Bdistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XIGZa2I8kxcJ&ei=DRZkYt3BH42ymgHg1rfQDQ&json=", "num_citations": 260, "citedby_url": "/scholar?cites=1698767877858492764&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XIGZa2I8kxcJ:scholar.google.com/&scioq=Large+scale+distributed+neural+network+training+through+online+distillation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.03235"}, "Kronecker Recurrent Units": {"container_type": "Publication", "bib": {"title": "Kronecker recurrent units", "author": ["C Jose", "M Ciss\u00e9", "F Fleuret"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "We present a flexible recurrent neural network model called Kronecker Recurrent Units (  At the heart of KRU, we use Kronecker factored recurrent matrix which provide an elegant way"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v80/jose18a.html", "author_id": ["dbIaOkQAAAAJ", "", "Bj1tRlsAAAAJ"], "url_scholarbib": "/scholar?q=info:p37al5bFhM0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKronecker%2BRecurrent%2BUnits%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p37al5bFhM0J&ei=ERZkYsTVHO-Sy9YPs_mY8AM&json=", "num_citations": 37, "citedby_url": "/scholar?cites=14809178725284478631&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p37al5bFhM0J:scholar.google.com/&scioq=Kronecker+Recurrent+Units&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/jose18a/jose18a.pdf"}, "Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks": {"container_type": "Publication", "bib": {"title": "Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks", "author": ["B Lake", "M Baroni"], "pub_year": "2018", "venue": "NA", "abstract": "Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb\" dax,\" he or she can immediately understand the meaning of\" dax twice\" or\" sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H18WqugAb", "author_id": ["vspmOX8AAAAJ", "l-xu2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:yVB-DYXoPR8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStill%2Bnot%2Bsystematic%2Bafter%2Ball%2Bthese%2Byears:%2BOn%2Bthe%2Bcompositional%2Bskills%2Bof%2Bsequence-to-sequence%2Brecurrent%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yVB-DYXoPR8J&ei=GBZkYvaEGMLZmQHnraWYCA&json=", "num_citations": 65, "citedby_url": "/scholar?cites=2251211046909792457&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yVB-DYXoPR8J:scholar.google.com/&scioq=Still+not+systematic+after+all+these+years:+On+the+compositional+skills+of+sequence-to-sequence+recurrent+networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H18WqugAb"}, "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms": {"container_type": "Publication", "bib": {"title": "Ensemble robustness and generalization of stochastic deep learning algorithms", "author": ["T Zahavy", "B Kang", "A Sivak", "J Feng", "H Xu"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing research interest. However, most of the well-established approaches, such as hypothesis capacity, stability or sparseness, have not provided complete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus on the robustness approach (Xu & Mannor, 2012), ie, if the error of a hypothesis will not change much due to perturbations of its training examples, then it will also generalize well. As most deep learning algorithms are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1602.02389", "author_id": ["9dXN6cMAAAAJ", "NmHgX-wAAAAJ", "", "Q8iay0gAAAAJ", "7vLwm84AAAAJ"], "url_scholarbib": "/scholar?q=info:5CszfYq_OEkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnsemble%2BRobustness%2Band%2BGeneralization%2Bof%2BStochastic%2BDeep%2BLearning%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5CszfYq_OEkJ&ei=HRZkYuSPA8LZmQHnraWYCA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=5276177564991433700&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5CszfYq_OEkJ:scholar.google.com/&scioq=Ensemble+Robustness+and+Generalization+of+Stochastic+Deep+Learning+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1602.02389"}, "Capturing Human Category Representations by Sampling in Deep Feature Spaces": {"container_type": "Publication", "bib": {"title": "Capturing human category representations by sampling in deep feature spaces", "author": ["JC Peterson", "JW Suchow", "K Aghi", "AY Ku"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Understanding how people represent categories is a core problem in cognitive science. Decades of research have yielded a variety of formal theories of categories, but validating them with naturalistic stimuli is difficult. The challenge is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires a workable representation of these stimuli. Deep neural networks have recently been successful in solving a range of computer vision tasks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.07644", "author_id": ["ncbyhdMAAAAJ", "S9xCl8EAAAAJ", "KLcqPPgAAAAJ", "Lh_ZqdcAAAAJ"], "url_scholarbib": "/scholar?q=info:cPnAHufJgpoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCapturing%2BHuman%2BCategory%2BRepresentations%2Bby%2BSampling%2Bin%2BDeep%2BFeature%2BSpaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cPnAHufJgpoJ&ei=IBZkYviEIOiSy9YPp-OyiAE&json=", "num_citations": 4, "citedby_url": "/scholar?cites=11133683223303879024&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cPnAHufJgpoJ:scholar.google.com/&scioq=Capturing+Human+Category+Representations+by+Sampling+in+Deep+Feature+Spaces&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.07644"}, "Cross-Corpus Training with TreeLSTM for the Extraction of Biomedical Relationships from Text": {"container_type": "Publication", "bib": {"title": "Cross-Corpus Training with TreeLSTM for the Extraction of Biomedical Relationships from Text", "author": ["L Jo\u00ebl", "Y Toussaint", "C Ra\u00efssi", "A Coulet"], "pub_year": "2018", "venue": "NA", "abstract": "A bottleneck problem in machine learning-based relationship extraction (RE) algorithms, and particularly of deep learning-based ones, is the availability of training data in the form of annotated corpora. For specific domains, such as biomedicine, the long time and high expertise required for the development of manually annotated corpora explain that most of the existing one are relatively small (ie, hundreds of sentences). Beside, larger corpora focusing on general or domain-specific relationships (such as citizenship or drug-drug"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1LXVnxRb", "author_id": ["", "", "iMXuJYsAAAAJ", "puI_AJgAAAAJ"], "url_scholarbib": "/scholar?q=info:Nsuj9wVfGoEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCross-Corpus%2BTraining%2Bwith%2BTreeLSTM%2Bfor%2Bthe%2BExtraction%2Bof%2BBiomedical%2BRelationships%2Bfrom%2BText%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Nsuj9wVfGoEJ&ei=JBZkYqHvJvmQ6rQP5OqKqAo&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:Nsuj9wVfGoEJ:scholar.google.com/&scioq=Cross-Corpus+Training+with+TreeLSTM+for+the+Extraction+of+Biomedical+Relationships+from+Text&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1LXVnxRb"}, "Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations": {"container_type": "Publication", "bib": {"title": "Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations", "author": ["Y Lu", "A Zhong", "Q Li", "B Dong"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v80/lu18d.html", "author_id": ["NmhvVBgAAAAJ", "pG5Zz4kAAAAJ", "", "zLXcC90AAAAJ"], "url_scholarbib": "/scholar?q=info:GALvUhM21tcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BFinite%2BLayer%2BNeural%2BNetworks:%2BBridging%2BDeep%2BArchitectures%2Band%2BNumerical%2BDifferential%2BEquations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GALvUhM21tcJ&ei=KRZkYr7HFKKUy9YP_JONiAY&json=", "num_citations": 323, "citedby_url": "/scholar?cites=15552677819794260504&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GALvUhM21tcJ:scholar.google.com/&scioq=Beyond+Finite+Layer+Neural+Networks:+Bridging+Deep+Architectures+and+Numerical+Differential+Equations&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/lu18d/lu18d.pdf"}, "Topic-Based Question Generation": {"container_type": "Publication", "bib": {"title": "Controllable Open-ended Question Generation with A New Question Type Ontology", "author": ["S Cao", "L Wang"], "pub_year": "2021", "venue": "arXiv preprint arXiv:2107.00152", "abstract": "Different from the topic-based ontology, our question types are more aligned with cognitive  levels. Moreover, our templates are automatically learned from training data. Recent work ("}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2107.00152", "author_id": ["OF3eat8AAAAJ", "uczqEdUAAAAJ"], "url_scholarbib": "/scholar?q=info:gVZrD4swuwoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTopic-Based%2BQuestion%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gVZrD4swuwoJ&ei=LBZkYs_QPIySyASZk6HgCA&json=", "num_citations": 4, "citedby_url": "/scholar?cites=773265134841452161&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gVZrD4swuwoJ:scholar.google.com/&scioq=Topic-Based+Question+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2107.00152"}, "Semantic Code Repair using Neuro-Symbolic Transformation Networks": {"container_type": "Publication", "bib": {"title": "Semantic code repair using neuro-symbolic transformation networks", "author": ["J Devlin", "J Uesato", "R Singh", "P Kohli"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11054", "abstract": "We study the problem of semantic code repair, which can be broadly defined as automatically fixing non-syntactic bugs in source code. The majority of past work in semantic code repair assumed access to unit tests against which candidate repairs could be validated. In contrast, the goal here is to develop a strong statistical model to accurately predict both bug locations and exact fixes without access to information about the intended correct behavior of the program. Achieving such a goal requires a robust contextual repair"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11054", "author_id": ["", "E_-4S6wAAAAJ", "5kVcNS4AAAAJ", "3pyzQQ8AAAAJ"], "url_scholarbib": "/scholar?q=info:lih1BDepiMkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemantic%2BCode%2BRepair%2Busing%2BNeuro-Symbolic%2BTransformation%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lih1BDepiMkJ&ei=NxZkYsjZA_mQ6rQP5OqKqAo&json=", "num_citations": 28, "citedby_url": "/scholar?cites=14522043052219246742&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lih1BDepiMkJ:scholar.google.com/&scioq=Semantic+Code+Repair+using+Neuro-Symbolic+Transformation+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11054"}, "Benefits of Depth for Long-Term Memory of Recurrent Networks": {"container_type": "Publication", "bib": {"title": "Benefits of depth for long-term memory of recurrent networks", "author": ["Y Levine", "O Sharir", "A Shashua"], "pub_year": "2018", "venue": "NA", "abstract": "The key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data, is their ever-improving ability to model intricate long-term temporal dependencies. However, a well established measure of RNNs' long-term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited. Though depth efficiency in convolutional networks is well established by now, it does not suffice in order to account for"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJKt3VyPz", "author_id": ["rVQo4JIAAAAJ", "2y5Am34AAAAJ", "dwi5wvYAAAAJ"], "url_scholarbib": "/scholar?q=info:-IHJgLjZ83MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBenefits%2Bof%2BDepth%2Bfor%2BLong-Term%2BMemory%2Bof%2BRecurrent%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-IHJgLjZ83MJ&ei=QBZkYrbGAvmQ6rQP5OqKqAo&json=", "num_citations": 19, "citedby_url": "/scholar?cites=8355261120160301560&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-IHJgLjZ83MJ:scholar.google.com/&scioq=Benefits+of+Depth+for+Long-Term+Memory+of+Recurrent+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJKt3VyPz"}, "Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction": {"container_type": "Publication", "bib": {"title": "Improving the universality and learnability of neural programmer-interpreters with combinator abstraction", "author": ["D Xiao", "JY Liao", "X Yuan"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.02696", "abstract": "To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction dramatically reduces the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.02696", "author_id": ["", "", ""], "url_scholarbib": "/scholar?q=info:RI4xj9liE9wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2Bthe%2BUniversality%2Band%2BLearnability%2Bof%2BNeural%2BProgrammer-Interpreters%2Bwith%2BCombinator%2BAbstraction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RI4xj9liE9wJ&ei=RBZkYr-PHYySyASZk6HgCA&json=", "num_citations": 12, "citedby_url": "/scholar?cites=15858127399451463236&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RI4xj9liE9wJ:scholar.google.com/&scioq=Improving+the+Universality+and+Learnability+of+Neural+Programmer-Interpreters+with+Combinator+Abstraction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.02696"}, "The loss surface and expressivity of deep convolutional neural networks": {"container_type": "Publication", "bib": {"title": "The loss surface and expressivity of deep convolutional neural networks", "author": ["Q Nguyen", "M Hein"], "pub_year": "2018", "venue": "NA", "abstract": "We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a \u201cwide\u201d layer which has more neurons than the number of training samples. This condition holds eg for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJjquybCW", "author_id": ["", "0ZAb3tsAAAAJ"], "url_scholarbib": "/scholar?q=info:Ne2ex3XPSikJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2Bloss%2Bsurface%2Band%2Bexpressivity%2Bof%2Bdeep%2Bconvolutional%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ne2ex3XPSikJ&ei=SBZkYqPdD5qSy9YP8pKNsAE&json=", "num_citations": 32, "citedby_url": "/scholar?cites=2975418608598838581&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ne2ex3XPSikJ:scholar.google.com/&scioq=The+loss+surface+and+expressivity+of+deep+convolutional+neural+networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJjquybCW"}, "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm": {"container_type": "Publication", "bib": {"title": "Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm", "author": ["C Finn", "S Levine"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11622", "abstract": "Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11622", "author_id": ["vfPE6hgAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:Bgd4xfERVh4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2Band%2BUniversality:%2BDeep%2BRepresentations%2Band%2BGradient%2BDescent%2Bcan%2BApproximate%2Bany%2BLearning%2BAlgorithm%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Bgd4xfERVh4J&ei=SxZkYuq7J-HDywSSipaYAg&json=", "num_citations": 192, "citedby_url": "/scholar?cites=2185954399232722694&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Bgd4xfERVh4J:scholar.google.com/&scioq=Meta-Learning+and+Universality:+Deep+Representations+and+Gradient+Descent+can+Approximate+any+Learning+Algorithm&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11622"}, "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor": {"container_type": "Publication", "bib": {"title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "author": ["T Haarnoja", "A Zhou", "P Abbeel"], "pub_year": "2018", "venue": "\u2026 conference on machine \u2026", "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v80/haarnoja18b", "author_id": ["VT7peyEAAAAJ", "1O83J5MAAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:ETPMR67DU7gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSoft%2BActor-Critic:%2BOff-Policy%2BMaximum%2BEntropy%2BDeep%2BReinforcement%2BLearning%2Bwith%2Ba%2BStochastic%2BActor%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ETPMR67DU7gJ&ei=ThZkYsaAH5qSy9YP8pKNsAE&json=", "num_citations": 3061, "citedby_url": "/scholar?cites=13282174879342015249&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ETPMR67DU7gJ:scholar.google.com/&scioq=Soft+Actor-Critic:+Off-Policy+Maximum+Entropy+Deep+Reinforcement+Learning+with+a+Stochastic+Actor&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf"}, "Learning Deep Generative Models of Graphs": {"container_type": "Publication", "bib": {"title": "Learning deep generative models of graphs", "author": ["Y Li", "O Vinyals", "C Dyer", "R Pascanu"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.03324", "author_id": ["UY7CtEwAAAAJ", "NkzyCvUAAAAJ", "W2DsnAkAAAAJ", "eSPY8LwAAAAJ"], "url_scholarbib": "/scholar?q=info:lc2E53hFDfMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDeep%2BGenerative%2BModels%2Bof%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lc2E53hFDfMJ&ei=URZkYvHZBuiSy9YPp-OyiAE&json=", "num_citations": 435, "citedby_url": "/scholar?cites=17513730911496359317&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lc2E53hFDfMJ:scholar.google.com/&scioq=Learning+Deep+Generative+Models+of+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.03324.pdf?ref=https://githubhelp.com"}, "Towards Provable Control for Unknown Linear Dynamical Systems": {"container_type": "Publication", "bib": {"title": "Towards provable control for unknown linear dynamical systems", "author": ["S Arora", "E Hazan", "H Lee", "K Singh", "C Zhang", "Y Zhang"], "pub_year": "2018", "venue": "NA", "abstract": "We study the control of symmetric linear dynamical systems with unknown dynamics and a hidden state. Using a recent spectral filtering technique for concisely representing such systems in a linear basis, we formulate optimal control in this setting as a convex program. This approach eliminates the need to solve the non-convex problem of explicit identification of the system and its latent state, and allows for provable optimality guarantees for the control signal. We give the first efficient algorithm for finding the optimal control signal with"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJGuXK1vM", "author_id": ["RUP4S68AAAAJ", "LnhCGNMAAAAJ", "hR9rFHgAAAAJ", "PZJIgZUAAAAJ", "sXtjq8IAAAAJ", "lc6CVqEAAAAJ"], "url_scholarbib": "/scholar?q=info:x0-9KRmfM4gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BProvable%2BControl%2Bfor%2BUnknown%2BLinear%2BDynamical%2BSystems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x0-9KRmfM4gJ&ei=VBZkYoHWH--Sy9YPs_mY8AM&json=", "num_citations": 20, "citedby_url": "/scholar?cites=9814362943393714119&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x0-9KRmfM4gJ:scholar.google.com/&scioq=Towards+Provable+Control+for+Unknown+Linear+Dynamical+Systems&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJGuXK1vM"}, "Fast Node Embeddings: Learning Ego-Centric Representations": {"container_type": "Publication", "bib": {"title": "Fast node embeddings: Learning ego-centric representations", "author": ["T Pimentel", "A Veloso", "N Ziviani"], "pub_year": "2018", "venue": "NA", "abstract": "Representation learning is one of the foundations of Deep Learning and allowed important improvements on several Machine Learning tasks, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. Several of these methods are based on the SkipGram algorithm, and they usually process a large number of multi-hop neighbors in order to produce the context from which node representations are learned. In"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B10ZSeWCb", "author_id": ["XjZ8NRsAAAAJ", "j2BEVSoAAAAJ", "OZ2ju_EAAAAJ"], "url_scholarbib": "/scholar?q=info:_5eQSn5tep8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFast%2BNode%2BEmbeddings:%2BLearning%2BEgo-Centric%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_5eQSn5tep8J&ei=VxZkYoK-FIuKmgGY1YjABQ&json=", "num_citations": 13, "citedby_url": "/scholar?cites=11491617788373538815&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_5eQSn5tep8J:scholar.google.com/&scioq=Fast+Node+Embeddings:+Learning+Ego-Centric+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJyfrl-0b"}, "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks": {"container_type": "Publication", "bib": {"title": "Empirical analysis of the hessian of over-parametrized neural networks", "author": ["L Sagun", "U Evci", "VU Guney", "Y Dauphin"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts:(1) the bulk centered near zero,(2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al.(2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.04454", "author_id": ["-iPZaBcAAAAJ", "8yGMMwcAAAAJ", "3aYVAZEAAAAJ", "XSforroAAAAJ"], "url_scholarbib": "/scholar?q=info:a673PIa1-fYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmpirical%2BAnalysis%2Bof%2Bthe%2BHessian%2Bof%2BOver-Parametrized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a673PIa1-fYJ&ei=WhZkYuqzCcLZmQHnraWYCA&json=", "num_citations": 204, "citedby_url": "/scholar?cites=17796454990684335723&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a673PIa1-fYJ:scholar.google.com/&scioq=Empirical+Analysis+of+the+Hessian+of+Over-Parametrized+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.04454"}, "DLVM: A modern compiler infrastructure for deep learning systems": {"container_type": "Publication", "bib": {"title": "DLVM: A modern compiler infrastructure for deep learning systems", "author": ["R Wei", "L Schwartz", "V Adve"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.03016", "abstract": "Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain-specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.03016", "author_id": ["WL9QGDUAAAAJ", "J8a1zEwAAAAJ", "VbruE20AAAAJ"], "url_scholarbib": "/scholar?q=info:GwjFzLCUiuEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDLVM:%2BA%2Bmodern%2Bcompiler%2Binfrastructure%2Bfor%2Bdeep%2Blearning%2Bsystems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GwjFzLCUiuEJ&ei=XhZkYuXxMOiSy9YPp-OyiAE&json=", "num_citations": 46, "citedby_url": "/scholar?cites=16251965692390475803&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GwjFzLCUiuEJ:scholar.google.com/&scioq=DLVM:+A+modern+compiler+infrastructure+for+deep+learning+systems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.03016"}, "Latent Space Oddity: on the Curvature of Deep Generative Models": {"container_type": "Publication", "bib": {"title": "Latent space oddity: on the curvature of deep generative models", "author": ["G Arvanitidis", "LK Hansen", "S Hauberg"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11379", "abstract": "Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear\" generator\" function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11379", "author_id": ["sFtJbSUAAAAJ", "gQVuJh8AAAAJ", "M1fMGOMAAAAJ"], "url_scholarbib": "/scholar?q=info:bKO5gdwarYsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BSpace%2BOddity:%2Bon%2Bthe%2BCurvature%2Bof%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bKO5gdwarYsJ&ei=YhZkYuSBFuiSy9YPp-OyiAE&json=", "num_citations": 140, "citedby_url": "/scholar?cites=10064730276614480748&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bKO5gdwarYsJ:scholar.google.com/&scioq=Latent+Space+Oddity:+on+the+Curvature+of+Deep+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11379"}, "Can Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?": {"container_type": "Publication", "bib": {"title": "Can deep reinforcement learning solve Erdos-Selfridge-Spencer games?", "author": ["M Raghu", "A Irpan", "J Andreas"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v80/raghu18a.html", "author_id": ["xdwK2NsAAAAJ", "", "dnZ8udEAAAAJ"], "url_scholarbib": "/scholar?q=info:wLOX6rsjBkYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCan%2BDeep%2BReinforcement%2BLearning%2Bsolve%2BErdos-Selfridge-Spencer%2BGames%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wLOX6rsjBkYJ&ei=ZhZkYpHqBouKmgGY1YjABQ&json=", "num_citations": 19, "citedby_url": "/scholar?cites=5045759722516886464&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wLOX6rsjBkYJ:scholar.google.com/&scioq=Can+Deep+Reinforcement+Learning+solve+Erdos-Selfridge-Spencer+Games%3F&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/raghu18a/raghu18a.pdf"}, "Deep Active Learning for Named Entity Recognition": {"container_type": "Publication", "bib": {"title": "Deep active learning for named entity recognition", "author": ["Y Shen", "H Yun", "ZC Lipton", "Y Kronrod"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.05928", "author_id": ["HJDxXUsAAAAJ", "W4oOmZEAAAAJ", "MN9Kfg8AAAAJ", "UMZrl1cAAAAJ"], "url_scholarbib": "/scholar?q=info:OjGbM5pRj6IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BActive%2BLearning%2Bfor%2BNamed%2BEntity%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OjGbM5pRj6IJ&ei=chZkYoX6F-HDywSSipaYAg&json=", "num_citations": 327, "citedby_url": "/scholar?cites=11713670878546571578&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OjGbM5pRj6IJ:scholar.google.com/&scioq=Deep+Active+Learning+for+Named+Entity+Recognition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.05928"}, "On the limitations of first order approximation in GAN dynamics": {"container_type": "Publication", "bib": {"title": "On the limitations of first-order approximation in GAN dynamics", "author": ["J Li", "A Madry", "J Peebles"], "pub_year": "2018", "venue": "\u2026 Conference on Machine \u2026", "abstract": "While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v80/li18d.html", "author_id": ["4zybTq4AAAAJ", "SupjsEUAAAAJ", ""], "url_scholarbib": "/scholar?q=info:P6ItfG1vYLIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2Blimitations%2Bof%2Bfirst%2Border%2Bapproximation%2Bin%2BGAN%2Bdynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=P6ItfG1vYLIJ&ei=jxZkYq_YE4uKmgGY1YjABQ&json=", "num_citations": 38, "citedby_url": "/scholar?cites=12853395852540879423&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:P6ItfG1vYLIJ:scholar.google.com/&scioq=On+the+limitations+of+first+order+approximation+in+GAN+dynamics&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/li18d/li18d.pdf"}, "Regret Minimization for Partially Observable Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Regret minimization for partially observable deep reinforcement learning", "author": ["P Jin", "K Keutzer", "S Levine"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial observations by using finite length observation histories or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v80/jin18c.html", "author_id": ["1ZT-nZEAAAAJ", "ID9QePIAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:gBVpkMrgmAAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegret%2BMinimization%2Bfor%2BPartially%2BObservable%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gBVpkMrgmAAJ&ei=lBZkYrHNI--Sy9YPs_mY8AM&json=", "num_citations": 40, "citedby_url": "/scholar?cites=43031357070841216&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gBVpkMrgmAAJ:scholar.google.com/&scioq=Regret+Minimization+for+Partially+Observable+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/jin18c/jin18c.pdf"}, "Semantic Interpolation in Implicit Models": {"container_type": "Publication", "bib": {"title": "Semantic interpolation in implicit models", "author": ["Y Kilcher", "A Lucchi", "T Hofmann"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11381", "abstract": "In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths. Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11381", "author_id": ["T3Ji6P4AAAAJ", "V1ONSgIAAAAJ", "T3hAyLkAAAAJ"], "url_scholarbib": "/scholar?q=info:oRM02qkDwKUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemantic%2BInterpolation%2Bin%2BImplicit%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oRM02qkDwKUJ&ei=mBZkYrjoCY6pywTd4KPADw&json=", "num_citations": 17, "citedby_url": "/scholar?cites=11943550239831757729&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oRM02qkDwKUJ:scholar.google.com/&scioq=Semantic+Interpolation+in+Implicit+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11381"}, "Adversarially Regularized Autoencoders": {"container_type": "Publication", "bib": {"title": "Adversarially regularized autoencoders", "author": ["J Zhao", "Y Kim", "K Zhang", "A Rush"], "pub_year": "2018", "venue": "\u2026 conference on machine \u2026", "abstract": "Impact of Regularization on Discrete Encoding We further examine the impact of adversarial  regularization on the encoded representation produced by the model as it is trained. Figure"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v80/zhao18b.html?ref=https://githubhelp.com", "author_id": ["8ipao8MAAAAJ", "n_ts4eYAAAAJ", "VDwprrsAAAAJ", "LIjnUGgAAAAJ"], "url_scholarbib": "/scholar?q=info:LqI8IQ9hu0UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarially%2BRegularized%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LqI8IQ9hu0UJ&ei=mxZkYvr9HI6pywTd4KPADw&json=", "num_citations": 197, "citedby_url": "/scholar?cites=5024716526871945774&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LqI8IQ9hu0UJ:scholar.google.com/&scioq=Adversarially+Regularized+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf"}, "Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies": {"container_type": "Publication", "bib": {"title": "Analyzing and exploiting NARX recurrent neural networks for long-term dependencies", "author": ["R DiPietro", "C Rupprecht", "N Navab"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work we analyzed NARX RNNs and introduced a variant which we  NARX RNNs; 2)  improve performance substantially over LSTM on tasks requiring very long-term dependencies"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.07805", "author_id": ["hgSJHaUAAAAJ", "IrYlproAAAAJ", "kzoVUPYAAAAJ"], "url_scholarbib": "/scholar?q=info:yAT9Gvl0w5oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalyzing%2Band%2BExploiting%2BNARX%2BRecurrent%2BNeural%2BNetworks%2Bfor%2BLong-Term%2BDependencies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yAT9Gvl0w5oJ&ei=nhZkYo7WHaKUy9YP_JONiAY&json=", "num_citations": 25, "citedby_url": "/scholar?cites=11151885715547948232&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yAT9Gvl0w5oJ:scholar.google.com/&scioq=Analyzing+and+Exploiting+NARX+Recurrent+Neural+Networks+for+Long-Term+Dependencies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.07805"}, "Neural Program Search: Solving Data Processing Tasks from Description and Examples": {"container_type": "Publication", "bib": {"title": "Neural program search: Solving data processing tasks from description and examples", "author": ["I Polosukhin", "A Skidanov"], "pub_year": "2018", "venue": "NA", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input/output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1KJJf-R-", "author_id": ["3SyxFIAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:9vEG3ojA-6YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BProgram%2BSearch:%2BSolving%2BData%2BProcessing%2BTasks%2Bfrom%2BDescription%2Band%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9vEG3ojA-6YJ&ei=pBZkYr_YE8LZmQHnraWYCA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=12032422523523494390&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9vEG3ojA-6YJ:scholar.google.com/&scioq=Neural+Program+Search:+Solving+Data+Processing+Tasks+from+Description+and+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1KJJf-R-"}, "Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks": {"container_type": "Publication", "bib": {"title": "Not-So-CLEVR: learning same\u2013different relations strains feedforward neural networks", "author": ["J Kim", "M Ricci", "T Serre"], "pub_year": "2018", "venue": "Interface focus", "abstract": "on visual recognition tasks like image classification and face recognition. However, here we  will show that feedforward neural networks struggle to learn abstract visual relations that are"}, "filled": false, "gsrank": 1, "pub_url": "https://royalsocietypublishing.org/doi/abs/10.1098/rsfs.2018.0011", "author_id": ["JBGADIwAAAAJ", "", "kZlPW4wAAAAJ"], "url_scholarbib": "/scholar?q=info:0oTv1EM_RyEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNot-So-CLEVR:%2BVisual%2BRelations%2BStrain%2BFeedforward%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0oTv1EM_RyEJ&ei=pxZkYp2HHO-Sy9YPs_mY8AM&json=", "num_citations": 45, "citedby_url": "/scholar?cites=2397954887165904082&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0oTv1EM_RyEJ:scholar.google.com/&scioq=Not-So-CLEVR:+Visual+Relations+Strain+Feedforward+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://royalsocietypublishing.org/doi/pdf/10.1098/rsfs.2018.0011"}, "No Spurious Local Minima in a Two Hidden Unit ReLU Network": {"container_type": "Publication", "bib": {"title": "No spurious local minima in a two hidden unit ReLU network", "author": ["C Wu", "J Luo", "JD Lee"], "pub_year": "2018", "venue": "NA", "abstract": "Deep learning models can be efficiently optimized via stochastic gradient descent, but there is little theoretical evidence to support this. A key question in optimization is to understand when the optimization landscape of a neural network is amenable to gradient-based optimization. We focus on a simple neural network two-layer ReLU network with two hidden units, and show that all local minimizers are global. This combined with recent work of Lee et al.(2017); Lee et al.(2016) show that gradient descent converges to the global minimizer."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B14uJzW0b", "author_id": ["WoB6M2cAAAAJ", "", "GR_DsT0AAAAJ"], "url_scholarbib": "/scholar?q=info:lyz8HO9jkboJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNo%2BSpurious%2BLocal%2BMinima%2Bin%2Ba%2BTwo%2BHidden%2BUnit%2BReLU%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lyz8HO9jkboJ&ei=tBZkYtj7NfmQ6rQP5OqKqAo&json=", "num_citations": 10, "citedby_url": "/scholar?cites=13443636241312263319&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lyz8HO9jkboJ:scholar.google.com/&scioq=No+Spurious+Local+Minima+in+a+Two+Hidden+Unit+ReLU+Network&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B14uJzW0b"}, "Stable Distribution Alignment Using the Dual of the Adversarial Distance": {"container_type": "Publication", "bib": {"title": "Stable distribution alignment using the dual of the adversarial distance", "author": ["B Usman", "K Saenko", "B Kulis"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.04046", "abstract": "Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results. However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization. We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.04046", "author_id": ["reUYd_0AAAAJ", "9xDADY4AAAAJ", "okcbLqoAAAAJ"], "url_scholarbib": "/scholar?q=info:JbvOSh3sXagJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStable%2BDistribution%2BAlignment%2BUsing%2Bthe%2BDual%2Bof%2Bthe%2BAdversarial%2BDistance%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JbvOSh3sXagJ&ei=yxZkYrf5BJqSy9YP8pKNsAE&json=", "num_citations": 3, "citedby_url": "/scholar?cites=12132112581759253285&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JbvOSh3sXagJ:scholar.google.com/&scioq=Stable+Distribution+Alignment+Using+the+Dual+of+the+Adversarial+Distance&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.04046"}, "Reinforcement Learning from Imperfect Demonstrations": {"container_type": "Publication", "bib": {"title": "Reinforcement learning from imperfect demonstrations", "author": ["Y Gao", "H Xu", "J Lin", "F Yu", "S Levine", "T Darrell"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration  data, since  We show that our unified reinforcement learning algorithm can learn robustly and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.05313", "author_id": ["k0jjQo8AAAAJ", "t9HPFawAAAAJ", "dVtzVVAAAAAJ", "-XCiamcAAAAJ", "8R35rCwAAAAJ", "bh-uRFMAAAAJ"], "url_scholarbib": "/scholar?q=info:2mECXTr4264J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforcement%2BLearning%2Bfrom%2BImperfect%2BDemonstrations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2mECXTr4264J&ei=2BZkYv_3CoOEmgHx-5DADA&json=", "num_citations": 170, "citedby_url": "/scholar?cites=12599937312051323354&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2mECXTr4264J:scholar.google.com/&scioq=Reinforcement+Learning+from+Imperfect+Demonstrations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.05313.pdf?ref=https://githubhelp.com"}, "Value Propagation Networks": {"container_type": "Publication", "bib": {"title": "Value propagation networks", "author": ["N Nardelli", "G Synnaeve", "Z Lin", "P Kohli"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning  modules built on Value Iteration which can successfully be trained using reinforcement"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11199", "author_id": ["iSI9-1oAAAAJ", "wN9rBkcAAAAJ", "ZDjmMuwAAAAJ", "3pyzQQ8AAAAJ"], "url_scholarbib": "/scholar?q=info:gVNlULy6Zn8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DValue%2BPropagation%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gVNlULy6Zn8J&ei=6xZkYp-uLoyuyAT-mrWwCA&json=", "num_citations": 21, "citedby_url": "/scholar?cites=9180230208406770561&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gVNlULy6Zn8J:scholar.google.com/&scioq=Value+Propagation+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11199"}, "Parametric Adversarial Divergences are Good Task Losses for Generative Modeling": {"container_type": "Publication", "bib": {"title": "Parametric adversarial divergences are good task losses for generative modeling", "author": ["G Huang", "H Berard", "A Touati", "G Gidel", "P Vincent"], "pub_year": "2018", "venue": "NA", "abstract": "Generative modeling of high dimensional data like images is a notoriously difficult and ill-defined problem. In particular, how to evaluate a learned generative model is unclear. In this paper, we argue that* adversarial learning*, pioneered with generative adversarial networks (GANs), provides an interesting framework to implicitly define more meaningful task losses for unsupervised tasks, such as for generating\" visually realistic\" images. By relating GANs and structured prediction under the framework of statistical decision theory, we put into light"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1TgRe-Iz", "author_id": ["_X1Tl-MAAAAJ", "P5d_140AAAAJ", "D4LT5xAAAAAJ", "bDrXQPUAAAAJ", "WBCKQMsAAAAJ"], "url_scholarbib": "/scholar?q=info:m768kybzoBkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParametric%2BAdversarial%2BDivergences%2Bare%2BGood%2BTask%2BLosses%2Bfor%2BGenerative%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m768kybzoBkJ&ei=7xZkYuSiNbKO6rQPy-CRsA8&json=", "num_citations": 11, "citedby_url": "/scholar?cites=1846743194234830491&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m768kybzoBkJ:scholar.google.com/&scioq=Parametric+Adversarial+Divergences+are+Good+Task+Losses+for+Generative+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1TgRe-Iz"}, "Time-Dependent Representation for Neural Event Sequence Prediction": {"container_type": "Publication", "bib": {"title": "Time-dependent representation for neural event sequence prediction", "author": ["Y Li", "N Du", "S Bengio"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.00065", "abstract": "Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, eg, dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.00065", "author_id": ["ZZdB48QAAAAJ", "v474hP4AAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:s1tTxuQz2DQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTime-Dependent%2BRepresentation%2Bfor%2BNeural%2BEvent%2BSequence%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s1tTxuQz2DQJ&ei=8hZkYo2iCcLZmQHnraWYCA&json=", "num_citations": 45, "citedby_url": "/scholar?cites=3807850542614666163&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s1tTxuQz2DQJ:scholar.google.com/&scioq=Time-Dependent+Representation+for+Neural+Event+Sequence+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.00065"}, "Learning Parametric Closed-Loop Policies for Markov Potential Games": {"container_type": "Publication", "bib": {"title": "Learning parametric closed-loop policies for markov potential games", "author": ["SV Macua", "J Zazo", "S Zazo"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.00899", "abstract": "Multiagent systems where agents interact among themselves and with a stochastic environment can be formalized as stochastic games. We study a subclass named Markov potential games (MPGs) that appear often in economic and engineering applications when the agents share a common resource. We consider MPGs with continuous state-action variables, coupled constraints and nonconvex rewards. Previous analysis followed a variational approach that is only valid for very simple cases (convex rewards, invertible"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.00899", "author_id": ["NBqFpgoAAAAJ", "nmvM8iAAAAAJ", "kNhqGMwAAAAJ"], "url_scholarbib": "/scholar?q=info:2pjrdMyOj18J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BParametric%2BClosed-Loop%2BPolicies%2Bfor%2BMarkov%2BPotential%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2pjrdMyOj18J&ei=9RZkYo7eKpLeyQTE46-QAg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=6885879364058847450&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2pjrdMyOj18J:scholar.google.com/&scioq=Learning+Parametric+Closed-Loop+Policies+for+Markov+Potential+Games&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.00899"}, "Inference Suboptimality in Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "Inference suboptimality in variational autoencoders", "author": ["C Cremer", "X Li", "D Duvenaud"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v80/cremer18a.html", "author_id": ["fxWW1OAAAAAJ", "GaYmpIgAAAAJ", "ZLpO3XQAAAAJ"], "url_scholarbib": "/scholar?q=info:cuKxgHLXr94J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInference%2BSuboptimality%2Bin%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cuKxgHLXr94J&ei=-hZkYqjEK8LZmQHnraWYCA&json=", "num_citations": 192, "citedby_url": "/scholar?cites=16046280884129751666&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cuKxgHLXr94J:scholar.google.com/&scioq=Inference+Suboptimality+in+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/cremer18a/cremer18a.pdf"}, "Investigating Human Priors for Playing Video Games": {"container_type": "Publication", "bib": {"title": "Investigating human priors for playing video games", "author": ["R Dubey", "P Agrawal", "D Pathak", "TL Griffiths"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.10217", "author_id": ["fJsSvzkAAAAJ", "UpZmJI0AAAAJ", "AEsPCAUAAAAJ", "UAwKvEsAAAAJ"], "url_scholarbib": "/scholar?q=info:GqBHlZXCjx4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInvestigating%2BHuman%2BPriors%2Bfor%2BPlaying%2BVideo%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GqBHlZXCjx4J&ei=_xZkYvykOvmQ6rQP5OqKqAo&json=", "num_citations": 122, "citedby_url": "/scholar?cites=2202192690517876762&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GqBHlZXCjx4J:scholar.google.com/&scioq=Investigating+Human+Priors+for+Playing+Video+Games&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.10217"}, "Learning Deep Models: Critical Points and Local Openness": {"container_type": "Publication", "bib": {"title": "Learning deep models: Critical points and local openness", "author": ["M Nouiehed", "M Razaviyayn"], "pub_year": "2021", "venue": "INFORMS Journal on \u2026", "abstract": "With the increasing popularity of nonconvex deep models, developing a unifying theory for studying the optimization problems that arise from training these models becomes very significant. Toward this end, we present in this paper a unifying landscape analysis framework that can be used when the training objective function is the composite of simple functions. Using the local openness property of the underlying training models, we provide simple sufficient conditions under which any local optimum of the resulting optimization"}, "filled": false, "gsrank": 1, "pub_url": "https://pubsonline.informs.org/doi/abs/10.1287/ijoo.2021.0062", "author_id": ["", "b5hImI4AAAAJ"], "url_scholarbib": "/scholar?q=info:LlD6Tzj0ZtEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDeep%2BModels:%2BCritical%2BPoints%2Band%2BLocal%2BOpenness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LlD6Tzj0ZtEJ&ei=BBdkYoCPCoySyASZk6HgCA&json=", "num_citations": 33, "citedby_url": "/scholar?cites=15089016124248576046&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LlD6Tzj0ZtEJ:scholar.google.com/&scioq=Learning+Deep+Models:+Critical+Points+and+Local+Openness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.02968"}, "Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control Gradient Explosion": {"container_type": "Publication", "bib": {"title": "Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion", "author": ["G Yang", "SS Schoenholz"], "pub_year": "2018", "venue": "NA", "abstract": "Based on insights from past works in deep mean field theory and information geometry, we  also provide a new perspective on the gradient explosion/vanishing problems: they lead to ill-"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJGY8GbR-", "author_id": ["Xz4RAJkAAAAJ", ""], "url_scholarbib": "/scholar?q=info:BFHzuix4yTYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BMean%2BField%2BTheory:%2BLayerwise%2BVariance%2Band%2BWidth%2BVariation%2Bas%2BMethods%2Bto%2BControl%2BGradient%2BExplosion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BFHzuix4yTYJ&ei=BhdkYvWnOO-Sy9YPs_mY8AM&json=", "num_citations": 8, "citedby_url": "/scholar?cites=3947818681877352708&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BFHzuix4yTYJ:scholar.google.com/&scioq=Deep+Mean+Field+Theory:+Layerwise+Variance+and+Width+Variation+as+Methods+to+Control+Gradient+Explosion&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJGY8GbR-"}, "Accelerating Neural Architecture Search using Performance Prediction": {"container_type": "Publication", "bib": {"title": "Accelerating neural architecture search using performance prediction", "author": ["B Baker", "O Gupta", "R Raskar", "N Naik"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.10823", "abstract": "Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.10823", "author_id": ["bMfPYdYAAAAJ", "2aEdHz0AAAAJ", "8hpOmVgAAAAJ", "M1IgIyMAAAAJ"], "url_scholarbib": "/scholar?q=info:sHBjqPdRuckJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccelerating%2BNeural%2BArchitecture%2BSearch%2Busing%2BPerformance%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sHBjqPdRuckJ&ei=CRdkYpShJI2ymgHg1rfQDQ&json=", "num_citations": 243, "citedby_url": "/scholar?cites=14535739396438847664&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sHBjqPdRuckJ:scholar.google.com/&scioq=Accelerating+Neural+Architecture+Search+using+Performance+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.10823"}, "Gradients explode - Deep Networks are shallow - ResNet explained": {"container_type": "Publication", "bib": {"title": "Gradients explode-deep networks are shallow-resnet explained", "author": ["G Philipp", "D Song", "JG Carbonell"], "pub_year": "2018", "venue": "NA", "abstract": "Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities\" solve\" the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the* collapsing domain problem*, which can arise in architectures that avoid exploding gradients. ResNets have"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJjcdFkPM", "author_id": ["IMi6NZMAAAAJ", "84WzBlYAAAAJ", "wlqqttEAAAAJ"], "url_scholarbib": "/scholar?q=info:PcUGvdht3OIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradients%2Bexplode%2B-%2BDeep%2BNetworks%2Bare%2Bshallow%2B-%2BResNet%2Bexplained%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PcUGvdht3OIJ&ei=DBdkYp7CAaKUy9YP_JONiAY&json=", "num_citations": 39, "citedby_url": "/scholar?cites=16347061525099758909&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PcUGvdht3OIJ:scholar.google.com/&scioq=Gradients+explode+-+Deep+Networks+are+shallow+-+ResNet+explained&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJjcdFkPM"}, "Predict Responsibly: Increasing Fairness by Learning to Defer": {"container_type": "Publication", "bib": {"title": "Predict responsibly: Increasing fairness by learning to defer", "author": ["D Madras", "T Pitassi", "R Zemel"], "pub_year": "2018", "venue": "NA", "abstract": "When machine learning models are used for high-stakes decisions, they should predict accurately, fairly, and responsibly. To fulfill these three requirements, a model must be able to output a reject option (ie say\"``I Don't Know\") when it is not qualified to make a prediction. In this work, we propose learning to defer, a method by which a model can defer judgment to a downstream decision-maker such as a human user. We show that learning to defer generalizes the rejection learning framework in two ways: by considering the effect of other"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyjVKLJwz", "author_id": ["MgnNDpkAAAAJ", "", "iBeDoRAAAAAJ"], "url_scholarbib": "/scholar?q=info:VoOLZ-xXUBgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPredict%2BResponsibly:%2BIncreasing%2BFairness%2Bby%2BLearning%2Bto%2BDefer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VoOLZ-xXUBgJ&ei=DxdkYsi6DZLeyQTE46-QAg&json=", "num_citations": 13, "citedby_url": "/scholar?cites=1751996927908217686&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VoOLZ-xXUBgJ:scholar.google.com/&scioq=Predict+Responsibly:+Increasing+Fairness+by+Learning+to+Defer&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyjVKLJwz"}, "Extending the Framework of Equilibrium Propagation to General Dynamics": {"container_type": "Publication", "bib": {"title": "Extending the framework of equilibrium propagation to general dynamics", "author": ["B Scellier", "A Goyal", "J Binas", "T Mesnard", "Y Bengio"], "pub_year": "2018", "venue": "NA", "abstract": "The biological plausibility of the backpropagation algorithm has long been doubted by neuroscientists. Two major reasons are that neurons would need to send two different types of signal in the forward and backward phases, and that pairs of neurons would need to communicate through symmetric bidirectional connections. We present a simple two-phase learning procedure for fixed point recurrent networks that addresses both these issues. In our model, neurons perform leaky integration and synaptic weights are updated through a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJ5V4ICIG", "author_id": ["GWyeAskAAAAJ", "krrh6OUAAAAJ", "oD1W8a4AAAAJ", "7qufP-8AAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:mJw5jtz-gtkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExtending%2Bthe%2BFramework%2Bof%2BEquilibrium%2BPropagation%2Bto%2BGeneral%2BDynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mJw5jtz-gtkJ&ei=EhdkYs_gHIySyASZk6HgCA&json=", "num_citations": 6, "citedby_url": "/scholar?cites=15673369876435147928&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mJw5jtz-gtkJ:scholar.google.com/&scioq=Extending+the+Framework+of+Equilibrium+Propagation+to+General+Dynamics&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJ5V4ICIG"}, "WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling": {"container_type": "Publication", "bib": {"title": "WHAI: Weibull hybrid autoencoding inference for deep topic modeling", "author": ["H Zhang", "B Chen", "D Guo", "M Zhou"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.01328", "abstract": "To train an inference network jointly with a deep generative topic model, making it both scalable to big corpora and fast in out-of-sample prediction, we develop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet allocation, which infers posterior samples via a hybrid of stochastic-gradient MCMC and autoencoding variational Bayes. The generative network of WHAI has a hierarchy of gamma distributions, while the inference network of WHAI is a Weibull upward-downward variational autoencoder, which integrates a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.01328", "author_id": ["Eo8e5icAAAAJ", "uv16_-UAAAAJ", "", "LXwCIisAAAAJ"], "url_scholarbib": "/scholar?q=info:70bxudNVlW8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWHAI:%2BWeibull%2BHybrid%2BAutoencoding%2BInference%2Bfor%2BDeep%2BTopic%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=70bxudNVlW8J&ei=FhdkYvfWPPmQ6rQP5OqKqAo&json=", "num_citations": 63, "citedby_url": "/scholar?cites=8040427077585946351&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:70bxudNVlW8J:scholar.google.com/&scioq=WHAI:+Weibull+Hybrid+Autoencoding+Inference+for+Deep+Topic+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.01328"}, "LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION": {"container_type": "Publication", "bib": {"title": "Lsh-sampling breaks the computation chicken-and-egg loop in adaptive stochastic gradient estimation", "author": ["B Chen", "Y Xu", "A Shrivastava"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.14162", "abstract": "Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch-wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient itself, which we call the chicken-and-the-egg loop. As a result, the false"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.14162", "author_id": ["jCNJhFcAAAAJ", "-CqyjXEAAAAJ", "SGT23RAAAAAJ"], "url_scholarbib": "/scholar?q=info:zwyhj272TrQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLSH-SAMPLING%2BBREAKS%2BTHE%2BCOMPUTATIONAL%2BCHICKEN-AND-EGG%2BLOOP%2BIN%2BADAPTIVE%2BSTOCHASTIC%2BGRADIENT%2BESTIMATION%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zwyhj272TrQJ&ei=GRdkYpmwOIyuyAT-mrWwCA&json=", "num_citations": 11, "citedby_url": "/scholar?cites=12992592929726991567&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zwyhj272TrQJ:scholar.google.com/&scioq=LSH-SAMPLING+BREAKS+THE+COMPUTATIONAL+CHICKEN-AND-EGG+LOOP+IN+ADAPTIVE+STOCHASTIC+GRADIENT+ESTIMATION&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.14162"}, "LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING": {"container_type": "Publication", "bib": {"title": "Learning to share: Simultaneous parameter tying and sparsification in deep learning", "author": ["D Zhang", "H Wang", "M Figueiredo"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. This has motivated a large body of work to reduce the complexity of the neural network by using sparsity-inducing regularizers. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rypT3fb0b", "author_id": ["klYBD5MAAAAJ", "8LsFD80AAAAJ", "S-pd0NwAAAAJ"], "url_scholarbib": "/scholar?q=info:ZGO9Zhb-0B8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLEARNING%2BTO%2BSHARE:%2BSIMULTANEOUS%2BPARAMETER%2BTYING%2BAND%2BSPARSIFICATION%2BIN%2BDEEP%2BLEARNING%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZGO9Zhb-0B8J&ei=HRdkYsO7A4OEmgHx-5DADA&json=", "num_citations": 27, "citedby_url": "/scholar?cites=2292611582498005860&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZGO9Zhb-0B8J:scholar.google.com/&scioq=LEARNING+TO+SHARE:+SIMULTANEOUS+PARAMETER+TYING+AND+SPARSIFICATION+IN+DEEP+LEARNING&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rypT3fb0b"}, "Regularization Neural Networks via Constrained Virtual  Movement Field": {"container_type": "Publication", "bib": {"title": "Regularization Neural Networks via Constrained Virtual Movement Field", "author": ["Z Zhang", "C Jung"], "pub_year": "2018", "venue": "NA", "abstract": "We provide a novel thinking of regularization neural networks. We smooth the objective of neural networks wrt small adversarial perturbations of the inputs. Different from previous works, we assume the adversarial perturbations are caused by the movement field. When the magnitude of movement field approaches 0, we call it virtual movement field. By introducing the movement field, we cast the problem of finding adversarial perturbations into the problem of finding adversarial movement field. By adding proper geometrical constraints"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkHl6MWC-", "author_id": ["", ""], "url_scholarbib": "/scholar?q=info:xlJXxEZp0gUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegularization%2BNeural%2BNetworks%2Bvia%2BConstrained%2BVirtual%2B%2BMovement%2BField%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xlJXxEZp0gUJ&ei=IBdkYsn8M42ymgHg1rfQDQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:xlJXxEZp0gUJ:scholar.google.com/&scioq=Regularization+Neural+Networks+via+Constrained+Virtual++Movement+Field&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkHl6MWC-"}, "Learning to Write by Learning the Objective": {"container_type": "Publication", "bib": {"title": "Learning to Write by Learning the Objective", "author": ["A Holtzman", "J Buys", "M Forbes", "A Bosselut", "Y Choi"], "pub_year": "2018", "venue": "NA", "abstract": "Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language. Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory. We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1lfpfZAb", "author_id": ["8veBOSIAAAAJ", "TlqDbGYAAAAJ", "sF1vZ0oAAAAJ", "", "vhP-tlcAAAAJ"], "url_scholarbib": "/scholar?q=info:Kj_IIFI7TCAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BWrite%2Bby%2BLearning%2Bthe%2BObjective%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Kj_IIFI7TCAJ&ei=IxdkYqCeGrKO6rQPy-CRsA8&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:Kj_IIFI7TCAJ:scholar.google.com/&scioq=Learning+to+Write+by+Learning+the+Objective&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1lfpfZAb"}, "Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy": {"container_type": "Publication", "bib": {"title": "Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy", "author": ["ST Kothen-Hill", "A Zviran", "RC Schulman", "S Deochand"], "pub_year": "2018", "venue": "NA", "abstract": "Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods. Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage. Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1DkN7ZCZ", "author_id": ["haiaxLYAAAAJ", "cmlgX_YAAAAJ", "4ewzqwgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:jrRKuiO3rzAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2Blearning%2Bmutation%2Bprediction%2Benables%2Bearly%2Bstage%2Blung%2Bcancer%2Bdetection%2Bin%2Bliquid%2Bbiopsy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jrRKuiO3rzAJ&ei=KRdkYrTKBJLeyQTE46-QAg&json=", "num_citations": 11, "citedby_url": "/scholar?cites=3508223998822102158&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jrRKuiO3rzAJ:scholar.google.com/&scioq=Deep+learning+mutation+prediction+enables+early+stage+lung+cancer+detection+in+liquid+biopsy&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1DkN7ZCZ"}, "Learning Efficient Tensor Representations with Ring Structure Networks": {"container_type": "Publication", "bib": {"title": "Learning efficient tensor representations with ring-structured networks", "author": ["Q Zhao", "M Sugiyama", "L Yuan"], "pub_year": "2019", "venue": "ICASSP 2019-2019 \u2026", "abstract": "Tensor train decomposition is a powerful representation for high-order tensors, which has been successfully applied to various machine learning tasks in recent years. In this paper, we study a more generalized tensor decomposition with a ring-structured network by employing circular multilinear products over a sequence of lower-order core tensors. We refer to such tensor decomposition as tensor ring (TR) representation. Our goal is to introduce learning algorithms including sequential singular value decompositions and"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8682231/", "author_id": ["cSQGe3YAAAAJ", "GkYIrlIAAAAJ", "h25R01wAAAAJ"], "url_scholarbib": "/scholar?q=info:0Y5LUXJDWlUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BEfficient%2BTensor%2BRepresentations%2Bwith%2BRing%2BStructure%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0Y5LUXJDWlUJ&ei=LhdkYuTzKYOEmgHx-5DADA&json=", "num_citations": 66, "citedby_url": "/scholar?cites=6150302399397072593&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0Y5LUXJDWlUJ:scholar.google.com/&scioq=Learning+Efficient+Tensor+Representations+with+Ring+Structure+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.08286"}, "Do GANs learn the distribution? Some Theory and Empirics": {"container_type": "Publication", "bib": {"title": "Do GANs learn the distribution? some theory and empirics", "author": ["S Arora", "A Risteski", "Y Zhang"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al.(2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al.(2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJehNfW0-", "author_id": ["RUP4S68AAAAJ", "NWPDSEsAAAAJ", "lc6CVqEAAAAJ"], "url_scholarbib": "/scholar?q=info:tZSJyfP0NdYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDo%2BGANs%2Blearn%2Bthe%2Bdistribution%253F%2BSome%2BTheory%2Band%2BEmpirics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tZSJyfP0NdYJ&ei=PxVkYuDaJpyO6rQP_qe3mAs&json=", "num_citations": 114, "citedby_url": "/scholar?cites=15435512625777710261&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tZSJyfP0NdYJ:scholar.google.com/&scioq=Do+GANs+learn+the+distribution%3F+Some+Theory+and+Empirics&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJehNfW0-"}, "On the Information Bottleneck Theory of Deep Learning": {"container_type": "Publication", "bib": {"title": "On the information bottleneck theory of deep learning", "author": ["AM Saxe", "Y Bansal", "J Dapello", "M Advani"], "pub_year": "2019", "venue": "Journal of Statistical \u2026", "abstract": "The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs"}, "filled": false, "gsrank": 1, "pub_url": "https://iopscience.iop.org/article/10.1088/1742-5468/ab3985/meta", "author_id": ["h0Al1fcAAAAJ", "uj1OljkAAAAJ", "4ZnsOa8AAAAJ", "0OgwPgcAAAAJ"], "url_scholarbib": "/scholar?q=info:vkMWKJs0TKoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BInformation%2BBottleneck%2BTheory%2Bof%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vkMWKJs0TKoJ&ei=RhVkYpaHCYOEmgHx-5DADA&json=", "num_citations": 317, "citedby_url": "/scholar?cites=12271240925674881982&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vkMWKJs0TKoJ:scholar.google.com/&scioq=On+the+Information+Bottleneck+Theory+of+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ry_WPG-A-"}, "A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs": {"container_type": "Publication", "bib": {"title": "A compressed sensing view of unsupervised text embeddings, bag-of-n-grams, and LSTMs", "author": ["S Arora", "M Khodak", "N Saunshi"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the \u201cmeaning\u201d of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of"}, "filled": false, "gsrank": 1, "pub_url": "https://oar.princeton.edu/handle/88435/pr13v8j", "author_id": ["RUP4S68AAAAJ", "gIH9P-8AAAAJ", "F24vXggAAAAJ"], "url_scholarbib": "/scholar?q=info:Odh44YyPJ20J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCompressed%2BSensing%2BView%2Bof%2BUnsupervised%2BText%2BEmbeddings,%2BBag-of-n-Grams,%2Band%2BLSTMs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Odh44YyPJ20J&ei=ShVkYt-fCJqSy9YP8pKNsAE&json=", "num_citations": 40, "citedby_url": "/scholar?cites=7865413109466847289&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Odh44YyPJ20J:scholar.google.com/&scioq=A+Compressed+Sensing+View+of+Unsupervised+Text+Embeddings,+Bag-of-n-Grams,+and+LSTMs&hl=en&as_sdt=0,33", "eprint_url": "https://oar.princeton.edu/bitstream/88435/pr13v8j/1/CompressedSensingView.pdf"}, "MGAN: Training Generative Adversarial Nets with Multiple Generators": {"container_type": "Publication", "bib": {"title": "MGAN: Training generative adversarial nets with multiple generators", "author": ["Q Hoang", "TD Nguyen", "T Le", "D Phung"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkmu5b0a-", "author_id": ["D4xcG3YAAAAJ", "4hT6E04AAAAJ", "gysdMxwAAAAJ", "OtA9SwIAAAAJ"], "url_scholarbib": "/scholar?q=info:vqhatF0KVdEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMGAN:%2BTraining%2BGenerative%2BAdversarial%2BNets%2Bwith%2BMultiple%2BGenerators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vqhatF0KVdEJ&ei=ThVkYpTCC6KUy9YP_JONiAY&json=", "num_citations": 163, "citedby_url": "/scholar?cites=15083973924521420990&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vqhatF0KVdEJ:scholar.google.com/&scioq=MGAN:+Training+Generative+Adversarial+Nets+with+Multiple+Generators&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkmu5b0a-"}, "MaskGAN: Better Text Generation via Filling in the _______": {"container_type": "Publication", "bib": {"title": "Maskgan: better text generation via filling in the_", "author": ["W Fedus", "I Goodfellow", "AM Dai"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1801.07736", "abstract": "Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi-mum likelihood and teacher forcing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.07736", "author_id": ["-ZfwQOkAAAAJ", "iYN86KEAAAAJ", "2r2NuDAAAAAJ"], "url_scholarbib": "/scholar?q=info:xWg1GSUhx28J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaskGAN:%2BBetter%2BText%2BGeneration%2Bvia%2BFilling%2Bin%2Bthe%2B_______%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xWg1GSUhx28J&ei=YRVkYtSZH4ySyASZk6HgCA&json=", "num_citations": 417, "citedby_url": "/scholar?cites=8054442901795858629&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xWg1GSUhx28J:scholar.google.com/&scioq=MaskGAN:+Better+Text+Generation+via+Filling+in+the+_______&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.07736.pdf%3C/p%3E?ref=https://githubhelp.com"}, "Memory Architectures in Recurrent Neural Network Language Models": {"container_type": "Publication", "bib": {"title": "Memory architectures in recurrent neural network language models", "author": ["D Yogatama", "Y Miao", "G Melis", "W Ling"], "pub_year": "2018", "venue": "International \u2026", "abstract": "We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov, 2015; Grefenstette et al., 2015) to allow a variable number of pop operations more naturally that further improves performance. We"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkFqf0lAZ", "author_id": ["IBlMTLwAAAAJ", "dxBYu1AAAAAJ", "TbLw2lcAAAAJ", "gl0PhvEAAAAJ"], "url_scholarbib": "/scholar?q=info:TFWCy7Awx5EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemory%2BArchitectures%2Bin%2BRecurrent%2BNeural%2BNetwork%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TFWCy7Awx5EJ&ei=ZBVkYqP8I-HDywSSipaYAg&json=", "num_citations": 51, "citedby_url": "/scholar?cites=10504418191751664972&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TFWCy7Awx5EJ:scholar.google.com/&scioq=Memory+Architectures+in+Recurrent+Neural+Network+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkFqf0lAZ"}, "Matrix capsules with EM routing": {"container_type": "Publication", "bib": {"title": "Matrix capsules with EM routing", "author": ["GE Hinton", "S Sabour", "N Frosst"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "The non-linearity implemented by a whole capsule layer is a form of cluster finding using  the EM algorithm, so we call it EM Routing. Procedure 1 Routing algorithm returns activation"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJWLfGWRb&noteId=rk5MadsMf&noteId=rk5MadsMf", "author_id": ["JicYPdAAAAAJ", "l8wQ39EAAAAJ", "1yVnaTgAAAAJ"], "url_scholarbib": "/scholar?q=info:vN7jXuUrduEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMatrix%2Bcapsules%2Bwith%2BEM%2Brouting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vN7jXuUrduEJ&ei=ZhVkYqu3NKKUy9YP_JONiAY&json=", "num_citations": 759, "citedby_url": "/scholar?cites=16246220969925140156&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vN7jXuUrduEJ:scholar.google.com/&scioq=Matrix+capsules+with+EM+routing&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJWLfGWRb"}, "NerveNet: Learning Structured Policy with Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Nervenet: Learning structured policy with graph neural networks", "author": ["T Wang", "R Liao", "J Ba", "S Fidler"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1sqHMZCb", "author_id": ["HvJj-pEAAAAJ", "2wrS35MAAAAJ", "ymzxRhAAAAAJ", "CUlqK5EAAAAJ"], "url_scholarbib": "/scholar?q=info:HwcBkjxJ5dEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNerveNet:%2BLearning%2BStructured%2BPolicy%2Bwith%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HwcBkjxJ5dEJ&ei=axVkYr_mAoOEmgHx-5DADA&json=", "num_citations": 148, "citedby_url": "/scholar?cites=15124575448090085151&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HwcBkjxJ5dEJ:scholar.google.com/&scioq=NerveNet:+Learning+Structured+Policy+with+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1sqHMZCb"}, "Decision Boundary Analysis of Adversarial Examples": {"container_type": "Publication", "bib": {"title": "Decision boundary analysis of adversarial examples", "author": ["W He", "B Li", "D Song"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs. Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models. However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls. In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkpiPMbA-", "author_id": ["", "K8vJkTcAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:Wdp4f1Ums80J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecision%2BBoundary%2BAnalysis%2Bof%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Wdp4f1Ums80J&ei=bhVkYvrVEe-Sy9YPs_mY8AM&json=", "num_citations": 92, "citedby_url": "/scholar?cites=14822232947259136601&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Wdp4f1Ums80J:scholar.google.com/&scioq=Decision+Boundary+Analysis+of+Adversarial+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkpiPMbA-"}, "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback": {"container_type": "Publication", "bib": {"title": "Residual loss prediction: Reinforcement learning with no incremental feedback", "author": ["H Daum\u00e9 III", "J Langford", "A Sharaf"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJNMYceCW", "author_id": ["PbEw81gAAAAJ", "LFiqVpwAAAAJ", "It3Gm1EAAAAJ"], "url_scholarbib": "/scholar?q=info:2ipvBMOTJJwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DResidual%2BLoss%2BPrediction:%2BReinforcement%2BLearning%2BWith%2BNo%2BIncremental%2BFeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2ipvBMOTJJwJ&ei=eBVkYvbyKvmQ6rQP5OqKqAo&json=", "num_citations": 5, "citedby_url": "/scholar?cites=11251280234880641754&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2ipvBMOTJJwJ:scholar.google.com/&scioq=Residual+Loss+Prediction:+Reinforcement+Learning+With+No+Incremental+Feedback&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJNMYceCW"}, "Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity": {"container_type": "Publication", "bib": {"title": "Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity", "author": ["T Zhou", "J Bilmes"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning. The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages. At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BywyFQlAW", "author_id": ["OKvgizMAAAAJ", "L9QufAsAAAAJ"], "url_scholarbib": "/scholar?q=info:7vqZFjjM-tEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimax%2BCurriculum%2BLearning:%2BMachine%2BTeaching%2Bwith%2BDesirable%2BDifficulties%2Band%2BScheduled%2BDiversity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7vqZFjjM-tEJ&ei=fRVkYrOcKcLZmQHnraWYCA&json=", "num_citations": 35, "citedby_url": "/scholar?cites=15130630439374027502&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7vqZFjjM-tEJ:scholar.google.com/&scioq=Minimax+Curriculum+Learning:+Machine+Teaching+with+Desirable+Difficulties+and+Scheduled+Diversity&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BywyFQlAW"}, "Variational image compression with a scale hyperprior": {"container_type": "Publication", "bib": {"title": "Variational image compression with a scale hyperprior", "author": ["J Ball\u00e9", "D Minnen", "S Singh", "SJ Hwang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.01436", "author_id": ["uKDe38UAAAAJ", "Pa8qzYQAAAAJ", "L7fTK1MAAAAJ", ""], "url_scholarbib": "/scholar?q=info:LYbsw0xLyPcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2Bimage%2Bcompression%2Bwith%2Ba%2Bscale%2Bhyperprior%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LYbsw0xLyPcJ&ei=gBVkYrqQL--Sy9YPs_mY8AM&json=", "num_citations": 614, "citedby_url": "/scholar?cites=17854603515786987053&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LYbsw0xLyPcJ:scholar.google.com/&scioq=Variational+image+compression+with+a+scale+hyperprior&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.01436"}, "Maximum a Posteriori Policy Optimisation": {"container_type": "Publication", "bib": {"title": "Maximum a posteriori policy optimisation", "author": ["A Abdolmaleki", "JT Springenberg", "Y Tassa"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.06920", "author_id": ["cCYTVWQAAAAJ", "MGXJkIAAAAAJ", "CjOTm_4AAAAJ"], "url_scholarbib": "/scholar?q=info:TUzAXDRAh8kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaximum%2Ba%2BPosteriori%2BPolicy%2BOptimisation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TUzAXDRAh8kJ&ei=ghVkYqmrO5GJmwGY-qmYDQ&json=", "num_citations": 265, "citedby_url": "/scholar?cites=14521646117118037069&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TUzAXDRAh8kJ:scholar.google.com/&scioq=Maximum+a+Posteriori+Policy+Optimisation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.06920"}, "The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning", "author": ["A Gruslys", "W Dabney", "MG Azar", "B Piot"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.04651", "author_id": ["", "dR-7QW8AAAAJ", "AlTQrFcAAAAJ", "fqxNUREAAAAJ"], "url_scholarbib": "/scholar?q=info:h1Hde-3283IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BReactor:%2BA%2Bfast%2Band%2Bsample-efficient%2BActor-Critic%2Bagent%2Bfor%2B%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=h1Hde-3283IJ&ei=hRVkYsrqDuiSy9YPp-OyiAE&json=", "num_citations": 63, "citedby_url": "/scholar?cites=8283235639510258055&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:h1Hde-3283IJ:scholar.google.com/&scioq=The+Reactor:+A+fast+and+sample-efficient+Actor-Critic+agent+for++Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.04651"}, "A Neural Representation of Sketch Drawings": {"container_type": "Publication", "bib": {"title": "A neural representation of sketch drawings", "author": ["D Ha", "D Eck"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1704.03477", "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.03477", "author_id": ["J1j92GsxVUMC", "bLb3VdIAAAAJ"], "url_scholarbib": "/scholar?q=info:GRalL5Y2byAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BNeural%2BRepresentation%2Bof%2BSketch%2BDrawings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GRalL5Y2byAJ&ei=iRVkYtmUEMLZmQHnraWYCA&json=", "num_citations": 573, "citedby_url": "/scholar?cites=2337146750300919321&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GRalL5Y2byAJ:scholar.google.com/&scioq=A+Neural+Representation+of+Sketch+Drawings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.03477?ref=https://githubhelp.com"}, "Unbiased Online Recurrent Optimization": {"container_type": "Publication", "bib": {"title": "Unbiased online recurrent optimization", "author": ["C Tallec", "Y Ollivier"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.05043", "abstract": "The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for online learning  of general recurrent computational graphs such as recurrent network models. It works in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.05043", "author_id": ["OPKX4GgLCxIC", ""], "url_scholarbib": "/scholar?q=info:gjzZZGqefDAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnbiased%2BOnline%2BRecurrent%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gjzZZGqefDAJ&ei=jBVkYqbgH4ySyASZk6HgCA&json=", "num_citations": 60, "citedby_url": "/scholar?cites=3493841590728342658&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gjzZZGqefDAJ:scholar.google.com/&scioq=Unbiased+Online+Recurrent+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.05043"}, "An Online Learning Approach to Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "An online learning approach to generative adversarial networks", "author": ["P Grnarova", "KY Levy", "A Lucchi", "T Hofmann"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1. On the theory side, we show that our method provably converges to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.03269", "author_id": ["fXf5gXEAAAAJ", "dG7KSW0AAAAJ", "V1ONSgIAAAAJ", "T3hAyLkAAAAJ"], "url_scholarbib": "/scholar?q=info:kTtJIU63-sAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BOnline%2BLearning%2BApproach%2Bto%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kTtJIU63-sAJ&ei=kRVkYoTZG5LeyQTE46-QAg&json=", "num_citations": 70, "citedby_url": "/scholar?cites=13905628345653607313&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kTtJIU63-sAJ:scholar.google.com/&scioq=An+Online+Learning+Approach+to+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.03269"}, "Temporally Efficient Deep Learning with Spikes": {"container_type": "Publication", "bib": {"title": "Temporally efficient deep learning with spikes", "author": ["P O'Connor", "E Gavves", "M Welling"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.04159", "abstract": "The vast majority of natural sensory data is temporally redundant. Video frames or audio samples which are sampled at nearby points in time tend to have similar values. Typically, deep learning algorithms take no advantage of this redundancy to reduce computation. This can be an obscene waste of energy. We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data-not the rate at which we process the data. We do this by having neurons communicate a combination of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.04159", "author_id": ["", "QqfCvsgAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:fXO84xZuI5gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTemporally%2BEfficient%2BDeep%2BLearning%2Bwith%2BSpikes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fXO84xZuI5gJ&ei=lRVkYrurBoyuyAT-mrWwCA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=10962726962539033469&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fXO84xZuI5gJ:scholar.google.com/&scioq=Temporally+Efficient+Deep+Learning+with+Spikes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.04159"}, "Depthwise Separable Convolutions for Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Depthwise separable convolutions for neural machine translation", "author": ["L Kaiser", "AN Gomez", "F Chollet"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.03059", "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.03059", "author_id": ["JWmiQR0AAAAJ", "2oq9614AAAAJ", "VfYhf2wAAAAJ"], "url_scholarbib": "/scholar?q=info:G9AC1V-wXWgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDepthwise%2BSeparable%2BConvolutions%2Bfor%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G9AC1V-wXWgJ&ei=mBVkYtSsM5qSy9YP8pKNsAE&json=", "num_citations": 211, "citedby_url": "/scholar?cites=7520360878420709403&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:G9AC1V-wXWgJ:scholar.google.com/&scioq=Depthwise+Separable+Convolutions+for+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.03059"}, "Distributional Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Distributional adversarial networks", "author": ["C Li", "D Alvarez-Melis", "K Xu", "S Jegelka", "S Sra"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "\u2022 We introduce a new distributional framework for adversarial training of neural networks, which   We propose two types of adversarial networks based on this distributional approach, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.09549", "author_id": ["", "XsxZrYYAAAAJ", "eV2tuR8AAAAJ", "gTWUZlsAAAAJ", "eyCw9goAAAAJ"], "url_scholarbib": "/scholar?q=info:K5SYOZV17zsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributional%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=K5SYOZV17zsJ&ei=nBVkYt_2AZLeyQTE46-QAg&json=", "num_citations": 24, "citedby_url": "/scholar?cites=4318799851448472619&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:K5SYOZV17zsJ:scholar.google.com/&scioq=Distributional+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.09549"}, "Regularizing and Optimizing LSTM Language Models": {"container_type": "Publication", "bib": {"title": "Regularizing and optimizing LSTM language models", "author": ["S Merity", "NS Keskar", "R Socher"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.02182", "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.02182", "author_id": ["AolIi4QAAAAJ", "CJ-_cEEAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:4HEsvaoWSZMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegularizing%2Band%2BOptimizing%2BLSTM%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4HEsvaoWSZMJ&ei=nxVkYvyACpqSy9YP8pKNsAE&json=", "num_citations": 938, "citedby_url": "/scholar?cites=10613038919449342432&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4HEsvaoWSZMJ:scholar.google.com/&scioq=Regularizing+and+Optimizing+LSTM+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.02182.pdf?ref=https://githubhelp.com"}, "SMASH: One-Shot Model Architecture Search through HyperNetworks": {"container_type": "Publication", "bib": {"title": "Smash: one-shot model architecture search through hypernetworks", "author": ["A Brock", "T Lim", "JM Ritchie", "N Weston"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.05344", "abstract": "Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.05344", "author_id": ["NIxD36wAAAAJ", "qpfMj-8AAAAJ", "", "PZSrJvIAAAAJ"], "url_scholarbib": "/scholar?q=info:qOPylis4HpEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSMASH:%2BOne-Shot%2BModel%2BArchitecture%2BSearch%2Bthrough%2BHyperNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qOPylis4HpEJ&ei=oxVkYsPdIcLZmQHnraWYCA&json=", "num_citations": 541, "citedby_url": "/scholar?cites=10456857144668119976&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qOPylis4HpEJ:scholar.google.com/&scioq=SMASH:+One-Shot+Model+Architecture+Search+through+HyperNetworks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.05344.pdf?ref=https://codemonkey.link"}, "CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training": {"container_type": "Publication", "bib": {"title": "Causalgan: Learning causal implicit generative models with adversarial training", "author": ["M Kocaoglu", "C Snyder", "AG Dimakis"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose an adversarial training procedure for learning a causal implicit generative model for a given causal graph. We show that adversarial training can be used to learn a generative model with true observational and interventional distributions if the generator architecture is consistent with the given causal graph. We consider the application of generating faces based on given binary labels where the dependency structure between the labels is preserved with a causal graph. This problem can be seen as learning a causal"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.02023", "author_id": ["7N7bzdwAAAAJ", "", "JSFmVQEAAAAJ"], "url_scholarbib": "/scholar?q=info:af3CFqB_x-gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCausalGAN:%2BLearning%2BCausal%2BImplicit%2BGenerative%2BModels%2Bwith%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=af3CFqB_x-gJ&ei=phVkYpTGPMLZmQHnraWYCA&json=", "num_citations": 130, "citedby_url": "/scholar?cites=16773515662718074217&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:af3CFqB_x-gJ:scholar.google.com/&scioq=CausalGAN:+Learning+Causal+Implicit+Generative+Models+with+Adversarial+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.02023.pdf?ref=https://githubhelp.com"}, "Learning how to explain neural networks: PatternNet and PatternAttribution": {"container_type": "Publication", "bib": {"title": "Learning how to explain neural networks: Patternnet and patternattribution", "author": ["PJ Kindermans", "KT Sch\u00fctt", "M Alber", "KR M\u00fcller"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.05598", "author_id": ["FpI8dFwAAAAJ", "0e49RfgAAAAJ", "pnOLBcoAAAAJ", "jplQac8AAAAJ"], "url_scholarbib": "/scholar?q=info:hAxX8SFGqCMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bhow%2Bto%2Bexplain%2Bneural%2Bnetworks:%2BPatternNet%2Band%2BPatternAttribution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hAxX8SFGqCMJ&ei=qRVkYrPHHZLeyQTE46-QAg&json=", "num_citations": 291, "citedby_url": "/scholar?cites=2569380699011746948&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hAxX8SFGqCMJ:scholar.google.com/&scioq=Learning+how+to+explain+neural+networks:+PatternNet+and+PatternAttribution&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.05598"}, "Rotational Unit of Memory ": {"container_type": "Publication", "bib": {"title": "Rotational unit of memory: a novel representation unit for RNNs with scalable applications", "author": ["R Dangovski", "L Jing", "P Nakov", "M Tatalovi\u0107"], "pub_year": "2019", "venue": "Transactions of the \u2026", "abstract": "memory state, Rotational Unit of Memory (RUM), that unifies the concepts of unitary learning  and associative memory tasks such as memory copying and memory recall much better than"}, "filled": false, "gsrank": 1, "pub_url": "https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00258/43512", "author_id": ["vFTpmUQAAAAJ", "VhxDLwcAAAAJ", "DfXsKZ4AAAAJ", ""], "url_scholarbib": "/scholar?q=info:TrvnJeU6a9sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRotational%2BUnit%2Bof%2BMemory%2B%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TrvnJeU6a9sJ&ei=rBVkYrvmL8LZmQHnraWYCA&json=", "num_citations": 19, "citedby_url": "/scholar?cites=15810795672672123726&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TrvnJeU6a9sJ:scholar.google.com/&scioq=Rotational+Unit+of+Memory+&hl=en&as_sdt=0,33", "eprint_url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00258/43512"}, "Searching for Activation Functions": {"container_type": "Publication", "bib": {"title": "Searching for activation functions", "author": ["P Ramachandran", "B Zoph", "QV Le"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.05941", "abstract": "to discover novel activation functions. We focus on finding new scalar activation functions,  which take in as input a scalar and output a scalar, because scalar activation functions can be"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.05941", "author_id": ["ktKXDuMAAAAJ", "H-BnRI0AAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:cQokjMydOgkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSearching%2Bfor%2BActivation%2BFunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cQokjMydOgkJ&ei=rxVkYs6dNpGJmwGY-qmYDQ&json=", "num_citations": 1797, "citedby_url": "/scholar?cites=665017396840630897&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cQokjMydOgkJ:scholar.google.com/&scioq=Searching+for+Activation+Functions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.05941;%20http://arxiv.org/abs/1710.05941"}, "Exponentially vanishing sub-optimal local minima in multilayer neural networks": {"container_type": "Publication", "bib": {"title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "author": ["D Soudry", "E Hoffer"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.05777", "abstract": "Background: Statistical mechanics results (Dauphin et al.(2014); Choromanska et al.(2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (eg,\" near\" linear separability), or an unrealistic hidden layer with $\\Omega\\left (N\\right) $ units. Results: We examine a MNN with one hidden layer of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.05777", "author_id": ["AEBWEm8AAAAJ", "iEfTH7AAAAAJ"], "url_scholarbib": "/scholar?q=info:D05E8xj2--UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExponentially%2Bvanishing%2Bsub-optimal%2Blocal%2Bminima%2Bin%2Bmultilayer%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D05E8xj2--UJ&ei=sxVkYpWaKfmQ6rQP5OqKqAo&json=", "num_citations": 89, "citedby_url": "/scholar?cites=16572109840860859919&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D05E8xj2--UJ:scholar.google.com/&scioq=Exponentially+vanishing+sub-optimal+local+minima+in+multilayer+neural+networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.05777"}, "The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings": {"container_type": "Publication", "bib": {"title": "The role of minimal complexity functions in unsupervised learning of semantic mappings", "author": ["T Galanti", "L Wolf", "S Benaim"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1709.00074", "abstract": "We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.00074", "author_id": ["ut_ISVIAAAAJ", "UbFrXTsAAAAJ", "-zSM2I8AAAAJ"], "url_scholarbib": "/scholar?q=info:IB-J_9MR6DsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BRole%2Bof%2BMinimal%2BComplexity%2BFunctions%2Bin%2BUnsupervised%2BLearning%2Bof%2BSemantic%2BMappings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IB-J_9MR6DsJ&ei=uBVkYt-xNZyO6rQP_qe3mAs&json=", "num_citations": 24, "citedby_url": "/scholar?cites=4316719845057568544&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IB-J_9MR6DsJ:scholar.google.com/&scioq=The+Role+of+Minimal+Complexity+Functions+in+Unsupervised+Learning+of+Semantic+Mappings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.00074"}, "Multi-View Data Generation Without View Supervision": {"container_type": "Publication", "bib": {"title": "Multi-view data generation without view supervision", "author": ["M Chen", "L Denoyer", "T Arti\u00e8res"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00305", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00305", "author_id": ["QnRpMJAAAAAJ", "", "p0-_w_AAAAAJ"], "url_scholarbib": "/scholar?q=info:PF1FgO24JdQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-View%2BData%2BGeneration%2BWithout%2BView%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PF1FgO24JdQJ&ei=vBVkYujeGIyuyAT-mrWwCA&json=", "num_citations": 20, "citedby_url": "/scholar?cites=15286827840377806140&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PF1FgO24JdQJ:scholar.google.com/&scioq=Multi-View+Data+Generation+Without+View+Supervision&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00305"}, "Challenges in Disentangling Independent Factors of Variation": {"container_type": "Publication", "bib": {"title": "Challenges in disentangling independent factors of variation", "author": ["A Szab\u00f3", "Q Hu", "T Portenier", "M Zwicker"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the problem of building models that disentangle independent factors of variation. Such models could be used to encode features that can efficiently be used for classification and to transfer attributes between different images in image synthesis. As data we use a weakly labeled training set. Our weak labels indicate what single factor has changed between two data samples, although the relative value of the change is unknown. This labeling is of particular interest as it may be readily available without annotation costs. To"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02245", "author_id": ["qdLLBHIAAAAJ", "KmolYVsAAAAJ", "b5Zoxi8AAAAJ", "KW0FmzgAAAAJ"], "url_scholarbib": "/scholar?q=info:_UvFhiLnAhEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DChallenges%2Bin%2BDisentangling%2BIndependent%2BFactors%2Bof%2BVariation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_UvFhiLnAhEJ&ei=wBVkYu-WKJqSy9YP8pKNsAE&json=", "num_citations": 44, "citedby_url": "/scholar?cites=1225796184074177533&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_UvFhiLnAhEJ:scholar.google.com/&scioq=Challenges+in+Disentangling+Independent+Factors+of+Variation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02245"}, "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations": {"container_type": "Publication", "bib": {"title": "Variational inference of disentangled latent concepts from unlabeled observations", "author": ["A Kumar", "P Sattigeri", "A Balakrishnan"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00848", "abstract": "Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00848", "author_id": ["6vghMS0AAAAJ", "m-s38ikAAAAJ", "q-Sc_MoAAAAJ"], "url_scholarbib": "/scholar?q=info:FpmeoMGLKwEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BInference%2Bof%2BDisentangled%2BLatent%2BConcepts%2Bfrom%2BUnlabeled%2BObservations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FpmeoMGLKwEJ&ei=xBVkYtmGD5qSy9YP8pKNsAE&json=", "num_citations": 302, "citedby_url": "/scholar?cites=84314681776183574&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FpmeoMGLKwEJ:scholar.google.com/&scioq=Variational+Inference+of+Disentangled+Latent+Concepts+from+Unlabeled+Observations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00848"}, "Intriguing Properties of Adversarial Examples": {"container_type": "Publication", "bib": {"title": "Intriguing properties of adversarial examples", "author": ["ED Cubuk", "B Zoph", "SS Schoenholz", "QV Le"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02846", "author_id": ["Mu_8iOEAAAAJ", "H-BnRI0AAAAJ", "mk-zQBsAAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:MQb-80Er4PwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntriguing%2BProperties%2Bof%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MQb-80Er4PwJ&ei=yBVkYv4aopTL1g_8k42IBg&json=", "num_citations": 65, "citedby_url": "/scholar?cites=18221611654607406641&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MQb-80Er4PwJ:scholar.google.com/&scioq=Intriguing+Properties+of+Adversarial+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02846"}, "DCN+: Mixed Objective And Deep Residual Coattention for Question Answering": {"container_type": "Publication", "bib": {"title": "Dcn+: Mixed objective and deep residual coattention for question answering", "author": ["C Xiong", "V Zhong", "R Socher"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00106", "abstract": "Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00106", "author_id": ["vaSdahkAAAAJ", "lT3YoNkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:7660lKmfBk0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDCN%252B:%2BMixed%2BObjective%2BAnd%2BDeep%2BResidual%2BCoattention%2Bfor%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7660lKmfBk0J&ei=yxVkYqSVEsLZmQHnraWYCA&json=", "num_citations": 93, "citedby_url": "/scholar?cites=5550299141473873647&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7660lKmfBk0J:scholar.google.com/&scioq=DCN%2B:+Mixed+Objective+And+Deep+Residual+Coattention+for+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00106"}, "Simple and efficient architecture search for Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Simple and efficient architecture search for convolutional neural networks", "author": ["T Elsken", "JH Metzen", "F Hutter"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.04528", "abstract": "Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.04528", "author_id": ["tzDC8FQAAAAJ", "w047VfEAAAAJ", "YUrxwrkAAAAJ"], "url_scholarbib": "/scholar?q=info:XFldqiHokZQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimple%2Band%2Befficient%2Barchitecture%2Bsearch%2Bfor%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XFldqiHokZQJ&ei=zhVkYuGnD-iSy9YPp-OyiAE&json=", "num_citations": 189, "citedby_url": "/scholar?cites=10705593020526188892&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XFldqiHokZQJ:scholar.google.com/&scioq=Simple+and+efficient+architecture+search+for+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.04528"}, "To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression": {"container_type": "Publication", "bib": {"title": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "author": ["M Zhu", "S Gupta"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.01878", "abstract": "-off in model size and accuracy. We investigate these two distinct paths for model compression   gradual pruning technique that is simple and straightforward to apply across a variety of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.01878", "author_id": ["", "cEb4k5IAAAAJ"], "url_scholarbib": "/scholar?q=info:W8zPbASshcYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTo%2BPrune,%2Bor%2BNot%2Bto%2BPrune:%2BExploring%2Bthe%2BEfficacy%2Bof%2BPruning%2Bfor%2BModel%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W8zPbASshcYJ&ei=1hVkYpjGIoOEmgHx-5DADA&json=", "num_citations": 651, "citedby_url": "/scholar?cites=14305028926417652827&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W8zPbASshcYJ:scholar.google.com/&scioq=To+Prune,+or+Not+to+Prune:+Exploring+the+Efficacy+of+Pruning+for+Model+Compression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.01878.pdf?source=post_page---------------------------"}, "A Deep Reinforced Model for Abstractive Summarization": {"container_type": "Publication", "bib": {"title": "A deep reinforced model for abstractive summarization", "author": ["R Paulus", "C Xiong", "R Socher"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.04304", "abstract": "We present a new abstractive summarization model that achieves state-of-the-art results on   To our knowledge, this is the first end-to-end model for abstractive summarization on the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.04304", "author_id": ["vn9sC6cAAAAJ", "vaSdahkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:Aig8ivHLFwYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDeep%2BReinforced%2BModel%2Bfor%2BAbstractive%2BSummarization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Aig8ivHLFwYJ&ei=2xVkYtDmBO-Sy9YPs_mY8AM&json=", "num_citations": 1216, "citedby_url": "/scholar?cites=439043726958667778&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Aig8ivHLFwYJ:scholar.google.com/&scioq=A+Deep+Reinforced+Model+for+Abstractive+Summarization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.04304.pdf?ref=hackernoon.com"}, "Sobolev GAN": {"container_type": "Publication", "bib": {"title": "Sobolev gan", "author": ["Y Mroueh", "CL Li", "T Sercu", "A Raj", "Y Cheng"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN  achieves  enforced on the critic by Sobolev GAN which relates to Laplacian regularization."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.04894", "author_id": ["6F90JHgAAAAJ", "vqHIt_sAAAAJ", "FMJePIUAAAAJ", "wpdabDgAAAAJ", "ORPxbV4AAAAJ"], "url_scholarbib": "/scholar?q=info:X30C69a2MuYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSobolev%2BGAN%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=X30C69a2MuYJ&ei=3hVkYpWqCvmQ6rQP5OqKqAo&json=", "num_citations": 99, "citedby_url": "/scholar?cites=16587521411741023583&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:X30C69a2MuYJ:scholar.google.com/&scioq=Sobolev+GAN&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.04894"}, "Compressing Word Embeddings via Deep Compositional Code Learning": {"container_type": "Publication", "bib": {"title": "Compressing word embeddings via deep compositional code learning", "author": ["R Shu", "H Nakayama"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.01068", "abstract": "\u2022 We propose to utilize the compositional coding approach for constructing the word em  compositional coding approach and analyze its merits for compressing word embeddings."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.01068", "author_id": ["qT2aZtsAAAAJ", "lZAYGJoAAAAJ"], "url_scholarbib": "/scholar?q=info:90AjQW4ZdWwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompressing%2BWord%2BEmbeddings%2Bvia%2BDeep%2BCompositional%2BCode%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=90AjQW4ZdWwJ&ei=4RVkYqq4OIySyASZk6HgCA&json=", "num_citations": 106, "citedby_url": "/scholar?cites=7815180689701290231&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:90AjQW4ZdWwJ:scholar.google.com/&scioq=Compressing+Word+Embeddings+via+Deep+Compositional+Code+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01068"}, "On the State of the Art of Evaluation in Neural Language Models": {"container_type": "Publication", "bib": {"title": "On the state of the art of evaluation in neural language models", "author": ["G Melis", "C Dyer", "P Blunsom"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.05589", "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.05589", "author_id": ["TbLw2lcAAAAJ", "W2DsnAkAAAAJ", "eJwbbXEAAAAJ"], "url_scholarbib": "/scholar?q=info:bsuEFbubAJIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BState%2Bof%2Bthe%2BArt%2Bof%2BEvaluation%2Bin%2BNeural%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bsuEFbubAJIJ&ei=5RVkYvibHpqSy9YP8pKNsAE&json=", "num_citations": 449, "citedby_url": "/scholar?cites=10520579957359692654&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bsuEFbubAJIJ:scholar.google.com/&scioq=On+the+State+of+the+Art+of+Evaluation+in+Neural+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.05589.pdf?ref=https://githubhelp.com"}, "Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Minimal-entropy correlation alignment for unsupervised deep domain adaptation", "author": ["P Morerio", "J Cavazza", "V Murino"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.10288", "abstract": "In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.10288", "author_id": ["lPV9rbkAAAAJ", "qJGSDAoAAAAJ", "yV3_PTkAAAAJ"], "url_scholarbib": "/scholar?q=info:hpM6mp0mp3YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimal-Entropy%2BCorrelation%2BAlignment%2Bfor%2BUnsupervised%2BDeep%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hpM6mp0mp3YJ&ei=7RVkYpabJ42ymgHg1rfQDQ&json=", "num_citations": 95, "citedby_url": "/scholar?cites=8549844875925427078&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hpM6mp0mp3YJ:scholar.google.com/&scioq=Minimal-Entropy+Correlation+Alignment+for+Unsupervised+Deep+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.10288"}, "Modular Continual Learning in a Unified Visual Environment": {"container_type": "Publication", "bib": {"title": "Modular continual learning in a unified visual environment", "author": ["KT Feigelis", "B Sheffer", "DLK Yamins"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.07425", "abstract": "A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.07425", "author_id": ["ywxy-ykAAAAJ", "TK4RicoAAAAJ", "qwC3vG8AAAAJ"], "url_scholarbib": "/scholar?q=info:_2l7R9pdXfsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModular%2BContinual%2BLearning%2Bin%2Ba%2BUnified%2BVisual%2BEnvironment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_2l7R9pdXfsJ&ei=8BVkYvPQHJyO6rQP_qe3mAs&json=", "num_citations": 1, "citedby_url": "/scholar?cites=18112736468437527039&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_2l7R9pdXfsJ:scholar.google.com/&scioq=Modular+Continual+Learning+in+a+Unified+Visual+Environment&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.07425"}, "N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "N2n learning: Network to network compression via policy gradient reinforcement learning", "author": ["A Ashok", "N Rhinehart", "F Beainy", "KM Kitani"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.06030", "author_id": ["wpci5noAAAAJ", "xUGZX_MAAAAJ", "mElEplEAAAAJ", "yv3sH74AAAAJ"], "url_scholarbib": "/scholar?q=info:uYmifHGjGAwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DN2N%2Blearning:%2BNetwork%2Bto%2BNetwork%2BCompression%2Bvia%2BPolicy%2BGradient%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uYmifHGjGAwJ&ei=8xVkYurSNoyuyAT-mrWwCA&json=", "num_citations": 133, "citedby_url": "/scholar?cites=871626235713849785&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uYmifHGjGAwJ:scholar.google.com/&scioq=N2N+learning:+Network+to+Network+Compression+via+Policy+Gradient+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.06030"}, "A Flexible Approach to Automated RNN Architecture Generation": {"container_type": "Publication", "bib": {"title": "A flexible approach to automated rnn architecture generation", "author": ["M Schrimpf", "S Merity", "J Bradbury", "R Socher"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "The process of designing neural architectures requires expert knowledge and extensive trial and error. While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components. We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width. The DSL is flexible enough to define standard architectures such as the Gated Recurrent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.07316", "author_id": ["RiZ-RdwAAAAJ", "AolIi4QAAAAJ", "GprA5UsAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:sIBfxO14k3AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BFlexible%2BApproach%2Bto%2BAutomated%2BRNN%2BArchitecture%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sIBfxO14k3AJ&ei=9xVkYrmjNpGJmwGY-qmYDQ&json=", "num_citations": 15, "citedby_url": "/scholar?cites=8111960316421570736&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sIBfxO14k3AJ:scholar.google.com/&scioq=A+Flexible+Approach+to+Automated+RNN+Architecture+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.07316"}, "Boosting the Actor with Dual Critic": {"container_type": "Publication", "bib": {"title": "Boosting the actor with dual critic", "author": ["B Dai", "A Shaw", "N He", "L Li", "L Song"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.10282", "abstract": "This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic. Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.10282", "author_id": ["TIKl_foAAAAJ", "Q9CPYwEAAAAJ", "iNcA81MAAAAJ", "Rqy5KDEAAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:I016i7AnfWgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosting%2Bthe%2BActor%2Bwith%2BDual%2BCritic%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=I016i7AnfWgJ&ei=_BVkYuPsBIySyASZk6HgCA&json=", "num_citations": 30, "citedby_url": "/scholar?cites=7529217791241112867&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:I016i7AnfWgJ:scholar.google.com/&scioq=Boosting+the+Actor+with+Dual+Critic&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.10282"}, "Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning": {"container_type": "Publication", "bib": {"title": "Routing networks: Adaptive selection of non-linear functions for multi-task learning", "author": ["C Rosenbaum", "T Klinger", "M Riemer"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.01239", "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network-for example a fully-connected or a convolutional layer"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.01239", "author_id": ["pP7H0fkAAAAJ", "dd8awr4AAAAJ", "PK7UzAwAAAAJ"], "url_scholarbib": "/scholar?q=info:amt2W747v9gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRouting%2BNetworks:%2BAdaptive%2BSelection%2Bof%2BNon-Linear%2BFunctions%2Bfor%2BMulti-Task%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=amt2W747v9gJ&ei=_xVkYrCoFYOEmgHx-5DADA&json=", "num_citations": 155, "citedby_url": "/scholar?cites=15618267721508481898&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:amt2W747v9gJ:scholar.google.com/&scioq=Routing+Networks:+Adaptive+Selection+of+Non-Linear+Functions+for+Multi-Task+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01239"}, "Covariant Compositional Networks For Learning Graphs": {"container_type": "Publication", "bib": {"title": "Covariant compositional networks for learning graphs", "author": ["R Kondor", "HT Son", "H Pan", "B Anderson"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "up the representation of the graph from the representations of a hierarchy of subgraphs. To  address the covariance issue, we study the covariance behavior of such networks in general,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.02144", "author_id": ["v12-jLUAAAAJ", "xEXxSN4AAAAJ", "9mmcE30AAAAJ", "QT0s-gkAAAAJ"], "url_scholarbib": "/scholar?q=info:c98qLOrMVDsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCovariant%2BCompositional%2BNetworks%2BFor%2BLearning%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c98qLOrMVDsJ&ei=AhZkYuKkLouKmgGY1YjABQ&json=", "num_citations": 65, "citedby_url": "/scholar?cites=4275267252416864115&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c98qLOrMVDsJ:scholar.google.com/&scioq=Covariant+Compositional+Networks+For+Learning+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.02144"}, "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge": {"container_type": "Publication", "bib": {"title": "Deep learning for physical processes: Incorporating prior scientific knowledge", "author": ["E De B\u00e9zenac", "A Pajot", "P Gallinari"], "pub_year": "2019", "venue": "Journal of Statistical \u2026", "abstract": "We consider the use of deep learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example"}, "filled": false, "gsrank": 1, "pub_url": "https://iopscience.iop.org/article/10.1088/1742-5468/ab3195/meta", "author_id": ["KvZw5gYAAAAJ", "9ff0JKYAAAAJ", "rFaxB20AAAAJ"], "url_scholarbib": "/scholar?q=info:fGtq8aFmtAQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bfor%2BPhysical%2BProcesses:%2BIncorporating%2BPrior%2BScientific%2BKnowledge%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fGtq8aFmtAQJ&ei=BhZkYtqIJIuKmgGY1YjABQ&json=", "num_citations": 180, "citedby_url": "/scholar?cites=339008717685681020&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fGtq8aFmtAQJ:scholar.google.com/&scioq=Deep+Learning+for+Physical+Processes:+Incorporating+Prior+Scientific+Knowledge&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.07970"}, "Spatially Transformed Adversarial Examples": {"container_type": "Publication", "bib": {"title": "Spatially transformed adversarial examples", "author": ["C Xiao", "JY Zhu", "B Li", "W He", "M Liu", "D Song"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "to generate adversarial examples based on spatial transformation instead of direct manipulation  of the pixel values, and we show realistic and effective adversarial examples on MNIST,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.02612", "author_id": ["Juoqtj8AAAAJ", "UdpacsMAAAAJ", "K8vJkTcAAAAJ", "", "WiIM-MgAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:pGYFRueHWb4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpatially%2BTransformed%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pGYFRueHWb4J&ei=ChZkYsr-KsLZmQHnraWYCA&json=", "num_citations": 341, "citedby_url": "/scholar?cites=13716143567515510436&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pGYFRueHWb4J:scholar.google.com/&scioq=Spatially+Transformed+Adversarial+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.02612"}, "Adversarial Spheres": {"container_type": "Publication", "bib": {"title": "Adversarial spheres", "author": ["J Gilmer", "L Metz", "F Faghri", "SS Schoenholz"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "back on the sphere by normalizing ||x||2. To find nearby adversarial examples we terminate  PGD  for all 1000 steps, potentially walking very far along the sphere from the starting point x."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.02774", "author_id": ["Ml_vQ8MAAAAJ", "jCOmCb4AAAAJ", "KUG_tG0AAAAJ", "mk-zQBsAAAAJ"], "url_scholarbib": "/scholar?q=info:javTQALJeOQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BSpheres%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=javTQALJeOQJ&ei=DhZkYozHBZyO6rQP_qe3mAs&json=", "num_citations": 268, "citedby_url": "/scholar?cites=16463129449367579533&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:javTQALJeOQJ:scholar.google.com/&scioq=Adversarial+Spheres&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.02774"}, "Unsupervised Cipher Cracking Using Discrete GANs": {"container_type": "Publication", "bib": {"title": "Unsupervised cipher cracking using discrete gans", "author": ["AN Gomez", "S Huang", "I Zhang", "BM Li", "M Osama"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This work details CipherGAN, an architecture inspired by CycleGAN used for inferring the underlying cipher mapping given banks of unpaired ciphertext and plaintext. We demonstrate that CipherGAN is capable of cracking language data enciphered using shift and Vigenere ciphers to a high degree of fidelity and for vocabularies much larger than previously achieved. We present how CycleGAN can be made compatible with discrete data and train in a stable way. We then prove that the technique used in CipherGAN avoids the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.04883", "author_id": ["2oq9614AAAAJ", "Qp2pme4AAAAJ", "qtPcPs4AAAAJ", "QQrzFdAAAAAJ", "Un736ecAAAAJ"], "url_scholarbib": "/scholar?q=info:mUSMEDH-hSoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BCipher%2BCracking%2BUsing%2BDiscrete%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mUSMEDH-hSoJ&ei=ERZkYo7cKu-Sy9YPs_mY8AM&json=", "num_citations": 55, "citedby_url": "/scholar?cites=3064134608179971225&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mUSMEDH-hSoJ:scholar.google.com/&scioq=Unsupervised+Cipher+Cracking+Using+Discrete+GANs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.04883"}, "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks": {"container_type": "Publication", "bib": {"title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks", "author": ["P Chaudhari", "S Soatto"], "pub_year": "2018", "venue": "2018 Information Theory and \u2026", "abstract": "Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8503224/", "author_id": ["c_z5hWEAAAAJ", "lH1PdF8AAAAJ"], "url_scholarbib": "/scholar?q=info:qwMfxZE-zhsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2Bgradient%2Bdescent%2Bperforms%2Bvariational%2Binference,%2Bconverges%2Bto%2Blimit%2Bcycles%2Bfor%2Bdeep%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qwMfxZE-zhsJ&ei=FBZkYtutCY2ymgHg1rfQDQ&json=", "num_citations": 211, "citedby_url": "/scholar?cites=2003607680024773547&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qwMfxZE-zhsJ:scholar.google.com/&scioq=Stochastic+gradient+descent+performs+variational+inference,+converges+to+limit+cycles+for+deep+networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11029"}, "Hyperparameter optimization: a spectral approach": {"container_type": "Publication", "bib": {"title": "Hyperparameter optimization: A spectral approach", "author": ["E Hazan", "A Klivans", "Y Yuan"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.00764", "abstract": "We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm---an iterative application of compressed sensing techniques for orthogonal polynomials---requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep neural networks on Cifar-10 show that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.00764", "author_id": ["LnhCGNMAAAAJ", "UD87zMYAAAAJ", "7o4wtKEAAAAJ"], "url_scholarbib": "/scholar?q=info:JCFbfyG175sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHyperparameter%2Boptimization:%2Ba%2Bspectral%2Bapproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JCFbfyG175sJ&ei=FxZkYprRDpLeyQTE46-QAg&json=", "num_citations": 86, "citedby_url": "/scholar?cites=11236398750787903780&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JCFbfyG175sJ:scholar.google.com/&scioq=Hyperparameter+optimization:+a+spectral+approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.00764"}, "Countering Adversarial Images using Input Transformations": {"container_type": "Publication", "bib": {"title": "Countering adversarial images using input transformations", "author": ["C Guo", "M Rana", "M Cisse", "L Van Der Maaten"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "transforming the inputs before feeding them to the system. Specifically, we study applying  image transformations  variance minimization, and image quilting before feeding the image to a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00117", "author_id": ["0gp5M-kAAAAJ", "EmPAC6YAAAAJ", "", "6GDfcqEAAAAJ"], "url_scholarbib": "/scholar?q=info:y5g9-Q_m2C4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCountering%2BAdversarial%2BImages%2Busing%2BInput%2BTransformations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y5g9-Q_m2C4J&ei=GhZkYr7UFZGJmwGY-qmYDQ&json=", "num_citations": 886, "citedby_url": "/scholar?cites=3375700876994648267&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y5g9-Q_m2C4J:scholar.google.com/&scioq=Countering+Adversarial+Images+using+Input+Transformations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00117"}, "Certified Defenses against Adversarial Examples ": {"container_type": "Publication", "bib": {"title": "Certified defenses against adversarial examples", "author": ["A Raghunathan", "J Steinhardt", "P Liang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1801.09344", "abstract": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.09344", "author_id": ["Ch9iRwQAAAAJ", "LKv32bgAAAAJ", "pouyVyUAAAAJ"], "url_scholarbib": "/scholar?q=info:cJFcm9dk8u0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCertified%2BDefenses%2Bagainst%2BAdversarial%2BExamples%2B%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cJFcm9dk8u0J&ei=HRZkYpHjBO-Sy9YPs_mY8AM&json=", "num_citations": 700, "citedby_url": "/scholar?cites=17145877608540180848&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cJFcm9dk8u0J:scholar.google.com/&scioq=Certified+Defenses+against+Adversarial+Examples+&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.09344"}, "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields": {"container_type": "Publication", "bib": {"title": "Coulomb gans: Provably optimal nash equilibria via potential fields", "author": ["T Unterthiner", "B Nessler", "C Seward"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field of charged particles, where generated samples are attracted to training set samples but repel"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.08819", "author_id": ["QCARd5gAAAAJ", "H_ICOW4AAAAJ", "4x2wF-EAAAAJ"], "url_scholarbib": "/scholar?q=info:SanAw7tTO80J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCoulomb%2BGANs:%2BProvably%2BOptimal%2BNash%2BEquilibria%2Bvia%2BPotential%2BFields%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SanAw7tTO80J&ei=HxZkYtO9HYySyASZk6HgCA&json=", "num_citations": 57, "citedby_url": "/scholar?cites=14788505867309328713&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SanAw7tTO80J:scholar.google.com/&scioq=Coulomb+GANs:+Provably+Optimal+Nash+Equilibria+via+Potential+Fields&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.08819"}, "Word translation without parallel data": {"container_type": "Publication", "bib": {"title": "Word translation without parallel data", "author": ["A Conneau", "G Lample", "MA Ranzato", "L Denoyer"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.04087", "author_id": ["45KfCpgAAAAJ", "H7sVDmIAAAAJ", "NbXF7T8AAAAJ", ""], "url_scholarbib": "/scholar?q=info:ELv-PTsxwZMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWord%2Btranslation%2Bwithout%2Bparallel%2Bdata%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ELv-PTsxwZMJ&ei=LRZkYpiGObKO6rQPy-CRsA8&json=", "num_citations": 1240, "citedby_url": "/scholar?cites=10646845124593498896&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ELv-PTsxwZMJ:scholar.google.com/&scioq=Word+translation+without+parallel+data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.04087"}, "Activation Maximization Generative Adversarial Nets": {"container_type": "Publication", "bib": {"title": "Activation maximization generative adversarial nets", "author": ["Z Zhou", "H Cai", "S Rong", "Y Song", "K Ren"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.02000", "author_id": ["b8YJ1EMAAAAJ", "x-AvvrYAAAAJ", "M4AhTM8AAAAJ", "xlnZ1OIAAAAJ", "USnQVWgAAAAJ"], "url_scholarbib": "/scholar?q=info:5LImCP_Al0cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DActivation%2BMaximization%2BGenerative%2BAdversarial%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5LImCP_Al0cJ&ei=MhZkYrqwFOiSy9YPp-OyiAE&json=", "num_citations": 65, "citedby_url": "/scholar?cites=5158804099762139876&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5LImCP_Al0cJ:scholar.google.com/&scioq=Activation+Maximization+Generative+Adversarial+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.02000"}, "Ensemble Adversarial Training: Attacks and Defenses": {"container_type": "Publication", "bib": {"title": "Ensemble adversarial training: Attacks and defenses", "author": ["F Tram\u00e8r", "A Kurakin", "N Papernot", "I Goodfellow"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.07204", "author_id": ["ijH0-a8AAAAJ", "nCh4qyMAAAAJ", "cGxq0cMAAAAJ", "iYN86KEAAAAJ"], "url_scholarbib": "/scholar?q=info:kI7wNzxR35EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnsemble%2BAdversarial%2BTraining:%2BAttacks%2Band%2BDefenses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kI7wNzxR35EJ&ei=QBZkYqDgOvmQ6rQP5OqKqAo&json=", "num_citations": 1866, "citedby_url": "/scholar?cites=10511209374384426640&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kI7wNzxR35EJ:scholar.google.com/&scioq=Ensemble+Adversarial+Training:+Attacks+and+Defenses&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07204.pdf)"}, "Generating Wikipedia by Summarizing Long Sequences": {"container_type": "Publication", "bib": {"title": "Generating wikipedia by summarizing long sequences", "author": ["PJ Liu", "M Saleh", "E Pot", "B Goodrich", "R Sepassi"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We show that generating English Wikipedia articles can be approached as a multi-document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.10198", "author_id": ["1EPxhywAAAAJ", "MmX7K38AAAAJ", "e1GAF6sAAAAJ", "mvojJ2MAAAAJ", "7vNmpO0AAAAJ"], "url_scholarbib": "/scholar?q=info:o3GSBtyykYMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BWikipedia%2Bby%2BSummarizing%2BLong%2BSequences%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o3GSBtyykYMJ&ei=QxZkYoW_M_mQ6rQP5OqKqAo&json=", "num_citations": 511, "citedby_url": "/scholar?cites=9480555348664414627&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o3GSBtyykYMJ:scholar.google.com/&scioq=Generating+Wikipedia+by+Summarizing+Long+Sequences&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.10198.pdf?ref=https://githubhelp.com"}, "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling": {"container_type": "Publication", "bib": {"title": "Fastgcn: fast learning with graph convolutional networks via importance sampling", "author": ["J Chen", "T Ma", "C Xiao"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1801.10247", "abstract": "The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.10247", "author_id": ["Z-lkme8AAAAJ", "9OvNakkAAAAJ", "ahaV25EAAAAJ"], "url_scholarbib": "/scholar?q=info:kTo2vC_SjPoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFastGCN:%2BFast%2BLearning%2Bwith%2BGraph%2BConvolutional%2BNetworks%2Bvia%2BImportance%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kTo2vC_SjPoJ&ei=SBZkYuSdPPmQ6rQP5OqKqAo&json=", "num_citations": 844, "citedby_url": "/scholar?cites=18054036108684442257&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kTo2vC_SjPoJ:scholar.google.com/&scioq=FastGCN:+Fast+Learning+with+Graph+Convolutional+Networks+via+Importance+Sampling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.10247?ref=https://githubhelp.com"}, "Progressive Growing of GANs for Improved Quality, Stability, and Variation": {"container_type": "Publication", "bib": {"title": "Progressive growing of gans for improved quality, stability, and variation", "author": ["T Karras", "T Aila", "S Laine", "J Lehtinen"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.10196", "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, eg, CelebA images at 1024^ 2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10196", "author_id": ["-50qJW8AAAAJ", "e7abmgkAAAAJ", "UCXJOTUAAAAJ", "Vpr6s3sAAAAJ"], "url_scholarbib": "/scholar?q=info:4gukjGnRZp8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgressive%2BGrowing%2Bof%2BGANs%2Bfor%2BImproved%2BQuality,%2BStability,%2Band%2BVariation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4gukjGnRZp8J&ei=ZRVkYraWAYyuyAT-mrWwCA&json=", "num_citations": 4423, "citedby_url": "/scholar?cites=11486098150916361186&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4gukjGnRZp8J:scholar.google.com/&scioq=Progressive+Growing+of+GANs+for+Improved+Quality,+Stability,+and+Variation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10196.pdf%C2%A0"}, "Loss-aware Weight Quantization of Deep Networks": {"container_type": "Publication", "bib": {"title": "Loss-aware weight quantization of deep networks", "author": ["L Hou", "JT Kwok"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.08635", "abstract": "The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m> 2) quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08635", "author_id": ["rnjoL5cAAAAJ", "-oTraZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:0uYD3T8zS_QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLoss-aware%2BWeight%2BQuantization%2Bof%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0uYD3T8zS_QJ&ei=aBVkYtGuC86E6rQP5-KmKA&json=", "num_citations": 103, "citedby_url": "/scholar?cites=17603219917891692242&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0uYD3T8zS_QJ:scholar.google.com/&scioq=Loss-aware+Weight+Quantization+of+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08635"}, "Deep Neural Networks as Gaussian Processes": {"container_type": "Publication", "bib": {"title": "Deep neural networks as gaussian processes", "author": ["J Lee", "Y Bahri", "R Novak", "SS Schoenholz"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "It has long been known that a single-layer fully-connected neural network with an iid prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00165", "author_id": ["d3YhiooAAAAJ", "p2_vHmAAAAAJ", "syG6krEAAAAJ", "mk-zQBsAAAAJ"], "url_scholarbib": "/scholar?q=info:wKbKFvL2HF0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BNeural%2BNetworks%2Bas%2BGaussian%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wKbKFvL2HF0J&ei=bBVkYrKTNfmQ6rQP5OqKqAo&json=", "num_citations": 656, "citedby_url": "/scholar?cites=6709509064500094656&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wKbKFvL2HF0J:scholar.google.com/&scioq=Deep+Neural+Networks+as+Gaussian+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00165"}, "Stochastic Variational Video Prediction": {"container_type": "Publication", "bib": {"title": "Stochastic variational video prediction", "author": ["M Babaeizadeh", "C Finn", "D Erhan", "RH Campbell"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Figure 1: Importance of stochasticity in video prediction. In each video, a random shape follows   Our main contribution in this paper is a stochastic variational method for video prediction,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11252", "author_id": ["3Y4egcYAAAAJ", "vfPE6hgAAAAJ", "wfGiqXEAAAAJ", "2ftJYXMAAAAJ"], "url_scholarbib": "/scholar?q=info:MYhkX7Q4-OEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BVariational%2BVideo%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MYhkX7Q4-OEJ&ei=bxVkYu3uK4ySyASZk6HgCA&json=", "num_citations": 368, "citedby_url": "/scholar?cites=16282826800103721009&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MYhkX7Q4-OEJ:scholar.google.com/&scioq=Stochastic+Variational+Video+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11252"}, "mixup: Beyond Empirical Risk Minimization": {"container_type": "Publication", "bib": {"title": "mixup: Beyond empirical risk minimization", "author": ["H Zhang", "M Cisse", "YN Dauphin"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "By doing so, mixup regularizes the neural network to favor simple linear  that mixup improves  the generalization of state-of-the-art neural network architectures. We also find that mixup"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.09412", "author_id": ["6Wg-hF4AAAAJ", "", "XSforroAAAAJ"], "url_scholarbib": "/scholar?q=info:zv-iX1Ff1K8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3Dmixup:%2BBeyond%2BEmpirical%2BRisk%2BMinimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zv-iX1Ff1K8J&ei=dBVkYqbnEKKUy9YP_JONiAY&json=", "num_citations": 3493, "citedby_url": "/scholar?cites=12669856454801555406&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zv-iX1Ff1K8J:scholar.google.com/&scioq=mixup:+Beyond+Empirical+Risk+Minimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.09412.pdf%C2%A0"}, "cGANs with Projection Discriminator": {"container_type": "Publication", "bib": {"title": "cGANs with projection discriminator", "author": ["T Miyato", "M Koyama"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.05637", "abstract": ", projection based way to incorporate the conditional information into the discriminator of   and we achieved this with a single pair of a discriminator and a generator. We were also able to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.05637", "author_id": ["s2lG0X0AAAAJ", "oY1gA10AAAAJ"], "url_scholarbib": "/scholar?q=info:bgNCDkw6FCEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DcGANs%2Bwith%2BProjection%2BDiscriminator%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bgNCDkw6FCEJ&ei=dxVkYvibGJLeyQTE46-QAg&json=", "num_citations": 361, "citedby_url": "/scholar?cites=2383594201116967790&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bgNCDkw6FCEJ:scholar.google.com/&scioq=cGANs+with+Projection+Discriminator&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.05637"}, "Spectral Normalization for Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Spectral normalization for generative adversarial networks", "author": ["T Miyato", "T Kataoka", "M Koyama", "Y Yoshida"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.05957", "author_id": ["s2lG0X0AAAAJ", "", "oY1gA10AAAAJ", "EIXTG_UAAAAJ"], "url_scholarbib": "/scholar?q=info:gIrP3JE_gg0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpectral%2BNormalization%2Bfor%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gIrP3JE_gg0J&ei=fRVkYq3gJ--Sy9YPs_mY8AM&json=", "num_citations": 2959, "citedby_url": "/scholar?cites=973410365172845184&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gIrP3JE_gg0J:scholar.google.com/&scioq=Spectral+Normalization+for+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.05957"}, "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning", "author": ["R Das", "S Dhuliawala", "M Zaheer", "L Vilnis"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Knowledge bases (KB), both automatically and manually constructed, are often incomplete---many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities or evaluating the truth of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05851", "author_id": ["FKoKAwIAAAAJ", "7O33ij4AAAAJ", "A33FhJMAAAAJ", "xWrOthYAAAAJ"], "url_scholarbib": "/scholar?q=info:x2Fw3QHn5kIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGo%2Bfor%2Ba%2BWalk%2Band%2BArrive%2Bat%2Bthe%2BAnswer:%2BReasoning%2BOver%2BPaths%2Bin%2BKnowledge%2BBases%2Busing%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x2Fw3QHn5kIJ&ei=gBVkYtPLII2ymgHg1rfQDQ&json=", "num_citations": 294, "citedby_url": "/scholar?cites=4820794446342808007&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x2Fw3QHn5kIJ:scholar.google.com/&scioq=Go+for+a+Walk+and+Arrive+at+the+Answer:+Reasoning+Over+Paths+in+Knowledge+Bases+using+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05851?ref=https://githubhelp.com"}, "Towards Deep Learning Models Resistant to Adversarial Attacks": {"container_type": "Publication", "bib": {"title": "Towards deep learning models resistant to adversarial attacks", "author": ["A Madry", "A Makelov", "L Schmidt", "D Tsipras"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Hence we believe that the class of attacks relying on first-order information is, in some  of  deep learning. Put together, these two ideas chart the way towards machine learning models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.06083", "author_id": ["SupjsEUAAAAJ", "haO4sKoAAAAJ", "SWMKy70AAAAJ", "26eh1jAAAAAJ"], "url_scholarbib": "/scholar?q=info:4SLudL17lMQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BDeep%2BLearning%2BModels%2BResistant%2Bto%2BAdversarial%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4SLudL17lMQJ&ei=gxVkYvC0H46pywTd4KPADw&json=", "num_citations": 5386, "citedby_url": "/scholar?cites=14165082781627851489&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4SLudL17lMQJ:scholar.google.com/&scioq=Towards+Deep+Learning+Models+Resistant+to+Adversarial+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.06083.pdf%E4%B8%AD%E6%9C%89%E4%BD%93%E7%8E%B0%EF%BC%8C%E4%BB%A5%E5%90%8E%E8%AF%B4%E5%88%B0CW%E6%94%BB%E5%87%BB%E5%86%8D%E7%BB%86%E8%AF%B4%E3%80%82"}, "Parameter Space Noise for Exploration": {"container_type": "Publication", "bib": {"title": "Parameter space noise for exploration", "author": ["M Plappert", "R Houthooft", "P Dhariwal", "S Sidor"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.01905", "author_id": ["MHQv5YUAAAAJ", "HBztuGIAAAAJ", "0pOgVVAAAAAJ", "UhmcQ7gAAAAJ"], "url_scholarbib": "/scholar?q=info:krtQXP-YkkwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParameter%2BSpace%2BNoise%2Bfor%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=krtQXP-YkkwJ&ei=hhVkYqPDLoOEmgHx-5DADA&json=", "num_citations": 467, "citedby_url": "/scholar?cites=5517640716015156114&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:krtQXP-YkkwJ:scholar.google.com/&scioq=Parameter+Space+Noise+for+Exploration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.01905.pdf?ref=https://githubhelp.com"}, "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model": {"container_type": "Publication", "bib": {"title": "Breaking the softmax bottleneck: A high-rank RNN language model", "author": ["Z Yang", "Z Dai", "R Salakhutdinov", "WW Cohen"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.03953", "author_id": ["7qXxyJkAAAAJ", "", "ITZ1e7MAAAAJ", "8ys-38kAAAAJ"], "url_scholarbib": "/scholar?q=info:l5I-n2FtpdcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBreaking%2Bthe%2BSoftmax%2BBottleneck:%2BA%2BHigh-Rank%2BRNN%2BLanguage%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=l5I-n2FtpdcJ&ei=iRVkYv3hNM6E6rQP5-KmKA&json=", "num_citations": 300, "citedby_url": "/scholar?cites=15538946355362697879&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:l5I-n2FtpdcJ:scholar.google.com/&scioq=Breaking+the+Softmax+Bottleneck:+A+High-Rank+RNN+Language+Model&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.03953.pdf%C3%AF%C2%BC%E2%80%B0"}, "Distributed Distributional Deterministic Policy Gradients": {"container_type": "Publication", "bib": {"title": "Distributed distributional deterministic policy gradients", "author": ["G Barth-Maron", "MW Hoffman", "D Budden"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of $ N $-step returns and prioritized experience replay. Experimentally we examine the contribution of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.08617", "author_id": ["mr1r8tgAAAAJ", "n2osZaoAAAAJ", "Tom5l8EAAAAJ"], "url_scholarbib": "/scholar?q=info:6qCGYuPkd8EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BDistributional%2BDeterministic%2BPolicy%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6qCGYuPkd8EJ&ei=jBVkYrrIK--Sy9YPs_mY8AM&json=", "num_citations": 351, "citedby_url": "/scholar?cites=13940862836810359018&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6qCGYuPkd8EJ:scholar.google.com/&scioq=Distributed+Distributional+Deterministic+Policy+Gradients&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.08617.pdf?ref=https://githubhelp.com"}, "Spherical CNNs": {"container_type": "Publication", "bib": {"title": "Spherical cnns", "author": ["TS Cohen", "M Geiger", "J K\u00f6hler", "M Welling"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Furthermore, we demonstrate the utility of spherical CNNs for rotation  spherical CNNs are  much better at rotation invariant classification of Spherical MNIST images than planar CNNs"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.10130", "author_id": ["a3q4YxEAAAAJ", "Py69D_8AAAAJ", "WNlTdm0AAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:i-Y7S4P-R1gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpherical%2BCNNs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i-Y7S4P-R1gJ&ei=kBVkYqz2JYOEmgHx-5DADA&json=", "num_citations": 599, "citedby_url": "/scholar?cites=6361332838540502667&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i-Y7S4P-R1gJ:scholar.google.com/&scioq=Spherical+CNNs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.10130.pdf%20http://arxiv.org/abs/1801.10130"}, "Graph Attention Networks": {"container_type": "Publication", "bib": {"title": "Graph attention networks", "author": ["P Veli\u010dkovi\u0107", "G Cucurull", "A Casanova"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present graph attention networks (GATs), novel neural network architectures that  operate on graph-structured data, leveraging masked self-attentional layers to address the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10903", "author_id": ["kcTK_FAAAAAJ", "dEtv5r4AAAAJ", "iFhSTbAAAAAJ"], "url_scholarbib": "/scholar?q=info:uQm0ZqKg100J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BAttention%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uQm0ZqKg100J&ei=mxVkYvXwLcLZmQHnraWYCA&json=", "num_citations": 3180, "citedby_url": "/scholar?cites=5609128480281463225&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uQm0ZqKg100J:scholar.google.com/&scioq=Graph+Attention+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10903?ref=https://githubhelp.com"}, "Expressive power of recurrent neural networks": {"container_type": "Publication", "bib": {"title": "Expressive power of recurrent neural networks", "author": ["V Khrulkov", "A Novikov", "I Oseledets"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00811", "abstract": "Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks--namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition--has been proven to have exponentially higher expressive power than shallow networks. Ie a shallow network of exponential width is required to realize the same score function as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00811", "author_id": ["GS5HTlkAAAAJ", "jMUkLqwAAAAJ", "5kMqBQEAAAAJ"], "url_scholarbib": "/scholar?q=info:VxDCwxbG8jkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExpressive%2Bpower%2Bof%2Brecurrent%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VxDCwxbG8jkJ&ei=nxVkYpy1NfmQ6rQP5OqKqAo&json=", "num_citations": 90, "citedby_url": "/scholar?cites=4175617605601726551&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VxDCwxbG8jkJ:scholar.google.com/&scioq=Expressive+power+of+recurrent+neural+networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00811"}, "META LEARNING SHARED HIERARCHIES": {"container_type": "Publication", "bib": {"title": "Meta learning shared hierarchies", "author": ["K Frans", "J Ho", "X Chen", "P Abbeel"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We develop a metalearning approach for learning hierarchically structured policies, improving  sample efficiency on unseen tasks through the use of shared primitives---policies that are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.09767", "author_id": ["NQ2ZWBoAAAAJ", "iVLAQysAAAAJ", "5tVuggUAAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:CACq5rZnGnQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMETA%2BLEARNING%2BSHARED%2BHIERARCHIES%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CACq5rZnGnQJ&ei=pBVkYteFHuiSy9YPp-OyiAE&json=", "num_citations": 281, "citedby_url": "/scholar?cites=8366113293045727240&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CACq5rZnGnQJ:scholar.google.com/&scioq=META+LEARNING+SHARED+HIERARCHIES&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.09767.pdf%20http://arxiv.org/abs/1710.09767"}, "Mixed Precision Training": {"container_type": "Publication", "bib": {"title": "Mixed precision training", "author": ["P Micikevicius", "S Narang", "J Alben", "G Diamos"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "and framework optimizations for mixed precision and are a focus of future work ( Furthermore,  automating loss-scaling factor selection would further simplify training with mixed precision"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.03740", "author_id": ["", "CWOixywAAAAJ", "", "Qq70O4UAAAAJ"], "url_scholarbib": "/scholar?q=info:FkBa2nKTMvwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMixed%2BPrecision%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FkBa2nKTMvwJ&ei=pxVkYqi1FIOEmgHx-5DADA&json=", "num_citations": 791, "citedby_url": "/scholar?cites=18172749567892275222&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FkBa2nKTMvwJ:scholar.google.com/&scioq=Mixed+Precision+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.03740.pdf?ref=https://githubhelp.com"}, "Learning to Represent Programs with Graphs": {"container_type": "Publication", "bib": {"title": "Learning to represent programs with graphs", "author": ["M Allamanis", "M Brockschmidt", "M Khademi"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Program Graphs We represent program source code as graphs and use different edge   The backbone of a program graph is the program\u2019s abstract syntax tree (AST), consisting of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00740", "author_id": ["rYsjwZgAAAAJ", "pF27eLMAAAAJ", ""], "url_scholarbib": "/scholar?q=info:6bzK_xIVqIEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BRepresent%2BPrograms%2Bwith%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6bzK_xIVqIEJ&ei=6RVkYqniBJqSy9YP8pKNsAE&json=", "num_citations": 498, "citedby_url": "/scholar?cites=9342740598325165289&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6bzK_xIVqIEJ:scholar.google.com/&scioq=Learning+to+Represent+Programs+with+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00740"}, "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models": {"container_type": "Publication", "bib": {"title": "Defense-gan: Protecting classifiers against adversarial attacks using generative models", "author": ["P Samangouei", "M Kabkab", "R Chellappa"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.06605", "abstract": "In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.06605", "author_id": ["l1VpBmUAAAAJ", "3l-T7HgAAAAJ", "L60tuywAAAAJ"], "url_scholarbib": "/scholar?q=info:6OUePHrldjwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDefense-GAN:%2BProtecting%2BClassifiers%2BAgainst%2BAdversarial%2BAttacks%2BUsing%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6OUePHrldjwJ&ei=7BVkYun9F4uKmgGY1YjABQ&json=", "num_citations": 874, "citedby_url": "/scholar?cites=4356922002684962280&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6OUePHrldjwJ:scholar.google.com/&scioq=Defense-GAN:+Protecting+Classifiers+Against+Adversarial+Attacks+Using+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.06605"}, "Unsupervised Representation Learning by Predicting Image Rotations": {"container_type": "Publication", "bib": {"title": "Unsupervised representation learning by predicting image rotations", "author": ["S Gidaris", "P Singh", "N Komodakis"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.07728", "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, ie, learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.07728", "author_id": ["7atfg7EAAAAJ", "12RkAfQAAAAJ", "xCPoT4EAAAAJ"], "url_scholarbib": "/scholar?q=info:3KNMiZfN67AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BRepresentation%2BLearning%2Bby%2BPredicting%2BImage%2BRotations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3KNMiZfN67AJ&ei=7xVkYsGOEcLZmQHnraWYCA&json=", "num_citations": 1627, "citedby_url": "/scholar?cites=12748509220929577948&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3KNMiZfN67AJ:scholar.google.com/&scioq=Unsupervised+Representation+Learning+by+Predicting+Image+Rotations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.07728.pdf?ref=hackernoon.com"}, "On the Convergence of Adam and Beyond": {"container_type": "Publication", "bib": {"title": "On the convergence of adam and beyond", "author": ["SJ Reddi", "S Kale", "S Kumar"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.09237", "abstract": "To resolve this issue, we propose new variants of ADAM which rely on long-term memory of   requirements as the original ADAM algorithm. We provide a convergence analysis for the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.09237", "author_id": ["70lgwYwAAAAJ", "gZgQLkgAAAAJ", "08CNqrYAAAAJ"], "url_scholarbib": "/scholar?q=info:B0s07Z6wFWkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BConvergence%2Bof%2BAdam%2Band%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=B0s07Z6wFWkJ&ei=-BVkYr7NG6KUy9YP_JONiAY&json=", "num_citations": 1694, "citedby_url": "/scholar?cites=7572152545124305671&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:B0s07Z6wFWkJ:scholar.google.com/&scioq=On+the+Convergence+of+Adam+and+Beyond&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.09237"}, "WSNet: Learning Compact and Efficient Networks with Weight Sampling": {"container_type": "Publication", "bib": {"title": "WSNet: Learning Compact and Efficient Networks with Weight Sampling", "author": ["X Jin", "Y Yang", "N Xu", "J Yang", "J Feng", "S Yan"], "pub_year": "2018", "venue": "NA", "abstract": "We present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via\\emph {ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1I3M7Z0b", "author_id": ["OEZ816YAAAAJ", "36vh2hgAAAAJ", "dRDZBoEAAAAJ", "HWFvq_wAAAAJ", "Q8iay0gAAAAJ", "DNuiPHwAAAAJ"], "url_scholarbib": "/scholar?q=info:Z1he6JzoL0sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWSNet:%2BLearning%2BCompact%2Band%2BEfficient%2BNetworks%2Bwith%2BWeight%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z1he6JzoL0sJ&ei=-xVkYpzQH-HDywSSipaYAg&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:Z1he6JzoL0sJ:scholar.google.com/&scioq=WSNet:+Learning+Compact+and+Efficient+Networks+with+Weight+Sampling&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1I3M7Z0b"}, "Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping": {"container_type": "Publication", "bib": {"title": "Fast and accurate text classification: Skimming, rereading and early stopping", "author": ["K Yu", "Y Liu", "AG Schwing", "J Peng"], "pub_year": "2018", "venue": "NA", "abstract": "a technique for skimming, re-reading, early stopping and  (ii) to enable fast and accurate text  classification. Specifically,  next token to read by skimming/skipping or to stop to form a final"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1wlAd1vM", "author_id": ["EgajpPkAAAAJ", "", "3B2c31wAAAAJ", "H2JX-RQAAAAJ"], "url_scholarbib": "/scholar?q=info:9HIiA6CYCdAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFast%2Band%2BAccurate%2BText%2BClassification:%2BSkimming,%2BRereading%2Band%2BEarly%2BStopping%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9HIiA6CYCdAJ&ei=ABZkYobTCYuKmgGY1YjABQ&json=", "num_citations": 16, "citedby_url": "/scholar?cites=14990680647694185204&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9HIiA6CYCdAJ:scholar.google.com/&scioq=Fast+and+Accurate+Text+Classification:+Skimming,+Rereading+and+Early+Stopping&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1wlAd1vM"}, "Neuron as an Agent": {"container_type": "Publication", "bib": {"title": "Neuron as an agent", "author": ["S Ohsawa", "K Akuzawa", "T Matsushima", "G Bezerra"], "pub_year": "2018", "venue": "NA", "abstract": "Auction theory is introduced because inter-agent reward  -agent environment from OpenAI  Gym and a multi-agent environment  We regard the situation as communication in multi-agent"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkfEzz-0-", "author_id": ["TrFlIx4AAAAJ", "4xoDD6YAAAAJ", "Wyn6BtsAAAAJ", ""], "url_scholarbib": "/scholar?q=info:sTk519eSr-IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeuron%2Bas%2Ban%2BAgent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sTk519eSr-IJ&ei=AhZkYqeJO5qSy9YP8pKNsAE&json=", "num_citations": 6, "citedby_url": "/scholar?cites=16334435829222554033&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sTk519eSr-IJ:scholar.google.com/&scioq=Neuron+as+an+Agent&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkfEzz-0-"}, "Cross-View Training for Semi-Supervised Learning": {"container_type": "Publication", "bib": {"title": "Cross-view training for semi-supervised learning", "author": ["K Clark", "T Luong", "QV Le"], "pub_year": "2018", "venue": "NA", "abstract": "We present Cross-View Training (CVT), a simple but effective method for deep semi-supervised learning. On labeled examples, the model is trained with standard cross-entropy loss. On an unlabeled example, the model first performs inference (acting as a\" teacher\") to produce soft targets. The model then learns from these soft targets (acting as a``\" student\"). We deviate from prior work by adding multiple auxiliary student prediction layers to the model. The input to each student layer is a sub-network of the full model that has a restricted"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJubPWZRW", "author_id": ["", "", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:VBDtvRvNDJkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCross-View%2BTraining%2Bfor%2BSemi-Supervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VBDtvRvNDJkJ&ei=BRZkYsyRHOHDywSSipaYAg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=11028415106557743188&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VBDtvRvNDJkJ:scholar.google.com/&scioq=Cross-View+Training+for+Semi-Supervised+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJubPWZRW"}, "Learning to Infer": {"container_type": "Publication", "bib": {"title": "A theory of learning to infer.", "author": ["I Dasgupta", "E Schulz", "JB Tenenbaum"], "pub_year": "2020", "venue": "Psychological \u2026", "abstract": "Specifically, we propose that people learn to infer. Instead of a domain-general inference  algorithm that treats all queries equally, we postulate an approximate recognition model (Dayan"}, "filled": false, "gsrank": 1, "pub_url": "https://psycnet.apa.org/doiLanding?doi=10.1037/rev0000178", "author_id": ["eJt6cSIAAAAJ", "74Cj5GYAAAAJ", "rRJ9wTJMUB8C"], "url_scholarbib": "/scholar?q=info:y0KmrdIL44IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BInfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y0KmrdIL44IJ&ei=CBZkYoyHOuiSy9YPp-OyiAE&json=", "num_citations": 34, "citedby_url": "/scholar?cites=9431395044128342731&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y0KmrdIL44IJ:scholar.google.com/&scioq=Learning+to+Infer&hl=en&as_sdt=0,33", "eprint_url": "https://dspace.mit.edu/bitstream/handle/1721.1/138352/644534v2.full.pdf?sequence=2&isAllowed=y"}, "Multiple Source Domain Adaptation with Adversarial Learning": {"container_type": "Publication", "bib": {"title": "Multiple source domain adaptation with adversarial learning", "author": ["H Zhao", "S Zhang", "G Wu", "GJ Gordon"], "pub_year": "2018", "venue": "NA", "abstract": "While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. We propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryDNZZZAW", "author_id": ["x942ipYAAAAJ", "voqw10cAAAAJ", "", "8LcYFjEAAAAJ"], "url_scholarbib": "/scholar?q=info:GlPFrtiaJxcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultiple%2BSource%2BDomain%2BAdaptation%2Bwith%2BAdversarial%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GlPFrtiaJxcJ&ei=DBZkYuzDHpLeyQTE46-QAg&json=", "num_citations": 42, "citedby_url": "/scholar?cites=1668472442399839002&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GlPFrtiaJxcJ:scholar.google.com/&scioq=Multiple+Source+Domain+Adaptation+with+Adversarial+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryDNZZZAW"}, "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design": {"container_type": "Publication", "bib": {"title": "Exploring deep recurrent models with reinforcement learning for molecule design", "author": ["D Neil", "M Segler", "L Guasch", "M Ahmed", "D Plumbley"], "pub_year": "2018", "venue": "NA", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery. However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works. This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkcTe-bR-&source=post_page-----a942a1c523a3----------------------", "author_id": ["IwolXLMAAAAJ", "imsL94QAAAAJ", "", "zqsIDngAAAAJ", ""], "url_scholarbib": "/scholar?q=info:vzO9YYxGg0MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploring%2BDeep%2BRecurrent%2BModels%2Bwith%2BReinforcement%2BLearning%2Bfor%2BMolecule%2BDesign%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vzO9YYxGg0MJ&ei=DxZkYsKpJ5yO6rQP_qe3mAs&json=", "num_citations": 55, "citedby_url": "/scholar?cites=4864809591239422911&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vzO9YYxGg0MJ:scholar.google.com/&scioq=Exploring+Deep+Recurrent+Models+with+Reinforcement+Learning+for+Molecule+Design&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkcTe-bR-"}, "Faster Discovery of Neural Architectures by Searching for Paths in a Large Model": {"container_type": "Publication", "bib": {"title": "Faster discovery of neural architectures by searching for paths in a large model", "author": ["H Pham", "MY Guan", "B Zoph", "QV Le", "J Dean"], "pub_year": "2018", "venue": "NA", "abstract": "We propose Efficient Neural Architecture Search (ENAS), a faster and less expensive approach to automated model design than previous methods. In ENAS, a controller learns to discover neural network architectures by searching for an optimal path within a larger model. The controller is trained with policy gradient to select a path that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected path is trained to minimize the cross entropy loss. On the Penn Treebank dataset, ENAS can"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByQZjx-0-", "author_id": ["GpcGdRkAAAAJ", "yt9bJUcAAAAJ", "H-BnRI0AAAAJ", "vfT6-XIAAAAJ", "NMS69lQAAAAJ"], "url_scholarbib": "/scholar?q=info:jLKr966s-QMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFaster%2BDiscovery%2Bof%2BNeural%2BArchitectures%2Bby%2BSearching%2Bfor%2BPaths%2Bin%2Ba%2BLarge%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jLKr966s-QMJ&ei=ExZkYsKKE46pywTd4KPADw&json=", "num_citations": 15, "citedby_url": "/scholar?cites=286449918794248844&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jLKr966s-QMJ:scholar.google.com/&scioq=Faster+Discovery+of+Neural+Architectures+by+Searching+for+Paths+in+a+Large+Model&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByQZjx-0-"}, "GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets": {"container_type": "Publication", "bib": {"title": "GANITE: Estimation of individualized treatment effects using generative adversarial nets", "author": ["J Yoon", "J Jordon", "M Van Der Schaar"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Estimating individualized treatment effects (ITE) is a challenging task due to the need for an individual's potential outcomes to be learned from biased data and without having access to the counterfactuals. We propose a novel method for inferring ITE based on the Generative Adversarial Nets (GANs) framework. Our method, termed Generative Adversarial Nets for inference of Individualized Treatment Effects (GANITE), is motivated by the possibility that we can capture the uncertainty in the counterfactual distributions by attempting to learn them"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByKWUeWA-", "author_id": ["kiFd6A8AAAAJ", "jiyonF0AAAAJ", "DZ3S--MAAAAJ"], "url_scholarbib": "/scholar?q=info:_Rgum8RROQwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGANITE:%2BEstimation%2Bof%2BIndividualized%2BTreatment%2BEffects%2Busing%2BGenerative%2BAdversarial%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_Rgum8RROQwJ&ei=GBZkYuHYC42ymgHg1rfQDQ&json=", "num_citations": 181, "citedby_url": "/scholar?cites=880825106986572029&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_Rgum8RROQwJ:scholar.google.com/&scioq=GANITE:+Estimation+of+Individualized+Treatment+Effects+using+Generative+Adversarial+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByKWUeWA-"}, "LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES": {"container_type": "Publication", "bib": {"title": "Learning to Organize Knowledge with N-Gram Machines", "author": ["QA End-to-end"], "venue": "NA", "pub_year": "NA", "abstract": "\u25cf Episodic memory can store and retrieve events through autoassociation--completing memory from partial cues, which enables efficient storage & retrieval.\u25cf The recurrent structure in CA3 collateral system allows the generation process to be informed by external input to provide a fuller and more coherent version."}, "filled": false, "gsrank": 1, "pub_url": "https://scholar.archive.org/work/hdu4pyqg3vecnnkjsvl2qyo7he/access/wayback/https://noon99jaki.github.io/publication/2017-ngm-ciai-poster.pdf", "author_id": [""], "url_scholarbib": "/scholar?q=info:obXeVoIk-JYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLEARNING%2BTO%2BORGANIZE%2BKNOWLEDGE%2BWITH%2BN-GRAM%2BMACHINES%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=obXeVoIk-JYJ&ei=HBZkYt-mOuHDywSSipaYAg&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:obXeVoIk-JYJ:scholar.google.com/&scioq=LEARNING+TO+ORGANIZE+KNOWLEDGE+WITH+N-GRAM+MACHINES&hl=en&as_sdt=0,33"}, "Convolutional Sequence Modeling Revisited": {"container_type": "Publication", "bib": {"title": "Convolutional sequence modeling revisited", "author": ["S Bai", "JZ Kolter", "V Koltun"], "pub_year": "2018", "venue": "NA", "abstract": "is to revisit this folk wisdom, and thereby make a counterclaim. We argue that with the tools  of modern convolutional  ), in fact convolutional architectures typically outperform recurrent"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJEX-H1Pf", "author_id": ["DLVP3PcAAAAJ", "UXh1I6UAAAAJ", "kg4bCpgAAAAJ"], "url_scholarbib": "/scholar?q=info:KnlrJCmaGxsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConvolutional%2BSequence%2BModeling%2BRevisited%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KnlrJCmaGxsJ&ei=IhZkYvuNFu-Sy9YPs_mY8AM&json=", "num_citations": 40, "citedby_url": "/scholar?cites=1953324364890601770&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KnlrJCmaGxsJ:scholar.google.com/&scioq=Convolutional+Sequence+Modeling+Revisited&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJEX-H1Pf"}, "Towards Effective GANs for Data Distributions with Diverse Modes": {"container_type": "Publication", "bib": {"title": "Towards Effective GANs for Data Distributions with Diverse Modes", "author": ["S Agrawal", "G Singh", "M Khapra"], "pub_year": "2018", "venue": "NA", "abstract": "Generative Adversarial Networks (GANs), when trained on large datasets with diverse modes, are known to produce conflated images which do not distinctly belong to any of the modes. We hypothesize that this problem occurs due to the interaction between two facts:(1) For datasets with large variety, it is likely that the modes lie on separate manifolds.(2) The generator (G) is formulated as a continuous function, and the input noise is derived from a connected set, due to which G's output is a connected set. If G covers all modes, then there"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyDMX0l0Z", "author_id": ["", "", "DV8z8DYAAAAJ"], "url_scholarbib": "/scholar?q=info:kUvHnXT9fQwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BEffective%2BGANs%2Bfor%2BData%2BDistributions%2Bwith%2BDiverse%2BModes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kUvHnXT9fQwJ&ei=JBZkYvvRO46pywTd4KPADw&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:kUvHnXT9fQwJ:scholar.google.com/&scioq=Towards+Effective+GANs+for+Data+Distributions+with+Diverse+Modes&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyDMX0l0Z"}, "Natural Language Inference with External Knowledge": {"container_type": "Publication", "bib": {"title": "Natural language inference with external knowledge", "author": ["Q Chen", "X Zhu", "ZH Ling", "D Inkpen"], "pub_year": "2018", "venue": "NA", "abstract": "Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Sy3XxCx0Z", "author_id": ["8eosmSQAAAAJ", "a6MYnuUAAAAJ", "f8jRR3EAAAAJ", "66pwIBcAAAAJ"], "url_scholarbib": "/scholar?q=info:KQ94-wqDXOcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNatural%2BLanguage%2BInference%2Bwith%2BExternal%2BKnowledge%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KQ94-wqDXOcJ&ei=KRZkYuv8Do6pywTd4KPADw&json=", "num_citations": 34, "citedby_url": "/scholar?cites=16671344003810594601&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KQ94-wqDXOcJ:scholar.google.com/&scioq=Natural+Language+Inference+with+External+Knowledge&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Sy3XxCx0Z"}, "DropMax: Adaptive Stochastic Softmax": {"container_type": "Publication", "bib": {"title": "DropMax: Adaptive variational softmax", "author": ["HB Lee", "J Lee", "S Kim", "E Yang"], "pub_year": "2018", "venue": "Advances in Neural \u2026", "abstract": "We propose DropMax, a stochastic version of softmax classifier  This stochastic regularization  has an effect of building an  accuracy over the regular softmax classifier and other baselines"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.neurips.cc/paper/2018/hash/389bc7bb1e1c2a5e7e147703232a88f6-Abstract.html", "author_id": ["50_nxq0AAAAJ", "Py4URJUAAAAJ", "_ZfueMIAAAAJ", "UWO1mloAAAAJ"], "url_scholarbib": "/scholar?q=info:rYnoi8tr2FQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDropMax:%2BAdaptive%2BStochastic%2BSoftmax%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rYnoi8tr2FQJ&ei=LBZkYqbDD5LeyQTE46-QAg&json=", "num_citations": 12, "citedby_url": "/scholar?cites=6113755016125254061&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rYnoi8tr2FQJ:scholar.google.com/&scioq=DropMax:+Adaptive+Stochastic+Softmax&hl=en&as_sdt=0,33", "eprint_url": "https://proceedings.neurips.cc/paper/2018/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf"}, "Autoregressive Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Autoregressive generative adversarial networks", "author": ["Y Yazici", "KH Yap", "S Winkler"], "pub_year": "2018", "venue": "NA", "abstract": "Generative Adversarial Networks (GANs) learn a generative model by playing an adversarial game between a generator and an auxiliary discriminator, which classifies data samples vs.\\generated ones. However, it does not explicitly model feature co-occurrences in samples. In this paper, we propose a novel Autoregressive Generative Adversarial Network (ARGAN), that models the latent distribution of data using an autoregressive model, rather than relying on binary classification of samples into data/generated categories. In this way"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hyo9zDuIz", "author_id": ["J4bayAsAAAAJ", "nr86m98AAAAJ", "R8XE-aoAAAAJ"], "url_scholarbib": "/scholar?q=info:ClBkcSESJkIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutoregressive%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ClBkcSESJkIJ&ei=LxZkYquFDIySyASZk6HgCA&json=", "num_citations": 4, "citedby_url": "/scholar?cites=4766517190463868938&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ClBkcSESJkIJ:scholar.google.com/&scioq=Autoregressive+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hyo9zDuIz"}, "Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Deep sensing: Active sensing using multi-directional recurrent neural networks", "author": ["J Yoon", "WR Zame", "M Van Der Schaar"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "For every prediction we might wish to make, we must decide what to observe (what source of information) and when to observe it. Because making observations is costly, this decision must trade off the value of information against the cost of observation. Making observations (sensing) should be an active choice. To solve the problem of active sensing we develop a novel deep learning architecture: Deep Sensing. At training time, Deep Sensing learns how to issue predictions at various cost-performance points. To do this, it creates multiple"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1SnX5xCb", "author_id": ["kiFd6A8AAAAJ", "aHa6fz4AAAAJ", "DZ3S--MAAAAJ"], "url_scholarbib": "/scholar?q=info:0GsHjWq8JcgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BSensing:%2BActive%2BSensing%2Busing%2BMulti-directional%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0GsHjWq8JcgJ&ei=MhZkYu-ODI6pywTd4KPADw&json=", "num_citations": 24, "citedby_url": "/scholar?cites=14422140547542510544&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0GsHjWq8JcgJ:scholar.google.com/&scioq=Deep+Sensing:+Active+Sensing+using+Multi-directional+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1SnX5xCb"}, "THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS": {"container_type": "Publication", "bib": {"title": "The Effectiveness of a two-Layer Neural Network for Recommendations", "author": ["O Rybakov", "V Mohan", "A Misra", "S LeGrand", "R Joseph"], "pub_year": "2018", "venue": "NA", "abstract": "We present a personalized recommender system using neural network for recommending products, such as eBooks, audio-books, Mobile Apps, Video and Music. It produces recommendations based on customer's implicit feedback history such as purchases, listens or watches. Our key contribution is to formulate recommendation problem as a model that encodes historical behavior to predict the future behavior using soft data split, combining predictor and auto-encoder models. We introduce convolutional layer for learning the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1lMMx1CW", "author_id": ["3qb7uO4AAAAJ", "NjNdMJMAAAAJ", "8vBEW_UAAAAJ", "", "BRUWuoUAAAAJ"], "url_scholarbib": "/scholar?q=info:qyTDMs8OVfMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTHE%2BEFFECTIVENESS%2BOF%2BA%2BTWO-LAYER%2BNEURAL%2BNETWORK%2BFOR%2BRECOMMENDATIONS%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qyTDMs8OVfMJ&ei=NBZkYoq3Jo2ymgHg1rfQDQ&json=", "num_citations": 2, "citedby_url": "/scholar?cites=17533937007309563051&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qyTDMs8OVfMJ:scholar.google.com/&scioq=THE+EFFECTIVENESS+OF+A+TWO-LAYER+NEURAL+NETWORK+FOR+RECOMMENDATIONS&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1lMMx1CW"}, "A New Method of Region Embedding for Text Classification": {"container_type": "Publication", "bib": {"title": "A New Method of Region Embedding for Text Classification.", "author": ["C Qiao", "B Huang", "G Niu", "D Li", "D Dong", "W He"], "pub_year": "2018", "venue": "ICLR (Poster \u2026", "abstract": "To represent a text as a bag of properly identified \u201cphrases\u201d and use the representation for processing the text is proved to be useful. The key question here is how to identify the phrases and represent them. The traditional method of utilizing n-grams can be regarded as an approximation of the approach. Such a method can suffer from data sparsity, however, particularly when the length of n-gram is large. In this paper, we propose a new method of learning and utilizing task-specific distributed representations of n-grams, referred to as"}, "filled": false, "gsrank": 1, "pub_url": "http://research.baidu.com/Public/uploads/5acc1e230d179.pdf", "author_id": ["", "", "tCDJiCMAAAAJ", "", "qe_yK7oAAAAJ", "E9V6nSwAAAAJ"], "url_scholarbib": "/scholar?q=info:9Jh8eyrapUEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BNew%2BMethod%2Bof%2BRegion%2BEmbedding%2Bfor%2BText%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9Jh8eyrapUEJ&ei=OxZkYpylKvmQ6rQP5OqKqAo&json=", "num_citations": 45, "citedby_url": "/scholar?cites=4730426859617818868&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9Jh8eyrapUEJ:scholar.google.com/&scioq=A+New+Method+of+Region+Embedding+for+Text+Classification&hl=en&as_sdt=0,33", "eprint_url": "http://research.baidu.com/Public/uploads/5acc1e230d179.pdf"}, "Decoupling the Layers in Residual Networks": {"container_type": "Publication", "bib": {"title": "Decoupling the Layers in Residual Networks", "author": ["R Fok", "A An", "Z Rashidi", "X Wang"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "We propose a Warped Residual Network (WarpNet) using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. We apply a perturbation theory on residual networks and decouple the interactions between residual units. The resulting warp operator is a first order approximation of the output over multiple layers. The first order perturbation theory exhibits properties such as binomial path lengths and exponential gradient scaling found"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SyMvJrdaW", "author_id": ["brhvb7MAAAAJ", "2eUL7eEAAAAJ", "O4FkvZcAAAAJ", ""], "url_scholarbib": "/scholar?q=info:3dzFL1BkfcMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecoupling%2Bthe%2BLayers%2Bin%2BResidual%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3dzFL1BkfcMJ&ei=PxZkYtz5O5yO6rQP_qe3mAs&json=", "num_citations": 1, "citedby_url": "/scholar?cites=14086525505046437085&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3dzFL1BkfcMJ:scholar.google.com/&scioq=Decoupling+the+Layers+in+Residual+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SyMvJrdaW"}, "Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation": {"container_type": "Publication", "bib": {"title": "Overcoming catastrophic interference using conceptor-aided backpropagation", "author": ["X He", "H Jaeger"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm,\" Conceptor-Aided Backprop\"(CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1al7jg0b", "author_id": ["", "0uztVbMAAAAJ"], "url_scholarbib": "/scholar?q=info:DuQEiMX-lyoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOvercoming%2BCatastrophic%2BInterference%2Busing%2BConceptor-Aided%2BBackpropagation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DuQEiMX-lyoJ&ei=QxZkYr_ZBM6E6rQP5-KmKA&json=", "num_citations": 72, "citedby_url": "/scholar?cites=3069201795420316686&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DuQEiMX-lyoJ:scholar.google.com/&scioq=Overcoming+Catastrophic+Interference+using+Conceptor-Aided+Backpropagation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1al7jg0b"}, "Few-Shot Learning with Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Few-shot learning with graph neural networks", "author": ["V Garcia", "J Bruna"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.04043", "abstract": "(2016), but extends the inference mechanism using the graph neural network formalism  that  explored graph neural representations for few-shot, semi-supervised and active learning."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.04043", "author_id": ["FPRvtUEAAAAJ", "L4bNmsMAAAAJ"], "url_scholarbib": "/scholar?q=info:4__u-THIANYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFew-Shot%2BLearning%2Bwith%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4__u-THIANYJ&ei=RhZkYoAy-ZDqtA_k6oqoCg&json=", "num_citations": 831, "citedby_url": "/scholar?cites=15420545241088720867&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4__u-THIANYJ:scholar.google.com/&scioq=Few-Shot+Learning+with+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.04043"}, "Wavelet Pooling for Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Wavelet pooling for convolutional neural networks", "author": ["T Williams", "R Li"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Convolutional Neural Networks continuously advance the progress of 2D and 3D image and object classification. The steadfast usage of this algorithm requires constant evaluation and upgrading of foundational concepts to maintain progress. Network regularization techniques typically focus on convolutional layer operations, while leaving pooling layer operations without suitable options. We introduce Wavelet Pooling as another alternative to traditional neighborhood pooling. This method decomposes features into a second level"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkhlb8lCZ", "author_id": ["ptEDgiIAAAAJ", "Pxu06zgAAAAJ"], "url_scholarbib": "/scholar?q=info:rK5JuuWr9ccJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWavelet%2BPooling%2Bfor%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rK5JuuWr9ccJ&ei=SRZkYp7IAbKO6rQPy-CRsA8&json=", "num_citations": 102, "citedby_url": "/scholar?cites=14408611586003021484&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rK5JuuWr9ccJ:scholar.google.com/&scioq=Wavelet+Pooling+for+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkhlb8lCZ"}, "Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference": {"container_type": "Publication", "bib": {"title": "Debiasing evidence approximations: On importance-weighted autoencoders and jackknife variational inference", "author": ["S Nowozin"], "pub_year": "2018", "venue": "International conference on learning representations", "abstract": "The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $ K $ samples we"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyZoi-WRb", "author_id": ["7-B7aQkAAAAJ"], "url_scholarbib": "/scholar?q=info:GVeqK_-E3n0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDebiasing%2BEvidence%2BApproximations:%2BOn%2BImportance-weighted%2BAutoencoders%2Band%2BJackknife%2BVariational%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GVeqK_-E3n0J&ei=TBZkYpXYBvmQ6rQP5OqKqAo&json=", "num_citations": 32, "citedby_url": "/scholar?cites=9069832931054868249&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GVeqK_-E3n0J:scholar.google.com/&scioq=Debiasing+Evidence+Approximations:+On+Importance-weighted+Autoencoders+and+Jackknife+Variational+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyZoi-WRb"}, "Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection": {"container_type": "Publication", "bib": {"title": "Deep autoencoding gaussian mixture model for unsupervised anomaly detection", "author": ["B Zong", "Q Song", "MR Min", "W Cheng"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Unsupervised anomaly detection on multi-or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJJLHbb0-", "author_id": ["GZKO2WoAAAAJ", "HFGNmPsAAAAJ", "", "PRrGVmoAAAAJ"], "url_scholarbib": "/scholar?q=info:WILZEHHOFEkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BAutoencoding%2BGaussian%2BMixture%2BModel%2Bfor%2BUnsupervised%2BAnomaly%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WILZEHHOFEkJ&ei=TxZkYvC4Bo2ymgHg1rfQDQ&json=", "num_citations": 714, "citedby_url": "/scholar?cites=5266060849312268888&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WILZEHHOFEkJ:scholar.google.com/&scioq=Deep+Autoencoding+Gaussian+Mixture+Model+for+Unsupervised+Anomaly+Detection&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJJLHbb0-"}, "Boundary Seeking GANs": {"container_type": "Publication", "bib": {"title": "Boundary-seeking generative adversarial networks", "author": ["RD Hjelm", "AP Jacob", "T Che", "A Trischler", "K Cho"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Here, we introduce the boundary-seeking GAN as a method for training GANs with  discrete data. We first introduce a policy gradient based on the KL-divergence which uses the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.08431", "author_id": ["68c5HfwAAAAJ", "XT3E7RoAAAAJ", "", "EvUM6UUAAAAJ", "0RAmmIAAAAAJ"], "url_scholarbib": "/scholar?q=info:cxnYtLdr1mgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoundary%2BSeeking%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cxnYtLdr1mgJ&ei=UhZkYq7vDeiSy9YPp-OyiAE&json=", "num_citations": 168, "citedby_url": "/scholar?cites=7554343861717834099&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cxnYtLdr1mgJ:scholar.google.com/&scioq=Boundary+Seeking+GANs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.08431?ref=https://githubhelp.com"}, "Multi-Task Learning for Document Ranking and Query Suggestion": {"container_type": "Publication", "bib": {"title": "Multi-task learning for document ranking and query suggestion", "author": ["WU Ahmad", "KW Chang", "H Wang"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "We propose a multi-task learning framework to jointly learn document ranking and query suggestion for web search. It consists of two major components, a document ranker, and a query recommender. Document ranker combines current query and session information and compares the combined representation with document representation to rank the documents. Query recommender tracks users' query reformulation sequence considering all previous in-session queries using a sequence to sequence approach. As both tasks are"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJ1nzBeA-", "author_id": ["YCHJZOMAAAAJ", "fqDBtzYAAAAJ", "qkdvKNoAAAAJ"], "url_scholarbib": "/scholar?q=info:puNJnGHQLccJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-Task%2BLearning%2Bfor%2BDocument%2BRanking%2Band%2BQuery%2BSuggestion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=puNJnGHQLccJ&ei=VhZkYr3zAZyO6rQP_qe3mAs&json=", "num_citations": 42, "citedby_url": "/scholar?cites=14352356705152132006&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:puNJnGHQLccJ:scholar.google.com/&scioq=Multi-Task+Learning+for+Document+Ranking+and+Query+Suggestion&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJ1nzBeA-"}, "Kronecker-factored Curvature Approximations for Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Kronecker-factored curvature approximations for recurrent neural networks", "author": ["J Martens", "J Ba", "M Johnson"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Kronecker-factor Approximate Curvature (Martens & Grosse, 2015)(K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017). It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized. The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyMTkQZAb", "author_id": ["LlK_saMAAAAJ", "ymzxRhAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:n8IC3neKjJIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKronecker-factored%2BCurvature%2BApproximations%2Bfor%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n8IC3neKjJIJ&ei=WBZkYqKoMO-Sy9YPs_mY8AM&json=", "num_citations": 49, "citedby_url": "/scholar?cites=10559967473707434655&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n8IC3neKjJIJ:scholar.google.com/&scioq=Kronecker-factored+Curvature+Approximations+for+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyMTkQZAb"}, "Variational Network Quantization": {"container_type": "Publication", "bib": {"title": "Variational network quantization", "author": ["J Achterhold", "JM Koehler", "A Schmeink"], "pub_year": "2018", "venue": "International \u2026", "abstract": "these weights causes the additional loss in accuracy that we observe when quantizing the  whole network. Without gradient stopping (ie, applying gradients to a shadow weight at the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=ry-TW-WAb", "author_id": ["PqkeMMEAAAAJ", "pJLq66sAAAAJ", ""], "url_scholarbib": "/scholar?q=info:yqFOHTf4T2AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BNetwork%2BQuantization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yqFOHTf4T2AJ&ei=WxZkYrqyIOHDywSSipaYAg&json=", "num_citations": 71, "citedby_url": "/scholar?cites=6940038466399805898&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yqFOHTf4T2AJ:scholar.google.com/&scioq=Variational+Network+Quantization&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ry-TW-WAb"}, "A Framework for the Quantitative Evaluation of Disentangled Representations": {"container_type": "Publication", "bib": {"title": "A framework for the quantitative evaluation of disentangled representations", "author": ["C Eastwood", "CKI Williams"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=By-7dz-AZ", "author_id": ["p7VFLEIAAAAJ", "rvKJDbIAAAAJ"], "url_scholarbib": "/scholar?q=info:W0zhYVZCviwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BFramework%2Bfor%2Bthe%2BQuantitative%2BEvaluation%2Bof%2BDisentangled%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W0zhYVZCviwJ&ei=XxZkYuLOB_mQ6rQP5OqKqAo&json=", "num_citations": 230, "citedby_url": "/scholar?cites=3224087322020629595&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W0zhYVZCviwJ:scholar.google.com/&scioq=A+Framework+for+the+Quantitative+Evaluation+of+Disentangled+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=By-7dz-AZ"}, "Monotonic Chunkwise Attention": {"container_type": "Publication", "bib": {"title": "Monotonic chunkwise attention", "author": ["CC Chiu", "C Raffel"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.05382", "abstract": "improved performance compared to a baseline monotonic attention-based model.  propose  a novel attention mechanism which we call MoChA, for Monotonic Chunkwise Attention. The"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.05382", "author_id": ["8bNM5WgAAAAJ", "I66ZBYwAAAAJ"], "url_scholarbib": "/scholar?q=info:yPMlrH-yFF4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMonotonic%2BChunkwise%2BAttention%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yPMlrH-yFF4J&ei=ZBZkYofEGrKO6rQPy-CRsA8&json=", "num_citations": 190, "citedby_url": "/scholar?cites=6779239600518198216&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yPMlrH-yFF4J:scholar.google.com/&scioq=Monotonic+Chunkwise+Attention&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.05382.pdf?undefined"}, "Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings": {"container_type": "Publication", "bib": {"title": "Simulated+ unsupervised learning with adaptive data generation and bidirectional mappings", "author": ["K Lee", "H Kim", "C Suh"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al.(2017) propose Simulated+ Unsupervised (S+ U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. Bousmalis et al.(2017) propose a similar framework that jointly trains a translation mapping and a learning model. While these algorithms are shown to achieve the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkHDoG-Cb", "author_id": ["sCEl8r-n5VEC", "52xvH88AAAAJ", "B1guGw8AAAAJ"], "url_scholarbib": "/scholar?q=info:v7ysYa7SbvgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimulated%252BUnsupervised%2BLearning%2BWith%2BAdaptive%2BData%2BGeneration%2Band%2BBidirectional%2BMappings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v7ysYa7SbvgJ&ei=bBZkYq_QMJLeyQTE46-QAg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=17901477215249153215&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v7ysYa7SbvgJ:scholar.google.com/&scioq=Simulated%2BUnsupervised+Learning+With+Adaptive+Data+Generation+and+Bidirectional+Mappings&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkHDoG-Cb"}, "Action-dependent Control Variates for Policy Optimization via Stein Identity": {"container_type": "Publication", "bib": {"title": "Action-depedent Control Variates for Policy Optimization via Stein's Identity", "author": ["H Liu", "Y Feng", "Y Mao", "D Zhou", "J Peng", "Q Liu"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11198", "author_id": ["wtK4Yh4AAAAJ", "", "9jLCbIYAAAAJ", "UwLsYw8AAAAJ", "H2JX-RQAAAAJ", "XEx1fZkAAAAJ"], "url_scholarbib": "/scholar?q=info:PpEWSspPfu4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAction-dependent%2BControl%2BVariates%2Bfor%2BPolicy%2BOptimization%2Bvia%2BStein%2BIdentity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PpEWSspPfu4J&ei=fxZkYoDnN5LeyQTE46-QAg&json=", "num_citations": 64, "citedby_url": "/scholar?cites=17185260958337372478&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PpEWSspPfu4J:scholar.google.com/&scioq=Action-dependent+Control+Variates+for+Policy+Optimization+via+Stein+Identity&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11198"}, "Large Scale Optimal Transport and Mapping Estimation": {"container_type": "Publication", "bib": {"title": "Large-scale optimal transport and mapping estimation", "author": ["V Seguy", "BB Damodaran", "R Flamary", "N Courty"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a\\textit {Monge map} as a deep neural network learned by approximating the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02283", "author_id": ["zmOWQLsAAAAJ", "DarhRtEAAAAJ", "zDnwxFQAAAAJ", "ibEREjcAAAAJ"], "url_scholarbib": "/scholar?q=info:ghO3p2HbHwcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2BScale%2BOptimal%2BTransport%2Band%2BMapping%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ghO3p2HbHwcJ&ei=hBZkYtn3EpGJmwGY-qmYDQ&json=", "num_citations": 139, "citedby_url": "/scholar?cites=513370095015629698&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ghO3p2HbHwcJ:scholar.google.com/&scioq=Large+Scale+Optimal+Transport+and+Mapping+Estimation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02283.pdf?ref=https://codemonkey.link"}, "Adaptive Quantization of Neural Networks": {"container_type": "Publication", "bib": {"title": "Adaptive quantization of neural networks", "author": ["S Khoram", "J Li"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SyOK1Sg0W", "author_id": ["5dd8i_YAAAAJ", "QGYL3tAAAAAJ"], "url_scholarbib": "/scholar?q=info:AqkYCVPMRJIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BQuantization%2Bof%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AqkYCVPMRJIJ&ei=iBZkYpLAAY6pywTd4KPADw&json=", "num_citations": 35, "citedby_url": "/scholar?cites=10539773684960766210&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AqkYCVPMRJIJ:scholar.google.com/&scioq=Adaptive+Quantization+of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SyOK1Sg0W"}, "Learning Sparse Neural Networks through L_0 Regularization": {"container_type": "Publication", "bib": {"title": "Learning Sparse Neural Networks through  Regularization", "author": ["C Louizos", "M Welling", "DP Kingma"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.01312", "abstract": "We propose a practical method for $ L_0 $ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $ L_0 $ regularization. However, since the $ L_0 $ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.01312", "author_id": ["xrSUChoAAAAJ", "8200InoAAAAJ", "yyIoQu4AAAAJ"], "url_scholarbib": "/scholar?q=info:OiDtsOdGMOoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BSparse%2BNeural%2BNetworks%2Bthrough%2BL_0%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OiDtsOdGMOoJ&ei=jBZkYqf3OJGJmwGY-qmYDQ&json=", "num_citations": 653, "citedby_url": "/scholar?cites=16875065764676968506&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OiDtsOdGMOoJ:scholar.google.com/&scioq=Learning+Sparse+Neural+Networks+through+L_0+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.01312.pdf)"}, "A Scalable Laplace Approximation for Neural Networks": {"container_type": "Publication", "bib": {"title": "A scalable laplace approximation for neural networks", "author": ["H Ritter", "A Botev", "D Barber"], "pub_year": "2018", "venue": "6th International Conference on \u2026", "abstract": "We leverage recent insights from second-order optimisation for neural networks to construct a Kronecker factored Laplace approximation to the posterior over the weights of a trained network. Our approximation requires no modification of the training procedure, enabling practitioners to estimate the uncertainty of their models currently used in production without having to retrain them. We extensively compare our method to using Dropout and a diagonal Laplace approximation for estimating the uncertainty of a network. We demonstrate that our"}, "filled": false, "gsrank": 1, "pub_url": "https://discovery.ucl.ac.uk/id/eprint/10080902/", "author_id": ["K_4fqx4AAAAJ", "8k7RD8QAAAAJ", "dqJPZHEAAAAJ"], "url_scholarbib": "/scholar?q=info:cIr5q_r-lSoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BScalable%2BLaplace%2BApproximation%2Bfor%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cIr5q_r-lSoJ&ei=lhZkYtHBJ-HDywSSipaYAg&json=", "num_citations": 180, "citedby_url": "/scholar?cites=3068639073703398000&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cIr5q_r-lSoJ:scholar.google.com/&scioq=A+Scalable+Laplace+Approximation+for+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://discovery.ucl.ac.uk/id/eprint/10080902/1/kflaplace.pdf"}, "Deep Learning with Logged Bandit Feedback": {"container_type": "Publication", "bib": {"title": "Deep learning with logged bandit feedback", "author": ["T Joachims", "A Swaminathan"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "We propose a new output layer for deep neural networks that permits the use of logged contextual bandit feedback for training. Such contextual bandit feedback can be available in huge quantities (eg, logs of search engines, recommender systems) at little cost, opening up a path for training deep networks on orders of magnitude more data. To this effect, we propose a Counterfactual Risk Minimization (CRM) approach for training deep networks using an equivariant empirical risk estimator with variance regularization, BanditNet, and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJaP_-xAb&source=post_page---------------------------", "author_id": ["5tk1PV8AAAAJ", "WNHLjp0AAAAJ"], "url_scholarbib": "/scholar?q=info:4im1QAlmAXsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bwith%2BLogged%2BBandit%2BFeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4im1QAlmAXsJ&ei=mRZkYrTSMJyO6rQP_qe3mAs&json=", "num_citations": 86, "citedby_url": "/scholar?cites=8863477731568200162&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4im1QAlmAXsJ:scholar.google.com/&scioq=Deep+Learning+with+Logged+Bandit+Feedback&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJaP_-xAb"}, "AmbientGAN: Generative models from lossy measurements": {"container_type": "Publication", "bib": {"title": "AmbientGAN: Generative models from lossy measurements", "author": ["A Bora", "E Price", "AG Dimakis"], "pub_year": "2018", "venue": "International conference on learning \u2026", "abstract": "Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, current techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fully-observed samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hy7fDog0b", "author_id": ["1z50O0MAAAAJ", "UE6z_m8AAAAJ", "JSFmVQEAAAAJ"], "url_scholarbib": "/scholar?q=info:j-CHG7yjYOoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAmbientGAN:%2BGenerative%2Bmodels%2Bfrom%2Blossy%2Bmeasurements%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j-CHG7yjYOoJ&ei=nBZkYu7gIY2ymgHg1rfQDQ&json=", "num_citations": 127, "citedby_url": "/scholar?cites=16888678630950428815&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:j-CHG7yjYOoJ:scholar.google.com/&scioq=AmbientGAN:+Generative+models+from+lossy+measurements&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hy7fDog0b"}, "Dynamic Neural Program Embeddings for Program Repair": {"container_type": "Publication", "bib": {"title": "Dynamic neural program embedding for program repair", "author": ["K Wang", "R Singh", "Z Su"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.07163", "abstract": "Neural program embeddings have shown much promise recently for a variety of program analysis tasks, including program synthesis, program repair, fault localization, etc. However, most existing program embeddings are based on syntactic features of programs, such as raw token sequences or abstract syntax trees. Unlike images and text, a program has an unambiguous semantic meaning that can be difficult to capture by only considering its syntax (ie syntactically similar pro-grams can exhibit vastly different run-time behavior)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.07163", "author_id": ["KIe98hIAAAAJ", "5kVcNS4AAAAJ", "RivxoIcAAAAJ"], "url_scholarbib": "/scholar?q=info:_aCSEyfz1ZgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BNeural%2BProgram%2BEmbeddings%2Bfor%2BProgram%2BRepair%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_aCSEyfz1ZgJ&ei=nhZkYoDzLc6E6rQP5-KmKA&json=", "num_citations": 105, "citedby_url": "/scholar?cites=11012975812962066685&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_aCSEyfz1ZgJ:scholar.google.com/&scioq=Dynamic+Neural+Program+Embeddings+for+Program+Repair&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.07163"}, "Attacking Binarized Neural Networks": {"container_type": "Publication", "bib": {"title": "Attacking binarized neural networks", "author": ["A Galloway", "GW Taylor", "M Moussa"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00449", "abstract": ": improved robustness against some adversarial attacks, and in the worst case, performance   the impact of iterative attacks. We observe that non-scaled binary neural networks exhibit a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00449", "author_id": ["Vl-8eZgAAAAJ", "PUeKU8kAAAAJ", "x-2sUG4AAAAJ"], "url_scholarbib": "/scholar?q=info:x7sDHZh95UQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttacking%2BBinarized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x7sDHZh95UQJ&ei=kxVkYqeoJO-Sy9YPs_mY8AM&json=", "num_citations": 80, "citedby_url": "/scholar?cites=4964512256521124807&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x7sDHZh95UQJ:scholar.google.com/&scioq=Attacking+Binarized+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00449?ref=https://githubhelp.com"}, "Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach": {"container_type": "Publication", "bib": {"title": "Evaluating the robustness of neural networks: An extreme value theory approach", "author": ["TW Weng", "H Zhang", "PY Chen", "J Yi", "D Su", "Y Gao"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide a theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.10578", "author_id": ["v8GM4xoAAAAJ", "LTa3GzEAAAAJ", "jxwlCUUAAAAJ", "lZxRZ84AAAAJ", "tzWZax0AAAAJ", "8GBUF1kAAAAJ"], "url_scholarbib": "/scholar?q=info:Dp3gITf31hwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvaluating%2Bthe%2BRobustness%2Bof%2BNeural%2BNetworks:%2BAn%2BExtreme%2BValue%2BTheory%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Dp3gITf31hwJ&ei=ohVkYvW5NIySyASZk6HgCA&json=", "num_citations": 268, "citedby_url": "/scholar?cites=2078120094241692942&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Dp3gITf31hwJ:scholar.google.com/&scioq=Evaluating+the+Robustness+of+Neural+Networks:+An+Extreme+Value+Theory+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.10578"}, "Alternating Multi-bit Quantization for Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Alternating multi-bit quantization for recurrent neural networks", "author": ["C Xu", "J Yao", "Z Lin", "W Ou", "Y Cao", "Z Wang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+ 1}. We formulate the quantization as an optimization problem"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.00150", "author_id": ["JqAQMVEAAAAJ", "6wcXSI0AAAAJ", "TanjFwoAAAAJ", "oP_7RCcAAAAJ", "UWDd6TkAAAAJ", ""], "url_scholarbib": "/scholar?q=info:NnSQKM1yiQIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAlternating%2BMulti-bit%2BQuantization%2Bfor%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NnSQKM1yiQIJ&ei=rhVkYrKbILKO6rQPy-CRsA8&json=", "num_citations": 93, "citedby_url": "/scholar?cites=182803485359633462&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NnSQKM1yiQIJ:scholar.google.com/&scioq=Alternating+Multi-bit+Quantization+for+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.00150.pdf?ref=https://githubhelp.com"}, "Polar Transformer Networks": {"container_type": "Publication", "bib": {"title": "Polar transformer networks", "author": ["C Esteves", "C Allen-Blanchette", "X Zhou"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "a polar origin predictor, the newly introduced polar transformer  demonstrate through the  Cylindrical Transformer Network.  In this paper, we propose the Polar Transformer Network (PTN)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.01889", "author_id": ["7vkMB0cAAAAJ", "0BbFHcEAAAAJ", "E1vVpg4AAAAJ"], "url_scholarbib": "/scholar?q=info:RR-xALCKv9gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPolar%2BTransformer%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RR-xALCKv9gJ&ei=sRVkYob9KJGJmwGY-qmYDQ&json=", "num_citations": 119, "citedby_url": "/scholar?cites=15618354521274654533&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RR-xALCKv9gJ:scholar.google.com/&scioq=Polar+Transformer+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.01889"}, "Lifelong Learning with Dynamically Expandable Networks": {"container_type": "Publication", "bib": {"title": "Lifelong learning with dynamically expandable networks", "author": ["J Yoon", "E Yang", "J Lee", "SJ Hwang"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.01547", "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.01547", "author_id": ["-5comoUAAAAJ", "UWO1mloAAAAJ", "", "RP4Qx3QAAAAJ"], "url_scholarbib": "/scholar?q=info:4talc9D_lowJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLifelong%2BLearning%2Bwith%2BDynamically%2BExpandable%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4talc9D_lowJ&ei=tRVkYqqDD4OEmgHx-5DADA&json=", "num_citations": 557, "citedby_url": "/scholar?cites=10130565682575038178&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4talc9D_lowJ:scholar.google.com/&scioq=Lifelong+Learning+with+Dynamically+Expandable+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.01547"}, "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop": {"container_type": "Publication", "bib": {"title": "Voiceloop: Voice fitting and synthesis via a phonological loop", "author": ["Y Taigman", "L Wolf", "A Polyak", "E Nachmani"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a new neural text to speech (TTS) method that is able to transform text to speech in voices that are sampled in the wild. Unlike other systems, our solution is able to deal with unconstrained voice samples and without requiring aligned phonemes or linguistic features. The network architecture is simpler than those in the existing literature and is based on a novel shifting buffer working memory. The same buffer is used for estimating the attention, computing the output audio, and for updating the buffer itself. The input sentence is encoded"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.06588", "author_id": ["mbB3MRIAAAAJ", "UbFrXTsAAAAJ", "CP62OTMAAAAJ", "qGs1u4YAAAAJ"], "url_scholarbib": "/scholar?q=info:KYD7113-gcQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVoiceLoop:%2BVoice%2BFitting%2Band%2BSynthesis%2Bvia%2Ba%2BPhonological%2BLoop%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KYD7113-gcQJ&ei=wRVkYsz1K--Sy9YPs_mY8AM&json=", "num_citations": 134, "citedby_url": "/scholar?cites=14159878382438547497&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KYD7113-gcQJ:scholar.google.com/&scioq=VoiceLoop:+Voice+Fitting+and+Synthesis+via+a+Phonological+Loop&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.06588?ref=https://githubhelp.com"}, "Adaptive Memory Networks": {"container_type": "Publication", "bib": {"title": "Adaptive memory networks", "author": ["D Li", "A Kadav"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.00510", "abstract": "Memory networks, a class of deep networks with explicit addressable memory, have   exponentially with the size of memory, memory networks are comparably parameter efficient and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.00510", "author_id": ["", "IphEjqcAAAAJ"], "url_scholarbib": "/scholar?q=info:h4NXVPWJGKIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BMemory%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=h4NXVPWJGKIJ&ei=xRVkYq2KDoOEmgHx-5DADA&json=", "num_citations": 6, "citedby_url": "/scholar?cites=11680237320360395655&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:h4NXVPWJGKIJ:scholar.google.com/&scioq=Adaptive+Memory+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.00510"}, "Multi-level Residual Networks from Dynamical Systems View": {"container_type": "Publication", "bib": {"title": "Multi-level residual networks from dynamical systems view", "author": ["B Chang", "L Meng", "E Haber", "F Tung"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep residual networks (ResNets) and their variants are widely used in many computer vision applications and natural language processing tasks. However, the theoretical principles for designing and training ResNets are still not fully understood. Recently, several points of view have emerged to try to interpret ResNet theoretically, such as unraveled view, unrolled iterative estimation and dynamical systems view. In this paper, we adopt the dynamical systems point of view, and analyze the lesioning properties of ResNet both"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10348", "author_id": ["s9F-vo0AAAAJ", "JzNMKQoAAAAJ", "NZmEIS8AAAAJ", "T4EeZ9gAAAAJ"], "url_scholarbib": "/scholar?q=info:6ktHKwgfnboJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-level%2BResidual%2BNetworks%2Bfrom%2BDynamical%2BSystems%2BView%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6ktHKwgfnboJ&ei=zRVkYp2mGYOEmgHx-5DADA&json=", "num_citations": 127, "citedby_url": "/scholar?cites=13446938182344461290&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6ktHKwgfnboJ:scholar.google.com/&scioq=Multi-level+Residual+Networks+from+Dynamical+Systems+View&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10348.pdf?ref=https://githubhelp.com"}, "Learning Discrete Weights Using the Local Reparameterization Trick": {"container_type": "Publication", "bib": {"title": "Learning discrete weights using the local reparameterization trick", "author": ["O Shayer", "D Levi", "E Fetaya"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.07739", "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge. One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.07739", "author_id": ["VxUNLdoAAAAJ", "jIRmxLkAAAAJ", "zLuqh-0AAAAJ"], "url_scholarbib": "/scholar?q=info:-Kl7DEy96rwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDiscrete%2BWeights%2BUsing%2Bthe%2BLocal%2BReparameterization%2BTrick%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-Kl7DEy96rwJ&ei=1BVkYtzMGoyuyAT-mrWwCA&json=", "num_citations": 80, "citedby_url": "/scholar?cites=13612900958005340664&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-Kl7DEy96rwJ:scholar.google.com/&scioq=Learning+Discrete+Weights+Using+the+Local+Reparameterization+Trick&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.07739"}, "Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers": {"container_type": "Publication", "bib": {"title": "Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers", "author": ["J Ye", "X Lu", "Z Lin", "JZ Wang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.00124", "abstract": "Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions in resource-limited scenarios. A widely-used practice in relevant work assumes that a smaller-norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs) that does not critically rely on this assumption. Instead, it focuses on direct simplification of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.00124", "author_id": ["2iStecQAAAAJ", "mFC0wp8AAAAJ", "R0bnqaAAAAAJ", "inVzWAcAAAAJ"], "url_scholarbib": "/scholar?q=info:jjHqs8t8U_cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2Bthe%2BSmaller-Norm-Less-Informative%2BAssumption%2Bin%2BChannel%2BPruning%2Bof%2BConvolution%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jjHqs8t8U_cJ&ei=1xVkYqy3O4OEmgHx-5DADA&json=", "num_citations": 301, "citedby_url": "/scholar?cites=17821725364773859726&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jjHqs8t8U_cJ:scholar.google.com/&scioq=Rethinking+the+Smaller-Norm-Less-Informative+Assumption+in+Channel+Pruning+of+Convolution+Layers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.00124"}, "FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension": {"container_type": "Publication", "bib": {"title": "Fusionnet: Fusing via fully-aware attention with application to machine comprehension", "author": ["HY Huang", "C Zhu", "Y Shen", "W Chen"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.07341", "abstract": "This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of\" history of word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the\" history of word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.07341", "author_id": ["2y5YF-gAAAAJ", "1b2kKWoAAAAJ", "S6OFEFEAAAAJ", "LG_E-4EAAAAJ"], "url_scholarbib": "/scholar?q=info:HbpF8JIZ8ewJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFusionNet:%2BFusing%2Bvia%2BFully-aware%2BAttention%2Bwith%2BApplication%2Bto%2BMachine%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HbpF8JIZ8ewJ&ei=2xVkYvu-AeHDywSSipaYAg&json=", "num_citations": 166, "citedby_url": "/scholar?cites=17073455781225282077&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HbpF8JIZ8ewJ:scholar.google.com/&scioq=FusionNet:+Fusing+via+Fully-aware+Attention+with+Application+to+Machine+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.07341.pdf?source=post_page---------------------------"}, "Deep Rewiring: Training very sparse deep networks": {"container_type": "Publication", "bib": {"title": "Deep rewiring: Training very sparse deep networks", "author": ["G Bellec", "D Kappel", "W Maass", "R Legenstein"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05136", "author_id": ["fSXUVvAAAAAJ", "csoW51sAAAAJ", "2WpvdH0AAAAJ", "nyUoJk0AAAAJ"], "url_scholarbib": "/scholar?q=info:JQKOm698Cl8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BRewiring:%2BTraining%2Bvery%2Bsparse%2Bdeep%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JQKOm698Cl8J&ei=3RVkYrOLOJLeyQTE46-QAg&json=", "num_citations": 141, "citedby_url": "/scholar?cites=6848423277041156645&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JQKOm698Cl8J:scholar.google.com/&scioq=Deep+Rewiring:+Training+very+sparse+deep+networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05136"}, "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Skip rnn: Learning to skip state updates in recurrent neural networks", "author": ["V Campos", "B Jou", "X Gir\u00f3-i-Nieto", "J Torres"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.06834", "author_id": ["8fzVqSkAAAAJ", "k7eC8-0AAAAJ", "M3ZUEc8AAAAJ", "waODsw0AAAAJ"], "url_scholarbib": "/scholar?q=info:IDJzsy-5yj0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkip%2BRNN:%2BLearning%2Bto%2BSkip%2BState%2BUpdates%2Bin%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IDJzsy-5yj0J&ei=4BVkYrPsLeiSy9YPp-OyiAE&json=", "num_citations": 185, "citedby_url": "/scholar?cites=4452574796134429216&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IDJzsy-5yj0J:scholar.google.com/&scioq=Skip+RNN:+Learning+to+Skip+State+Updates+in+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.06834"}, "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training": {"container_type": "Publication", "bib": {"title": "Deep gradient compression: Reducing the communication bandwidth for distributed training", "author": ["Y Lin", "S Han", "H Mao", "Y Wang", "WJ Dally"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.01887", "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.01887", "author_id": ["Drc8L5EAAAAJ", "E0iCaa4AAAAJ", "r5WezOYAAAAJ", "j8JGVvoAAAAJ", "YZHj-Y4AAAAJ"], "url_scholarbib": "/scholar?q=info:BpKtUmbXfSIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BGradient%2BCompression:%2BReducing%2Bthe%2BCommunication%2BBandwidth%2Bfor%2BDistributed%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BpKtUmbXfSIJ&ei=4xVkYqGnO46pywTd4KPADw&json=", "num_citations": 843, "citedby_url": "/scholar?cites=2485379403852124678&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BpKtUmbXfSIJ:scholar.google.com/&scioq=Deep+Gradient+Compression:+Reducing+the+Communication+Bandwidth+for+Distributed+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.01887.pdf?ref=https://githubhelp.com"}, "Stabilizing Adversarial Nets with Prediction Methods": {"container_type": "Publication", "bib": {"title": "Stabilizing adversarial nets with prediction methods", "author": ["A Yadav", "S Shah", "Z Xu", "D Jacobs"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.07364", "author_id": ["pbRu-GQAAAAJ", "OwT_8sQAAAAJ", "TfWlMTYAAAAJ", "WH2KmRgAAAAJ"], "url_scholarbib": "/scholar?q=info:76ntgYQxHBIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStabilizing%2BAdversarial%2BNets%2Bwith%2BPrediction%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=76ntgYQxHBIJ&ei=5xVkYo6lGoOEmgHx-5DADA&json=", "num_citations": 81, "citedby_url": "/scholar?cites=1304972437215881711&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:76ntgYQxHBIJ:scholar.google.com/&scioq=Stabilizing+Adversarial+Nets+with+Prediction+Methods&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07364"}, "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization": {"container_type": "Publication", "bib": {"title": "Learning latent representations in neural networks for clustering through pseudo supervision and graph-based activity regularization", "author": ["O Kilinc", "I Uysal"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.03063", "abstract": "In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.03063", "author_id": ["XHqdGk8AAAAJ", "gGOMMsEAAAAJ"], "url_scholarbib": "/scholar?q=info:6lPKOKBy4J0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLatent%2BRepresentations%2Bin%2BNeural%2BNetworks%2Bfor%2BClustering%2Bthrough%2BPseudo%2BSupervision%2Band%2BGraph-based%2BActivity%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6lPKOKBy4J0J&ei=6xVkYuyxDIySyASZk6HgCA&json=", "num_citations": 36, "citedby_url": "/scholar?cites=11376218691210990570&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6lPKOKBy4J0J:scholar.google.com/&scioq=Learning+Latent+Representations+in+Neural+Networks+for+Clustering+through+Pseudo+Supervision+and+Graph-based+Activity+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.03063"}, "Learning Intrinsic Sparse Structures within Long Short-Term Memory": {"container_type": "Publication", "bib": {"title": "Learning intrinsic sparse structures within long short-term memory", "author": ["W Wen", "Y He", "S Rajbhandari", "M Zhang"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.05027", "author_id": ["JYD36ocAAAAJ", "SB3_eb0AAAAJ", "", "98vX7S8AAAAJ"], "url_scholarbib": "/scholar?q=info:xKvHKHdVvIMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BIntrinsic%2BSparse%2BStructures%2Bwithin%2BLong%2BShort-Term%2BMemory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xKvHKHdVvIMJ&ei=7xVkYsSpKZGJmwGY-qmYDQ&json=", "num_citations": 122, "citedby_url": "/scholar?cites=9492556084863806404&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xKvHKHdVvIMJ:scholar.google.com/&scioq=Learning+Intrinsic+Sparse+Structures+within+Long+Short-Term+Memory&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.05027.pdf?ref=https://githubhelp.com"}, "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering": {"container_type": "Publication", "bib": {"title": "Beyond shared hierarchies: Deep multitask learning through soft layer ordering", "author": ["E Meyerson", "R Miikkulainen"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00108", "abstract": "Existing deep multitask learning (MTL) approaches align layers shared between tasks in a parallel ordering. Such an organization significantly constricts the types of shared structure that can be learned. The necessity of parallel ordering for deep MTL is first tested by comparing it with permuted ordering of shared layers. The results indicate that a flexible ordering can enable more effective sharing, thus motivating the development of a soft ordering approach, which learns how shared layers are applied in different ways for different"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00108", "author_id": ["RT_LBQ4AAAAJ", "2SmbjHAAAAAJ"], "url_scholarbib": "/scholar?q=info:TqYzTf4WIHYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BShared%2BHierarchies:%2BDeep%2BMultitask%2BLearning%2Bthrough%2BSoft%2BLayer%2BOrdering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TqYzTf4WIHYJ&ei=8hVkYvKQKZLeyQTE46-QAg&json=", "num_citations": 98, "citedby_url": "/scholar?cites=8511828577202972238&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TqYzTf4WIHYJ:scholar.google.com/&scioq=Beyond+Shared+Hierarchies:+Deep+Multitask+Learning+through+Soft+Layer+Ordering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00108"}, "Training and Inference with Integers in Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Training and inference with integers in deep neural networks", "author": ["S Wu", "G Li", "F Chen", "L Shi"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.04680", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as\" WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.04680", "author_id": ["_MtBmxkAAAAJ", "qCfE--MAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:BiLcQ1q7JtMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2Band%2BInference%2Bwith%2BIntegers%2Bin%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BiLcQ1q7JtMJ&ei=_hVkYou5JPmQ6rQP5OqKqAo&json=", "num_citations": 320, "citedby_url": "/scholar?cites=15215054387477750278&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BiLcQ1q7JtMJ:scholar.google.com/&scioq=Training+and+Inference+with+Integers+in+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.04680.pdf?ref=https://githubhelp.com"}, "Training GANs with Optimism": {"container_type": "Publication", "bib": {"title": "Training gans with optimism", "author": ["C Daskalakis", "A Ilyas", "V Syrgkanis", "H Zeng"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Finally, we apply optimism to training GANs for images and introduce the Optimistic Adam  algo better performance than Adam, in terms of inception score, when trained on CIFAR10."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00141", "author_id": ["iTv2cOgAAAAJ", "Dtw3YBoAAAAJ", "G1WMpcUAAAAJ", "5z2rh_oAAAAJ"], "url_scholarbib": "/scholar?q=info:UVGtc8B6AwoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BGANs%2Bwith%2BOptimism%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UVGtc8B6AwoJ&ei=ARZkYtfAM6KUy9YP_JONiAY&json=", "num_citations": 320, "citedby_url": "/scholar?cites=721555332302459217&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UVGtc8B6AwoJ:scholar.google.com/&scioq=Training+GANs+with+Optimism&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00141"}, "Towards Reverse-Engineering Black-Box Neural Networks": {"container_type": "Publication", "bib": {"title": "Towards reverse-engineering black-box neural networks", "author": ["SJ Oh", "B Schiele", "M Fritz"], "pub_year": "2019", "venue": "Explainable AI: Interpreting, Explaining and \u2026", "abstract": "Much progress in interpretable AI is built around scenarios where the user, one who interprets the model, has a full ownership of the model to be diagnosed. The user either owns the training data and computing resources to train an interpretable model herself or owns a full access to an already trained model to be interpreted post-hoc. In this chapter, we consider a less investigated scenario of diagnosing black-box neural networks, where the user can only send queries and read off outputs. Black-box access is a common deployment"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-28954-6_7", "author_id": ["kmXOOdsAAAAJ", "z76PBfYAAAAJ", "4V1nNm4AAAAJ"], "url_scholarbib": "/scholar?q=info:cx3gj4N1FLUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BReverse-Engineering%2BBlack-Box%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cx3gj4N1FLUJ&ei=BBZkYrbNO--Sy9YPs_mY8AM&json=", "num_citations": 232, "citedby_url": "/scholar?cites=13048183228314164595&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cx3gj4N1FLUJ:scholar.google.com/&scioq=Towards+Reverse-Engineering+Black-Box+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01768"}, "Noisy Networks For Exploration": {"container_type": "Publication", "bib": {"title": "Noisy networks for exploration", "author": ["M Fortunato", "MG Azar", "B Piot", "J Menick"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "of the noise distributions for linear layers in a noisy network.  Gaussian noise, which uses  an independent Gaussian noise  Gaussian noise, which uses an independent noise per each"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.10295", "author_id": ["_fMHSIUAAAAJ", "AlTQrFcAAAAJ", "fqxNUREAAAAJ", "te_gpaEAAAAJ"], "url_scholarbib": "/scholar?q=info:G0A6s-4sIsEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNoisy%2BNetworks%2BFor%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G0A6s-4sIsEJ&ei=CBZkYoLFE5qSy9YP8pKNsAE&json=", "num_citations": 637, "citedby_url": "/scholar?cites=13916735202249031707&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:G0A6s-4sIsEJ:scholar.google.com/&scioq=Noisy+Networks+For+Exploration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.10295.pdf?source=post_page---------------------------"}, "Model compression via distillation and quantization": {"container_type": "Publication", "bib": {"title": "Model compression via distillation and quantization", "author": ["A Polino", "R Pascanu", "D Alistarh"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.05668", "abstract": "Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.05668", "author_id": ["m9NARs8AAAAJ", "eSPY8LwAAAAJ", "75q-6ZQAAAAJ"], "url_scholarbib": "/scholar?q=info:BCHVk0993YgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel%2Bcompression%2Bvia%2Bdistillation%2Band%2Bquantization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BCHVk0993YgJ&ei=FBZkYo2PE4OEmgHx-5DADA&json=", "num_citations": 419, "citedby_url": "/scholar?cites=9862176539747361028&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BCHVk0993YgJ:scholar.google.com/&scioq=Model+compression+via+distillation+and+quantization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.05668"}, "Learning to Count Objects in Natural Images for Visual Question Answering": {"container_type": "Publication", "bib": {"title": "Learning to count objects in natural images for visual question answering", "author": ["Y Zhang", "J Hare", "A Pr\u00fcgel-Bennett"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.05766", "abstract": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.05766", "author_id": ["XtCqbfEAAAAJ", "UFeON5oAAAAJ", "oQgxYjkAAAAJ"], "url_scholarbib": "/scholar?q=info:FqzNwpQwb0kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BCount%2BObjects%2Bin%2BNatural%2BImages%2Bfor%2BVisual%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FqzNwpQwb0kJ&ei=GRZkYofGLu-Sy9YPs_mY8AM&json=", "num_citations": 160, "citedby_url": "/scholar?cites=5291501502665174038&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FqzNwpQwb0kJ:scholar.google.com/&scioq=Learning+to+Count+Objects+in+Natural+Images+for+Visual+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.05766"}, "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models": {"container_type": "Publication", "bib": {"title": "Decision-based adversarial attacks: Reliable attacks against black-box machine learning models", "author": ["W Brendel", "J Rauber", "M Bethge"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.04248", "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.04248", "author_id": ["v-JL-hsAAAAJ", "1ujJpuMAAAAJ", "0z0fNxUAAAAJ"], "url_scholarbib": "/scholar?q=info:JRl1-z9B9xAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecision-Based%2BAdversarial%2BAttacks:%2BReliable%2BAttacks%2BAgainst%2BBlack-Box%2BMachine%2BLearning%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JRl1-z9B9xAJ&ei=HRZkYpQWjK7IBP6atbAI&json=", "num_citations": 791, "citedby_url": "/scholar?cites=1222517566911879461&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JRl1-z9B9xAJ:scholar.google.com/&scioq=Decision-Based+Adversarial+Attacks:+Reliable+Attacks+Against+Black-Box+Machine+Learning+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.04248"}, "Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Jointly learning to construct and control agents using deep reinforcement learning", "author": ["C Schaff", "D Yunis", "A Chakrabarti"], "pub_year": "2019", "venue": "\u2026 on Robotics and \u2026", "abstract": "The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learning-based approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical-ie, by picking a design and training a control policy for it. Since training these policies is time-consuming, it"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8793537/", "author_id": ["MxantswAAAAJ", "", "0v5utcwAAAAJ"], "url_scholarbib": "/scholar?q=info:lRR449uAUeUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DJointly%2BLearning%2Bto%2BConstruct%2Band%2BControl%2BAgents%2Busing%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lRR449uAUeUJ&ei=IBZkYtO7FeHDywSSipaYAg&json=", "num_citations": 46, "citedby_url": "/scholar?cites=16524130189701551253&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lRR449uAUeUJ:scholar.google.com/&scioq=Jointly+Learning+to+Construct+and+Control+Agents+using+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.01432"}, "Efficient Sparse-Winograd Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Efficient sparse-winograd convolutional neural networks", "author": ["X Liu", "J Pool", "S Han", "WJ Dally"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.06367", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd's minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be directly combined $-$ applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.06367", "author_id": ["ZVABLi8AAAAJ", "DagH37xI9soC", "E0iCaa4AAAAJ", "YZHj-Y4AAAAJ"], "url_scholarbib": "/scholar?q=info:QMEfDLOTdUsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BSparse-Winograd%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QMEfDLOTdUsJ&ei=IhZkYtPdN4uKmgGY1YjABQ&json=", "num_citations": 110, "citedby_url": "/scholar?cites=5437414522331578688&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QMEfDLOTdUsJ:scholar.google.com/&scioq=Efficient+Sparse-Winograd+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.06367?ref=https://githubhelp.com"}, "Neural Language Modeling by Jointly Learning Syntax and Lexicon": {"container_type": "Publication", "bib": {"title": "Neural language modeling by jointly learning syntax and lexicon", "author": ["Y Shen", "Z Lin", "CW Huang", "A Courville"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.02013", "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02013", "author_id": ["qff5rRYAAAAJ", "LNZ4efwAAAAJ", "0sxcBnwAAAAJ", "km6CP8cAAAAJ"], "url_scholarbib": "/scholar?q=info:g11gp_DhjdIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BLanguage%2BModeling%2Bby%2BJointly%2BLearning%2BSyntax%2Band%2BLexicon%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g11gp_DhjdIJ&ei=JRZkYoH-O4ySyASZk6HgCA&json=", "num_citations": 132, "citedby_url": "/scholar?cites=15172031143397580163&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:g11gp_DhjdIJ:scholar.google.com/&scioq=Neural+Language+Modeling+by+Jointly+Learning+Syntax+and+Lexicon&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02013.pdf%20http://arxiv.org/abs/1711.02013"}, "Variance-based Gradient Compression for Efficient Distributed Deep Learning": {"container_type": "Publication", "bib": {"title": "Variance-based gradient compression for efficient distributed deep learning", "author": ["Y Tsuzuku", "H Imachi", "T Akiba"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.06058", "abstract": "Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.06058", "author_id": ["MKQHkuwAAAAJ", "", "WW-22F8AAAAJ"], "url_scholarbib": "/scholar?q=info:b2BYsuGizbMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance-based%2BGradient%2BCompression%2Bfor%2BEfficient%2BDistributed%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=b2BYsuGizbMJ&ei=NBZkYq7sC5qSy9YP8pKNsAE&json=", "num_citations": 42, "citedby_url": "/scholar?cites=12956190793258262639&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:b2BYsuGizbMJ:scholar.google.com/&scioq=Variance-based+Gradient+Compression+for+Efficient+Distributed+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.06058"}, "Self-ensembling for visual domain adaptation": {"container_type": "Publication", "bib": {"title": "Self-ensembling for visual domain adaptation", "author": ["G French", "M Mackiewicz", "M Fisher"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.05208", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.05208", "author_id": ["wD2nHRsAAAAJ", "4jsSbFIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:f4vqkWbfuH8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-ensembling%2Bfor%2Bvisual%2Bdomain%2Badaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=f4vqkWbfuH8J&ei=OhZkYqbtE7KO6rQPy-CRsA8&json=", "num_citations": 348, "citedby_url": "/scholar?cites=9203351470159334271&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:f4vqkWbfuH8J:scholar.google.com/&scioq=Self-ensembling+for+visual+domain+adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.05208.pdf?ref=https://githubhelp.com"}, "i-RevNet: Deep Invertible Networks": {"container_type": "Publication", "bib": {"title": "i-revnet: Deep invertible networks", "author": ["JH Jacobsen", "A Smeulders", "E Oyallon"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.07088", "abstract": "i-RevNet, a network that can be fully inverted up to the final projection onto the classes, ie no  information is discarded. Building an invertible  by providing an explicit inverse. An analysis"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.07088", "author_id": ["c1FYGAQAAAAJ", "aa5Ou7gAAAAJ", "Y8XGVkYAAAAJ"], "url_scholarbib": "/scholar?q=info:aNGWuSkrvcoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3Di-RevNet:%2BDeep%2BInvertible%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aNGWuSkrvcoJ&ei=PhZkYpnYIoOEmgHx-5DADA&json=", "num_citations": 233, "citedby_url": "/scholar?cites=14608880224467079528&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aNGWuSkrvcoJ:scholar.google.com/&scioq=i-RevNet:+Deep+Invertible+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.07088"}, "Proximal Backpropagation": {"container_type": "Publication", "bib": {"title": "Proximal backpropagation", "author": ["T Frerix", "T M\u00f6llenhoff", "M Moeller", "D Cremers"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose proximal backpropagation (ProxProp) as a  point of view on the backpropagation  algorithm, currently the  Specifically, we show that backpropagation of a prediction error"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.04638", "author_id": ["vUaDEG0AAAAJ", "KAqmeqAAAAAJ", "sxzdAGUAAAAJ", "cXQciMEAAAAJ"], "url_scholarbib": "/scholar?q=info:Ip20Dt7mK8EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProximal%2BBackpropagation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ip20Dt7mK8EJ&ei=QRZkYq33Ko6pywTd4KPADw&json=", "num_citations": 45, "citedby_url": "/scholar?cites=13919472914722495778&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ip20Dt7mK8EJ:scholar.google.com/&scioq=Proximal+Backpropagation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.04638"}, "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step": {"container_type": "Publication", "bib": {"title": "Many paths to equilibrium: GANs do not need to decrease a divergence at every step", "author": ["W Fedus", "M Rosca", "B Lakshminarayanan"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.08446", "author_id": ["-ZfwQOkAAAAJ", "MxkDwD0AAAAJ", "QYn8RbgAAAAJ"], "url_scholarbib": "/scholar?q=info:FNIyzG5f9NwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMany%2BPaths%2Bto%2BEquilibrium:%2BGANs%2BDo%2BNot%2BNeed%2Bto%2BDecrease%2Ba%2BDivergence%2BAt%2BEvery%2BStep%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FNIyzG5f9NwJ&ei=RRZkYoLVAu-Sy9YPs_mY8AM&json=", "num_citations": 180, "citedby_url": "/scholar?cites=15921455512138469908&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FNIyzG5f9NwJ:scholar.google.com/&scioq=Many+Paths+to+Equilibrium:+GANs+Do+Not+Need+to+Decrease+a+Divergence+At+Every+Step&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.08446"}, "Smooth Loss Functions for Deep Top-k Classification": {"container_type": "Publication", "bib": {"title": "Smooth loss functions for deep top-k classification", "author": ["L Berrada", "A Zisserman", "MP Kumar"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.07595", "abstract": "The top-k error is a common measure of performance in machine learning and computer vision. In practice, top-k classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-k classification can bring significant improvements. Our empirical evidence suggests that the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.07595", "author_id": ["zoae83AAAAAJ", "UZ5wscMAAAAJ", "BfmcfEAAAAAJ"], "url_scholarbib": "/scholar?q=info:Ss4cVW6QYx8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmooth%2BLoss%2BFunctions%2Bfor%2BDeep%2BTop-k%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ss4cVW6QYx8J&ei=RxZkYvqYGYOEmgHx-5DADA&json=", "num_citations": 58, "citedby_url": "/scholar?cites=2261810241418874442&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ss4cVW6QYx8J:scholar.google.com/&scioq=Smooth+Loss+Functions+for+Deep+Top-k+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.07595"}, "Guide Actor-Critic for Continuous Control": {"container_type": "Publication", "bib": {"title": "Guide actor-critic for continuous control", "author": ["V Tangkaratt", "A Abdolmaleki", "M Sugiyama"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.07606", "author_id": ["cj9n5hwAAAAJ", "cCYTVWQAAAAJ", "GkYIrlIAAAAJ"], "url_scholarbib": "/scholar?q=info:JjXLdLeVp1cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGuide%2BActor-Critic%2Bfor%2BContinuous%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JjXLdLeVp1cJ&ei=SxZkYsikLvmQ6rQP5OqKqAo&json=", "num_citations": 21, "citedby_url": "/scholar?cites=6316181617581438246&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JjXLdLeVp1cJ:scholar.google.com/&scioq=Guide+Actor-Critic+for+Continuous+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07606"}, "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning": {"container_type": "Publication", "bib": {"title": "Deep voice 3: Scaling text-to-speech with convolutional sequence learning", "author": ["W Ping", "K Peng", "A Gibiansky", "SO Arik"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.07654", "author_id": ["6gKEYRgAAAAJ", "Fhg8dSwAAAAJ", "j9N53_QAAAAJ", "-EZBCBAAAAAJ"], "url_scholarbib": "/scholar?q=info:o6WwTt3QXxkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BVoice%2B3:%2BScaling%2BText-to-Speech%2Bwith%2BConvolutional%2BSequence%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o6WwTt3QXxkJ&ei=ThZkYpLpOJLeyQTE46-QAg&json=", "num_citations": 326, "citedby_url": "/scholar?cites=1828409622662260131&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o6WwTt3QXxkJ:scholar.google.com/&scioq=Deep+Voice+3:+Scaling+Text-to-Speech+with+Convolutional+Sequence+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.07654.pdf?ref=https://githubhelp.com"}, "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Semantically decomposing the latent spaces of generative adversarial networks", "author": ["C Donahue", "ZC Lipton", "A Balsubramani"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (eg individual humans) and observations (eg specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.07904", "author_id": ["MgzHAPQAAAAJ", "MN9Kfg8AAAAJ", "Hd80zWMAAAAJ"], "url_scholarbib": "/scholar?q=info:0LvgNOqkPXgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemantically%2BDecomposing%2Bthe%2BLatent%2BSpaces%2Bof%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0LvgNOqkPXgJ&ei=UhZkYv2BAoyuyAT-mrWwCA&json=", "num_citations": 106, "citedby_url": "/scholar?cites=8664262583947148240&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0LvgNOqkPXgJ:scholar.google.com/&scioq=Semantically+Decomposing+the+Latent+Spaces+of+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07904"}, "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting": {"container_type": "Publication", "bib": {"title": "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting", "author": ["Y Li", "R Yu", "C Shahabi", "Y Liu"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.01926", "abstract": "Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks,(2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.01926", "author_id": ["fv5TMfIAAAAJ", "4HTITaMAAAAJ", "jEdhxGMAAAAJ", "UUKLPMYAAAAJ"], "url_scholarbib": "/scholar?q=info:oLhVkGO4clcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiffusion%2BConvolutional%2BRecurrent%2BNeural%2BNetwork:%2BData-Driven%2BTraffic%2BForecasting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oLhVkGO4clcJ&ei=VBZkYp7qNIySyASZk6HgCA&json=", "num_citations": 1232, "citedby_url": "/scholar?cites=6301301566407555232&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oLhVkGO4clcJ:scholar.google.com/&scioq=Diffusion+Convolutional+Recurrent+Neural+Network:+Data-Driven+Traffic+Forecasting&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.01926"}, "Trust-PCL: An Off-Policy Trust Region Method for Continuous Control": {"container_type": "Publication", "bib": {"title": "Trust-pcl: An off-policy trust region method for continuous control", "author": ["O Nachum", "M Norouzi", "K Xu", "D Schuurmans"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a prohibitively large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL. The algorithm is the result of observing that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.01891", "author_id": ["C-ZlBWMAAAAJ", "Lncr-VoAAAAJ", "GyoKzFwAAAAJ", "xaQuPloAAAAJ"], "url_scholarbib": "/scholar?q=info:zVhhUN7kIpkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTrust-PCL:%2BAn%2BOff-Policy%2BTrust%2BRegion%2BMethod%2Bfor%2BContinuous%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zVhhUN7kIpkJ&ei=WBZkYvb2FeiSy9YPp-OyiAE&json=", "num_citations": 88, "citedby_url": "/scholar?cites=11034633680493566157&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zVhhUN7kIpkJ:scholar.google.com/&scioq=Trust-PCL:+An+Off-Policy+Trust+Region+Method+for+Continuous+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.01891"}, "FigureQA: An Annotated Figure Dataset for Visual Reasoning": {"container_type": "Publication", "bib": {"title": "Figureqa: An annotated figure dataset for visual reasoning", "author": ["SE Kahou", "V Michalski", "A Atkinson", "\u00c1 K\u00e1d\u00e1r"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce FigureQA, a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. We formulate our reasoning task by generating questions from 15 templates; questions concern various relationships between plot elements and examine characteristics like the maximum, the minimum, area-under-the-curve, smoothness, and intersection. To resolve, such"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.07300", "author_id": ["F99FuaAAAAAJ", "9BGzHdUAAAAJ", "Ow-xbmYAAAAJ", ""], "url_scholarbib": "/scholar?q=info:AcIa1QWeNWEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFigureQA:%2BAn%2BAnnotated%2BFigure%2BDataset%2Bfor%2BVisual%2BReasoning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AcIa1QWeNWEJ&ei=XBZkYo7sCOHDywSSipaYAg&json=", "num_citations": 103, "citedby_url": "/scholar?cites=7004678543332000257&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AcIa1QWeNWEJ:scholar.google.com/&scioq=FigureQA:+An+Annotated+Figure+Dataset+for+Visual+Reasoning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.07300"}, "Training wide residual networks for deployment using a single bit for each weight": {"container_type": "Publication", "bib": {"title": "Training wide residual networks for deployment using a single bit for each weight", "author": ["MD McDonnell"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.08530", "abstract": "For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08530", "author_id": ["29OHpnEAAAAJ"], "url_scholarbib": "/scholar?q=info:1atKzBZPrGoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2Bwide%2Bresidual%2Bnetworks%2Bfor%2Bdeployment%2Busing%2Ba%2Bsingle%2Bbit%2Bfor%2Beach%2Bweight%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1atKzBZPrGoJ&ei=YBZkYsypGsLZmQHnraWYCA&json=", "num_citations": 57, "citedby_url": "/scholar?cites=7686605623349914581&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1atKzBZPrGoJ:scholar.google.com/&scioq=Training+wide+residual+networks+for+deployment+using+a+single+bit+for+each+weight&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08530.pdf?ref=https://githubhelp.com"}, "Kernel Implicit Variational Inference": {"container_type": "Publication", "bib": {"title": "Kernel implicit variational inference", "author": ["J Shi", "S Sun", "J Zhu"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.10119", "abstract": "VI, we propose to replace the discriminator with a kernel method for DRE. The advantages  of  We present an implicit VI method named Kernel Implicit Variational Inference (KIVI), which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.10119", "author_id": ["juZXbFoAAAAJ", "", "axsP38wAAAAJ"], "url_scholarbib": "/scholar?q=info:zHwHfZFE0KgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKernel%2BImplicit%2BVariational%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zHwHfZFE0KgJ&ei=ZBZkYq38GY2ymgHg1rfQDQ&json=", "num_citations": 43, "citedby_url": "/scholar?cites=12164297985186299084&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zHwHfZFE0KgJ:scholar.google.com/&scioq=Kernel+Implicit+Variational+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.10119"}, "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments": {"container_type": "Publication", "bib": {"title": "Continuous adaptation via meta-learning in nonstationary and competitive environments", "author": ["M Al-Shedivat", "T Bansal", "Y Burda", "I Sutskever"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.03641", "author_id": ["iUe4TdgAAAAJ", "hHPeXmYAAAAJ", "Amky96kAAAAJ", "x04W_mMAAAAJ"], "url_scholarbib": "/scholar?q=info:Sn8KmyCh5JUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContinuous%2BAdaptation%2Bvia%2BMeta-Learning%2Bin%2BNonstationary%2Band%2BCompetitive%2BEnvironments%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Sn8KmyCh5JUJ&ei=bxZkYpuDN--Sy9YPs_mY8AM&json=", "num_citations": 282, "citedby_url": "/scholar?cites=10800934967753473866&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Sn8KmyCh5JUJ:scholar.google.com/&scioq=Continuous+Adaptation+via+Meta-Learning+in+Nonstationary+and+Competitive+Environments&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.03641.pdf?source=post_page---------------------------&ref=https://githubhelp.com"}, "Mixed Precision Training of Convolutional Neural Networks using Integer Operations": {"container_type": "Publication", "bib": {"title": "Mixed precision training of convolutional neural networks using integer operations", "author": ["D Das", "N Mellempudi", "D Mudigere", "D Kalamkar"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al.(2017). On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.00930", "author_id": ["J7qGGjsAAAAJ", "YsJnkagAAAAJ", "p2R-FRoAAAAJ", ""], "url_scholarbib": "/scholar?q=info:hT84hOqx2iIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMixed%2BPrecision%2BTraining%2Bof%2BConvolutional%2BNeural%2BNetworks%2Busing%2BInteger%2BOperations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hT84hOqx2iIJ&ei=cxZkYsXbF6KUy9YP_JONiAY&json=", "num_citations": 120, "citedby_url": "/scholar?cites=2511515363011215237&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hT84hOqx2iIJ:scholar.google.com/&scioq=Mixed+Precision+Training+of+Convolutional+Neural+Networks+using+Integer+Operations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.00930.pdf?ref=https://githubhelp.com"}, "Learning Latent Permutations with Gumbel-Sinkhorn Networks": {"container_type": "Publication", "bib": {"title": "Learning latent permutations with gumbel-sinkhorn networks", "author": ["G Mena", "D Belanger", "S Linderman", "J Snoek"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data. Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable. In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator. Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08665", "author_id": ["TJlkOdcAAAAJ", "AGnp8NAAAAAJ", "6mD3I24AAAAJ", "FM2DTXwAAAAJ"], "url_scholarbib": "/scholar?q=info:bWodrrqbvPkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLatent%2BPermutations%2Bwith%2BGumbel-Sinkhorn%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bWodrrqbvPkJ&ei=dxZkYtGwBqKUy9YP_JONiAY&json=", "num_citations": 139, "citedby_url": "/scholar?cites=17995429437153045101&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bWodrrqbvPkJ:scholar.google.com/&scioq=Learning+Latent+Permutations+with+Gumbel-Sinkhorn+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08665.pdf?ref=https://githubhelp.com"}, "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples": {"container_type": "Publication", "bib": {"title": "Training confidence-calibrated classifiers for detecting out-of-distribution samples", "author": ["K Lee", "H Lee", "K Lee", "J Shin"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.09325", "abstract": "The problem of detecting whether a test sample is from in-distribution (ie, training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, ie, do not distinguish in-and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.09325", "author_id": ["92M8xv4AAAAJ", "fmSHtE8AAAAJ", "6wwWRdEAAAAJ", "m3eDp7kAAAAJ"], "url_scholarbib": "/scholar?q=info:PxKUyVyKYMYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BConfidence-calibrated%2BClassifiers%2Bfor%2BDetecting%2BOut-of-Distribution%2BSamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PxKUyVyKYMYJ&ei=ehZkYvOVMuiSy9YPp-OyiAE&json=", "num_citations": 501, "citedby_url": "/scholar?cites=14294577348397503039&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PxKUyVyKYMYJ:scholar.google.com/&scioq=Training+Confidence-calibrated+Classifiers+for+Detecting+Out-of-Distribution+Samples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.09325.pdf?ref=https://githubhelp.com"}, "Twin Networks: Matching the Future for Sequence Generation": {"container_type": "Publication", "bib": {"title": "Twin networks: Matching the future for sequence generation", "author": ["D Serdyuk", "NR Ke", "A Sordoni", "A Trischler", "C Pal"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a simple technique for encouraging generative RNNs to plan ahead. We train a\" backward\" recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.06742", "author_id": ["PsKlNzUAAAAJ", "dxwPYhQAAAAJ", "DJon7w4AAAAJ", "EvUM6UUAAAAJ", "1ScWJOoAAAAJ"], "url_scholarbib": "/scholar?q=info:Fj8vNvTAXfoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTwin%2BNetworks:%2BMatching%2Bthe%2BFuture%2Bfor%2BSequence%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Fj8vNvTAXfoJ&ei=fhZkYom-CeHDywSSipaYAg&json=", "num_citations": 50, "citedby_url": "/scholar?cites=18040787837429694230&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Fj8vNvTAXfoJ:scholar.google.com/&scioq=Twin+Networks:+Matching+the+Future+for+Sequence+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.06742"}, "FearNet: Brain-Inspired Model for Incremental Learning": {"container_type": "Publication", "bib": {"title": "Fearnet: Brain-inspired model for incremental learning", "author": ["R Kemker", "C Kanan"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.10563", "abstract": "Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.10563", "author_id": ["", "jMxZjBoAAAAJ"], "url_scholarbib": "/scholar?q=info:p6BkjOs5oTQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFearNet:%2BBrain-Inspired%2BModel%2Bfor%2BIncremental%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p6BkjOs5oTQJ&ei=hxZkYrmtG5LeyQTE46-QAg&json=", "num_citations": 285, "citedby_url": "/scholar?cites=3792376045058171047&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p6BkjOs5oTQJ:scholar.google.com/&scioq=FearNet:+Brain-Inspired+Model+for+Incremental+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.10563"}, "Eigenoption Discovery through the Deep Successor Representation": {"container_type": "Publication", "bib": {"title": "Eigenoption discovery through the deep successor representation", "author": ["MC Machado", "C Rosenbaum", "X Guo", "M Liu"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11089", "author_id": ["xf_n4xUAAAAJ", "pP7H0fkAAAAJ", "DIBOO50AAAAJ", "7QHvAEYAAAAJ"], "url_scholarbib": "/scholar?q=info:b0lPysTlDE4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEigenoption%2BDiscovery%2Bthrough%2Bthe%2BDeep%2BSuccessor%2BRepresentation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=b0lPysTlDE4J&ei=jRZkYvagIuiSy9YPp-OyiAE&json=", "num_citations": 88, "citedby_url": "/scholar?cites=5624122668049451375&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:b0lPysTlDE4J:scholar.google.com/&scioq=Eigenoption+Discovery+through+the+Deep+Successor+Representation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11089"}, "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation": {"container_type": "Publication", "bib": {"title": "Backpropagation through the void: Optimizing control variates for black-box gradient estimation", "author": ["W Grathwohl", "D Choi", "Y Wu", "G Roeder"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our method uses gradients of a neural network trained jointly with model parameters or policies, and is applicable in both discrete and continuous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00123", "author_id": ["ZbClz98AAAAJ", "giuZW04AAAAJ", "bOQGfFIAAAAJ", "sIfE5HIAAAAJ"], "url_scholarbib": "/scholar?q=info:lbpDmgME5scJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBackpropagation%2Bthrough%2Bthe%2BVoid:%2BOptimizing%2Bcontrol%2Bvariates%2Bfor%2Bblack-box%2Bgradient%2Bestimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lbpDmgME5scJ&ei=lBZkYtSyBO-Sy9YPs_mY8AM&json=", "num_citations": 222, "citedby_url": "/scholar?cites=14404204871710653077&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lbpDmgME5scJ:scholar.google.com/&scioq=Backpropagation+through+the+Void:+Optimizing+control+variates+for+black-box+gradient+estimation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00123.pdf?ref=https://githubhelp.com"}, "Generating Natural Adversarial Examples": {"container_type": "Publication", "bib": {"title": "Generating natural adversarial examples", "author": ["Z Zhao", "D Dua", "S Singh"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11342", "abstract": "In this paper, we propose a framework to generate natural and legible adversarial examples   recent advances in generative adversarial networks. We present generated adversaries to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11342", "author_id": ["1jUruh0AAAAJ", "RDky42sAAAAJ", "-hGZC54AAAAJ"], "url_scholarbib": "/scholar?q=info:7k0S4WZjB1oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BNatural%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7k0S4WZjB1oJ&ei=mBZkYpWmFpqSy9YP8pKNsAE&json=", "num_citations": 416, "citedby_url": "/scholar?cites=6487263081764376046&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7k0S4WZjB1oJ:scholar.google.com/&scioq=Generating+Natural+Adversarial+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11342"}, "Don't Decay the Learning Rate, Increase the Batch Size": {"container_type": "Publication", "bib": {"title": "Don't decay the learning rate, increase the batch size", "author": ["SL Smith", "PJ Kindermans", "C Ying", "QV Le"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00489", "abstract": "It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00489", "author_id": ["fyEqU5oAAAAJ", "FpI8dFwAAAAJ", "d9c0JeMAAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:mm5_YiY3SzUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDon%2527t%2BDecay%2Bthe%2BLearning%2BRate,%2BIncrease%2Bthe%2BBatch%2BSize%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mm5_YiY3SzUJ&ei=nBZkYpq6CPmQ6rQP5OqKqAo&json=", "num_citations": 732, "citedby_url": "/scholar?cites=3840223745264283290&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mm5_YiY3SzUJ:scholar.google.com/&scioq=Don%27t+Decay+the+Learning+Rate,+Increase+the+Batch+Size&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00489.pdf?source=post_page---------------------------"}, "Syntax-Directed Variational Autoencoder for Structured Data": {"container_type": "Publication", "bib": {"title": "Syntax-directed variational autoencoder for structured data", "author": ["H Dai", "Y Tian", "B Dai", "S Skiena", "L Song"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.08786", "abstract": "Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08786", "author_id": ["obpl7GQAAAAJ", "17Fe5K0AAAAJ", "TIKl_foAAAAJ", "fnE2dSoAAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:mehRbeOQ6G4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSyntax-Directed%2BVariational%2BAutoencoder%2Bfor%2BStructured%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mehRbeOQ6G4J&ei=nxZkYoqzBPmQ6rQP5OqKqAo&json=", "num_citations": 239, "citedby_url": "/scholar?cites=7991796845235005593&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mehRbeOQ6G4J:scholar.google.com/&scioq=Syntax-Directed+Variational+Autoencoder+for+Structured+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08786"}, "Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration": {"container_type": "Publication", "bib": {"title": "Reinforcement learning on web interfaces using workflow-guided exploration", "author": ["EZ Liu", "K Guu", "P Pasupat", "T Shi", "P Liang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.08802", "abstract": "By comparing DOMNET+BC+RL and DOMNET+WGE, we find that workflow-guided exploration  enables DOMNET to perform even better on the more difficult tasks, which we analyze in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08802", "author_id": ["qjDVoqQAAAAJ", "w8ZDbO8AAAAJ", "BqKXIA8AAAAJ", "qRAQ5BsAAAAJ", "pouyVyUAAAAJ"], "url_scholarbib": "/scholar?q=info:KIk77-9T4zsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforcement%2BLearning%2Bon%2BWeb%2BInterfaces%2Busing%2BWorkflow-Guided%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KIk77-9T4zsJ&ei=phZkYsjjDIuKmgGY1YjABQ&json=", "num_citations": 50, "citedby_url": "/scholar?cites=4315385157927012648&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KIk77-9T4zsJ:scholar.google.com/&scioq=Reinforcement+Learning+on+Web+Interfaces+using+Workflow-Guided+Exploration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08802"}, "On the Expressive Power of Overlapping Architectures of Deep Learning": {"container_type": "Publication", "bib": {"title": "On the expressive power of overlapping architectures of deep learning", "author": ["O Sharir", "A Shashua"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.02065", "abstract": "Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.02065", "author_id": ["2y5Am34AAAAJ", "dwi5wvYAAAAJ"], "url_scholarbib": "/scholar?q=info:C0_2u7y37_cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BExpressive%2BPower%2Bof%2BOverlapping%2BArchitectures%2Bof%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=C0_2u7y37_cJ&ei=qhZkYrfnJMLZmQHnraWYCA&json=", "num_citations": 30, "citedby_url": "/scholar?cites=17865700268037263115&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:C0_2u7y37_cJ:scholar.google.com/&scioq=On+the+Expressive+Power+of+Overlapping+Architectures+of+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.02065"}, "Synthetic and Natural Noise Both Break Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Synthetic and natural noise both break neural machine translation", "author": ["Y Belinkov", "Y Bisk"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.02173", "abstract": "Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02173", "author_id": ["K-6ujU4AAAAJ", "bWoGh8UAAAAJ"], "url_scholarbib": "/scholar?q=info:VfT24yMYn5EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSynthetic%2Band%2BNatural%2BNoise%2BBoth%2BBreak%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VfT24yMYn5EJ&ei=vRZkYs-CKo6pywTd4KPADw&json=", "num_citations": 472, "citedby_url": "/scholar?cites=10493132199224079445&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VfT24yMYn5EJ:scholar.google.com/&scioq=Synthetic+and+Natural+Noise+Both+Break+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02173.pdf?ref=https://githubhelp.com"}, "Scalable Private Learning with PATE": {"container_type": "Publication", "bib": {"title": "Scalable private learning with pate", "author": ["N Papernot", "S Song", "I Mironov", "A Raghunathan"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a\" student\" model the knowledge of an ensemble of\" teacher\" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08908", "author_id": ["cGxq0cMAAAAJ", "ZHEGw9sAAAAJ", "hg3A9TgAAAAJ", "KQNjz6cAAAAJ"], "url_scholarbib": "/scholar?q=info:eUcScM3DHNcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScalable%2BPrivate%2BLearning%2Bwith%2BPATE%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eUcScM3DHNcJ&ei=whZkYtr0EbKO6rQPy-CRsA8&json=", "num_citations": 358, "citedby_url": "/scholar?cites=15500479304618362745&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eUcScM3DHNcJ:scholar.google.com/&scioq=Scalable+Private+Learning+with+PATE&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08908.pdf?ref=hackernoon.com"}, "A Simple Neural Attentive Meta-Learner": {"container_type": "Publication", "bib": {"title": "A simple neural attentive meta-learner", "author": ["N Mishra", "M Rohaninejad", "X Chen", "P Abbeel"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "how the meta-learner solves the task. We propose a class of simple and generic meta-learner   to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.03141", "author_id": ["STFR8uYAAAAJ", "0qW2Sr8AAAAJ", "5tVuggUAAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:sDYW9_aEkrYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BSimple%2BNeural%2BAttentive%2BMeta-Learner%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sDYW9_aEkrYJ&ei=xRZkYv6GG-HDywSSipaYAg&json=", "num_citations": 922, "citedby_url": "/scholar?cites=13155723657744889520&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sDYW9_aEkrYJ:scholar.google.com/&scioq=A+Simple+Neural+Attentive+Meta-Learner&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.03141.pdf?ref=https://githubhelp.com"}, "Generative Models of Visually Grounded Imagination": {"container_type": "Publication", "bib": {"title": "Generative models of visually grounded imagination", "author": ["R Vedantam", "I Fischer", "J Huang", "K Murphy"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.10762", "author_id": ["v1CRzeAAAAAJ", "Z63Zf_0AAAAJ", "-pu6i_4AAAAJ", "MxxZkEcAAAAJ"], "url_scholarbib": "/scholar?q=info:PyNyyncXwGQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BModels%2Bof%2BVisually%2BGrounded%2BImagination%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PyNyyncXwGQJ&ei=yxZkYoPZA46pywTd4KPADw&json=", "num_citations": 103, "citedby_url": "/scholar?cites=7259828402586264383&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PyNyyncXwGQJ:scholar.google.com/&scioq=Generative+Models+of+Visually+Grounded+Imagination&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.10762"}, "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks": {"container_type": "Publication", "bib": {"title": "Enhancing the reliability of out-of-distribution image detection in neural networks", "author": ["S Liang", "Y Li", "R Srikant"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.02690", "abstract": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in-and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.02690", "author_id": ["L8r9ox4AAAAJ", "QSTd1oUAAAAJ", "tDWt_MQAAAAJ"], "url_scholarbib": "/scholar?q=info:3naMPG-alWgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnhancing%2BThe%2BReliability%2Bof%2BOut-of-distribution%2BImage%2BDetection%2Bin%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3naMPG-alWgJ&ei=zRZkYvS_M4yuyAT-mrWwCA&json=", "num_citations": 824, "citedby_url": "/scholar?cites=7536099354022278878&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3naMPG-alWgJ:scholar.google.com/&scioq=Enhancing+The+Reliability+of+Out-of-distribution+Image+Detection+in+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.02690.pdf?ref=https://githubhelp.com"}, "Deep Complex Networks": {"container_type": "Publication", "bib": {"title": "Complex networks and deep learning for EEG signal analysis", "author": ["Z Gao", "W Dang", "X Wang", "X Hong", "L Hou", "K Ma"], "pub_year": "2021", "venue": "Cognitive \u2026", "abstract": "Next, we will discuss the application of complex networks and deep learning in EEG analysis.  And a framework combining complex network and deep learning is proposed to achieve"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/article/10.1007/s11571-020-09626-1", "author_id": ["IOZ7pBgAAAAJ", "", "DN-iV-YAAAAJ", "", "", "FSSXeyAAAAAJ"], "url_scholarbib": "/scholar?q=info:qwYD0TMuamYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BComplex%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qwYD0TMuamYJ&ei=mhVkYvnhEoySyASZk6HgCA&json=", "num_citations": 30, "citedby_url": "/scholar?cites=7379761739484825259&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qwYD0TMuamYJ:scholar.google.com/&scioq=Deep+Complex+Networks&hl=en&as_sdt=0,33", "eprint_url": "http://www.matjazperc.com/publications/CognNeurodyn_15_369.pdf"}, "Online Learning Rate Adaptation with Hypergradient Descent": {"container_type": "Publication", "bib": {"title": "Online learning rate adaptation with hypergradient descent", "author": ["AG Baydin", "R Cornish", "DM Rubio", "M Schmidt"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms. Our method works by dynamically updating the learning rate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.04782", "author_id": ["GWBSOj4AAAAJ", "ZDVQRN0AAAAJ", "dMwpf-4AAAAJ", "5BtEUJcAAAAJ"], "url_scholarbib": "/scholar?q=info:-yQcv-RBwSYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOnline%2BLearning%2BRate%2BAdaptation%2Bwith%2BHypergradient%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-yQcv-RBwSYJ&ei=oBVkYvqjM46pywTd4KPADw&json=", "num_citations": 137, "citedby_url": "/scholar?cites=2792585694661059835&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-yQcv-RBwSYJ:scholar.google.com/&scioq=Online+Learning+Rate+Adaptation+with+Hypergradient+Descent&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.04782.pdf?ref=https://githubhelp.com"}, "Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling": {"container_type": "Publication", "bib": {"title": "Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling", "author": ["C Riquelme", "G Tucker", "J Snoek"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.09127", "abstract": "Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.09127", "author_id": ["Es2BBeYAAAAJ", "-gJkPHIAAAAJ", "FM2DTXwAAAAJ"], "url_scholarbib": "/scholar?q=info:eGAL4YF2T9EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BBayesian%2BBandits%2BShowdown:%2B%2BAn%2BEmpirical%2BComparison%2Bof%2BBayesian%2BDeep%2BNetworks%2Bfor%2BThompson%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eGAL4YF2T9EJ&ei=phVkYvTCOOHDywSSipaYAg&json=", "num_citations": 206, "citedby_url": "/scholar?cites=15082403977285558392&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eGAL4YF2T9EJ:scholar.google.com/&scioq=Deep+Bayesian+Bandits+Showdown:++An+Empirical+Comparison+of+Bayesian+Deep+Networks+for+Thompson+Sampling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.09127"}, "Unsupervised Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Unsupervised neural machine translation", "author": ["M Artetxe", "G Labaka", "E Agirre", "K Cho"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11041", "abstract": "In spite of the recent success of neural machine translation ( unsupervised manner, relying  on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11041", "author_id": ["N5InzP8AAAAJ", "xZdN_ZIAAAAJ", "kSuqts0AAAAJ", "0RAmmIAAAAAJ"], "url_scholarbib": "/scholar?q=info:ToKI4aUsyFQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ToKI4aUsyFQJ&ei=qhVkYoeGHOHDywSSipaYAg&json=", "num_citations": 648, "citedby_url": "/scholar?cites=6109181985493123662&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ToKI4aUsyFQJ:scholar.google.com/&scioq=Unsupervised+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11041.pdf?source=post_page---------------------------"}, "Understanding image motion with group representations ": {"container_type": "Publication", "bib": {"title": "Understanding image motion with group representations", "author": ["A Jaegle", "S Phillips", "D Ippolito", "K Daniilidis"], "pub_year": "2016", "venue": "arXiv preprint arXiv \u2026", "abstract": "Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1612.00472", "author_id": ["2iBYdwEAAAAJ", "onOVjV0AAAAJ", "COEsqLYAAAAJ", "dGs2BcIAAAAJ"], "url_scholarbib": "/scholar?q=info:ADwLFx3xjzgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2Bimage%2Bmotion%2Bwith%2Bgroup%2Brepresentations%2B%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ADwLFx3xjzgJ&ei=rhVkYpaQLpqSy9YP8pKNsAE&json=", "num_citations": 4, "citedby_url": "/scholar?cites=4075741295036546048&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ADwLFx3xjzgJ:scholar.google.com/&scioq=Understanding+image+motion+with+group+representations+&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1612.00472"}, "Not-So-Random Features": {"container_type": "Publication", "bib": {"title": "Not-so-random features", "author": ["B Bullins", "C Zhang", "Y Zhang"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.10230", "abstract": "We propose a principled method for kernel learning, which relies on a Fourier-analytic  characterization of translation-invariant or rotation-invariant kernels. Our method produces a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10230", "author_id": ["PCUwf-8AAAAJ", "sXtjq8IAAAAJ", "lc6CVqEAAAAJ"], "url_scholarbib": "/scholar?q=info:VTAKE3CmreYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNot-So-Random%2BFeatures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VTAKE3CmreYJ&ei=shVkYof2Bu-Sy9YPs_mY8AM&json=", "num_citations": 14, "citedby_url": "/scholar?cites=16622124799980351573&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VTAKE3CmreYJ:scholar.google.com/&scioq=Not-So-Random+Features&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10230"}, "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking": {"container_type": "Publication", "bib": {"title": "Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking", "author": ["A Bojchevski", "S G\u00fcnnemann"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.03815", "abstract": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss-an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.03815", "author_id": ["F1APiN4AAAAJ", "npqoAWwAAAAJ"], "url_scholarbib": "/scholar?q=info:fysXhv_Q3YoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BGaussian%2BEmbedding%2Bof%2BGraphs:%2BUnsupervised%2BInductive%2BLearning%2Bvia%2BRanking%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fysXhv_Q3YoJ&ei=tBVkYuX4OvmQ6rQP5OqKqAo&json=", "num_citations": 271, "citedby_url": "/scholar?cites=10006383742972013439&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fysXhv_Q3YoJ:scholar.google.com/&scioq=Deep+Gaussian+Embedding+of+Graphs:+Unsupervised+Inductive+Learning+via+Ranking&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.03815.pdf?ref=https://githubhelp.com"}, "Detecting Statistical Interactions from Neural Network Weights": {"container_type": "Publication", "bib": {"title": "Detecting statistical interactions from neural network weights", "author": ["M Tsang", "D Cheng", "Y Liu"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.04977", "abstract": "Interpreting neural networks is a crucial and challenging task in machine learning. In this paper, we develop a novel framework for detecting statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights. Depending on the desired interactions, our method can achieve significantly better or similar interaction detection performance compared to the state-of-the-art without searching an exponential solution space of possible interactions. We obtain this accuracy and efficiency by observing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.04977", "author_id": ["gLZfSR4AAAAJ", "bqlnOOsAAAAJ", "UUKLPMYAAAAJ"], "url_scholarbib": "/scholar?q=info:JObXJh-gehwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDetecting%2BStatistical%2BInteractions%2Bfrom%2BNeural%2BNetwork%2BWeights%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JObXJh-gehwJ&ei=txVkYvzzGJLeyQTE46-QAg&json=", "num_citations": 115, "citedby_url": "/scholar?cites=2052128635876795940&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JObXJh-gehwJ:scholar.google.com/&scioq=Detecting+Statistical+Interactions+from+Neural+Network+Weights&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.04977.pdf?ref=https://githubhelp.com"}, "Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions": {"container_type": "Publication", "bib": {"title": "Relational neural expectation maximization: Unsupervised discovery of objects and their interactions", "author": ["S Van Steenkiste", "M Chang", "K Greff"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely\\emph {unsupervised} fashion. It"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.10353", "author_id": ["i-AStBYAAAAJ", "vgfGtykAAAAJ", "OcownLgAAAAJ"], "url_scholarbib": "/scholar?q=info:nup4eWmWJZ0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelational%2BNeural%2BExpectation%2BMaximization:%2BUnsupervised%2BDiscovery%2Bof%2BObjects%2Band%2Btheir%2BInteractions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nup4eWmWJZ0J&ei=vBVkYqTgKIySyASZk6HgCA&json=", "num_citations": 216, "citedby_url": "/scholar?cites=11323622217846680222&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nup4eWmWJZ0J:scholar.google.com/&scioq=Relational+Neural+Expectation+Maximization:+Unsupervised+Discovery+of+Objects+and+their+Interactions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.10353"}, "Learning from Between-class Examples for Deep Sound Recognition": {"container_type": "Publication", "bib": {"title": "Learning from between-class examples for deep sound recognition", "author": ["Y Tokozume", "Y Ushiku", "T Harada"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.10282", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.10282", "author_id": ["", "kxUld9MAAAAJ", "k8rlJ8AAAAAJ"], "url_scholarbib": "/scholar?q=info:6WSS7PqXercJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bfrom%2BBetween-class%2BExamples%2Bfor%2BDeep%2BSound%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6WSS7PqXercJ&ei=vxVkYrvFGOiSy9YPp-OyiAE&json=", "num_citations": 175, "citedby_url": "/scholar?cites=13221046760066147561&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6WSS7PqXercJ:scholar.google.com/&scioq=Learning+from+Between-class+Examples+for+Deep+Sound+Recognition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.10282"}, "Memory-based Parameter Adaptation": {"container_type": "Publication", "bib": {"title": "Memory-based parameter adaptation", "author": ["P Sprechmann", "SM Jayakumar", "JW Rae"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": ", Memory-based Parameter Adaptation, stores  adaptation, reneging the need for many  iterations over similar data before good predictions can be made. As our method is memory-based"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.10542", "author_id": ["YCPycGAAAAAJ", "rJUAY8QAAAAJ", "s0YmMPcAAAAJ"], "url_scholarbib": "/scholar?q=info:6F7EWfXCnbYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemory-based%2BParameter%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6F7EWfXCnbYJ&ei=wxVkYoTTHO-Sy9YPs_mY8AM&json=", "num_citations": 64, "citedby_url": "/scholar?cites=13158888045275274984&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6F7EWfXCnbYJ:scholar.google.com/&scioq=Memory-based+Parameter+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.10542"}, "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions": {"container_type": "Publication", "bib": {"title": "Few-shot autoregressive density estimation: Towards learning to learn distributions", "author": ["S Reed", "Y Chen", "T Paine", "A Oord", "SM Eslami"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10304", "author_id": ["jEANvfgAAAAJ", "fAWKizAAAAAJ", "oFIvUSQAAAAJ", "TqUN-LwAAAAJ", "skyUvycAAAAJ"], "url_scholarbib": "/scholar?q=info:MtMamDh9H6cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFew-shot%2BAutoregressive%2BDensity%2BEstimation:%2BTowards%2BLearning%2Bto%2BLearn%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MtMamDh9H6cJ&ei=zhVkYu7cOO-Sy9YPs_mY8AM&json=", "num_citations": 74, "citedby_url": "/scholar?cites=12042481610635531058&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MtMamDh9H6cJ:scholar.google.com/&scioq=Few-shot+Autoregressive+Density+Estimation:+Towards+Learning+to+Learn+Distributions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10304.pdf?source=post_page---------------------------"}, "Model-Ensemble Trust-Region Policy Optimization": {"container_type": "Publication", "bib": {"title": "Model-ensemble trust-region policy optimization", "author": ["T Kurutach", "I Clavera", "Y Duan", "A Tamar"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.10592", "author_id": ["IbcqwaoAAAAJ", "yABlzrsAAAAJ", "EMDboA4AAAAJ", "kppa2vgAAAAJ"], "url_scholarbib": "/scholar?q=info:9nXnhLkb-08J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel-Ensemble%2BTrust-Region%2BPolicy%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9nXnhLkb-08J&ei=0RVkYu3TGuHDywSSipaYAg&json=", "num_citations": 308, "citedby_url": "/scholar?cites=5763230631763342838&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9nXnhLkb-08J:scholar.google.com/&scioq=Model-Ensemble+Trust-Region+Policy+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.10592"}, "Mitigating Adversarial Effects Through Randomization": {"container_type": "Publication", "bib": {"title": "Mitigating adversarial effects through randomization", "author": ["C Xie", "J Wang", "Z Zhang", "Z Ren", "A Yuille"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.01991", "abstract": "Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.01991", "author_id": ["X3vVZPcAAAAJ", "CR4T0pkAAAAJ", "8gRM3xMAAAAJ", "0cygTk0AAAAJ", "FJ-huxgAAAAJ"], "url_scholarbib": "/scholar?q=info:ZekLItr4iA8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMitigating%2BAdversarial%2BEffects%2BThrough%2BRandomization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZekLItr4iA8J&ei=1BVkYsqbOvmQ6rQP5OqKqAo&json=", "num_citations": 694, "citedby_url": "/scholar?cites=1119418123159333221&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZekLItr4iA8J:scholar.google.com/&scioq=Mitigating+Adversarial+Effects+Through+Randomization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01991"}, "Adversarial Dropout Regularization": {"container_type": "Publication", "bib": {"title": "Adversarial dropout regularization", "author": ["K Saito", "Y Ushiku", "T Harada", "K Saenko"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.01575", "abstract": "to regularize the main classifier and make it insensitive to noise. Instead, we use dropout in  an adversarial way,  Compared to previous adversarial feature alignment methods, where the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.01575", "author_id": ["2X0cwhkAAAAJ", "kxUld9MAAAAJ", "k8rlJ8AAAAAJ", "9xDADY4AAAAJ"], "url_scholarbib": "/scholar?q=info:Lnsie6gpAFgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BDropout%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Lnsie6gpAFgJ&ei=2BVkYumzBo2ymgHg1rfQDQ&json=", "num_citations": 195, "citedby_url": "/scholar?cites=6341114078934760238&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Lnsie6gpAFgJ:scholar.google.com/&scioq=Adversarial+Dropout+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01575"}, "Meta-Learning for Semi-Supervised Few-Shot Classification": {"container_type": "Publication", "bib": {"title": "Meta-learning for semi-supervised few-shot classification", "author": ["M Ren", "E Triantafillou", "S Ravi", "J Snell"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.00676", "author_id": ["XcQ9WqMAAAAJ", "Y5x2ZgQAAAAJ", "cr53lHIAAAAJ", "MbXKAK8AAAAJ"], "url_scholarbib": "/scholar?q=info:MhcxDt9qFAsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2Bfor%2BSemi-Supervised%2BFew-Shot%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MhcxDt9qFAsJ&ei=2xVkYruSI-iSy9YPp-OyiAE&json=", "num_citations": 778, "citedby_url": "/scholar?cites=798380540199769906&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MhcxDt9qFAsJ:scholar.google.com/&scioq=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.00676.pdf?utm_campaign=affiliate-ir-Optimise%20media%28%20South%20East%20Asia%29%20Pte.%20ltd._156_-99_national_R_all_ACQ_cpa_en&utm_content=&utm_source=%20388939"}, "Interpretable Counting for Visual Question Answering": {"container_type": "Publication", "bib": {"title": "Interpretable counting for visual question answering", "author": ["A Trott", "C Xiong", "R Socher"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.08697", "abstract": "Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.08697", "author_id": ["rB4bvV0AAAAJ", "vaSdahkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:H-ZhGjHQ1sIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInterpretable%2BCounting%2Bfor%2BVisual%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H-ZhGjHQ1sIJ&ei=3hVkYuW7OrKO6rQPy-CRsA8&json=", "num_citations": 52, "citedby_url": "/scholar?cites=14039637797688698399&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:H-ZhGjHQ1sIJ:scholar.google.com/&scioq=Interpretable+Counting+for+Visual+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.08697"}, "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Ask the right questions: Active question reformulation with reinforcement learning", "author": ["C Buck", "J Bulian", "M Ciaramita", "W Gajewski"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.07830", "author_id": ["DSb_wQ8AAAAJ", "Yq32OuIAAAAJ", "PwsKWfUAAAAJ", "HnQ1yE8AAAAJ"], "url_scholarbib": "/scholar?q=info:J_pGssydyD4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAsk%2Bthe%2BRight%2BQuestions:%2BActive%2BQuestion%2BReformulation%2Bwith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J_pGssydyD4J&ei=4hVkYs-WI4OEmgHx-5DADA&json=", "num_citations": 134, "citedby_url": "/scholar?cites=4524039328183548455&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:J_pGssydyD4J:scholar.google.com/&scioq=Ask+the+Right+Questions:+Active+Question+Reformulation+with+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07830"}, "Distributed Prioritized Experience Replay": {"container_type": "Publication", "bib": {"title": "Distributed prioritized experience replay", "author": ["D Horgan", "J Quan", "D Budden", "G Barth-Maron"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "3 OUR CONTRIBUTION: DISTRIBUTED PRIORITIZED EXPERIENCE REPLAY In this  paper we extend prioritized experience replay to the distributed setting and show that this is a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.00933", "author_id": ["5__jLn4AAAAJ", "", "Tom5l8EAAAAJ", "mr1r8tgAAAAJ"], "url_scholarbib": "/scholar?q=info:lg0ASeezaKYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BPrioritized%2BExperience%2BReplay%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lg0ASeezaKYJ&ei=5hVkYuyoFZqSy9YP8pKNsAE&json=", "num_citations": 496, "citedby_url": "/scholar?cites=11991031813817503126&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lg0ASeezaKYJ:scholar.google.com/&scioq=Distributed+Prioritized+Experience+Replay&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.00933"}, "Generalizing Hamiltonian Monte Carlo with Neural Networks": {"container_type": "Publication", "bib": {"title": "Generalizing hamiltonian monte carlo with neural networks", "author": ["D Levy", "MD Hoffman", "J Sohl-Dickstein"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.09268", "abstract": "We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.09268", "author_id": ["xmOaZ8EAAAAJ", "IeHKeGYAAAAJ", "-3zYIjQAAAAJ"], "url_scholarbib": "/scholar?q=info:diXTIuC-5VUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizing%2BHamiltonian%2BMonte%2BCarlo%2Bwith%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=diXTIuC-5VUJ&ei=6RVkYuK2HpyO6rQP_qe3mAs&json=", "num_citations": 94, "citedby_url": "/scholar?cites=6189563132756829558&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:diXTIuC-5VUJ:scholar.google.com/&scioq=Generalizing+Hamiltonian+Monte+Carlo+with+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.09268"}, "Some Considerations on Learning to Explore via Meta-Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Some considerations on learning to explore via meta-reinforcement learning", "author": ["BC Stadie", "G Yang", "R Houthooft", "X Chen"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\\text {RL}^ 2$. Results are presented on a novel environment we callKrazy World'and a set of maze environments. We show E-MAML and E-$\\text {RL}^ 2$ deliver better performance on tasks where exploration is important."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.01118", "author_id": ["lEV5F5kAAAAJ", "vaQcF6kAAAAJ", "HBztuGIAAAAJ", "5tVuggUAAAAJ"], "url_scholarbib": "/scholar?q=info:r0dIi1r7UDwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSome%2BConsiderations%2Bon%2BLearning%2Bto%2BExplore%2Bvia%2BMeta-Reinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r0dIi1r7UDwJ&ei=8BVkYqSVH42ymgHg1rfQDQ&json=", "num_citations": 78, "citedby_url": "/scholar?cites=4346250006714927023&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r0dIi1r7UDwJ:scholar.google.com/&scioq=Some+Considerations+on+Learning+to+Explore+via+Meta-Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.01118"}, "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect": {"container_type": "Publication", "bib": {"title": "Improving the improved training of wasserstein gans: A consistency term and its dual effect", "author": ["X Wei", "B Gong", "Z Liu", "W Lu", "L Wang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.01541", "abstract": "Despite being impactful on a variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by\\cite {arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, called Wasserstein GAN (WGAN), hinges on the 1-Lipschitz continuity of the discriminator. In this paper, we propose a novel approach to enforcing the Lipschitz continuity in the training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.01541", "author_id": ["SydSP3gAAAAJ", "lv9ZeVUAAAAJ", "", "", "mZKxB10AAAAJ"], "url_scholarbib": "/scholar?q=info:0Vd1fmwNySsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2Bthe%2BImproved%2BTraining%2Bof%2BWasserstein%2BGANs:%2BA%2BConsistency%2BTerm%2Band%2BIts%2BDual%2BEffect%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0Vd1fmwNySsJ&ei=9hVkYpSLJouKmgGY1YjABQ&json=", "num_citations": 192, "citedby_url": "/scholar?cites=3155067773578991569&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0Vd1fmwNySsJ:scholar.google.com/&scioq=Improving+the+Improved+Training+of+Wasserstein+GANs:+A+Consistency+Term+and+Its+Dual+Effect&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.01541"}, "On the regularization of Wasserstein GANs": {"container_type": "Publication", "bib": {"title": "On the regularization of wasserstein gans", "author": ["H Petzka", "A Fischer", "D Lukovnicov"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1709.08894", "abstract": "Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.08894", "author_id": ["lgSAcz0AAAAJ", "FyZbyIUAAAAJ", "V9Ru7-sAAAAJ"], "url_scholarbib": "/scholar?q=info:UpxUoKw7SOQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2Bregularization%2Bof%2BWasserstein%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UpxUoKw7SOQJ&ei=-RVkYpDXPJGJmwGY-qmYDQ&json=", "num_citations": 174, "citedby_url": "/scholar?cites=16449463251581049938&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UpxUoKw7SOQJ:scholar.google.com/&scioq=On+the+regularization+of+Wasserstein+GANs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.08894"}, "Visualizing the Loss Landscape of Neural Nets": {"container_type": "Publication", "bib": {"title": "Visualizing the loss landscape of neural nets", "author": ["H Li", "Z Xu", "G Taylor", "C Studer"], "pub_year": "2018", "venue": "Advances in neural \u2026", "abstract": "Neural network training relies on our ability to find\" good\" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (eg, skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.neurips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets", "author_id": ["BNEeEosAAAAJ", "TfWlMTYAAAAJ", "hDqVCIoAAAAJ", "Jco5C7sAAAAJ"], "url_scholarbib": "/scholar?q=info:iuhR1mHVrqEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisualizing%2Bthe%2BLoss%2BLandscape%2Bof%2BNeural%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iuhR1mHVrqEJ&ei=_hVkYt3tDOiSy9YPp-OyiAE&json=", "num_citations": 917, "citedby_url": "/scholar?cites=11650483902238288010&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iuhR1mHVrqEJ:scholar.google.com/&scioq=Visualizing+the+Loss+Landscape+of+Neural+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf"}, "Understanding Short-Horizon Bias in Stochastic Meta-Optimization": {"container_type": "Publication", "bib": {"title": "Understanding short-horizon bias in stochastic meta-optimization", "author": ["Y Wu", "M Ren", "R Liao", "R Grosse"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.02021", "abstract": "Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.02021", "author_id": ["bOQGfFIAAAAJ", "XcQ9WqMAAAAJ", "2wrS35MAAAAJ", "xgQd1qgAAAAJ"], "url_scholarbib": "/scholar?q=info:3GsafZ07-5EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BShort-Horizon%2BBias%2Bin%2BStochastic%2BMeta-Optimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3GsafZ07-5EJ&ei=ARZkYp2VLouKmgGY1YjABQ&json=", "num_citations": 77, "citedby_url": "/scholar?cites=10519066902248713180&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3GsafZ07-5EJ:scholar.google.com/&scioq=Understanding+Short-Horizon+Bias+in+Stochastic+Meta-Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.02021.pdf?utm_source="}, "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration": {"container_type": "Publication", "bib": {"title": "Unsupervised learning of goal spaces for intrinsically motivated goal exploration", "author": ["A P\u00e9r\u00e9", "S Forestier", "O Sigaud", "PY Oudeyer"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered feature space, limiting their autonomy. In this work, we propose to use deep representation learning algorithms to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.00781", "author_id": ["", "AJcwwzUAAAAJ", "elLfDv0AAAAJ", "gCqGj4sAAAAJ"], "url_scholarbib": "/scholar?q=info:ZwQyNcYYpvcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2Bof%2BGoal%2BSpaces%2Bfor%2BIntrinsically%2BMotivated%2BGoal%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZwQyNcYYpvcJ&ei=BhZkYuXsG86E6rQP5-KmKA&json=", "num_citations": 71, "citedby_url": "/scholar?cites=17844977813077230695&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZwQyNcYYpvcJ:scholar.google.com/&scioq=Unsupervised+Learning+of+Goal+Spaces+for+Intrinsically+Motivated+Goal+Exploration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.00781"}, "HexaConv": {"container_type": "Publication", "bib": {"title": "Hexaconv", "author": ["E Hoogeboom", "JWT Peters", "TS Cohen"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The effectiveness of Convolutional Neural Networks stems in large part from their ability to  exploit the translation invariance that is inherent in many learning problems. Recently, it was"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.02108", "author_id": ["nkTd_BIAAAAJ", "VgYX7w4AAAAJ", "a3q4YxEAAAAJ"], "url_scholarbib": "/scholar?q=info:WUM3BpRcnzAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHexaConv%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WUM3BpRcnzAJ&ei=CRZkYrKJGYuKmgGY1YjABQ&json=", "num_citations": 61, "citedby_url": "/scholar?cites=3503620825946735449&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WUM3BpRcnzAJ:scholar.google.com/&scioq=HexaConv&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.02108"}, "Towards better understanding of gradient-based attribution methods for Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Towards better understanding of gradient-based attribution methods for deep neural networks", "author": ["M Ancona", "E Ceolini", "C \u00d6ztireli", "M Gross"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.06104", "abstract": "Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.06104", "author_id": ["QSutBuYAAAAJ", "yJ8Q2XsAAAAJ", "dXt1WOUAAAAJ", "uxk0GmUAAAAJ"], "url_scholarbib": "/scholar?q=info:GcGAHUjM8GIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2Bbetter%2Bunderstanding%2Bof%2Bgradient-based%2Battribution%2Bmethods%2Bfor%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GcGAHUjM8GIJ&ei=DRZkYpnCIIuKmgGY1YjABQ&json=", "num_citations": 579, "citedby_url": "/scholar?cites=7129422820232184089&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GcGAHUjM8GIJ:scholar.google.com/&scioq=Towards+better+understanding+of+gradient-based+attribution+methods+for+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.06104.pdf?ref=https://githubhelp.com"}, "An efficient framework for learning sentence representations": {"container_type": "Publication", "bib": {"title": "An efficient framework for learning sentence representations", "author": ["L Logeswaran", "H Lee"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.02893", "abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.02893", "author_id": ["dcv4kpIAAAAJ", "fmSHtE8AAAAJ"], "url_scholarbib": "/scholar?q=info:fAiM1BSxpLMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2Befficient%2Bframework%2Bfor%2Blearning%2Bsentence%2Brepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fAiM1BSxpLMJ&ei=EBZkYufgLYuKmgGY1YjABQ&json=", "num_citations": 381, "citedby_url": "/scholar?cites=12944665931993057404&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fAiM1BSxpLMJ:scholar.google.com/&scioq=An+efficient+framework+for+learning+sentence+representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.02893.pdf,"}, "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Treeqn and atreec: Differentiable tree-structured models for deep reinforcement learning", "author": ["G Farquhar", "T Rockt\u00e4schel", "M Igl"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11417", "author_id": ["6Z-RC-QAAAAJ", "mWBY8aIAAAAJ", "rFcdDJEAAAAJ"], "url_scholarbib": "/scholar?q=info:TsycUah4xJMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTreeQN%2Band%2BATreeC:%2BDifferentiable%2BTree-Structured%2BModels%2Bfor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TsycUah4xJMJ&ei=FBZkYqvLHY6pywTd4KPADw&json=", "num_citations": 107, "citedby_url": "/scholar?cites=10647768083329764430&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TsycUah4xJMJ:scholar.google.com/&scioq=TreeQN+and+ATreeC:+Differentiable+Tree-Structured+Models+for+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11417?ref=https://githubhelp.com"}, "Residual Connections Encourage Iterative Inference": {"container_type": "Publication", "bib": {"title": "Residual connections encourage iterative inference", "author": ["S Jastrz\u0119bski", "D Arpit", "N Ballas", "V Verma", "T Che"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual connections naturally encourage features of residual blocks to move along the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.04773", "author_id": ["wbJxGQ8AAAAJ", "oU0jQIAAAAAJ", "euUV4iUAAAAJ", "wo_M4uQAAAAJ", ""], "url_scholarbib": "/scholar?q=info:9Or79pD2lcgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DResidual%2BConnections%2BEncourage%2BIterative%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9Or79pD2lcgJ&ei=FxZkYu-UAo6pywTd4KPADw&json=", "num_citations": 74, "citedby_url": "/scholar?cites=14453729681594903284&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9Or79pD2lcgJ:scholar.google.com/&scioq=Residual+Connections+Encourage+Iterative+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.04773"}, "Non-Autoregressive Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Non-autoregressive neural machine translation", "author": ["J Gu", "J Bradbury", "C Xiong", "VOK Li", "R Socher"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02281", "author_id": ["cB1mFBsAAAAJ", "GprA5UsAAAAJ", "vaSdahkAAAAJ", "GKL_i6sAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:s_wGcDqBVTAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNon-Autoregressive%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s_wGcDqBVTAJ&ei=JRZkYsW-N6KUy9YP_JONiAY&json=", "num_citations": 422, "citedby_url": "/scholar?cites=3482831974828539059&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s_wGcDqBVTAJ:scholar.google.com/&scioq=Non-Autoregressive+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02281"}, "Learning Approximate Inference Networks for Structured Prediction": {"container_type": "Publication", "bib": {"title": "Learning approximate inference networks for structured prediction", "author": ["L Tu", "K Gimpel"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.03376", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\" inference network\" outputs continuous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.03376", "author_id": ["", "kDHs7DYAAAAJ"], "url_scholarbib": "/scholar?q=info:kyCgTGQfGdIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BApproximate%2BInference%2BNetworks%2Bfor%2BStructured%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kyCgTGQfGdIJ&ei=LRZkYrqiGZqSy9YP8pKNsAE&json=", "num_citations": 46, "citedby_url": "/scholar?cites=15139166138025386131&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kyCgTGQfGdIJ:scholar.google.com/&scioq=Learning+Approximate+Inference+Networks+for+Structured+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.03376"}, "Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks": {"container_type": "Publication", "bib": {"title": "Memorization precedes generation: Learning unsupervised gans with memory networks", "author": ["Y Kim", "M Kim", "G Kim"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.01500", "abstract": "We propose an approach to address two issues that commonly occur during training of unsupervised GANs. First, since GANs use only a continuous latent distribution to embed multiple classes or clusters of data, they often do not correctly handle the structural discontinuity between disparate classes in a latent space. Second, discriminators of GANs easily forget about past generated samples by generators, incurring instability during adversarial training. We argue that these two infamous problems of unsupervised GAN"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.01500", "author_id": ["N54w7AIAAAAJ", "lqFnRfsAAAAJ", "CiSdOV0AAAAJ"], "url_scholarbib": "/scholar?q=info:PU7Agg79wWgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemorization%2BPrecedes%2BGeneration:%2BLearning%2BUnsupervised%2BGANs%2Bwith%2BMemory%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PU7Agg79wWgJ&ei=MhZkYrrvPIuKmgGY1YjABQ&json=", "num_citations": 30, "citedby_url": "/scholar?cites=7548592689214672445&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PU7Agg79wWgJ:scholar.google.com/&scioq=Memorization+Precedes+Generation:+Learning+Unsupervised+GANs+with+Memory+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.01500"}, "Wasserstein Auto-Encoders": {"container_type": "Publication", "bib": {"title": "Wasserstein auto-encoders", "author": ["I Tolstikhin", "O Bousquet", "S Gelly"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "\u2022 A new family of regularized auto-encoders (Algorithms 1, 2 and Eq. 4), which we call  Wasserstein Auto-Encoders (WAE), that minimize the optimal transport Wc(PX,PG) for any cost"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.01558", "author_id": ["n4k9D7QAAAAJ", "CHO-UV8AAAAJ", "m7LvuTkAAAAJ"], "url_scholarbib": "/scholar?q=info:wTPuiGeYLBcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWasserstein%2BAuto-Encoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wTPuiGeYLBcJ&ei=PRZkYqW4JpqSy9YP8pKNsAE&json=", "num_citations": 789, "citedby_url": "/scholar?cites=1669877132293977025&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wTPuiGeYLBcJ:scholar.google.com/&scioq=Wasserstein+Auto-Encoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01558.pdf?ref=https://githubhelp.com"}, "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality": {"container_type": "Publication", "bib": {"title": "Characterizing adversarial subspaces using local intrinsic dimensionality", "author": ["X Ma", "B Li", "Y Wang", "SM Erfani", "S Wijewickrema"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called'adversarial subspaces') in which adversarial examples lie. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.02613", "author_id": ["XQViiyYAAAAJ", "K8vJkTcAAAAJ", "uMWPDboAAAAJ", "Jq9ocx4AAAAJ", "MjgOHPYAAAAJ"], "url_scholarbib": "/scholar?q=info:CetEPlO1yO0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCharacterizing%2BAdversarial%2BSubspaces%2BUsing%2BLocal%2BIntrinsic%2BDimensionality%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CetEPlO1yO0J&ei=SBZkYvDmIsLZmQHnraWYCA&json=", "num_citations": 435, "citedby_url": "/scholar?cites=17134144151462669065&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CetEPlO1yO0J:scholar.google.com/&scioq=Characterizing+Adversarial+Subspaces+Using+Local+Intrinsic+Dimensionality&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.02613"}, "Emergent Complexity via Multi-Agent Competition": {"container_type": "Publication", "bib": {"title": "Emergent complexity via multi-agent competition", "author": ["T Bansal", "J Pachocki", "S Sidor", "I Sutskever"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.03748", "author_id": ["hHPeXmYAAAAJ", "", "UhmcQ7gAAAAJ", "x04W_mMAAAAJ"], "url_scholarbib": "/scholar?q=info:X4VDNNDHi7IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BComplexity%2Bvia%2BMulti-Agent%2BCompetition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=X4VDNNDHi7IJ&ei=VBZkYq7TKOHDywSSipaYAg&json=", "num_citations": 296, "citedby_url": "/scholar?cites=12865596457557919071&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:X4VDNNDHi7IJ:scholar.google.com/&scioq=Emergent+Complexity+via+Multi-Agent+Competition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.03748.pdf%3Cp%3EKEYWORDS:&nbsp;Artificial"}, "Shifting Mean Activation Towards Zero with Bipolar Activation Functions": {"container_type": "Publication", "bib": {"title": "Shifting mean activation towards zero with bipolar activation functions", "author": ["L Eidnes", "A N\u00f8kland"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1709.04054", "abstract": "We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.04054", "author_id": ["", ""], "url_scholarbib": "/scholar?q=info:kr1MJ0ryTkQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DShifting%2BMean%2BActivation%2BTowards%2BZero%2Bwith%2BBipolar%2BActivation%2BFunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kr1MJ0ryTkQJ&ei=VxZkYvL-CY2ymgHg1rfQDQ&json=", "num_citations": 16, "citedby_url": "/scholar?cites=4922137843063373202&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kr1MJ0ryTkQJ:scholar.google.com/&scioq=Shifting+Mean+Activation+Towards+Zero+with+Bipolar+Activation+Functions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.04054"}, "Improving GANs Using Optimal Transport": {"container_type": "Publication", "bib": {"title": "Improving GANs using optimal transport", "author": ["T Salimans", "H Zhang", "A Radford", "D Metaxas"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.05573", "author_id": ["w68-7AYAAAAJ", "cxEoVL4AAAAJ", "dOad5HoAAAAJ", "a7VNhCIAAAAJ"], "url_scholarbib": "/scholar?q=info:_7ZHSMR9FDEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGANs%2BUsing%2BOptimal%2BTransport%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_7ZHSMR9FDEJ&ei=WRZkYrOTFOiSy9YPp-OyiAE&json=", "num_citations": 212, "citedby_url": "/scholar?cites=3536589889372403455&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_7ZHSMR9FDEJ:scholar.google.com/&scioq=Improving+GANs+Using+Optimal+Transport&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.05573"}, "On the insufficiency of existing momentum schemes for Stochastic Optimization": {"container_type": "Publication", "bib": {"title": "On the insufficiency of existing momentum schemes for stochastic optimization", "author": ["R Kidambi", "P Netrapalli", "P Jain"], "pub_year": "2018", "venue": "2018 Information Theory \u2026", "abstract": "Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). In general,\u201cfast gradient\u201d methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8503173/", "author_id": ["vSxL7K8AAAAJ", "mim8FQkAAAAJ", "qYhRbJoAAAAJ"], "url_scholarbib": "/scholar?q=info:A2zL_pSz218J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2Binsufficiency%2Bof%2Bexisting%2Bmomentum%2Bschemes%2Bfor%2BStochastic%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=A2zL_pSz218J&ei=XBZkYqU8jbKaAeDWt9AN&json=", "num_citations": 75, "citedby_url": "/scholar?cites=6907311906014063619&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:A2zL_pSz218J:scholar.google.com/&scioq=On+the+insufficiency+of+existing+momentum+schemes+for+Stochastic+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.05591"}, "Graph Partition Neural Networks for Semi-Supervised Classification": {"container_type": "Publication", "bib": {"title": "Graph partition neural networks for semi-supervised classification", "author": ["R Liao", "M Brockschmidt", "D Tarlow", "AL Gaunt"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present graph partition neural networks (GPNN), an extension of graph neural networks   A mix of the two ideas leads to our Graph Partition Neural Networks (GPNN), which we will"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.06272", "author_id": ["2wrS35MAAAAJ", "pF27eLMAAAAJ", "oavgGaMAAAAJ", "IzwvqPIAAAAJ"], "url_scholarbib": "/scholar?q=info:UJ5FOo_pnk8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BPartition%2BNeural%2BNetworks%2Bfor%2BSemi-Supervised%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UJ5FOo_pnk8J&ei=YBZkYtHdJKKUy9YP_JONiAY&json=", "num_citations": 50, "citedby_url": "/scholar?cites=5737279776683826768&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UJ5FOo_pnk8J:scholar.google.com/&scioq=Graph+Partition+Neural+Networks+for+Semi-Supervised+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.06272.pdf?utm_term="}, "Learning to cluster in order to transfer across domains and tasks": {"container_type": "Publication", "bib": {"title": "Learning to cluster in order to transfer across domains and tasks", "author": ["YC Hsu", "Z Lv", "Z Kira"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.10125", "abstract": "This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not. This similarity is category"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.10125", "author_id": ["7QWAiigAAAAJ", "fSb94nAAAAAJ", "2a5XgNAAAAAJ"], "url_scholarbib": "/scholar?q=info:rU8EIG1IRkYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2Bcluster%2Bin%2Border%2Bto%2Btransfer%2Bacross%2Bdomains%2Band%2Btasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rU8EIG1IRkYJ&ei=bhZkYpPgJYuKmgGY1YjABQ&json=", "num_citations": 92, "citedby_url": "/scholar?cites=5063814464550490029&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rU8EIG1IRkYJ:scholar.google.com/&scioq=Learning+to+cluster+in+order+to+transfer+across+domains+and+tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.10125"}, "Cascade Adversarial Machine Learning Regularized with a Unified Embedding": {"container_type": "Publication", "bib": {"title": "Cascade adversarial machine learning regularized with a unified embedding", "author": ["T Na", "JH Ko", "S Mukhopadhyay"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.02582", "abstract": "Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.02582", "author_id": ["z5kbXTUAAAAJ", "UN_OIs4AAAAJ", "5KRtMEkAAAAJ"], "url_scholarbib": "/scholar?q=info:R4u1000tEKMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCascade%2BAdversarial%2BMachine%2BLearning%2BRegularized%2Bwith%2Ba%2BUnified%2BEmbedding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=R4u1000tEKMJ&ei=cxZkYpmeHeiSy9YPp-OyiAE&json=", "num_citations": 84, "citedby_url": "/scholar?cites=11749941240097246023&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:R4u1000tEKMJ:scholar.google.com/&scioq=Cascade+Adversarial+Machine+Learning+Regularized+with+a+Unified+Embedding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.02582"}, "A DIRT-T Approach to Unsupervised Domain Adaptation": {"container_type": "Publication", "bib": {"title": "A dirt-t approach to unsupervised domain adaptation", "author": ["R Shu", "HH Bui", "H Narui", "S Ermon"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.08735", "abstract": "Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08735", "author_id": ["UB7UZEYAAAAJ", "mDLwSZAAAAAJ", "ZDkv4DgAAAAJ", "ogXTOZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:Y1eYu2vcWnwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDIRT-T%2BApproach%2Bto%2BUnsupervised%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Y1eYu2vcWnwJ&ei=dhZkYpHmK-HDywSSipaYAg&json=", "num_citations": 402, "citedby_url": "/scholar?cites=8960716763873957731&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Y1eYu2vcWnwJ:scholar.google.com/&scioq=A+DIRT-T+Approach+to+Unsupervised+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08735"}, "All-but-the-Top: Simple and Effective Postprocessing for Word Representations": {"container_type": "Publication", "bib": {"title": "All-but-the-top: Simple and effective postprocessing for word representations", "author": ["J Mu", "S Bhat", "P Viswanath"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.01417", "abstract": "Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\\em very simple}, and yet counter-intuitive, postprocessing technique--eliminate the common mean vector and a few top dominating directions from the word vectors--that renders off-the-shelf representations {\\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.01417", "author_id": ["7koUBUIAAAAJ", "F6x3R1oAAAAJ", "lPycXNcAAAAJ"], "url_scholarbib": "/scholar?q=info:K-hMVgsAZN0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAll-but-the-Top:%2BSimple%2Band%2BEffective%2BPostprocessing%2Bfor%2BWord%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=K-hMVgsAZN0J&ei=ehZkYsOfCouKmgGY1YjABQ&json=", "num_citations": 188, "citedby_url": "/scholar?cites=15952875828745660459&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:K-hMVgsAZN0J:scholar.google.com/&scioq=All-but-the-Top:+Simple+and+Effective+Postprocessing+for+Word+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.01417"}, "Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines": {"container_type": "Publication", "bib": {"title": "Variance reduction for policy gradient with action-dependent factorized baselines", "author": ["C Wu", "A Rajeswaran", "Y Duan", "V Kumar"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.07246", "author_id": ["wyjZbeMAAAAJ", "_EJrRVAAAAAJ", "EMDboA4AAAAJ", "nu3W--sAAAAJ"], "url_scholarbib": "/scholar?q=info:vi-KDXqHrJgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance%2BReduction%2Bfor%2BPolicy%2BGradient%2Bwith%2BAction-Dependent%2BFactorized%2BBaselines%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vi-KDXqHrJgJ&ei=fRZkYvqwD7KO6rQPy-CRsA8&json=", "num_citations": 115, "citedby_url": "/scholar?cites=11001316948042198974&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vi-KDXqHrJgJ:scholar.google.com/&scioq=Variance+Reduction+for+Policy+Gradient+with+Action-Dependent+Factorized+Baselines&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.07246"}, "Fix your classifier: the marginal value of training the last weight layer": {"container_type": "Publication", "bib": {"title": "Fix your classifier: the marginal value of training the last weight layer", "author": ["E Hoffer", "I Hubara", "D Soudry"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1801.04540", "abstract": "Neural networks are commonly used as models for classification for a wide variety of tasks. Typically, a learned affine transformation is placed at the end of such models, yielding a per-class value used for classification. This classifier can have a vast number of parameters, which grows linearly with the number of possible classes, thus requiring increasingly more resources. In this work we argue that this classifier can be fixed, up to a global scale constant, with little or no loss of accuracy for most tasks, allowing memory and computational"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.04540", "author_id": ["iEfTH7AAAAAJ", "dyYryZYAAAAJ", "AEBWEm8AAAAJ"], "url_scholarbib": "/scholar?q=info:6lA8T2T0BI0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFix%2Byour%2Bclassifier:%2Bthe%2Bmarginal%2Bvalue%2Bof%2Btraining%2Bthe%2Blast%2Bweight%2Blayer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6lA8T2T0BI0J&ei=gBZkYpq3KI6pywTd4KPADw&json=", "num_citations": 68, "citedby_url": "/scholar?cites=10161515370917941482&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6lA8T2T0BI0J:scholar.google.com/&scioq=Fix+your+classifier:+the+marginal+value+of+training+the+last+weight+layer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.04540"}, "Demystifying MMD GANs": {"container_type": "Publication", "bib": {"title": "Demystifying mmd gans", "author": ["M Bi\u0144kowski", "DJ Sutherland", "M Arbel"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In our experiments (Section 5), we find that MMD GANs achieve the same generator  ,  resulting in GANs with fewer parameters and computationally faster training. Thus, the MMD GAN"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.01401", "author_id": ["wVZXAk0AAAAJ", "uO_NqicAAAAJ", "NsOqVtkAAAAJ"], "url_scholarbib": "/scholar?q=info:IJsDTXrDDY4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDemystifying%2BMMD%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IJsDTXrDDY4J&ei=gxZkYsiKM5qSy9YP8pKNsAE&json=", "num_citations": 498, "citedby_url": "/scholar?cites=10236052458128513824&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IJsDTXrDDY4J:scholar.google.com/&scioq=Demystifying+MMD+GANs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.01401.pdf?ref=https://githubhelp.com"}, "The Implicit Bias of Gradient Descent on Separable Data": {"container_type": "Publication", "bib": {"title": "The implicit bias of gradient descent on separable data", "author": ["D Soudry", "E Hoffer", "MS Nacson", "S Gunasekar"], "pub_year": "2018", "venue": "The Journal of Machine \u2026", "abstract": "We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the"}, "filled": false, "gsrank": 1, "pub_url": "https://www.jmlr.org/papers/volume19/18-188/18-188.pdf", "author_id": ["AEBWEm8AAAAJ", "iEfTH7AAAAAJ", "wrozdTYAAAAJ", "EkREu_QAAAAJ"], "url_scholarbib": "/scholar?q=info:CYRqEXYrEHQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BImplicit%2BBias%2Bof%2BGradient%2BDescent%2Bon%2BSeparable%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CYRqEXYrEHQJ&ei=jhZkYqHVF4yuyAT-mrWwCA&json=", "num_citations": 506, "citedby_url": "/scholar?cites=8363232294125339657&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CYRqEXYrEHQJ:scholar.google.com/&scioq=The+Implicit+Bias+of+Gradient+Descent+on+Separable+Data&hl=en&as_sdt=0,33", "eprint_url": "https://www.jmlr.org/papers/volume19/18-188/18-188.pdf"}, "Data Augmentation Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Data augmentation generative adversarial networks", "author": ["A Antoniou", "A Storkey", "H Edwards"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.04340", "abstract": "Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.04340", "author_id": ["LNO4XZQAAAAJ", "3Rlc8EAAAAAJ", "0o470HsAAAAJ"], "url_scholarbib": "/scholar?q=info:xnEy6niHoocJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DData%2BAugmentation%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xnEy6niHoocJ&ei=kxZkYorhMYOEmgHx-5DADA&json=", "num_citations": 787, "citedby_url": "/scholar?cites=9773523094742397382&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xnEy6niHoocJ:scholar.google.com/&scioq=Data+Augmentation+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.04340"}, "DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images": {"container_type": "Publication", "bib": {"title": "Dna-gan: Learning disentangled representations from multi-attribute images", "author": ["T Xiao", "J Hong", "J Ma"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.05415", "abstract": "Disentangling factors of variation has become a very challenging problem on representation learning. Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, poor quality of generated images from encodings, lack of identity information, etc. In this paper, we propose a supervised learning model called DNA-GAN which tries to disentangle different factors or attributes of images. The latent representations of images are DNA-like, in which each individual piece (of the encoding) represents an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05415", "author_id": ["Op_tr2IAAAAJ", "JzR4ezEAAAAJ", ""], "url_scholarbib": "/scholar?q=info:EO_3YqYrLY8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDNA-GAN:%2BLearning%2BDisentangled%2BRepresentations%2Bfrom%2BMulti-Attribute%2BImages%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EO_3YqYrLY8J&ei=lxZkYpI6so7qtA_L4JGwDw&json=", "num_citations": 61, "citedby_url": "/scholar?cites=10316950315000655632&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EO_3YqYrLY8J:scholar.google.com/&scioq=DNA-GAN:+Learning+Disentangled+Representations+from+Multi-Attribute+Images&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05415"}, "Fraternal Dropout": {"container_type": "Publication", "bib": {"title": "Fraternal dropout", "author": ["K Zolna", "D Arpit", "D Suhubdy", "Y Bengio"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00066", "abstract": "on dropout that we call fraternal dropout, where we minimize an equally weighted sum of  prediction losses from two identical copies of the same LSTM with different dropout  iid dropout"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00066", "author_id": ["Kg_f9PwAAAAJ", "oU0jQIAAAAAJ", "YgGzSlMAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:dM2Bm9IQvj8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFraternal%2BDropout%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dM2Bm9IQvj8J&ei=mhZkYv7DBOHDywSSipaYAg&json=", "num_citations": 33, "citedby_url": "/scholar?cites=4593127166702636404&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dM2Bm9IQvj8J:scholar.google.com/&scioq=Fraternal+Dropout&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00066"}, "Neural Speed Reading via Skim-RNN": {"container_type": "Publication", "bib": {"title": "Neural speed reading via skim-rnn", "author": ["M Seo", "S Min", "A Farhadi", "H Hajishirzi"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.02085", "abstract": "of speed reading, we introduce Skim-RNN, a recurrent neural network ( Skim-RNN gives  computational advantage over an RNN that always updates the entire hidden state. Skim-RNN"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02085", "author_id": ["zYze5fIAAAAJ", "jU4IZs4AAAAJ", "jeOFRDsAAAAJ", "LOV6_WIAAAAJ"], "url_scholarbib": "/scholar?q=info:i3RhUWG4I3sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BSpeed%2BReading%2Bvia%2BSkim-RNN%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i3RhUWG4I3sJ&ei=nRZkYoaQLM6E6rQP5-KmKA&json=", "num_citations": 26, "citedby_url": "/scholar?cites=8873138418966688907&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i3RhUWG4I3sJ:scholar.google.com/&scioq=Neural+Speed+Reading+via+Skim-RNN&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02085.pdf?fbclid=IwAR3EeFsKM_b5p9Ox7X9mH-1oI3U3oOKPBy3xUOBN0XvJa7QW2ZeJJ9ypQVo"}, "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning": {"container_type": "Publication", "bib": {"title": "Learning general purpose distributed sentence representations via large scale multi-task learning", "author": ["S Subramanian", "A Trischler", "Y Bengio"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.00079", "author_id": ["7i101rcAAAAJ", "EvUM6UUAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:U_gMVt71O8cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BGeneral%2BPurpose%2BDistributed%2BSentence%2BRepresentations%2Bvia%2BLarge%2BScale%2BMulti-task%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=U_gMVt71O8cJ&ei=oBZkYofDMuHDywSSipaYAg&json=", "num_citations": 315, "citedby_url": "/scholar?cites=14356338572448823379&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:U_gMVt71O8cJ:scholar.google.com/&scioq=Learning+General+Purpose+Distributed+Sentence+Representations+via+Large+Scale+Multi-task+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.00079?ref=https://githubhelp.com"}, "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches": {"container_type": "Publication", "bib": {"title": "Flipout: Efficient pseudo-independent weight perturbations on mini-batches", "author": ["Y Wen", "P Vicol", "J Ba", "D Tran", "R Grosse"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.04386", "abstract": "Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.04386", "author_id": ["J2GzNAkAAAAJ", "jywEQ-AAAAAJ", "ymzxRhAAAAAJ", "wVazIm8AAAAJ", "xgQd1qgAAAAJ"], "url_scholarbib": "/scholar?q=info:89yXb-xRZjQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFlipout:%2BEfficient%2BPseudo-Independent%2BWeight%2BPerturbations%2Bon%2BMini-Batches%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=89yXb-xRZjQJ&ei=pBZkYvuwAaKUy9YP_JONiAY&json=", "num_citations": 180, "citedby_url": "/scholar?cites=3775795413523094771&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:89yXb-xRZjQJ:scholar.google.com/&scioq=Flipout:+Efficient+Pseudo-Independent+Weight+Perturbations+on+Mini-Batches&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.04386"}, "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling": {"container_type": "Publication", "bib": {"title": "Bi-directional block self-attention for fast and memory-efficient sequence modeling", "author": ["T Shen", "T Zhou", "G Long", "J Jiang", "C Zhang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but does not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called\" bi-directional"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.00857", "author_id": ["SegyX9AAAAAJ", "OKvgizMAAAAJ", "Pl8m7hMAAAAJ", "XFtCe08AAAAJ", "B6lBmqEAAAAJ"], "url_scholarbib": "/scholar?q=info:ZUmlEOGG92MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBi-Directional%2BBlock%2BSelf-Attention%2Bfor%2BFast%2Band%2BMemory-Efficient%2BSequence%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZUmlEOGG92MJ&ei=qBZkYsWYA-HDywSSipaYAg&json=", "num_citations": 131, "citedby_url": "/scholar?cites=7203374430207428965&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZUmlEOGG92MJ:scholar.google.com/&scioq=Bi-Directional+Block+Self-Attention+for+Fast+and+Memory-Efficient+Sequence+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.00857"}, "SpectralNet: Spectral Clustering using Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Spectralnet: Spectral clustering using deep neural networks", "author": ["U Shaham", "K Stanton", "H Li", "B Nadler", "R Basri"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Spectral clustering is a leading and popular technique in unsupervised data analysis. Two of its major limitations are scalability and generalization of the spectral embedding (ie, out-of-sample-extension). In this paper we introduce a deep learning approach to spectral clustering that overcomes the above shortcomings. Our network, which we call SpectralNet, learns a map that embeds input data points into the eigenspace of their associated graph Laplacian matrix and subsequently clusters them. We train SpectralNet using a procedure"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.01587", "author_id": ["IVapKkkAAAAJ", "", "o7-TIlcAAAAJ", "N3Jj5_cAAAAJ", "d6vuvHIAAAAJ"], "url_scholarbib": "/scholar?q=info:7LsQqet7Mz8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpectralNet:%2BSpectral%2BClustering%2Busing%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7LsQqet7Mz8J&ei=qxZkYufyGY6pywTd4KPADw&json=", "num_citations": 193, "citedby_url": "/scholar?cites=4554119900285680620&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7LsQqet7Mz8J:scholar.google.com/&scioq=SpectralNet:+Spectral+Clustering+using+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.01587"}, "Compositional Obverter Communication Learning from Raw Visual Input": {"container_type": "Publication", "bib": {"title": "Compositional obverter communication learning from raw visual input", "author": ["E Choi", "A Lazaridou", "N De Freitas"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.02341", "abstract": "One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (eg hand-engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.02341", "author_id": ["GUlGIPkAAAAJ", "BMgUIC0AAAAJ", "nzEluBwAAAAJ"], "url_scholarbib": "/scholar?q=info:MYZkoDO7YJ0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompositional%2BObverter%2BCommunication%2BLearning%2Bfrom%2BRaw%2BVisual%2BInput%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MYZkoDO7YJ0J&ei=rhZkYpLbO-HDywSSipaYAg&json=", "num_citations": 50, "citedby_url": "/scholar?cites=11340269692127577649&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MYZkoDO7YJ0J:scholar.google.com/&scioq=Compositional+Obverter+Communication+Learning+from+Raw+Visual+Input&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.02341"}, "Building Generalizable Agents with a Realistic and Rich 3D Environment": {"container_type": "Publication", "bib": {"title": "Building generalizable agents with a realistic and rich 3d environment", "author": ["Y Wu", "Y Wu", "G Gkioxari", "Y Tian"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1801.02209", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (eg color, texture, object changes), and also high-level variations (eg layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.02209", "author_id": ["dusV5HMAAAAJ", "mJQI-gUAAAAJ", "kQisE-gAAAAJ", "0mgEF28AAAAJ"], "url_scholarbib": "/scholar?q=info:hR9MRzyduDoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBuilding%2BGeneralizable%2BAgents%2Bwith%2Ba%2BRealistic%2Band%2BRich%2B3D%2BEnvironment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hR9MRzyduDoJ&ei=sxZkYrrxDpGJmwGY-qmYDQ&json=", "num_citations": 177, "citedby_url": "/scholar?cites=4231304732134350725&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hR9MRzyduDoJ:scholar.google.com/&scioq=Building+Generalizable+Agents+with+a+Realistic+and+Rich+3D+Environment&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.02209"}, "Emergent Translation in Multi-Agent Communication": {"container_type": "Publication", "bib": {"title": "Emergent translation in multi-agent communication", "author": ["J Lee", "K Cho", "J Weston", "D Kiela"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.06922", "abstract": "While most machine translation systems to date are trained on large parallel corpora, humans learn language in a different way: by being grounded in an environment and interacting with other humans. In this work, we propose a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task. We find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals. The emergent translation is interactive and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.06922", "author_id": ["adUC3vkAAAAJ", "0RAmmIAAAAAJ", "lMkTx0EAAAAJ", "Q0piorUAAAAJ"], "url_scholarbib": "/scholar?q=info:2vCE5ZTLMuoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BTranslation%2Bin%2BMulti-Agent%2BCommunication%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2vCE5ZTLMuoJ&ei=thZkYveHFvmQ6rQP5OqKqAo&json=", "num_citations": 55, "citedby_url": "/scholar?cites=16875774594076963034&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2vCE5ZTLMuoJ:scholar.google.com/&scioq=Emergent+Translation+in+Multi-Agent+Communication&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.06922"}, "DORA The Explorer: Directed Outreaching Reinforcement Action-Selection": {"container_type": "Publication", "bib": {"title": "Dora the explorer: Directed outreaching reinforcement action-selection", "author": ["L Choshen", "L Fox", "Y Loewenstein"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.04012", "abstract": "Exploration is a fundamental aspect of Reinforcement Learning, typically implemented using stochastic action-selection. Exploration, however, can be more efficient if directed toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality. While there are a few model-based solutions to this shortcoming, a model-free approach is still missing. We propose $ E $-values, a generalization of counters that can be used to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.04012", "author_id": ["8b8IhUYAAAAJ", "6mCoDiUAAAAJ", "Isnd7NgAAAAJ"], "url_scholarbib": "/scholar?q=info:D1YVbbE46ZMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDORA%2BThe%2BExplorer:%2BDirected%2BOutreaching%2BReinforcement%2BAction-Selection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D1YVbbE46ZMJ&ei=sRVkYuOuOpGJmwGY-qmYDQ&json=", "num_citations": 41, "citedby_url": "/scholar?cites=10658112327839471119&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D1YVbbE46ZMJ:scholar.google.com/&scioq=DORA+The+Explorer:+Directed+Outreaching+Reinforcement+Action-Selection&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.04012"}, "Neural Sketch Learning for Conditional Program Generation": {"container_type": "Publication", "bib": {"title": "Neural sketch learning for conditional program generation", "author": ["V Murali", "L Qi", "S Chaudhuri", "C Jermaine"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.05698", "abstract": "We study the problem of generating source code in a strongly typed, Java-like programming language, given a label (for example a set of API calls or types) carrying a small amount of information about the code that is desired. The generated programs are expected to respect a\" realistic\" relationship between programs and labels, as exemplified by a corpus of labeled programs available during training. Two challenges in such conditional program generation are that the generated programs must satisfy a rich set of syntactic and semantic constraints"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.05698", "author_id": ["wlp0v4kAAAAJ", "", "9j6RBYQAAAAJ", "D2P2B0MAAAAJ"], "url_scholarbib": "/scholar?q=info:K7fwD_O-hJoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BSketch%2BLearning%2Bfor%2BConditional%2BProgram%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=K7fwD_O-hJoJ&ei=tRVkYqvuNMLZmQHnraWYCA&json=", "num_citations": 106, "citedby_url": "/scholar?cites=11134234129920472875&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:K7fwD_O-hJoJ:scholar.google.com/&scioq=Neural+Sketch+Learning+for+Conditional+Program+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.05698"}, "Unsupervised Machine Translation Using Monolingual Corpora Only": {"container_type": "Publication", "bib": {"title": "Unsupervised machine translation using monolingual corpora only", "author": ["G Lample", "A Conneau", "L Denoyer"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00043", "author_id": ["H7sVDmIAAAAJ", "45KfCpgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:WCvoXbNYegkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BMachine%2BTranslation%2BUsing%2BMonolingual%2BCorpora%2BOnly%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WCvoXbNYegkJ&ei=vBVkYsSDNo6pywTd4KPADw&json=", "num_citations": 857, "citedby_url": "/scholar?cites=682955820897938264&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WCvoXbNYegkJ:scholar.google.com/&scioq=Unsupervised+Machine+Translation+Using+Monolingual+Corpora+Only&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00043?ref=https://githubhelp.com"}, "Learning a Generative Model for Validity in Complex Discrete Structures": {"container_type": "Publication", "bib": {"title": "Learning a generative model for validity in complex discrete structures", "author": ["D Janz", "J van der Westhuizen", "B Paige"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.01664", "author_id": ["rI5XB7sAAAAJ", "-BXuPM4AAAAJ", "JrFJmx0AAAAJ"], "url_scholarbib": "/scholar?q=info:68XnpCJz0EgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ba%2BGenerative%2BModel%2Bfor%2BValidity%2Bin%2BComplex%2BDiscrete%2BStructures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=68XnpCJz0EgJ&ei=wRVkYrGLNZGJmwGY-qmYDQ&json=", "num_citations": 19, "citedby_url": "/scholar?cites=5246820158519363051&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:68XnpCJz0EgJ:scholar.google.com/&scioq=Learning+a+Generative+Model+for+Validity+in+Complex+Discrete+Structures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.01664"}, "Emergent Communication in a Multi-Modal, Multi-Step Referential Game": {"container_type": "Publication", "bib": {"title": "Emergent communication in a multi-modal, multi-step referential game", "author": ["K Evtimova", "A Drozdov", "D Kiela", "K Cho"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.10369", "abstract": "Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct modalities of an object, and their information exchange is bidirectional and of arbitrary duration. The multi-modal multi-step setting allows agents to develop an internal communication significantly closer to natural language, in that they share a single set of messages, and that the length of the conversation may vary according to the difficulty of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.10369", "author_id": ["d4xi2HIAAAAJ", "RsU18o4AAAAJ", "Q0piorUAAAAJ", "0RAmmIAAAAAJ"], "url_scholarbib": "/scholar?q=info:WBLLeEN0V1sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BCommunication%2Bin%2Ba%2BMulti-Modal,%2BMulti-Step%2BReferential%2BGame%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WBLLeEN0V1sJ&ei=xRVkYu63F-HDywSSipaYAg&json=", "num_citations": 96, "citedby_url": "/scholar?cites=6581857213563474520&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WBLLeEN0V1sJ:scholar.google.com/&scioq=Emergent+Communication+in+a+Multi-Modal,+Multi-Step+Referential+Game&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.10369"}, "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem": {"container_type": "Publication", "bib": {"title": "Deep learning as a mixed convex-combinatorial optimization problem", "author": ["AL Friesen", "P Domingos"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11573", "abstract": "As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn networks of them in a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11573", "author_id": ["sfvCNiEAAAAJ", "KOrhfVMAAAAJ"], "url_scholarbib": "/scholar?q=info:DvaymUAJY8MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bas%2Ba%2BMixed%2BConvex-Combinatorial%2BOptimization%2BProblem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DvaymUAJY8MJ&ei=yBVkYveCOKKUy9YP_JONiAY&json=", "num_citations": 17, "citedby_url": "/scholar?cites=14079107033151501838&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DvaymUAJY8MJ:scholar.google.com/&scioq=Deep+Learning+as+a+Mixed+Convex-Combinatorial+Optimization+Problem&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11573"}, "Synthesizing realistic neural population activity patterns using Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Synthesizing realistic neural population activity patterns using generative adversarial networks", "author": ["M Molano-Mazon", "A Onken", "E Piasini"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.00338", "author_id": ["hfA8aYIAAAAJ", "JQh31ekAAAAJ", "dGKi9Q4AAAAJ"], "url_scholarbib": "/scholar?q=info:4NJBw6sUsi0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSynthesizing%2Brealistic%2Bneural%2Bpopulation%2Bactivity%2Bpatterns%2Busing%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4NJBw6sUsi0J&ei=yxVkYuC1I5GJmwGY-qmYDQ&json=", "num_citations": 23, "citedby_url": "/scholar?cites=3292717005509087968&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4NJBw6sUsi0J:scholar.google.com/&scioq=Synthesizing+realistic+neural+population+activity+patterns+using+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.00338"}, "Learning Awareness Models": {"container_type": "Publication", "bib": {"title": "Learning awareness models", "author": ["B Amos", "L Dinh", "S Cabi", "T Roth\u00f6rl"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "these models to reason about the global state st even though no information about this state  is available during training, which we refer to as awareness models we use auxiliary models,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.06318", "author_id": ["d8gdZR4AAAAJ", "h7OHSkoAAAAJ", "l-HhJaUAAAAJ", ""], "url_scholarbib": "/scholar?q=info:LXQa_Qj-MiwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BAwareness%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LXQa_Qj-MiwJ&ei=zxVkYoiDI_mQ6rQP5OqKqAo&json=", "num_citations": 38, "citedby_url": "/scholar?cites=3184887201063924781&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LXQa_Qj-MiwJ:scholar.google.com/&scioq=Learning+Awareness+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.06318"}, "Towards Neural Phrase-based Machine Translation": {"container_type": "Publication", "bib": {"title": "Towards neural phrase-based machine translation", "author": ["PS Huang", "C Wang", "S Huang", "D Zhou"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.05565", "author_id": ["4oJB32YAAAAJ", "vRI2blsAAAAJ", "KduOKeMAAAAJ", "UwLsYw8AAAAJ"], "url_scholarbib": "/scholar?q=info:vN-shLZc8M0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BNeural%2BPhrase-based%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vN-shLZc8M0J&ei=1BVkYp3TNouKmgGY1YjABQ&json=", "num_citations": 86, "citedby_url": "/scholar?cites=14839462711165509564&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vN-shLZc8M0J:scholar.google.com/&scioq=Towards+Neural+Phrase-based+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.05565"}, "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension": {"container_type": "Publication", "bib": {"title": "Qanet: Combining local convolution with global self-attention for reading comprehension", "author": ["AW Yu", "D Dohan", "MT Luong", "R Zhao", "K Chen"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.09541", "author_id": ["-hW6cvgAAAAJ", "iZ5cY0AAAAAJ", "Bmbkv6sAAAAJ", "", "TKvd_Z4AAAAJ"], "url_scholarbib": "/scholar?q=info:ob3bCm54g9oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQANet:%2BCombining%2BLocal%2BConvolution%2Bwith%2BGlobal%2BSelf-Attention%2Bfor%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ob3bCm54g9oJ&ei=4xVkYpGPN--Sy9YPs_mY8AM&json=", "num_citations": 748, "citedby_url": "/scholar?cites=15745561136241294753&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ob3bCm54g9oJ:scholar.google.com/&scioq=QANet:+Combining+Local+Convolution+with+Global+Self-Attention+for+Reading+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.09541.pdf?ref=https://githubhelp.com"}, "Zero-Shot Visual Imitation": {"container_type": "Publication", "bib": {"title": "Zero-shot visual imitation", "author": ["D Pathak", "P Mahmoudieh", "G Luo"], "pub_year": "2018", "venue": "Proceedings of the \u2026", "abstract": "to imitate) after seeing just a sequence of images demonstrating the desired task. Our method  is' zero-shot'in  We evaluate our zero-shot imitator in two real-world settings: complex rope"}, "filled": false, "gsrank": 1, "pub_url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Pathak_Zero-Shot_Visual_Imitation_CVPR_2018_paper.html", "author_id": ["AEsPCAUAAAAJ", "FyQAwaEAAAAJ", ""], "url_scholarbib": "/scholar?q=info:a4Puhm4tAdQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DZero-Shot%2BVisual%2BImitation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a4Puhm4tAdQJ&ei=6BVkYqqRGY2ymgHg1rfQDQ&json=", "num_citations": 205, "citedby_url": "/scholar?cites=15276541363750863723&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a4Puhm4tAdQJ:scholar.google.com/&scioq=Zero-Shot+Visual+Imitation&hl=en&as_sdt=0,33", "eprint_url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w40/Pathak_Zero-Shot_Visual_Imitation_CVPR_2018_paper.pdf"}, "Measuring the Intrinsic Dimension of Objective Landscapes": {"container_type": "Publication", "bib": {"title": "Measuring the intrinsic dimension of objective landscapes", "author": ["C Li", "H Farkhoor", "R Liu", "J Yosinski"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.08838", "abstract": "Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.08838", "author_id": ["Zd7WmXUAAAAJ", "", "_GzrRGwAAAAJ", "gxL1qj8AAAAJ"], "url_scholarbib": "/scholar?q=info:q56SYgmsc-4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2Bthe%2BIntrinsic%2BDimension%2Bof%2BObjective%2BLandscapes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=q56SYgmsc-4J&ei=6xVkYvOwHYuKmgGY1YjABQ&json=", "num_citations": 157, "citedby_url": "/scholar?cites=17182266159657033387&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:q56SYgmsc-4J:scholar.google.com/&scioq=Measuring+the+Intrinsic+Dimension+of+Objective+Landscapes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.08838"}, "Compositional Attention Networks for Machine Reasoning": {"container_type": "Publication", "bib": {"title": "Compositional attention networks for machine reasoning", "author": ["DA Hudson", "CD Manning"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.03067", "abstract": "We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.03067", "author_id": ["q_trRV0AAAAJ", "1zmDOdwAAAAJ"], "url_scholarbib": "/scholar?q=info:AXNrIIon61YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompositional%2BAttention%2BNetworks%2Bfor%2BMachine%2BReasoning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AXNrIIon61YJ&ei=7xVkYtr4DoOEmgHx-5DADA&json=", "num_citations": 396, "citedby_url": "/scholar?cites=6263143180991689473&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AXNrIIon61YJ:scholar.google.com/&scioq=Compositional+Attention+Networks+for+Machine+Reasoning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.03067"}, "Interactive Grounded Language Acquisition and Generalization in a 2D World": {"container_type": "Publication", "bib": {"title": "Interactive grounded language acquisition and generalization in a 2d world", "author": ["H Yu", "H Zhang", "W Xu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.01433", "abstract": "We build a virtual agent for learning language in a 2D maze-like world. The agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher's language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the world, the language, and the action control. By disentangling language grounding from other computational routines and sharing a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.01433", "author_id": ["Army5cEAAAAJ", "_OsT-RgAAAAJ", "Gxz1fqwAAAAJ"], "url_scholarbib": "/scholar?q=info:4EPbZD2hLUEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInteractive%2BGrounded%2BLanguage%2BAcquisition%2Band%2BGeneralization%2Bin%2Ba%2B2D%2BWorld%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4EPbZD2hLUEJ&ei=8hVkYvnOOc6E6rQP5-KmKA&json=", "num_citations": 59, "citedby_url": "/scholar?cites=4696587271474463712&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4EPbZD2hLUEJ:scholar.google.com/&scioq=Interactive+Grounded+Language+Acquisition+and+Generalization+in+a+2D+World&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.01433"}, "Hierarchical Density Order Embeddings": {"container_type": "Publication", "bib": {"title": "Hierarchical density order embeddings", "author": ["B Athiwaratkun", "AG Wilson"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.09843", "abstract": "density order embeddings, which learn hierarchical representations through encapsulation  of probability densities samples to better learn hierarchical density representations. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.09843", "author_id": ["KZpZTTQAAAAJ", "twWX2LIAAAAJ"], "url_scholarbib": "/scholar?q=info:37IOMZ_XeKwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BDensity%2BOrder%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=37IOMZ_XeKwJ&ei=-xVkYp7vKo6pywTd4KPADw&json=", "num_citations": 43, "citedby_url": "/scholar?cites=12427920250451702495&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:37IOMZ_XeKwJ:scholar.google.com/&scioq=Hierarchical+Density+Order+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.09843.pdf,"}, "Gradient Estimators for Implicit Models": {"container_type": "Publication", "bib": {"title": "Gradient estimators for implicit models", "author": ["Y Li", "RE Turner"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.07107", "abstract": "Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.07107", "author_id": ["gcfs8N8AAAAJ", "DgLEyZgAAAAJ"], "url_scholarbib": "/scholar?q=info:sIQ8ANuOagAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient%2BEstimators%2Bfor%2BImplicit%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sIQ8ANuOagAJ&ei=_xVkYpvXBo2ymgHg1rfQDQ&json=", "num_citations": 68, "citedby_url": "/scholar?cites=29993418784277680&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sIQ8ANuOagAJ:scholar.google.com/&scioq=Gradient+Estimators+for+Implicit+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07107"}, "Learn to Pay Attention": {"container_type": "Publication", "bib": {"title": "Learn to pay attention", "author": ["S Jetley", "NA Lord", "N Lee", "PHS Torr"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.02391", "abstract": "attention-weighted features, we propose the use of attention over different spatial resolutions.  The combination of the two factors stated above results in our deploying the attention units"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.02391", "author_id": ["jdWaKDIAAAAJ", "WKmXsyEAAAAJ", "wi9q5T8AAAAJ", "kPxa2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:REroEteML4IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearn%2Bto%2BPay%2BAttention%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=REroEteML4IJ&ei=BRZkYqGiPI6pywTd4KPADw&json=", "num_citations": 319, "citedby_url": "/scholar?cites=9380871404199103044&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:REroEteML4IJ:scholar.google.com/&scioq=Learn+to+Pay+Attention&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.02391.pdf?ref=https://githubhelp.com"}, "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering": {"container_type": "Publication", "bib": {"title": "Evidence aggregation for answer re-ranking in open-domain question answering", "author": ["S Wang", "M Yu", "J Jiang", "W Zhang", "X Guo"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "A popular recent approach to answering open-domain questions is to first search for question-related passages and then apply reading comprehension models to extract answers. Existing methods usually extract answers from single passages independently. But some questions require a combination of evidence from across different sources to answer correctly. In this paper, we propose two models which make use of multiple passages to generate their answers. Both use an answer-reranking approach which reorders the answer"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05116", "author_id": ["mN-IO6wAAAAJ", "vC8DssQAAAAJ", "hVTK2YwAAAAJ", "_b4LTZ8AAAAJ", "DIBOO50AAAAJ"], "url_scholarbib": "/scholar?q=info:PHcmiPWMHlIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvidence%2BAggregation%2Bfor%2BAnswer%2BRe-Ranking%2Bin%2BOpen-Domain%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PHcmiPWMHlIJ&ei=CRZkYvf1NvmQ6rQP5OqKqAo&json=", "num_citations": 156, "citedby_url": "/scholar?cites=5917321946590508860&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PHcmiPWMHlIJ:scholar.google.com/&scioq=Evidence+Aggregation+for+Answer+Re-Ranking+in+Open-Domain+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05116.pdf?ref=https://githubhelp.com"}, "Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs": {"container_type": "Publication", "bib": {"title": "Combining symbolic expressions and black-box function evaluations in neural programs", "author": ["F Arabshahi", "S Singh", "A Anandkumar"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1801.04342", "abstract": "Neural programming involves training neural networks to learn programs, mathematics, or logic from data. Previous works have failed to achieve good generalization performance, especially on problems and programs with high complexity or on large domains. This is because they mostly rely either on black-box function evaluations that do not capture the structure of the program, or on detailed execution traces that are expensive to obtain, and hence the training data has poor coverage of the domain under consideration. We present a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.04342", "author_id": ["5Og5oi4AAAAJ", "-hGZC54AAAAJ", "bEcLezcAAAAJ"], "url_scholarbib": "/scholar?q=info:03Zx7LmKULAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCombining%2BSymbolic%2BExpressions%2Band%2BBlack-box%2BFunction%2BEvaluations%2Bin%2BNeural%2BPrograms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=03Zx7LmKULAJ&ei=DBZkYvGRGeiSy9YPp-OyiAE&json=", "num_citations": 28, "citedby_url": "/scholar?cites=12704807079952611027&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:03Zx7LmKULAJ:scholar.google.com/&scioq=Combining+Symbolic+Expressions+and+Black-box+Function+Evaluations+in+Neural+Programs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.04342"}, "Divide-and-Conquer Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Divide-and-conquer reinforcement learning", "author": ["D Ghosh", "A Singh", "A Rajeswaran", "V Kumar"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "The main contribution of this paper is a reinforcement learning algorithm specifically  We  term this approach as divide-and-conquer (DnC) reinforcement learning. Detailed empirical"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.09874", "author_id": ["znnl0kwAAAAJ", "C2_ZXdcAAAAJ", "_EJrRVAAAAAJ", "nu3W--sAAAAJ"], "url_scholarbib": "/scholar?q=info:Vshg7E_pV3YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDivide-and-Conquer%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Vshg7E_pV3YJ&ei=GBZkYpysFsLZmQHnraWYCA&json=", "num_citations": 85, "citedby_url": "/scholar?cites=8527540948926777430&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Vshg7E_pV3YJ:scholar.google.com/&scioq=Divide-and-Conquer+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.09874"}, "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play": {"container_type": "Publication", "bib": {"title": "Intrinsic motivation and automatic curricula via asymmetric self-play", "author": ["S Sukhbaatar", "Z Lin", "I Kostrikov", "G Synnaeve"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments:(nearly) reversible environments and environments that can be reset. Alice will\" propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.05407", "author_id": ["ri1sE34AAAAJ", "ZDjmMuwAAAAJ", "PTS2AOgAAAAJ", "wN9rBkcAAAAJ"], "url_scholarbib": "/scholar?q=info:RAeCe82IvjkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntrinsic%2BMotivation%2Band%2BAutomatic%2BCurricula%2Bvia%2BAsymmetric%2BSelf-Play%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RAeCe82IvjkJ&ei=HBZkYrX0MJyO6rQP_qe3mAs&json=", "num_citations": 237, "citedby_url": "/scholar?cites=4160913521858709316&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RAeCe82IvjkJ:scholar.google.com/&scioq=Intrinsic+Motivation+and+Automatic+Curricula+via+Asymmetric+Self-Play&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.05407.pdf%20http://arxiv.org/abs/1703.05407"}, "Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs": {"container_type": "Publication", "bib": {"title": "Beyond word importance: Contextual decomposition to extract interactions from lstms", "author": ["WJ Murdoch", "PJ Liu", "B Yu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1801.05453", "abstract": "The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.05453", "author_id": ["3JNHcMwAAAAJ", "1EPxhywAAAAJ", "xT19Jc0AAAAJ"], "url_scholarbib": "/scholar?q=info:-d7gDUyYAIAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BWord%2BImportance:%2B%2BContextual%2BDecomposition%2Bto%2BExtract%2BInteractions%2Bfrom%2BLSTMs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-d7gDUyYAIAJ&ei=HxZkYsCSNs6E6rQP5-KmKA&json=", "num_citations": 159, "citedby_url": "/scholar?cites=9223539489272553209&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-d7gDUyYAIAJ:scholar.google.com/&scioq=Beyond+Word+Importance:++Contextual+Decomposition+to+Extract+Interactions+from+LSTMs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.05453"}, "Quantitatively Evaluating GANs With Divergences Proposed for Training": {"container_type": "Publication", "bib": {"title": "Quantitatively evaluating GANs with divergences proposed for training", "author": ["DJ Im", "H Ma", "G Taylor", "K Branson"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.01045", "abstract": "Generative adversarial networks (GANs) have been extremely effective in approximating complex distributions of high-dimensional, input data samples, and substantial progress has been made in understanding and improving GAN performance in terms of both theory and application. However, we currently lack quantitative methods for model assessment. Because of this, while many GAN variants are being proposed, we have relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.01045", "author_id": ["bzmGSYIAAAAJ", "D7ypICQAAAAJ", "PUeKU8kAAAAJ", "g558OVoAAAAJ"], "url_scholarbib": "/scholar?q=info:JZGBULCk30QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuantitatively%2BEvaluating%2BGANs%2BWith%2BDivergences%2BProposed%2Bfor%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JZGBULCk30QJ&ei=IhZkYu2BKKKUy9YP_JONiAY&json=", "num_citations": 72, "citedby_url": "/scholar?cites=4962866391557443877&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JZGBULCk30QJ:scholar.google.com/&scioq=Quantitatively+Evaluating+GANs+With+Divergences+Proposed+for+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.01045"}, "Gaussian Process Behaviour in Wide Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Gaussian process behaviour in wide deep neural networks", "author": ["AGG Matthews", "M Rowland", "J Hron", "RE Turner"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.11271", "author_id": ["3OFgQKcAAAAJ", "-0U84zMAAAAJ", "Jp7hKlAAAAAJ", "DgLEyZgAAAAJ"], "url_scholarbib": "/scholar?q=info:rB3hYA1Yx8QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGaussian%2BProcess%2BBehaviour%2Bin%2BWide%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rB3hYA1Yx8QJ&ei=LRZkYseGCOHDywSSipaYAg&json=", "num_citations": 180, "citedby_url": "/scholar?cites=14179398766282481068&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rB3hYA1Yx8QJ:scholar.google.com/&scioq=Gaussian+Process+Behaviour+in+Wide+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.11271"}, "Generalizing Across Domains via Cross-Gradient Training": {"container_type": "Publication", "bib": {"title": "Generalizing across domains via cross-gradient training", "author": ["S Shankar", "V Piratla", "S Chakrabarti"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.10745", "author_id": ["yK56jugAAAAJ", "DQddccYAAAAJ", "LfF2zfQAAAAJ"], "url_scholarbib": "/scholar?q=info:kRNfsruZ1DkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizing%2BAcross%2BDomains%2Bvia%2BCross-Gradient%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kRNfsruZ1DkJ&ei=MBZkYsT4N5GJmwGY-qmYDQ&json=", "num_citations": 216, "citedby_url": "/scholar?cites=4167124586655060881&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kRNfsruZ1DkJ:scholar.google.com/&scioq=Generalizing+Across+Domains+via+Cross-Gradient+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.10745"}, "Learning to Teach": {"container_type": "Publication", "bib": {"title": "Learning to teach.", "author": ["H Borko", "RT Putnam"], "pub_year": "1996", "venue": "NA", "abstract": "address... how knowledge & beliefs change over time as novice teachers learn to teach and  experienced teachers attempt to make changes in their teaching practices/selected studies"}, "filled": false, "gsrank": 1, "pub_url": "https://psycnet.apa.org/record/1996-98614-019", "author_id": ["XbKq79sAAAAJ", "WZ-JV3IAAAAJ"], "url_scholarbib": "/scholar?q=info:wmimqC_b7d4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BTeach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wmimqC_b7d4J&ei=PRZkYpL4BeiSy9YPp-OyiAE&json=", "num_citations": 2507, "citedby_url": "/scholar?cites=16063736443639851202&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wmimqC_b7d4J:scholar.google.com/&scioq=Learning+to+Teach&hl=en&as_sdt=0,33"}, "Improving GAN Training via Binarized Representation Entropy (BRE) Regularization": {"container_type": "Publication", "bib": {"title": "Improving GAN training via binarized representation entropy (BRE) regularization", "author": ["Y Cao", "GW Ding", "KYC Lui", "R Huang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.03644", "abstract": "We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.03644", "author_id": ["RTVRTSsAAAAJ", "f7AS33oAAAAJ", "", "-tNbYyQAAAAJ"], "url_scholarbib": "/scholar?q=info:2Rj8wOJ-x8gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGAN%2BTraining%2Bvia%2BBinarized%2BRepresentation%2BEntropy%2B(BRE)%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2Rj8wOJ-x8gJ&ei=QRZkYu-hJvmQ6rQP5OqKqAo&json=", "num_citations": 17, "citedby_url": "/scholar?cites=14467671840316463321&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2Rj8wOJ-x8gJ:scholar.google.com/&scioq=Improving+GAN+Training+via+Binarized+Representation+Entropy+(BRE)+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.03644"}, "Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks": {"container_type": "Publication", "bib": {"title": "Decoding decoders: Finding optimal representation spaces for unsupervised similarity tasks", "author": ["V Zhelezniak", "D Busbridge", "A Shen", "SL Smith"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. We provide a simple yet rigorous explanation for this behaviour by introducing the concept of an optimal representation space, in which semantically close symbols are mapped to representations that are close under a similarity measure induced by the model's objective function. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.03435", "author_id": ["HnnnfW4AAAAJ", "CvA9jjMAAAAJ", "", "fyEqU5oAAAAJ"], "url_scholarbib": "/scholar?q=info:qs0PSWBrP_kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecoding%2BDecoders:%2BFinding%2BOptimal%2BRepresentation%2BSpaces%2Bfor%2BUnsupervised%2BSimilarity%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qs0PSWBrP_kJ&ei=RBZkYtzzLIOEmgHx-5DADA&json=", "num_citations": 7, "citedby_url": "/scholar?cites=17960191900263632298&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qs0PSWBrP_kJ:scholar.google.com/&scioq=Decoding+Decoders:+Finding+Optimal+Representation+Spaces+for+Unsupervised+Similarity+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.03435"}, "Generative networks as inverse problems with Scattering transforms": {"container_type": "Publication", "bib": {"title": "Generative networks as inverse problems with scattering transforms", "author": ["T Angles", "S Mallat"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.06621", "abstract": "Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood. We compute deep convolutional network generators by inverting a fixed embedding operator. Therefore, they do not require to be optimized with a discriminator or an encoder. The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.06621", "author_id": ["RIH68D0AAAAJ", "g_YTmSgAAAAJ"], "url_scholarbib": "/scholar?q=info:6x8O0iYeiSIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2Bnetworks%2Bas%2Binverse%2Bproblems%2Bwith%2BScattering%2Btransforms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6x8O0iYeiSIJ&ei=RxZkYt7EHKKUy9YP_JONiAY&json=", "num_citations": 26, "citedby_url": "/scholar?cites=2488553421180641259&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6x8O0iYeiSIJ:scholar.google.com/&scioq=Generative+networks+as+inverse+problems+with+Scattering+transforms&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.06621"}, "Learning From Noisy Singly-labeled Data": {"container_type": "Publication", "bib": {"title": "Learning from noisy singly-labeled data", "author": ["A Khetan", "ZC Lipton", "A Anandkumar"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.04577", "abstract": "Supervised learning depends on annotated examples, which are taken to be the\\emph {ground truth}. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions:(1) How can we best learn from"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.04577", "author_id": ["AaauqDAAAAAJ", "MN9Kfg8AAAAJ", "bEcLezcAAAAJ"], "url_scholarbib": "/scholar?q=info:NIdGvPQOcRgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFrom%2BNoisy%2BSingly-labeled%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NIdGvPQOcRgJ&ei=TBZkYsKWDpGJmwGY-qmYDQ&json=", "num_citations": 121, "citedby_url": "/scholar?cites=1761205373572122420&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NIdGvPQOcRgJ:scholar.google.com/&scioq=Learning+From+Noisy+Singly-labeled+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.04577"}, "Variational Continual Learning": {"container_type": "Publication", "bib": {"title": "Variational continual learning", "author": ["CV Nguyen", "Y Li", "TD Bui", "RE Turner"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.10628", "abstract": "(VCL), developed in this paper, is an approach in this vein that extends online variational  inference to handle more general continual learning tasks and complex neural network models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10628", "author_id": ["CG9yOXoAAAAJ", "gcfs8N8AAAAJ", "SVq3y1sAAAAJ", "DgLEyZgAAAAJ"], "url_scholarbib": "/scholar?q=info:BOg3XBauKSQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BContinual%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BOg3XBauKSQJ&ei=URZkYuX7NZLeyQTE46-QAg&json=", "num_citations": 447, "citedby_url": "/scholar?cites=2605805270470223876&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BOg3XBauKSQJ:scholar.google.com/&scioq=Variational+Continual+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10628"}, "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples": {"container_type": "Publication", "bib": {"title": "Pixeldefend: Leveraging generative models to understand and defend against adversarial examples", "author": ["Y Song", "T Kim", "S Nowozin", "S Ermon"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10766", "author_id": ["o_J2CroAAAAJ", "7V7yNeoAAAAJ", "7-B7aQkAAAAJ", "ogXTOZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:l95gfWyvpIAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPixelDefend:%2BLeveraging%2BGenerative%2BModels%2Bto%2BUnderstand%2Band%2BDefend%2Bagainst%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=l95gfWyvpIAJ&ei=VBZkYpm7D42ymgHg1rfQDQ&json=", "num_citations": 553, "citedby_url": "/scholar?cites=9269726813530152599&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:l95gfWyvpIAJ:scholar.google.com/&scioq=PixelDefend:+Leveraging+Generative+Models+to+Understand+and+Defend+against+Adversarial+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10766"}, "Divide and Conquer Networks": {"container_type": "Publication", "bib": {"title": "Divide and conquer networks", "author": ["A Nowak", "D Folqu\u00e9", "J Bruna"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "to the principle of divide and conquer, and study what are its implications in terms of learning.   We demonstrate the flexibility and efficiency of the Divideand-Conquer Network on several"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1jscMbAW", "author_id": ["f1IHuhcAAAAJ", "", "L4bNmsMAAAAJ"], "url_scholarbib": "/scholar?q=info:i8wO59jGrQQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDivide%2Band%2BConquer%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i8wO59jGrQQJ&ei=WBZkYsLYNJGJmwGY-qmYDQ&json=", "num_citations": 10, "citedby_url": "/scholar?cites=337144182014397579&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i8wO59jGrQQJ:scholar.google.com/&scioq=Divide+and+Conquer+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1jscMbAW"}, "Communication Algorithms via Deep Learning": {"container_type": "Publication", "bib": {"title": "Communication algorithms via deep learning", "author": ["H Kim", "Y Jiang", "R Rana", "S Kannan", "S Oh"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "discovery of decoding algorithms via deep learning. We study a  algorithms of our times  (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.09317", "author_id": ["uK5wa_EAAAAJ", "0Wb80ScAAAAJ", "", "RrYw5jkAAAAJ", "N6AWeX0AAAAJ"], "url_scholarbib": "/scholar?q=info:HdU6ZBm7-jMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCommunication%2BAlgorithms%2Bvia%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HdU6ZBm7-jMJ&ei=WxZkYrGvHuiSy9YPp-OyiAE&json=", "num_citations": 173, "citedby_url": "/scholar?cites=3745511757842142493&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HdU6ZBm7-jMJ:scholar.google.com/&scioq=Communication+Algorithms+via+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.09317"}, "Natural Language Inference over Interaction Space": {"container_type": "Publication", "bib": {"title": "Natural language inference over interaction space", "author": ["Y Gong", "H Luo", "J Zhang"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1709.04348", "abstract": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.04348", "author_id": ["e3bTLycAAAAJ", "0xIrC1cAAAAJ", ""], "url_scholarbib": "/scholar?q=info:yeI3M8O-OjQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNatural%2BLanguage%2BInference%2Bover%2BInteraction%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yeI3M8O-OjQJ&ei=XhZkYry9AfmQ6rQP5OqKqAo&json=", "num_citations": 240, "citedby_url": "/scholar?cites=3763530184208671433&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yeI3M8O-OjQJ:scholar.google.com/&scioq=Natural+Language+Inference+over+Interaction+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.04348"}, "TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING": {"container_type": "Publication", "bib": {"title": "Truncated horizon policy search: Combining reinforcement learning & imitation learning", "author": ["W Sun", "JA Bagnell", "B Boots"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.11240", "abstract": "In this paper, we propose to combine imitation and reinforcement learning via the idea of reward shaping using an oracle. We study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner's planning horizon as function of its accuracy: a globally optimal oracle can shorten the planning horizon to one, leading to a one-step greedy Markov Decision Process which is much easier to optimize, while an oracle that is far away from the optimality requires"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11240", "author_id": ["iOLC30YAAAAJ", "7t4jbPQAAAAJ", "kXB8FBoAAAAJ"], "url_scholarbib": "/scholar?q=info:vU_LOdSc3QoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTRUNCATED%2BHORIZON%2BPOLICY%2BSEARCH:%2BCOMBINING%2BREINFORCEMENT%2BLEARNING%2B%2526%2BIMITATION%2BLEARNING%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vU_LOdSc3QoJ&ei=YRZkYpyyOI2ymgHg1rfQDQ&json=", "num_citations": 60, "citedby_url": "/scholar?cites=782954345548959677&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vU_LOdSc3QoJ:scholar.google.com/&scioq=TRUNCATED+HORIZON+POLICY+SEARCH:+COMBINING+REINFORCEMENT+LEARNING+%26+IMITATION+LEARNING&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11240"}, "Multi-Mention Learning for Reading Comprehension with Neural Cascades": {"container_type": "Publication", "bib": {"title": "Multi-mention learning for reading comprehension with neural cascades", "author": ["S Swayamdipta", "AP Parikh", "T Kwiatkowski"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal, since by focusing on a specific passage, it becomes difficult to leverage multiple"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00894", "author_id": ["3uTVQt0AAAAJ", "bRpjhycAAAAJ", "MpZ6dTEAAAAJ"], "url_scholarbib": "/scholar?q=info:NOodxNEw5nMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-Mention%2BLearning%2Bfor%2BReading%2BComprehension%2Bwith%2BNeural%2BCascades%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NOodxNEw5nMJ&ei=bBZkYuPtM7KO6rQPy-CRsA8&json=", "num_citations": 29, "citedby_url": "/scholar?cites=8351416236501756468&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NOodxNEw5nMJ:scholar.google.com/&scioq=Multi-Mention+Learning+for+Reading+Comprehension+with+Neural+Cascades&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00894"}, "Active Learning for Convolutional Neural Networks: A Core-Set Approach": {"container_type": "Publication", "bib": {"title": "Active learning for convolutional neural networks: A core-set approach", "author": ["O Sener", "S Savarese"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.00489", "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.00489", "author_id": ["BI8xFr4AAAAJ", "ImpbxLsAAAAJ"], "url_scholarbib": "/scholar?q=info:jw9F4FKR2qUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DActive%2BLearning%2Bfor%2BConvolutional%2BNeural%2BNetworks:%2BA%2BCore-Set%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jw9F4FKR2qUJ&ei=cBZkYqbjAuHDywSSipaYAg&json=", "num_citations": 687, "citedby_url": "/scholar?cites=11951024346317000591&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jw9F4FKR2qUJ:scholar.google.com/&scioq=Active+Learning+for+Convolutional+Neural+Networks:+A+Core-Set+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.00489.pdf),"}, "TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "TD or not TD: Analyzing the role of temporal differencing in deep reinforcement learning", "author": ["A Amiranashvili", "A Dosovitskiy", "V Koltun"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.01175", "author_id": ["BgYdP1sAAAAJ", "FXNJRDoAAAAJ", "kg4bCpgAAAAJ"], "url_scholarbib": "/scholar?q=info:BMfcQ4-FOPAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTD%2Bor%2Bnot%2BTD:%2BAnalyzing%2Bthe%2BRole%2Bof%2BTemporal%2BDifferencing%2Bin%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BMfcQ4-FOPAJ&ei=cxZkYqftEo2ymgHg1rfQDQ&json=", "num_citations": 15, "citedby_url": "/scholar?cites=17309732018163861252&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BMfcQ4-FOPAJ:scholar.google.com/&scioq=TD+or+not+TD:+Analyzing+the+Role+of+Temporal+Differencing+in+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.01175"}, "SCAN: Learning Hierarchical Compositional Visual Concepts": {"container_type": "Publication", "bib": {"title": "Scan: Learning hierarchical compositional visual concepts", "author": ["I Higgins", "N Sonnerat", "L Matthey", "A Pal"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.03389", "author_id": ["YWVuCKUAAAAJ", "MRwi3K8AAAAJ", "f520HmwAAAAJ", ""], "url_scholarbib": "/scholar?q=info:JJCEut1xXIgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSCAN:%2BLearning%2BHierarchical%2BCompositional%2BVisual%2BConcepts%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JJCEut1xXIgJ&ei=dhZkYrTALI6pywTd4KPADw&json=", "num_citations": 86, "citedby_url": "/scholar?cites=9825853684146540580&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JJCEut1xXIgJ:scholar.google.com/&scioq=SCAN:+Learning+Hierarchical+Compositional+Visual+Concepts&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.03389"}, "Multi-Scale Dense Networks for Resource Efficient Image Classification": {"container_type": "Publication", "bib": {"title": "Multi-scale dense networks for resource efficient image classification", "author": ["G Huang", "D Chen", "T Li", "F Wu"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network's prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across\" easier\" and\" harder\" inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.09844", "author_id": ["-P9LwcgAAAAJ", "3Mj2v2oAAAAJ", "AI2f3dkAAAAJ", "sNL8SSoAAAAJ"], "url_scholarbib": "/scholar?q=info:8L5hOCipbHkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-Scale%2BDense%2BNetworks%2Bfor%2BResource%2BEfficient%2BImage%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8L5hOCipbHkJ&ei=eRZkYrfiJMLZmQHnraWYCA&json=", "num_citations": 391, "citedby_url": "/scholar?cites=8749554166283747056&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8L5hOCipbHkJ:scholar.google.com/&scioq=Multi-Scale+Dense+Networks+for+Resource+Efficient+Image+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.09844"}, "An image representation based convolutional network for DNA classification": {"container_type": "Publication", "bib": {"title": "An image representation based convolutional network for DNA classification", "author": ["B Yin", "M Balvert", "D Zambrano", "A Sch\u00f6nhuth"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that takes an image-representation of primary DNA sequence as its input, and predicts key determinants of chromatin structure. The method is developed such that it is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.04931", "author_id": ["5fpuCPYAAAAJ", "Sc9AMMgAAAAJ", "WVWngZYAAAAJ", "pEtR1C4AAAAJ"], "url_scholarbib": "/scholar?q=info:8jFTPcOghkEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2Bimage%2Brepresentation%2Bbased%2Bconvolutional%2Bnetwork%2Bfor%2BDNA%2Bclassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8jFTPcOghkEJ&ei=fBZkYrW0F4ySyASZk6HgCA&json=", "num_citations": 25, "citedby_url": "/scholar?cites=4721638019752473074&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8jFTPcOghkEJ:scholar.google.com/&scioq=An+image+representation+based+convolutional+network+for+DNA+classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.04931"}, "Variational Message Passing with Structured Inference Networks": {"container_type": "Publication", "bib": {"title": "Variational message passing with structured inference networks", "author": ["W Lin", "N Hubacher", "ME Khan"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.05589", "abstract": "Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.05589", "author_id": ["sGl6muoAAAAJ", "", "Yv6wq2kAAAAJ"], "url_scholarbib": "/scholar?q=info:AMein3XudEIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BMessage%2BPassing%2Bwith%2BStructured%2BInference%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AMein3XudEIJ&ei=fhZkYturN86E6rQP5-KmKA&json=", "num_citations": 38, "citedby_url": "/scholar?cites=4788714492758509312&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AMein3XudEIJ:scholar.google.com/&scioq=Variational+Message+Passing+with+Structured+Inference+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.05589"}, "Certifying Some Distributional Robustness with Principled Adversarial Training": {"container_type": "Publication", "bib": {"title": "Certifying some distributional robustness with principled adversarial training", "author": ["A Sinha", "H Namkoong", "R Volpi", "J Duchi"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.10571", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10571", "author_id": ["ZCa4VDcAAAAJ", "dyXX1EgAAAAJ", "YkeS_SoAAAAJ", "i5srt20AAAAJ"], "url_scholarbib": "/scholar?q=info:2RzSQTpOZEwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCertifying%2BSome%2BDistributional%2BRobustness%2Bwith%2BPrincipled%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2RzSQTpOZEwJ&ei=ghZkYtnNLpyO6rQP_qe3mAs&json=", "num_citations": 540, "citedby_url": "/scholar?cites=5504610656672947417&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2RzSQTpOZEwJ:scholar.google.com/&scioq=Certifying+Some+Distributional+Robustness+with+Principled+Adversarial+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10571.pdf]"}, "Weightless: Lossy Weight Encoding For Deep Neural Network Compression": {"container_type": "Publication", "bib": {"title": "Weightless: Lossy weight encoding for deep neural network compression", "author": ["B Reagan", "U Gupta", "B Adolf"], "pub_year": "2018", "venue": "International \u2026", "abstract": "The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random"}, "filled": false, "gsrank": 1, "pub_url": "http://proceedings.mlr.press/v80/reagan18a.html?ref=https://githubhelp.com", "author_id": ["cO2uYoAAAAAJ", "f_q71XMAAAAJ", "oWFqet4AAAAJ"], "url_scholarbib": "/scholar?q=info:MWY8-7dtcNcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWeightless:%2BLossy%2BWeight%2BEncoding%2BFor%2BDeep%2BNeural%2BNetwork%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MWY8-7dtcNcJ&ei=hRZkYrS9POHDywSSipaYAg&json=", "num_citations": 34, "citedby_url": "/scholar?cites=15524028552507582001&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MWY8-7dtcNcJ:scholar.google.com/&scioq=Weightless:+Lossy+Weight+Encoding+For+Deep+Neural+Network+Compression&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/reagan18a/reagan18a.pdf"}, "The Kanerva Machine: A Generative Distributed Memory": {"container_type": "Publication", "bib": {"title": "The kanerva machine: A generative distributed memory", "author": ["Y Wu", "G Wayne", "A Graves", "T Lillicrap"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.01756", "abstract": "We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.01756", "author_id": ["vYmSd0UAAAAJ", "", "DaFHynwAAAAJ", "htPVdRMAAAAJ"], "url_scholarbib": "/scholar?q=info:w5lYMSMqOokJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BKanerva%2BMachine:%2BA%2BGenerative%2BDistributed%2BMemory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w5lYMSMqOokJ&ei=iBZkYsGFGI2ymgHg1rfQDQ&json=", "num_citations": 38, "citedby_url": "/scholar?cites=9888262262485457347&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w5lYMSMqOokJ:scholar.google.com/&scioq=The+Kanerva+Machine:+A+Generative+Distributed+Memory&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.01756"}, "On the importance of single directions for generalization": {"container_type": "Publication", "bib": {"title": "On the importance of single directions for generalization", "author": ["AS Morcos", "DGT Barrett", "NC Rabinowitz"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Here, we provide a rough intuition for why a network\u2019s reliance upon single directions might  be related to generalization performance. Consider two networks trained on a large, labeled"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.06959", "author_id": ["v-A_7UsAAAAJ", "Whh_d2EAAAAJ", "AgUYQMwAAAAJ"], "url_scholarbib": "/scholar?q=info:SWFZem_TrTMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2Bimportance%2Bof%2Bsingle%2Bdirections%2Bfor%2Bgeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SWFZem_TrTMJ&ei=ixZkYp7LKvmQ6rQP5OqKqAo&json=", "num_citations": 237, "citedby_url": "/scholar?cites=3723864942652776777&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SWFZem_TrTMJ:scholar.google.com/&scioq=On+the+importance+of+single+directions+for+generalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.06959.pdf?ref=https://githubhelp.com"}, "Simulating Action Dynamics with Neural Process Networks": {"container_type": "Publication", "bib": {"title": "Simulating action dynamics with neural process networks", "author": ["A Bosselut", "O Levy", "A Holtzman", "C Ennis", "D Fox"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics. Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05313", "author_id": ["tK9K1D4AAAAJ", "PZVd2h8AAAAJ", "8veBOSIAAAAJ", "", "DqXsbPAAAAAJ"], "url_scholarbib": "/scholar?q=info:grAGtD8o2NYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimulating%2BAction%2BDynamics%2Bwith%2BNeural%2BProcess%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=grAGtD8o2NYJ&ei=jhZkYqO0DO-Sy9YPs_mY8AM&json=", "num_citations": 79, "citedby_url": "/scholar?cites=15481167973154467970&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:grAGtD8o2NYJ:scholar.google.com/&scioq=Simulating+Action+Dynamics+with+Neural+Process+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05313"}, "Reinforcement Learning Algorithm Selection": {"container_type": "Publication", "bib": {"title": "Reinforcement learning algorithm selection", "author": ["R Laroche", "R Feraud"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.08810", "abstract": "online algorithm selection in the context of Reinforcement Learning. The setup is as follows:  given an episodic task and a finite number of off-policy RL algorithms, a meta-algorithm has"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.08810", "author_id": ["RiIOKJMAAAAJ", "4YwuLUMAAAAJ"], "url_scholarbib": "/scholar?q=info:QveU4NXWzaYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforcement%2BLearning%2BAlgorithm%2BSelection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QveU4NXWzaYJ&ei=lRZkYvScAsLZmQHnraWYCA&json=", "num_citations": 20, "citedby_url": "/scholar?cites=12019499194605958978&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QveU4NXWzaYJ:scholar.google.com/&scioq=Reinforcement+Learning+Algorithm+Selection&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.08810"}, "Feature Incay for Representation Regularization": {"container_type": "Publication", "bib": {"title": "Feature incay for representation regularization", "author": ["Y Yuan", "K Yang", "C Zhang"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.10284", "abstract": "Softmax loss is widely used in deep neural networks for multi-class classification, where each class is represented by a weight vector, a sample is represented as a feature vector, and the feature vector has the largest projection on the weight vector of the correct category when the model correctly classifies a sample. To ensure generalization, weight decay that shrinks the weight norm is often used as regularizer. Different from traditional learning algorithms where features are fixed and only weights are tunable, features are also tunable"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.10284", "author_id": ["PzyvzksAAAAJ", "g2gAY_0AAAAJ", "NeCCx-kAAAAJ"], "url_scholarbib": "/scholar?q=info:jVT-eaDhsU0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeature%2BIncay%2Bfor%2BRepresentation%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jVT-eaDhsU0J&ei=mhZkYqeGIOHDywSSipaYAg&json=", "num_citations": 9, "citedby_url": "/scholar?cites=5598503891155965069&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jVT-eaDhsU0J:scholar.google.com/&scioq=Feature+Incay+for+Representation+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.10284"}, "Distributed Fine-tuning of Language Models on Private Data": {"container_type": "Publication", "bib": {"title": "Distributed fine-tuning of language models on private data", "author": ["V Popov", "M Kudinov", "I Piontkovskaya"], "pub_year": "2018", "venue": "International \u2026", "abstract": "One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm. In language modeling, users' language (eg in private messaging) could change in a year and be completely different from what we observe in publicly available data. At the same time, public data can be used for obtaining general knowledge (ie general model of English). We study approaches to distributed fine-tuning of a general model on user private data with the additional"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkgNdt26Z", "author_id": ["", "", ""], "url_scholarbib": "/scholar?q=info:xMHNKKs6YiIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BFine-tuning%2Bof%2BLanguage%2BModels%2Bon%2BPrivate%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xMHNKKs6YiIJ&ei=nRZkYquFJouKmgGY1YjABQ&json=", "num_citations": 13, "citedby_url": "/scholar?cites=2477607251805585860&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xMHNKKs6YiIJ:scholar.google.com/&scioq=Distributed+Fine-tuning+of+Language+Models+on+Private+Data&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkgNdt26Z"}, "Parametrized Hierarchical Procedures for Neural Programming": {"container_type": "Publication", "bib": {"title": "Parametrized hierarchical procedures for neural programming", "author": ["R Fox", "R Shin", "S Krishnan", "K Goldberg", "D Song"], "pub_year": "2018", "venue": "ICLR 2018", "abstract": "Neural programs are highly accurate and structured policies that perform algorith-mic tasks by controlling the behavior of a computation mechanism. Despite the potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic do-mains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/biblio/10063833", "author_id": ["FH9nKOAAAAAJ", "xPnkc80AAAAJ", "Yxh9WWoAAAAJ", "8fztli4AAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:GSl_k0ILo5kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParametrized%2BHierarchical%2BProcedures%2Bfor%2BNeural%2BProgramming%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GSl_k0ILo5kJ&ei=oRZkYu6AHpGJmwGY-qmYDQ&json=", "num_citations": 22, "citedby_url": "/scholar?cites=11070704689577142553&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GSl_k0ILo5kJ:scholar.google.com/&scioq=Parametrized+Hierarchical+Procedures+for+Neural+Programming&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10063833"}, "Learning Sparse Latent Representations with the Deep Copula Information Bottleneck": {"container_type": "Publication", "bib": {"title": "Learning sparse latent representations with the deep copula information bottleneck", "author": ["A Wieczorek", "M Wieser", "D Murezzan", "V Roth"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.06216", "author_id": ["U8SgEVUAAAAJ", "", "UGfBiYEAAAAJ", "v1qj03cAAAAJ"], "url_scholarbib": "/scholar?q=info:ca_zqkb_Zy8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BSparse%2BLatent%2BRepresentations%2Bwith%2Bthe%2BDeep%2BCopula%2BInformation%2BBottleneck%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ca_zqkb_Zy8J&ei=pBZkYpWpB5qSy9YP8pKNsAE&json=", "num_citations": 24, "citedby_url": "/scholar?cites=3415979521364701041&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ca_zqkb_Zy8J:scholar.google.com/&scioq=Learning+Sparse+Latent+Representations+with+the+Deep+Copula+Information+Bottleneck&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.06216"}, "Tree-to-tree Neural Networks for Program Translation": {"container_type": "Publication", "bib": {"title": "Tree-to-tree neural networks for program translation", "author": ["X Chen", "C Liu", "D Song"], "pub_year": "2018", "venue": "Advances in neural information \u2026", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.neurips.cc/paper/2018/hash/d759175de8ea5b1d9a2660e45554894f-Abstract.html", "author_id": ["d4W1UT0AAAAJ", "Zrbs8hIAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:jxj7xYmAdkYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTree-to-tree%2BNeural%2BNetworks%2Bfor%2BProgram%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jxj7xYmAdkYJ&ei=pxZkYrCnCo6pywTd4KPADw&json=", "num_citations": 159, "citedby_url": "/scholar?cites=5077386959127255183&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jxj7xYmAdkYJ:scholar.google.com/&scioq=Tree-to-tree+Neural+Networks+for+Program+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://proceedings.neurips.cc/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf"}, "Towards Synthesizing Complex Programs From Input-Output Examples": {"container_type": "Publication", "bib": {"title": "Towards synthesizing complex programs from input-output examples", "author": ["X Chen", "C Liu", "D Song"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.01284", "abstract": "In recent years, deep learning techniques have been developed to improve the performance of program synthesis from input-output examples. Albeit its significant progress, the programs that can be synthesized by state-of-the-art approaches are still simple in terms of their complexity. In this work, we move a significant step forward along this direction by proposing a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.01284", "author_id": ["d4W1UT0AAAAJ", "Zrbs8hIAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:WtIICjI8fcgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BSynthesizing%2BComplex%2BPrograms%2BFrom%2BInput-Output%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WtIICjI8fcgJ&ei=qxZkYsapKO-Sy9YPs_mY8AM&json=", "num_citations": 38, "citedby_url": "/scholar?cites=14446769365288800858&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WtIICjI8fcgJ:scholar.google.com/&scioq=Towards+Synthesizing+Complex+Programs+From+Input-Output+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.01284"}, "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data": {"container_type": "Publication", "bib": {"title": "SGD learns over-parameterized networks that provably generalize on linearly separable data", "author": ["A Brutzkus", "A Globerson", "E Malach"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.10174", "author_id": ["m1wmXdgAAAAJ", "5JserkUAAAAJ", "I15dUOwAAAAJ"], "url_scholarbib": "/scholar?q=info:_SQuKwAzWUgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSGD%2BLearns%2BOver-parameterized%2BNetworks%2Bthat%2BProvably%2BGeneralize%2Bon%2BLinearly%2BSeparable%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_SQuKwAzWUgJ&ei=rhZkYqGqLuHDywSSipaYAg&json=", "num_citations": 207, "citedby_url": "/scholar?cites=5213254119475520765&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_SQuKwAzWUgJ:scholar.google.com/&scioq=SGD+Learns+Over-parameterized+Networks+that+Provably+Generalize+on+Linearly+Separable+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.10174"}, "Universal Agent for Disentangling Environments and Tasks": {"container_type": "Publication", "bib": {"title": "Universal agent for disentangling environments and tasks", "author": ["J Mao", "H Dong", "JJ Lim"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task. Hence, both environment and task specific knowledge are entangled into one framework. However, there are often scenarios where the environment (eg the physical world) is fixed while only the target task changes. Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units. The"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1mvVm-C-", "author_id": ["-xaOIZIAAAAJ", "MrGN4oMAAAAJ", "jTnQTBoAAAAJ"], "url_scholarbib": "/scholar?q=info:LRwrZZEbfdgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BAgent%2Bfor%2BDisentangling%2BEnvironments%2Band%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LRwrZZEbfdgJ&ei=sRZkYvbvBcLZmQHnraWYCA&json=", "num_citations": 5, "citedby_url": "/scholar?cites=15599654995562798125&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LRwrZZEbfdgJ:scholar.google.com/&scioq=Universal+Agent+for+Disentangling+Environments+and+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1mvVm-C-"}, "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy": {"container_type": "Publication", "bib": {"title": "Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy", "author": ["A Mishra", "D Marr"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.05852", "abstract": "Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems-the models (often deep networks or wide networks or both) are compute and memory intensive. Low-precision numerics and model compression using knowledge distillation are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05852", "author_id": ["SUh3W84AAAAJ", ""], "url_scholarbib": "/scholar?q=info:yID2JVb7-z0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DApprentice:%2BUsing%2BKnowledge%2BDistillation%2BTechniques%2BTo%2BImprove%2BLow-Precision%2BNetwork%2BAccuracy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yID2JVb7-z0J&ei=tRZkYsOpIKKUy9YP_JONiAY&json=", "num_citations": 249, "citedby_url": "/scholar?cites=4466439802890649800&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yID2JVb7-z0J:scholar.google.com/&scioq=Apprentice:+Using+Knowledge+Distillation+Techniques+To+Improve+Low-Precision+Network+Accuracy&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05852?ref=https://githubhelp.com"}, "Semi-parametric topological memory for navigation": {"container_type": "Publication", "bib": {"title": "Semi-parametric topological memory for navigation", "author": ["N Savinov", "A Dosovitskiy", "V Koltun"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.00653", "abstract": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semi-parametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.00653", "author_id": ["qUIOyQYAAAAJ", "FXNJRDoAAAAJ", "kg4bCpgAAAAJ"], "url_scholarbib": "/scholar?q=info:PqguZ-fn8ZUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-parametric%2Btopological%2Bmemory%2Bfor%2Bnavigation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PqguZ-fn8ZUJ&ei=uhZkYpiLCs6E6rQP5-KmKA&json=", "num_citations": 242, "citedby_url": "/scholar?cites=10804671962093103166&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PqguZ-fn8ZUJ:scholar.google.com/&scioq=Semi-parametric+topological+memory+for+navigation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.00653.pdf?ref=https://githubhelp.com"}, "Automatically Inferring Data Quality for Spatiotemporal Forecasting": {"container_type": "Publication", "bib": {"title": "Automatically inferring data quality for spatiotemporal forecasting", "author": ["S Seo", "A Mohegh", "G Ban-Weiss", "Y Liu"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Spatiotemporal forecasting has become an increasingly important prediction task in machine learning and statistics due to its vast applications, such as climate modeling, traffic prediction, video caching predictions, and so on. While numerous studies have been conducted, most existing works assume that the data from different sources or across different locations are equally reliable. Due to cost, accessibility, or other factors, it is inevitable that the data quality could vary, which introduces significant biases into the model"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByJIWUnpW", "author_id": ["spYH0tEAAAAJ", "xnAoPpAAAAAJ", "KMuPRZYAAAAJ", "UUKLPMYAAAAJ"], "url_scholarbib": "/scholar?q=info:68-xPC5Y4KYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomatically%2BInferring%2BData%2BQuality%2Bfor%2BSpatiotemporal%2BForecasting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=68-xPC5Y4KYJ&ei=vxZkYsjSC46pywTd4KPADw&json=", "num_citations": 6, "citedby_url": "/scholar?cites=12024707960689250283&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:68-xPC5Y4KYJ:scholar.google.com/&scioq=Automatically+Inferring+Data+Quality+for+Spatiotemporal+Forecasting&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByJIWUnpW"}, "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Hierarchical and interpretable skill acquisition in multi-task reinforcement learning", "author": ["T Shu", "C Xiong", "R Socher"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.07294", "abstract": "Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.07294", "author_id": ["YT_ffdwAAAAJ", "vaSdahkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:TF2-Ro-jkWcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2Band%2BInterpretable%2BSkill%2BAcquisition%2Bin%2BMulti-task%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TF2-Ro-jkWcJ&ei=whZkYrOFOuiSy9YPp-OyiAE&json=", "num_citations": 93, "citedby_url": "/scholar?cites=7462925893292154188&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TF2-Ro-jkWcJ:scholar.google.com/&scioq=Hierarchical+and+Interpretable+Skill+Acquisition+in+Multi-task+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.07294"}, "Learning to Multi-Task by Active Sampling": {"container_type": "Publication", "bib": {"title": "Learning to multi-task by active sampling", "author": ["S Sharma", "A Jha", "P Hegde", "B Ravindran"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.06053", "abstract": "One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.06053", "author_id": ["P6l5tBcAAAAJ", "IhhuIZcAAAAJ", "g8mEfXQAAAAJ", "nGUcGrYAAAAJ"], "url_scholarbib": "/scholar?q=info:mI_kOWFfOG4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BMulti-Task%2Bby%2BActive%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mI_kOWFfOG4J&ei=zxVkYo6oOY2ymgHg1rfQDQ&json=", "num_citations": 29, "citedby_url": "/scholar?cites=7942202814055616408&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mI_kOWFfOG4J:scholar.google.com/&scioq=Learning+to+Multi-Task+by+Active+Sampling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.06053"}, "Consequentialist conditional cooperation in social dilemmas with imperfect information": {"container_type": "Publication", "bib": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "author": ["A Peysakhovich", "A Lerer"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.06975", "abstract": "have introduced consequentialist conditionally cooperative strategies and shown that they  are useful heuristics in social dilemmas, even in those where information is imperfect either"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.06975", "author_id": ["zwLePrsAAAAJ", "Ad6O4-0AAAAJ"], "url_scholarbib": "/scholar?q=info:7fgyxa9ysEwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConsequentialist%2Bconditional%2Bcooperation%2Bin%2Bsocial%2Bdilemmas%2Bwith%2Bimperfect%2Binformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7fgyxa9ysEwJ&ei=0hVkYq72O4yuyAT-mrWwCA&json=", "num_citations": 41, "citedby_url": "/scholar?cites=5526042842036893933&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7fgyxa9ysEwJ:scholar.google.com/&scioq=Consequentialist+conditional+cooperation+in+social+dilemmas+with+imperfect+information&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.06975"}, "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis": {"container_type": "Publication", "bib": {"title": "Leveraging grammar and reinforcement learning for neural program synthesis", "author": ["R Bunel", "M Hausknecht", "J Devlin", "R Singh"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.04276", "author_id": ["7cqQFSoAAAAJ", "lutJce0AAAAJ", "", "5kVcNS4AAAAJ"], "url_scholarbib": "/scholar?q=info:o9KIwViEDMcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLeveraging%2BGrammar%2Band%2BReinforcement%2BLearning%2Bfor%2BNeural%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o9KIwViEDMcJ&ei=2BVkYsr9CIuKmgGY1YjABQ&json=", "num_citations": 133, "citedby_url": "/scholar?cites=14342984430007145123&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o9KIwViEDMcJ:scholar.google.com/&scioq=Leveraging+Grammar+and+Reinforcement+Learning+for+Neural+Program+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.04276"}, "Stochastic Activation Pruning for Robust Adversarial Defense": {"container_type": "Publication", "bib": {"title": "Stochastic activation pruning for robust adversarial defense", "author": ["GS Dhillon", "K Azizzadenesheli", "ZC Lipton"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural networks are known to be vulnerable to adversarial examples. Carefully chosen perturbations to real images, while imperceptible to humans, induce misclassification and threaten the reliability of deep learning systems in the wild. To guard against adversarial examples, we take inspiration from game theory and cast the problem as a minimax zero-sum game between the adversary and the model. In general, for such games, the optimal strategy for both players requires a stochastic policy, also known as a mixed strategy. In this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.01442", "author_id": ["m68VEtIAAAAJ", "Odek140AAAAJ", "MN9Kfg8AAAAJ"], "url_scholarbib": "/scholar?q=info:ndKDT5cnVdIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BActivation%2BPruning%2Bfor%2BRobust%2BAdversarial%2BDefense%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ndKDT5cnVdIJ&ei=2xVkYufkLZqSy9YP8pKNsAE&json=", "num_citations": 418, "citedby_url": "/scholar?cites=15156063651812856477&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ndKDT5cnVdIJ:scholar.google.com/&scioq=Stochastic+Activation+Pruning+for+Robust+Adversarial+Defense&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.01442.pdf?ref=https://githubhelp.com"}, "Temporal Difference Models: Model-Free Deep RL for Model-Based Control": {"container_type": "Publication", "bib": {"title": "Temporal difference models: Model-free deep rl for model-based control", "author": ["V Pong", "S Gu", "M Dalal", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.09081", "abstract": "Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.09081", "author_id": ["BIwrJuQAAAAJ", "B8wslVsAAAAJ", "5dBp2f4AAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:O4-mYlsQsD0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTemporal%2BDifference%2BModels:%2BModel-Free%2BDeep%2BRL%2Bfor%2BModel-Based%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O4-mYlsQsD0J&ei=3xVkYsO1DKKUy9YP_JONiAY&json=", "num_citations": 188, "citedby_url": "/scholar?cites=4445070816897830715&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:O4-mYlsQsD0J:scholar.google.com/&scioq=Temporal+Difference+Models:+Model-Free+Deep+RL+for+Model-Based+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.09081"}, "On Unifying Deep Generative Models": {"container_type": "Publication", "bib": {"title": "On unifying deep generative models", "author": ["Z Hu", "Z Yang", "R Salakhutdinov", "EP Xing"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1706.00550", "abstract": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.00550", "author_id": ["N7_xhHoAAAAJ", "siCYLcUAAAAJ", "ITZ1e7MAAAAJ", "5pKTRxEAAAAJ"], "url_scholarbib": "/scholar?q=info:03ckNp-rNikJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BUnifying%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=03ckNp-rNikJ&ei=4xVkYom5CJqSy9YP8pKNsAE&json=", "num_citations": 121, "citedby_url": "/scholar?cites=2969749704593930195&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:03ckNp-rNikJ:scholar.google.com/&scioq=On+Unifying+Deep+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.00550.pdf?source=post_page---------------------------"}, "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization": {"container_type": "Publication", "bib": {"title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization", "author": ["CJ Cueva", "XX Wei"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.07770", "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.07770", "author_id": ["BxLgolsAAAAJ", "7Pd1QzwAAAAJ"], "url_scholarbib": "/scholar?q=info:21Ay_6wwjvkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergence%2Bof%2Bgrid-like%2Brepresentations%2Bby%2Btraining%2Brecurrent%2Bneural%2Bnetworks%2Bto%2Bperform%2Bspatial%2Blocalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=21Ay_6wwjvkJ&ei=5hVkYt6JJZyO6rQP_qe3mAs&json=", "num_citations": 119, "citedby_url": "/scholar?cites=17982363881710964955&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:21Ay_6wwjvkJ:scholar.google.com/&scioq=Emergence+of+grid-like+representations+by+training+recurrent+neural+networks+to+perform+spatial+localization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.07770"}, "Emergence of Linguistic Communication from  Referential Games with Symbolic and Pixel Input": {"container_type": "Publication", "bib": {"title": "Emergence of linguistic communication from referential games with symbolic and pixel input", "author": ["A Lazaridou", "KM Hermann", "K Tuyls", "S Clark"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.03984", "author_id": ["BMgUIC0AAAAJ", "WmH1GQoAAAAJ", "cxRqeVwAAAAJ", "bBnvK8cAAAAJ"], "url_scholarbib": "/scholar?q=info:yY2jTPGoWbAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergence%2Bof%2BLinguistic%2BCommunication%2Bfrom%2B%2BReferential%2BGames%2Bwith%2BSymbolic%2Band%2BPixel%2BInput%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yY2jTPGoWbAJ&ei=6hVkYt-KAY2ymgHg1rfQDQ&json=", "num_citations": 149, "citedby_url": "/scholar?cites=12707373577928936905&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yY2jTPGoWbAJ:scholar.google.com/&scioq=Emergence+of+Linguistic+Communication+from++Referential+Games+with+Symbolic+and+Pixel+Input&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.03984"}, "Auto-Encoding Sequential Monte Carlo": {"container_type": "Publication", "bib": {"title": "Auto-encoding sequential monte carlo", "author": ["TA Le", "M Igl", "T Rainforth", "T Jin", "F Wood"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.10306", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.10306", "author_id": ["tkceMM0AAAAJ", "rFcdDJEAAAAJ", "ieLRNKMAAAAJ", "", "d4yNzXIAAAAJ"], "url_scholarbib": "/scholar?q=info:iecH8J2yWRYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAuto-Encoding%2BSequential%2BMonte%2BCarlo%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iecH8J2yWRYJ&ei=7RVkYpfiHe-Sy9YPs_mY8AM&json=", "num_citations": 130, "citedby_url": "/scholar?cites=1610514733168322441&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iecH8J2yWRYJ:scholar.google.com/&scioq=Auto-Encoding+Sequential+Monte+Carlo&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.10306"}, "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes": {"container_type": "Publication", "bib": {"title": "Recasting gradient-based meta-learning as hierarchical bayes", "author": ["E Grant", "C Finn", "S Levine", "T Darrell"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al.(2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.08930", "author_id": ["OSg3D9MAAAAJ", "vfPE6hgAAAAJ", "8R35rCwAAAAJ", "bh-uRFMAAAAJ"], "url_scholarbib": "/scholar?q=info:TtWLVht85U0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecasting%2BGradient-Based%2BMeta-Learning%2Bas%2BHierarchical%2BBayes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TtWLVht85U0J&ei=8hVkYsVNwtmZAeetpZgI&json=", "num_citations": 380, "citedby_url": "/scholar?cites=5613028967445157198&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TtWLVht85U0J:scholar.google.com/&scioq=Recasting+Gradient-Based+Meta-Learning+as+Hierarchical+Bayes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.08930"}, "Policy Optimization by Genetic Distillation ": {"container_type": "Publication", "bib": {"title": "Policy optimization by genetic distillation", "author": ["T Gangwani", "J Peng"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.01012", "abstract": "Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.01012", "author_id": ["IUY5oVkAAAAJ", "H2JX-RQAAAAJ"], "url_scholarbib": "/scholar?q=info:xfWj6gFULdwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPolicy%2BOptimization%2Bby%2BGenetic%2BDistillation%2B%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xfWj6gFULdwJ&ei=9RVkYtz2E46pywTd4KPADw&json=", "num_citations": 27, "citedby_url": "/scholar?cites=15865429429504439749&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xfWj6gFULdwJ:scholar.google.com/&scioq=Policy+Optimization+by+Genetic+Distillation+&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01012"}, "Active Neural Localization": {"container_type": "Publication", "bib": {"title": "Active neural localization", "author": ["DS Chaplot", "E Parisotto", "R Salakhutdinov"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "\u201cActive Neural Localizer\u201d, a fully differentiable neural network  of traditional filtering-based  localization methods, by using a  minimizing the number of steps required for localization."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1801.08214", "author_id": ["1MSpdmQAAAAJ", "-GduGkcAAAAJ", "ITZ1e7MAAAAJ"], "url_scholarbib": "/scholar?q=info:3dItliKSSdYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DActive%2BNeural%2BLocalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3dItliKSSdYJ&ei=-BVkYsSJO46pywTd4KPADw&json=", "num_citations": 72, "citedby_url": "/scholar?cites=15441033474662585053&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3dItliKSSdYJ:scholar.google.com/&scioq=Active+Neural+Localization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1801.08214"}, "The power of deeper networks for expressing natural functions": {"container_type": "Publication", "bib": {"title": "The power of deeper networks for expressing natural functions", "author": ["D Rolnick", "M Tegmark"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.05502", "abstract": "It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons $ m $ required to approximate natural classes of multivariate polynomials of $ n $ variables grows only linearly with $ n $ for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from $1 $ to $ k $, the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.05502", "author_id": ["P_luG3cAAAAJ", "eBXEZxgAAAAJ"], "url_scholarbib": "/scholar?q=info:cP_RrORXGpwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2Bpower%2Bof%2Bdeeper%2Bnetworks%2Bfor%2Bexpressing%2Bnatural%2Bfunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cP_RrORXGpwJ&ei=BRZkYpLEApGJmwGY-qmYDQ&json=", "num_citations": 141, "citedby_url": "/scholar?cites=11248399658974838640&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cP_RrORXGpwJ:scholar.google.com/&scioq=The+power+of+deeper+networks+for+expressing+natural+functions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.05502"}, "Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis": {"container_type": "Publication", "bib": {"title": "Auto-conditioned recurrent networks for extended complex human motion synthesis", "author": ["Z Li", "Y Zhou", "S Xiao", "C He", "Z Huang", "H Li"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a real-time method for synthesizing highly complex human motions using a novel training regime we call the auto-conditioned Recurrent Neural Network (acRNN). Recently, researchers have attempted to synthesize new motion by using autoregressive techniques, but existing methods tend to freeze or diverge after a couple of seconds due to an accumulation of errors that are fed back into the network. Furthermore, such methods have only been shown to be reliable for relatively simple human motions, such as walking or"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.05363", "author_id": ["118OteUAAAAJ", "a8P5CVIAAAAJ", "", "", "B3FqpjQAAAAJ", "NFeigSoAAAAJ"], "url_scholarbib": "/scholar?q=info:y70QjK9T8EoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAuto-Conditioned%2BRecurrent%2BNetworks%2Bfor%2BExtended%2BComplex%2BHuman%2BMotion%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y70QjK9T8EoJ&ei=DBZkYo3QLu-Sy9YPs_mY8AM&json=", "num_citations": 147, "citedby_url": "/scholar?cites=5399907966651514315&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y70QjK9T8EoJ:scholar.google.com/&scioq=Auto-Conditioned+Recurrent+Networks+for+Extended+Complex+Human+Motion+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.05363.pdf?ref=https://githubhelp.com"}, "A Hierarchical Model for Device Placement": {"container_type": "Publication", "bib": {"title": "A hierarchical model for device placement", "author": ["A Mirhoseini", "A Goldie", "H Pham", "B Steiner"], "pub_year": "2018", "venue": "International \u2026", "abstract": "a hierarchical model for efficient placement of computational graphs onto hardware devices,   of CPUs, GPUs, and other computational devices. Our method learns to assign graph"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hkc-TeZ0W", "author_id": ["PjEHjPEAAAAJ", "uyFDSDwAAAAJ", "GpcGdRkAAAAJ", "rT11mdcAAAAJ"], "url_scholarbib": "/scholar?q=info:4aujMSHdjvgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BHierarchical%2BModel%2Bfor%2BDevice%2BPlacement%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4aujMSHdjvgJ&ei=FBZkYujjK4uKmgGY1YjABQ&json=", "num_citations": 114, "citedby_url": "/scholar?cites=17910495902735510497&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4aujMSHdjvgJ:scholar.google.com/&scioq=A+Hierarchical+Model+for+Device+Placement&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hkc-TeZ0W"}, "Understanding Deep Neural Networks with Rectified Linear Units": {"container_type": "Publication", "bib": {"title": "Understanding deep neural networks with rectified linear units", "author": ["R Arora", "A Basu", "P Mianjy", "A Mukherjee"], "pub_year": "2016", "venue": "arXiv preprint arXiv:1611.01491", "abstract": "In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to* global optimality* with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of\" hard\""}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1611.01491", "author_id": ["Spe0xdkAAAAJ", "jV2sAB0AAAAJ", "PTG3GAsAAAAJ", "V32KjH0AAAAJ"], "url_scholarbib": "/scholar?q=info:zQGIyekdjscJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BDeep%2BNeural%2BNetworks%2Bwith%2BRectified%2BLinear%2BUnits%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zQGIyekdjscJ&ei=FxZkYoXUM_mQ6rQP5OqKqAo&json=", "num_citations": 444, "citedby_url": "/scholar?cites=14379463550186291661&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zQGIyekdjscJ:scholar.google.com/&scioq=Understanding+Deep+Neural+Networks+with+Rectified+Linear+Units&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1611.01491"}, "Learning a neural response metric for retinal prosthesis": {"container_type": "Publication", "bib": {"title": "Learning a neural response metric for retinal prosthesis", "author": ["NP Shah", "S Madugula", "EJ Chichilnisky", "J Shlens"], "pub_year": "2018", "venue": "bioRxiv", "abstract": "Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons, causing them to send artificial visual signals to the brain. However, electrical stimulation generally cannot precisely reproduce typical patterns of neural activity in the retina. Therefore, an electrical stimulus must be selected so as to produce a neural response as close as possible to the desired response. This requires a technique for computing the distance between a desired response and an achievable response that is"}, "filled": false, "gsrank": 1, "pub_url": "https://www.biorxiv.org/content/10.1101/226530.abstract", "author_id": ["ifFcxR4AAAAJ", "", "PMM9J2AAAAAJ", "sm1q2bYAAAAJ"], "url_scholarbib": "/scholar?q=info:bdLspqEXHxEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ba%2Bneural%2Bresponse%2Bmetric%2Bfor%2Bretinal%2Bprosthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bdLspqEXHxEJ&ei=GhZkYsviN4ySyASZk6HgCA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=1233730805980516973&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bdLspqEXHxEJ:scholar.google.com/&scioq=Learning+a+neural+response+metric+for+retinal+prosthesis&hl=en&as_sdt=0,33", "eprint_url": "https://www.biorxiv.org/content/10.1101/226530.full.pdf"}, "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples": {"container_type": "Publication", "bib": {"title": "Neural-guided deductive search for real-time program synthesis from examples", "author": ["A Kalyan", "A Mohta", "O Polozov", "D Batra", "P Jain"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.01186", "author_id": ["KYHL9aIAAAAJ", "2_oau4cAAAAJ", "-SuHe48AAAAJ", "_bs7PqgAAAAJ", "qYhRbJoAAAAJ"], "url_scholarbib": "/scholar?q=info:_Lr4lsDZg1wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural-Guided%2BDeductive%2BSearch%2Bfor%2BReal-Time%2BProgram%2BSynthesis%2Bfrom%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_Lr4lsDZg1wJ&ei=IBZkYtjXEYOEmgHx-5DADA&json=", "num_citations": 117, "citedby_url": "/scholar?cites=6666411294628297468&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_Lr4lsDZg1wJ:scholar.google.com/&scioq=Neural-Guided+Deductive+Search+for+Real-Time+Program+Synthesis+from+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.01186"}, "Learning an Embedding Space for Transferable Robot Skills": {"container_type": "Publication", "bib": {"title": "Learning an embedding space for transferable robot skills", "author": ["K Hausman", "JT Springenberg", "Z Wang"], "pub_year": "2018", "venue": "International \u2026", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rk07ZXZRb", "author_id": ["yy0UFOwAAAAJ", "MGXJkIAAAAAJ", "Rne0FzEAAAAJ"], "url_scholarbib": "/scholar?q=info:g_jZX3_UNBkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ban%2BEmbedding%2BSpace%2Bfor%2BTransferable%2BRobot%2BSkills%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g_jZX3_UNBkJ&ei=JRZkYvqrO4yuyAT-mrWwCA&json=", "num_citations": 210, "citedby_url": "/scholar?cites=1816310193271208067&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:g_jZX3_UNBkJ:scholar.google.com/&scioq=Learning+an+Embedding+Space+for+Transferable+Robot+Skills&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rk07ZXZRb"}, "Learning Differentially Private Recurrent Language Models": {"container_type": "Publication", "bib": {"title": "Learning differentially private recurrent language models", "author": ["HB McMahan", "D Ramage", "K Talwar"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes\" large step\" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.06963", "author_id": ["iKPWydkAAAAJ", "D0NeJxMAAAAJ", "XD_01h8AAAAJ"], "url_scholarbib": "/scholar?q=info:wK92_1qMSw8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDifferentially%2BPrivate%2BRecurrent%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wK92_1qMSw8J&ei=KhZkYuflBIuKmgGY1YjABQ&json=", "num_citations": 577, "citedby_url": "/scholar?cites=1102128856283131840&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wK92_1qMSw8J:scholar.google.com/&scioq=Learning+Differentially+Private+Recurrent+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.06963"}, "Emergent Communication through Negotiation": {"container_type": "Publication", "bib": {"title": "Emergent communication through negotiation", "author": ["K Cao", "A Lazaridou", "M Lanctot", "JZ Leibo"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "a way to study how communication could emerge in  of communication in the negotiation  environment, a semi-cooperative model of agent interaction. We introduce two communication"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.03980", "author_id": ["_KBP8ZgAAAAJ", "BMgUIC0AAAAJ", "E_oZZj8AAAAJ", "3y_M1cUAAAAJ"], "url_scholarbib": "/scholar?q=info:kdAC4eHJe3oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BCommunication%2Bthrough%2BNegotiation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kdAC4eHJe3oJ&ei=LBZkYpuYIo2ymgHg1rfQDQ&json=", "num_citations": 117, "citedby_url": "/scholar?cites=8825869866742501521&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kdAC4eHJe3oJ:scholar.google.com/&scioq=Emergent+Communication+through+Negotiation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.03980"}, "Global Optimality Conditions for Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Global optimality conditions for deep neural networks", "author": ["C Yun", "S Sra", "A Jadbabaie"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.02444", "abstract": "We study the error landscape of deep linear and nonlinear neural networks with the squared error loss. Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete. For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum. Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.02444", "author_id": ["Ukl64ggAAAAJ", "eyCw9goAAAAJ", "ZBc_WwYAAAAJ"], "url_scholarbib": "/scholar?q=info:3N_GYm6Y8RoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGlobal%2BOptimality%2BConditions%2Bfor%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3N_GYm6Y8RoJ&ei=OBZkYrKSJ8LZmQHnraWYCA&json=", "num_citations": 93, "citedby_url": "/scholar?cites=1941500514244419548&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3N_GYm6Y8RoJ:scholar.google.com/&scioq=Global+Optimality+Conditions+for+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.02444"}, "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Leave no trace: Learning to reset for safe and autonomous reinforcement learning", "author": ["B Eysenbach", "S Gu", "J Ibarz", "S Levine"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.06782", "abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.06782", "author_id": ["DRnOvU8AAAAJ", "B8wslVsAAAAJ", "l-la0GQAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:OW8UYB8xJawJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLeave%2Bno%2BTrace:%2BLearning%2Bto%2BReset%2Bfor%2BSafe%2Band%2BAutonomous%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OW8UYB8xJawJ&ei=OxZkYqrKJKKUy9YP_JONiAY&json=", "num_citations": 97, "citedby_url": "/scholar?cites=12404374759487598393&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OW8UYB8xJawJ:scholar.google.com/&scioq=Leave+no+Trace:+Learning+to+Reset+for+Safe+and+Autonomous+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.06782.pdf?ref=https://githubhelp.com"}, "Sensitivity and Generalization in Neural Networks: an Empirical Study": {"container_type": "Publication", "bib": {"title": "Sensitivity and generalization in neural networks: an empirical study", "author": ["R Novak", "Y Bahri", "DA Abolafia", "J Pennington"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08760", "author_id": ["syG6krEAAAAJ", "p2_vHmAAAAAJ", "Oo32AkIAAAAJ", "cn_FoswAAAAJ"], "url_scholarbib": "/scholar?q=info:aaidqd5KojQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSensitivity%2Band%2BGeneralization%2Bin%2BNeural%2BNetworks:%2Ban%2BEmpirical%2BStudy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aaidqd5KojQJ&ei=PxZkYofHIM6E6rQP5-KmKA&json=", "num_citations": 292, "citedby_url": "/scholar?cites=3792676156388255849&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aaidqd5KojQJ:scholar.google.com/&scioq=Sensitivity+and+Generalization+in+Neural+Networks:+an+Empirical+Study&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08760"}, "Fidelity-Weighted Learning": {"container_type": "Publication", "bib": {"title": "Fidelity-weighted learning", "author": ["M Dehghani", "A Mehrjou", "S Gouws", "J Kamps"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "versus-quantity trade-off in the learning process. Do we learn from the small amount of high-  take the label-quality into account when learning the data representation, we could get the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02799", "author_id": ["MiHOX3QAAAAJ", "pnypNygAAAAJ", "lLTdYUYAAAAJ", "bWlQ2uEAAAAJ"], "url_scholarbib": "/scholar?q=info:ua45olmvslwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFidelity-Weighted%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ua45olmvslwJ&ei=QhZkYq66IYOEmgHx-5DADA&json=", "num_citations": 55, "citedby_url": "/scholar?cites=6679593996852506297&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ua45olmvslwJ:scholar.google.com/&scioq=Fidelity-Weighted+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02799.pdf?ref=https://githubhelp.com"}, "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks": {"container_type": "Publication", "bib": {"title": "A pac-bayesian approach to spectrally-normalized margin bounds for neural networks", "author": ["B Neyshabur", "S Bhojanapalli", "N Srebro"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1707.09564", "abstract": "In this paper we present and prove a margin based generalization bound for feedforward  neural networks with ReLU activations, that depends on the product of the spectral norm of the  weights in each layer, as well as the Frobenius norm of the weights.  Toward this we use the  following lemma that gives a margin-based generalization bound derived from the PAC-Bayesian  bound (2):  In this section we present our generalization bound for feedfoward networks with  ReLU activations, derived using the PAC-Bayesian framework. Langford& Caruana (2001)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1707.09564", "author_id": ["e1ucbCYAAAAJ", "bpSF_9EAAAAJ", "ZnT-QpMAAAAJ"], "url_scholarbib": "/scholar?q=info:Gl86qd8Xob8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BPAC-Bayesian%2BApproach%2Bto%2BSpectrally-Normalized%2BMargin%2BBounds%2Bfor%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Gl86qd8Xob8J&ei=RRZkYsaWJZyO6rQP_qe3mAs&json=", "num_citations": 393, "citedby_url": "/scholar?cites=13808344181878972186&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Gl86qd8Xob8J:scholar.google.com/&scioq=A+PAC-Bayesian+Approach+to+Spectrally-Normalized+Margin+Bounds+for+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1707.09564"}, "On the Discrimination-Generalization Tradeoff in GANs": {"container_type": "Publication", "bib": {"title": "On the discrimination-generalization tradeoff in GANs", "author": ["P Zhang", "Q Liu", "D Zhou", "T Xu", "X He"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.02771", "abstract": "We characterize the generalizability of GANs using the Rademacher complexity of discriminator  set F and put together bounds between the true distributions \u00b5 and GAN estimators \u03bdm"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.02771", "author_id": ["3VZ_E64AAAAJ", "XEx1fZkAAAAJ", "", "_-vYZ68AAAAJ", "W5WbqgoAAAAJ"], "url_scholarbib": "/scholar?q=info:WDOlvzk0LUsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BDiscrimination-Generalization%2BTradeoff%2Bin%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WDOlvzk0LUsJ&ei=SBZkYoqbLvmQ6rQP5OqKqAo&json=", "num_citations": 82, "citedby_url": "/scholar?cites=5417043349429629784&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WDOlvzk0LUsJ:scholar.google.com/&scioq=On+the+Discrimination-Generalization+Tradeoff+in+GANs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.02771"}, "Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data": {"container_type": "Publication", "bib": {"title": "Predicting floor-level for 911 calls with neural networks and smartphone sensor data", "author": ["W Falcon", "H Schulzrinne"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11122", "abstract": "In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. Unlike impractical previous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11122", "author_id": ["0ngL-30AAAAJ", "6IHX8J4AAAAJ"], "url_scholarbib": "/scholar?q=info:TZRJCclk9qIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPredicting%2BFloor-Level%2Bfor%2B911%2BCalls%2Bwith%2BNeural%2BNetworks%2Band%2BSmartphone%2BSensor%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TZRJCclk9qIJ&ei=TBZkYqGfB4ySyASZk6HgCA&json=", "num_citations": 8, "citedby_url": "/scholar?cites=11742683893022168141&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TZRJCclk9qIJ:scholar.google.com/&scioq=Predicting+Floor-Level+for+911+Calls+with+Neural+Networks+and+Smartphone+Sensor+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11122"}, "Towards Image Understanding from Deep Compression Without Decoding": {"container_type": "Publication", "bib": {"title": "Towards image understanding from deep compression without decoding", "author": ["R Torfason", "F Mentzer", "E Agustsson"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Motivated by recent work on deep neural network (DNN)-based image compression methods showing potential improvements in image quality, savings in storage, and bandwidth reduction, we propose to perform image understanding tasks such as classification and segmentation directly on the compressed representations produced by these compression methods. Since the encoders and decoders in DNN-based compression methods are neural networks with feature-maps as internal representations of the images"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.06131", "author_id": ["", "R80F8XUAAAAJ", "Uhvyua4AAAAJ"], "url_scholarbib": "/scholar?q=info:LtCiCqqE35MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BImage%2BUnderstanding%2Bfrom%2BDeep%2BCompression%2BWithout%2BDecoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LtCiCqqE35MJ&ei=UBZkYv3-IpLeyQTE46-QAg&json=", "num_citations": 90, "citedby_url": "/scholar?cites=10655381109239631918&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LtCiCqqE35MJ:scholar.google.com/&scioq=Towards+Image+Understanding+from+Deep+Compression+Without+Decoding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.06131"}, "Learning One-hidden-layer Neural Networks with Landscape Design": {"container_type": "Publication", "bib": {"title": "Learning one-hidden-layer neural networks with landscape design", "author": ["R Ge", "JD Lee", "T Ma"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.00501", "abstract": "We consider the problem of learning a one-hidden-layer neural network: we assume the input $ x\\in\\mathbb {R}^ d $ is from Gaussian distribution and the label $ y= a^\\top\\sigma (Bx)+\\xi $, where $ a $ is a nonnegative vector in $\\mathbb {R}^ m $ with $ m\\le d $, $ B\\in\\mathbb {R}^{m\\times d} $ is a full-rank weight matrix, and $\\xi $ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00501", "author_id": ["MVxcjEoAAAAJ", "GR_DsT0AAAAJ", "i38QlUwAAAAJ"], "url_scholarbib": "/scholar?q=info:CoWOel2lPc0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BOne-hidden-layer%2BNeural%2BNetworks%2Bwith%2BLandscape%2BDesign%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CoWOel2lPc0J&ei=XBZkYpfsOIyuyAT-mrWwCA&json=", "num_citations": 219, "citedby_url": "/scholar?cites=14789158572261278986&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CoWOel2lPc0J:scholar.google.com/&scioq=Learning+One-hidden-layer+Neural+Networks+with+Landscape+Design&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00501"}, "Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip": {"container_type": "Publication", "bib": {"title": "Sparse persistent RNNs: Squeezing large recurrent networks on-chip", "author": ["F Zhu", "J Pool", "M Andersch", "J Appleyard"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.10223", "author_id": ["", "DagH37xI9soC", "", ""], "url_scholarbib": "/scholar?q=info:YK5y0JQxo-sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSparse%2BPersistent%2BRNNs:%2BSqueezing%2BLarge%2BRecurrent%2BNetworks%2BOn-Chip%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YK5y0JQxo-sJ&ei=ZRZkYprKFM6E6rQP5-KmKA&json=", "num_citations": 23, "citedby_url": "/scholar?cites=16979469535338999392&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YK5y0JQxo-sJ:scholar.google.com/&scioq=Sparse+Persistent+RNNs:+Squeezing+Large+Recurrent+Networks+On-Chip&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.10223"}, "Robustness of Classifiers to Universal Perturbations: A Geometric Perspective": {"container_type": "Publication", "bib": {"title": "Robustness of classifiers to universal perturbations: A geometric perspective", "author": ["SM Moosavi-Dezfooli", "A Fawzi", "O Fawzi"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we propose the first quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.09554", "author_id": ["qosS83IAAAAJ", "BAtMB04AAAAJ", "eAHAxi0AAAAJ"], "url_scholarbib": "/scholar?q=info:kCTCavHlraQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobustness%2Bof%2BClassifiers%2Bto%2BUniversal%2BPerturbations:%2BA%2BGeometric%2BPerspective%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kCTCavHlraQJ&ei=cBZkYp68HZLeyQTE46-QAg&json=", "num_citations": 41, "citedby_url": "/scholar?cites=11866393418232112272&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kCTCavHlraQJ:scholar.google.com/&scioq=Robustness+of+Classifiers+to+Universal+Perturbations:+A+Geometric+Perspective&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.09554"}, "Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent": {"container_type": "Publication", "bib": {"title": "Mastering the dungeon: Grounded language learning by mechanical turker descent", "author": ["Z Yang", "S Zhang", "J Urbanek", "W Feng", "AH Miller"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Contrary to most natural language processing research, which makes use of static datasets, humans learn language interactively, grounded in an environment. In this work we propose an interactive learning procedure called Mechanical Turker Descent (MTD) and use it to train agents to execute natural language commands grounded in a fantasy text adventure game. In MTD, Turkers compete to train better agents in the short term, and collaborate by sharing their agents' skills in the long term. This results in a gamified, engaging experience"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.07950", "author_id": ["7qXxyJkAAAAJ", "", "8LN5NoQAAAAJ", "", "3b0l5LEAAAAJ"], "url_scholarbib": "/scholar?q=info:LVIuzOLL2rYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMastering%2Bthe%2BDungeon:%2BGrounded%2BLanguage%2BLearning%2Bby%2BMechanical%2BTurker%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LVIuzOLL2rYJ&ei=cxZkYunYPOiSy9YPp-OyiAE&json=", "num_citations": 24, "citedby_url": "/scholar?cites=13176067834774442541&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LVIuzOLL2rYJ:scholar.google.com/&scioq=Mastering+the+Dungeon:+Grounded+Language+Learning+by+Mechanical+Turker+Descent&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.07950"}, " Neural Map: Structured Memory for Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Neural map: Structured memory for deep reinforcement learning", "author": ["E Parisotto", "R Salakhutdinov"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1702.08360", "abstract": "A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1702.08360", "author_id": ["-GduGkcAAAAJ", "ITZ1e7MAAAAJ"], "url_scholarbib": "/scholar?q=info:c2OAKzWre8QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BNeural%2BMap:%2BStructured%2BMemory%2Bfor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c2OAKzWre8QJ&ei=fxZkYrDrCeHDywSSipaYAg&json=", "num_citations": 224, "citedby_url": "/scholar?cites=14158098098420736883&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c2OAKzWre8QJ:scholar.google.com/&scioq=+Neural+Map:+Structured+Memory+for+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1702.08360"}, "Learning Wasserstein Embeddings": {"container_type": "Publication", "bib": {"title": "Learning wasserstein embeddings", "author": ["N Courty", "R Flamary", "M Ducoffe"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.07457", "abstract": "to learn an Euclidean embedding of distributions where the Euclidean norm approximates  the Wasserstein  Wasserstein Barycenters Next we evaluate our embedding on the task of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.07457", "author_id": ["ibEREjcAAAAJ", "zDnwxFQAAAAJ", "jqG6J-AAAAAJ"], "url_scholarbib": "/scholar?q=info:YAVOsxODw8UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BWasserstein%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YAVOsxODw8UJ&ei=gxZkYuuaKIyuyAT-mrWwCA&json=", "num_citations": 33, "citedby_url": "/scholar?cites=14250377766566233440&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YAVOsxODw8UJ:scholar.google.com/&scioq=Learning+Wasserstein+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.07457"}, "Hierarchical Subtask Discovery with Non-Negative Matrix Factorization": {"container_type": "Publication", "bib": {"title": "Hierarchical subtask discovery with non-negative matrix factorization", "author": ["AC Earle", "AM Saxe", "B Rosman"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.00463", "abstract": "Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.00463", "author_id": ["gCE6yEQAAAAJ", "h0Al1fcAAAAJ", "pWJ0SocAAAAJ"], "url_scholarbib": "/scholar?q=info:XGK95ae7mGcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BSubtask%2BDiscovery%2Bwith%2BNon-Negative%2BMatrix%2BFactorization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XGK95ae7mGcJ&ei=hhZkYsbLJJLeyQTE46-QAg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=7464922712154923612&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XGK95ae7mGcJ:scholar.google.com/&scioq=Hierarchical+Subtask+Discovery+with+Non-Negative+Matrix+Factorization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.00463"}, "Can Neural Networks Understand Logical Entailment?": {"container_type": "Publication", "bib": {"title": "Can neural networks understand logical entailment?", "author": ["R Evans", "D Saxton", "D Amos", "P Kohli"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a\" convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.08535", "author_id": ["2tbESYUAAAAJ", "ekterPoAAAAJ", "", "3pyzQQ8AAAAJ"], "url_scholarbib": "/scholar?q=info:kwZxaF4uxusJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCan%2BNeural%2BNetworks%2BUnderstand%2BLogical%2BEntailment%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kwZxaF4uxusJ&ei=ixZkYu7iGIuKmgGY1YjABQ&json=", "num_citations": 103, "citedby_url": "/scholar?cites=16989317627315816083&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kwZxaF4uxusJ:scholar.google.com/&scioq=Can+Neural+Networks+Understand+Logical+Entailment%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.08535"}, "SEARNN: Training RNNs with global-local losses": {"container_type": "Publication", "bib": {"title": "SEARNN: Training RNNs with global-local losses", "author": ["R Leblond", "JB Alayrac", "A Osokin"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the\" learning to search\"(L2S) approach to structured prediction. RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE). Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1706.04499", "author_id": ["6UPPnIQAAAAJ", "_VmflIEAAAAJ", "R9MXY8wAAAAJ"], "url_scholarbib": "/scholar?q=info:ZYJ9P_zpcpIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSEARNN:%2BTraining%2BRNNs%2Bwith%2Bglobal-local%2Blosses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZYJ9P_zpcpIJ&ei=jxZkYueoF5qSy9YP8pKNsAE&json=", "num_citations": 39, "citedby_url": "/scholar?cites=10552754146488713829&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZYJ9P_zpcpIJ:scholar.google.com/&scioq=SEARNN:+Training+RNNs+with+global-local+losses&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1706.04499"}, "Initialization matters: Orthogonal Predictive State Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Initialization matters: Orthogonal predictive state recurrent neural networks", "author": ["K Choromanski", "C Downey", "B Boots"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs)(Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model. PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJJ23bW0b", "author_id": ["J8OgouwAAAAJ", "-wflT2wAAAAJ", "kXB8FBoAAAAJ"], "url_scholarbib": "/scholar?q=info:ugNtdkfwjFwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInitialization%2Bmatters:%2BOrthogonal%2BPredictive%2BState%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ugNtdkfwjFwJ&ei=kxZkYv73FeHDywSSipaYAg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=6668969337949062074&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ugNtdkfwjFwJ:scholar.google.com/&scioq=Initialization+matters:+Orthogonal+Predictive+State+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJJ23bW0b"}, "Learning Deep Mean Field Games for Modeling Large Population Behavior": {"container_type": "Publication", "bib": {"title": "Learning deep mean field games for modeling large population behavior", "author": ["J Yang", "X Ye", "R Trivedi", "H Xu", "H Zha"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.03156", "abstract": "We consider the problem of representing collective behavior of large populations and predicting the evolution of a population distribution over a discrete state space. A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions. We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP. This enables us to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.03156", "author_id": ["7spgYVAAAAAJ", "abyGrwwAAAAJ", "Jq1MCAYAAAAJ", "7vLwm84AAAAJ", "n1DQMIsAAAAJ"], "url_scholarbib": "/scholar?q=info:qg6hCN5gfPAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDeep%2BMean%2BField%2BGames%2Bfor%2BModeling%2BLarge%2BPopulation%2BBehavior%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qg6hCN5gfPAJ&ei=lhZkYvmpMu-Sy9YPs_mY8AM&json=", "num_citations": 31, "citedby_url": "/scholar?cites=17328831972958604970&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qg6hCN5gfPAJ:scholar.google.com/&scioq=Learning+Deep+Mean+Field+Games+for+Modeling+Large+Population+Behavior&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.03156"}, "Thermometer Encoding: One Hot Way To Resist Adversarial Examples": {"container_type": "Publication", "bib": {"title": "Thermometer encoding: One hot way to resist adversarial examples", "author": ["J Buckman", "A Roy", "C Raffel"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "It is well known that it is possible to construct\" adversarial examples\" for neural networks: inputs which are misclassified by the network yet indistinguishable from true data. We propose a simple modification to standard neural network architectures, thermometer encoding, which significantly increases the robustness of the network to adversarial examples. We demonstrate this robustness with experiments on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that models with thermometer-encoded inputs"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1SBu-WRW", "author_id": ["IDSGZNYAAAAJ", "mCmda68AAAAJ", "I66ZBYwAAAAJ"], "url_scholarbib": "/scholar?q=info:ZQ7KBBUAW8gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThermometer%2BEncoding:%2BOne%2BHot%2BWay%2BTo%2BResist%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZQ7KBBUAW8gJ&ei=mxZkYuqCBZyO6rQP_qe3mAs&json=", "num_citations": 437, "citedby_url": "/scholar?cites=14437133120740920933&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZQ7KBBUAW8gJ:scholar.google.com/&scioq=Thermometer+Encoding:+One+Hot+Way+To+Resist+Adversarial+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S18Su--CW"}, "Hierarchical Representations for Efficient Architecture Search": {"container_type": "Publication", "bib": {"title": "Hierarchical representations for efficient architecture search", "author": ["H Liu", "K Simonyan", "O Vinyals", "C Fernando"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.00436", "author_id": ["IMkVH_8AAAAJ", "L7lMQkQAAAAJ", "NkzyCvUAAAAJ", "dAuGHpsAAAAJ"], "url_scholarbib": "/scholar?q=info:_u5DUWb1H3kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BRepresentations%2Bfor%2BEfficient%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_u5DUWb1H3kJ&ei=nhZkYryfA_mQ6rQP5OqKqAo&json=", "num_citations": 721, "citedby_url": "/scholar?cites=8727964422666186494&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_u5DUWb1H3kJ:scholar.google.com/&scioq=Hierarchical+Representations+for+Efficient+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.00436"}, "Empirical Risk Landscape Analysis for Understanding Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Empirical risk landscape analysis for understanding deep neural networks", "author": ["P Zhou", "J Feng"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "This work aims to provide comprehensive landscape analysis of empirical risk in deep neural networks (DNNs), including the convergence behavior of its gradient, its stationary points and the empirical risk itself to their corresponding population counterparts, which reveals how various network parameters determine the convergence performance. In particular, for an $ l $-layer linear neural network consisting of $\\dm_i $ neurons in the $ i $-th layer, we prove the gradient of its empirical risk uniformly converges to the one of its"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkzlEFsTb", "author_id": ["0b7ZqlcAAAAJ", "Q8iay0gAAAAJ"], "url_scholarbib": "/scholar?q=info:-kV8MOxKsT0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmpirical%2BRisk%2BLandscape%2BAnalysis%2Bfor%2BUnderstanding%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-kV8MOxKsT0J&ei=oRZkYp-GMMLZmQHnraWYCA&json=", "num_citations": 20, "citedby_url": "/scholar?cites=4445416685477578234&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-kV8MOxKsT0J:scholar.google.com/&scioq=Empirical+Risk+Landscape+Analysis+for+Understanding+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1QgVti6Z"}, "Adaptive Dropout with Rademacher Complexity Regularization": {"container_type": "Publication", "bib": {"title": "Adaptive dropout with rademacher complexity regularization", "author": ["K Zhai", "H Wang"], "pub_year": "2018", "venue": "International conference on learning \u2026", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1wgiJgAb", "author_id": ["m8tMLsMAAAAJ", "7NpTttkAAAAJ"], "url_scholarbib": "/scholar?q=info:-k1X2TAuulgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BDropout%2Bwith%2BRademacher%2BComplexity%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-k1X2TAuulgJ&ei=pBZkYtfYF-HDywSSipaYAg&json=", "num_citations": 20, "citedby_url": "/scholar?cites=6393473408345525754&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-k1X2TAuulgJ:scholar.google.com/&scioq=Adaptive+Dropout+with+Rademacher+Complexity+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1uxsye0Z"}, "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning robust rewards with adversarial inverse reinforcement learning", "author": ["J Fu", "K Luo", "S Levine"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.11248", "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.11248", "author_id": ["T9To2C0AAAAJ", "qlmK27YAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:JqbZSu8yWNUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRobust%2BRewards%2Bwith%2BAdverserial%2BInverse%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JqbZSu8yWNUJ&ei=pxZkYuOvJIuKmgGY1YjABQ&json=", "num_citations": 473, "citedby_url": "/scholar?cites=15373093331363538470&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JqbZSu8yWNUJ:scholar.google.com/&scioq=Learning+Robust+Rewards+with+Adverserial+Inverse+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.11248"}, "Identifying Analogies Across Domains": {"container_type": "Publication", "bib": {"title": "Identifying analogies across domains", "author": ["Y Hoshen", "L Wolf"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "aims to identify analogies between datasets without supervision. Analogy identification as   As we perform matching by synthesis across domains, our method is related to unsupervised"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkN_r2lR-&", "author_id": ["6y1-qS4AAAAJ", "UbFrXTsAAAAJ"], "url_scholarbib": "/scholar?q=info:gWCsh7h93cIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIdentifying%2BAnalogies%2BAcross%2BDomains%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gWCsh7h93cIJ&ei=shZkYuyjO5yO6rQP_qe3mAs&json=", "num_citations": 13, "citedby_url": "/scholar?cites=14041517444714750081&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gWCsh7h93cIJ:scholar.google.com/&scioq=Identifying+Analogies+Across+Domains&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkN_r2lR-"}, "Can recurrent neural networks warp time?": {"container_type": "Publication", "bib": {"title": "Can recurrent neural networks warp time?", "author": ["C Tallec", "Y Ollivier"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.11188", "abstract": "Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. We prove that learnable gates in a recurrent model formally provide quasi-invariance to general time transformations in the input data. We recover part of the LSTM architecture from a simple axiomatic approach. This result leads to a new way of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.11188", "author_id": ["OPKX4GgLCxIC", ""], "url_scholarbib": "/scholar?q=info:02mClMmxpu8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCan%2Brecurrent%2Bneural%2Bnetworks%2Bwarp%2Btime%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=02mClMmxpu8J&ei=tRZkYo_VJY6pywTd4KPADw&json=", "num_citations": 87, "citedby_url": "/scholar?cites=17268685300536863187&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:02mClMmxpu8J:scholar.google.com/&scioq=Can+recurrent+neural+networks+warp+time%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.11188"}, "PixelNN: Example-based Image Synthesis": {"container_type": "Publication", "bib": {"title": "Pixelnn: Example-based image synthesis", "author": ["A Bansal", "Y Sheikh", "D Ramanan"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1708.05349", "abstract": "PixelNN: One-to-Many Mappings We define the problem of conditional image synthesis as   or low-resolution image), synthesize a high-quality output image(s). To describe our approach"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1708.05349", "author_id": ["848cQhkAAAAJ", "Yd4KvooAAAAJ", "9B8PoXUAAAAJ"], "url_scholarbib": "/scholar?q=info:vknJeKiWl-kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPixelNN:%2BExample-based%2BImage%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vknJeKiWl-kJ&ei=uhZkYquhFpLeyQTE46-QAg&json=", "num_citations": 45, "citedby_url": "/scholar?cites=16832087782645647806&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vknJeKiWl-kJ:scholar.google.com/&scioq=PixelNN:+Example-based+Image+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1708.05349"}, "Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design": {"container_type": "Publication", "bib": {"title": "Deep learning and quantum entanglement: Fundamental connections with implications to network design", "author": ["Y Levine", "D Yakira", "N Cohen", "A Shashua"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep convolutional networks have witnessed unprecedented success in various machine learning applications. Formal understanding on what makes these networks so successful is gradually unfolding, but for the most part there are still significant mysteries to unravel. The inductive bias, which reflects prior knowledge embedded in the network architecture, is one of them. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning. We use this connection for asserting novel theoretical"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.01552", "author_id": ["rVQo4JIAAAAJ", "", "DmzoCRMAAAAJ", "dwi5wvYAAAAJ"], "url_scholarbib": "/scholar?q=info:Os4mdk_FUQMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Band%2BQuantum%2BEntanglement:%2BFundamental%2BConnections%2Bwith%2BImplications%2Bto%2BNetwork%2BDesign%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Os4mdk_FUQMJ&ei=vRZkYsqFG7KO6rQPy-CRsA8&json=", "num_citations": 93, "citedby_url": "/scholar?cites=239189200302689850&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Os4mdk_FUQMJ:scholar.google.com/&scioq=Deep+Learning+and+Quantum+Entanglement:+Fundamental+Connections+with+Implications+to+Network+Design&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.01552"}, "Critical Percolation as a Framework to Analyze the Training of Deep Networks": {"container_type": "Publication", "bib": {"title": "Critical Percolation as a Framework to Analyze the Training of Deep Networks", "author": ["Z Ringel", "R de Bem"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.02154", "abstract": "In this paper we approach two relevant deep learning topics: i) tackling of graph structured input data and ii) a better understanding and analysis of deep networks and related learning algorithms. With this in mind we focus on the topological classification of reachability in a particular subset of planar graphs (Mazes). Doing so, we are able to model the topology of data while staying in Euclidean space, thus allowing its processing with standard CNN architectures. We suggest a suitable architecture for this problem and show that it can"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.02154", "author_id": ["8-8VIDgAAAAJ", "LqsiZNoAAAAJ"], "url_scholarbib": "/scholar?q=info:ucoUHIXdtIkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCritical%2BPercolation%2Bas%2Ba%2BFramework%2Bto%2BAnalyze%2Bthe%2BTraining%2Bof%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ucoUHIXdtIkJ&ei=wRZkYpyOMrKO6rQPy-CRsA8&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:ucoUHIXdtIkJ:scholar.google.com/&scioq=Critical+Percolation+as+a+Framework+to+Analyze+the+Training+of+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.02154"}, "Parallelizing Linear Recurrent Neural Nets Over Sequence Length": {"container_type": "Publication", "bib": {"title": "Parallelizing linear recurrent neural nets over sequence length", "author": ["E Martin", "C Cundy"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1709.04057", "abstract": "Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. We develop a parallel linear recurrence CUDA kernel and show that it can be applied to immediately speed up"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.04057", "author_id": ["", "TWAn5XoAAAAJ"], "url_scholarbib": "/scholar?q=info:WQCvcEGOH34J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParallelizing%2BLinear%2BRecurrent%2BNeural%2BNets%2BOver%2BSequence%2BLength%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WQCvcEGOH34J&ei=xRZkYoWOMc6E6rQP5-KmKA&json=", "num_citations": 23, "citedby_url": "/scholar?cites=9088138984771485785&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WQCvcEGOH34J:scholar.google.com/&scioq=Parallelizing+Linear+Recurrent+Neural+Nets+Over+Sequence+Length&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.04057"}, "When is a Convolutional Filter Easy to Learn?": {"container_type": "Publication", "bib": {"title": "When is a convolutional filter easy to learn?", "author": ["SS Du", "JD Lee", "Y Tian"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1709.06129", "abstract": "We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.06129", "author_id": ["OttawxUAAAAJ", "GR_DsT0AAAAJ", "0mgEF28AAAAJ"], "url_scholarbib": "/scholar?q=info:pRHBLBHAhAcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhen%2Bis%2Ba%2BConvolutional%2BFilter%2BEasy%2Bto%2BLearn%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pRHBLBHAhAcJ&ei=zhZkYoCQGLKO6rQPy-CRsA8&json=", "num_citations": 114, "citedby_url": "/scholar?cites=541769035189129637&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pRHBLBHAhAcJ:scholar.google.com/&scioq=When+is+a+Convolutional+Filter+Easy+to+Learn%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.06129"}, "WRPN: Wide Reduced-Precision Networks": {"container_type": "Publication", "bib": {"title": "WRPN: Wide reduced-precision networks", "author": ["A Mishra", "E Nurvitadhi", "JJ Cook", "D Marr"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1709.01134", "abstract": "We call networks using this scheme wide reduced-precision networks (WRPN) and find that  this scheme compensates or surpasses the accuracy of the baseline full-precision network."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.01134", "author_id": ["SUh3W84AAAAJ", "09XsEl4AAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:6bOHdRWPDmkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWRPN:%2BWide%2BReduced-Precision%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6bOHdRWPDmkJ&ei=0RZkYqenKO-Sy9YPs_mY8AM&json=", "num_citations": 268, "citedby_url": "/scholar?cites=7570145345985295337&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6bOHdRWPDmkJ:scholar.google.com/&scioq=WRPN:+Wide+Reduced-Precision+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.01134"}, "Skip Connections Eliminate Singularities": {"container_type": "Publication", "bib": {"title": "Skip connections eliminate singularities", "author": ["AE Orhan", "X Pitkow"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1701.09175", "abstract": "In Supplementary Note 5 we show that skip connections do not eliminate the singularities  in  from the singularities. Thus, in linear networks, any benefits of skip connections are due"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1701.09175", "author_id": ["YkT8jLoAAAAJ", "ony4DjAAAAAJ"], "url_scholarbib": "/scholar?q=info:4QCR18BjMpAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkip%2BConnections%2BEliminate%2BSingularities%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4QCR18BjMpAJ&ei=1RZkYqXSCqKUy9YP_JONiAY&json=", "num_citations": 191, "citedby_url": "/scholar?cites=10390476970198630625&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4QCR18BjMpAJ:scholar.google.com/&scioq=Skip+Connections+Eliminate+Singularities&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1701.09175?ref=https://githubhelp.com"}, "The High-Dimensional Geometry of Binary Neural Networks": {"container_type": "Publication", "bib": {"title": "The high-dimensional geometry of binary neural networks", "author": ["AG Anderson", "CP Berg"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.07199", "abstract": "Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al.(2016)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.07199", "author_id": ["g9kGy0QAAAAJ", ""], "url_scholarbib": "/scholar?q=info:i3_-iji-G-QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BHigh-Dimensional%2BGeometry%2Bof%2BBinary%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i3_-iji-G-QJ&ei=2RZkYrDNJc6E6rQP5-KmKA&json=", "num_citations": 62, "citedby_url": "/scholar?cites=16436940415078137739&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i3_-iji-G-QJ:scholar.google.com/&scioq=The+High-Dimensional+Geometry+of+Binary+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.07199"}, "Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties": {"container_type": "Publication", "bib": {"title": "Critical points of linear neural networks: Analytical forms and landscape properties", "author": ["Y Zhou", "Y Liang"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SysEexbRb", "author_id": ["4fK8bYIAAAAJ", "lGgLAiIAAAAJ"], "url_scholarbib": "/scholar?q=info:Ge9Jv5Cvb_8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCritical%2BPoints%2Bof%2BLinear%2BNeural%2BNetworks:%2BAnalytical%2BForms%2Band%2BLandscape%2BProperties%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ge9Jv5Cvb_8J&ei=3RZkYtCFBIOEmgHx-5DADA&json=", "num_citations": 44, "citedby_url": "/scholar?cites=18406123238305951513&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ge9Jv5Cvb_8J:scholar.google.com/&scioq=Critical+Points+of+Linear+Neural+Networks:+Analytical+Forms+and+Landscape+Properties&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SysEexbRb"}, "Imitation Learning from Visual Data with Multiple Intentions": {"container_type": "Publication", "bib": {"title": "Imitation learning from visual data with multiple intentions", "author": ["A Tamar", "K Rohanimanesh", "Y Chow"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. LfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hk3ddfWRW", "author_id": ["kppa2vgAAAAJ", "", "BFlpS-8AAAAJ"], "url_scholarbib": "/scholar?q=info:xthBm61hTxcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImitation%2BLearning%2Bfrom%2BVisual%2BData%2Bwith%2BMultiple%2BIntentions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xthBm61hTxcJ&ei=4BZkYqrKLIOEmgHx-5DADA&json=", "num_citations": 8, "citedby_url": "/scholar?cites=1679668584294504646&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xthBm61hTxcJ:scholar.google.com/&scioq=Imitation+Learning+from+Visual+Data+with+Multiple+Intentions&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hk3ddfWRW"}, "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Neumann optimizer: A practical optimization algorithm for deep neural networks", "author": ["S Krishnan", "Y Xiao", "RA Saurous"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.03298", "abstract": "Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.03298", "author_id": ["IJan8QwAAAAJ", "BUBwb-kAAAAJ", "QNnjg7YAAAAJ"], "url_scholarbib": "/scholar?q=info:ovEWouTPpsIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeumann%2BOptimizer:%2BA%2BPractical%2BOptimization%2BAlgorithm%2Bfor%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ovEWouTPpsIJ&ei=5BZkYqvvBYOEmgHx-5DADA&json=", "num_citations": 16, "citedby_url": "/scholar?cites=14026126670370894242&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ovEWouTPpsIJ:scholar.google.com/&scioq=Neumann+Optimizer:+A+Practical+Optimization+Algorithm+for+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.03298"}, "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models": {"container_type": "Publication", "bib": {"title": "Latent constraints: Learning to generate conditionally from unconditional generative models", "author": ["J Engel", "M Hoffman", "A Roberts"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.05772", "abstract": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05772", "author_id": ["Sc7qOfcAAAAJ", "IeHKeGYAAAAJ", "U5UpKq8AAAAJ"], "url_scholarbib": "/scholar?q=info:ORQZDRRBokIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BConstraints:%2BLearning%2Bto%2BGenerate%2BConditionally%2Bfrom%2BUnconditional%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ORQZDRRBokIJ&ei=6RZkYoykMvmQ6rQP5OqKqAo&json=", "num_citations": 107, "citedby_url": "/scholar?cites=4801471707105268793&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ORQZDRRBokIJ:scholar.google.com/&scioq=Latent+Constraints:+Learning+to+Generate+Conditionally+from+Unconditional+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05772"}, "Learning Representations and Generative Models for 3D Point Clouds": {"container_type": "Publication", "bib": {"title": "Learning representations and generative models for 3d point clouds", "author": ["P Achlioptas", "O Diamanti"], "pub_year": "2018", "venue": "\u2026 on machine learning", "abstract": "Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.mlr.press/v80/achlioptas18a.html", "author_id": ["Z975RkYAAAAJ", "PKSWrbwAAAAJ"], "url_scholarbib": "/scholar?q=info:XhqdVQoEbokJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRepresentations%2Band%2BGenerative%2BModels%2Bfor%2B3D%2BPoint%2BClouds%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XhqdVQoEbokJ&ei=7hZkYoPpBJGJmwGY-qmYDQ&json=", "num_citations": 685, "citedby_url": "/scholar?cites=9902857073066842718&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XhqdVQoEbokJ:scholar.google.com/&scioq=Learning+Representations+and+Generative+Models+for+3D+Point+Clouds&hl=en&as_sdt=0,33", "eprint_url": "http://proceedings.mlr.press/v80/achlioptas18a/achlioptas18a.pdf"}, "Memory Augmented Control Networks": {"container_type": "Publication", "bib": {"title": "Memory augmented control networks", "author": ["A Khan", "C Zhang", "N Atanasov", "K Karydis"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "aforementioned challenges we propose the Memory Augmented Control Network (MACN),   a local policy which is used to augment the neural memory to produce an optimal policy for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1709.05706", "author_id": ["u0mdue8AAAAJ", "TVQRi8wAAAAJ", "RTkSatQAAAAJ", "4Urexvi1sIcC"], "url_scholarbib": "/scholar?q=info:-6qMeThU0bkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemory%2BAugmented%2BControl%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-6qMeThU0bkJ&ei=8hZkYsD4KpyO6rQP_qe3mAs&json=", "num_citations": 88, "citedby_url": "/scholar?cites=13389575768683358971&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-6qMeThU0bkJ:scholar.google.com/&scioq=Memory+Augmented+Control+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1709.05706"}, "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions": {"container_type": "Publication", "bib": {"title": "Boosting dilated convolutional networks with mixed tensor decompositions", "author": ["N Cohen", "R Tamari", "A Shashua"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1703.06846", "abstract": "The driving force behind deep networks is their ability to compactly represent rich classes of functions. The primary notion for formally reasoning about this phenomenon is expressive efficiency, which refers to a situation where one network must grow unfeasibly large in order to realize (or approximate) functions of another. To date, expressive efficiency analyses focused on the architectural feature of depth, showing that deep networks are representationally superior to shallow ones. In this paper we study the expressive efficiency"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1703.06846", "author_id": ["DmzoCRMAAAAJ", "AW26zZcAAAAJ", "dwi5wvYAAAAJ"], "url_scholarbib": "/scholar?q=info:LSKds1rylFEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosting%2BDilated%2BConvolutional%2BNetworks%2Bwith%2BMixed%2BTensor%2BDecompositions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LSKds1rylFEJ&ei=9hZkYriXFOiSy9YPp-OyiAE&json=", "num_citations": 17, "citedby_url": "/scholar?cites=5878589884999737901&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LSKds1rylFEJ:scholar.google.com/&scioq=Boosting+Dilated+Convolutional+Networks+with+Mixed+Tensor+Decompositions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1703.06846"}}