{"Exploration By Random Network Distillation": {"container_type": "Publication", "bib": {"title": "Exploration by random network distillation", "author": ["Y Burda", "H Edwards", "A Storkey", "O Klimov"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.12894", "author_id": ["Amky96kAAAAJ", "0o470HsAAAAJ", "3Rlc8EAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:hnioaab9vwEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploration%2BBy%2BRandom%2BNetwork%2BDistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hnioaab9vwEJ&ei=3WBeYpCOMc2Ny9YPqPyUgAs&json=", "num_citations": 607, "citedby_url": "/scholar?cites=126098205768710278&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hnioaab9vwEJ:scholar.google.com/&scioq=Exploration+By+Random+Network+Distillation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.12894.pdf?ref=https://codemonkey.link"}, "Generating Multi-agent Trajectories Using Programmatic Weak Supervision": {"container_type": "Publication", "bib": {"title": "Generating multi-agent trajectories using programmatic weak supervision", "author": ["E Zhan", "S Zheng", "Y Yue", "L Sha", "P Lucey"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.07612", "abstract": "We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.07612", "author_id": ["WBKsZwgAAAAJ", "akO_dVoAAAAJ", "tEk4qo8AAAAJ", "5BB_vQsAAAAJ", "bn4x2d8AAAAJ"], "url_scholarbib": "/scholar?q=info:JA3HGnsWNf4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BMulti-agent%2BTrajectories%2BUsing%2BProgrammatic%2BWeak%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JA3HGnsWNf4J&ei=5mBeYojXAo-bmAGmiqCIBw&json=", "num_citations": 42, "citedby_url": "/scholar?cites=18317571777385401636&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JA3HGnsWNf4J:scholar.google.com/&scioq=Generating+Multi-agent+Trajectories+Using+Programmatic+Weak+Supervision&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.07612"}, "Stable Opponent Shaping In Differentiable Games": {"container_type": "Publication", "bib": {"title": "Stable opponent shaping in differentiable games", "author": ["A Letcher", "J Foerster", "D Balduzzi", "T Rockt\u00e4schel"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "A growing number of learning methods are actually differentiable games whose players optimise multiple, interdependent objectives in parallel--from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in these games, accounting for player influence on others' updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm that exploits this response and leads to cooperation in settings like the Iterated Prisoner's Dilemma. Although experimentally"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.08469", "author_id": ["o28w0mwAAAAJ", "6z4lQzMAAAAJ", "xA3Jd5gAAAAJ", "mWBY8aIAAAAJ"], "url_scholarbib": "/scholar?q=info:Cl3wOR8jCYQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStable%2BOpponent%2BShaping%2BIn%2BDifferentiable%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Cl3wOR8jCYQJ&ei=6WBeYrGODYvMsQK69Y7ABg&json=", "num_citations": 60, "citedby_url": "/scholar?cites=9514174304819895562&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Cl3wOR8jCYQJ:scholar.google.com/&scioq=Stable+Opponent+Shaping+In+Differentiable+Games&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.08469"}, "A Closer Look At Deep Learning Heuristics: Learning Rate Restarts, Warmup And Distillation": {"container_type": "Publication", "bib": {"title": "A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation", "author": ["A Gotmare", "NS Keskar", "C Xiong", "R Socher"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.13243", "author_id": ["2S-aFwIAAAAJ", "CJ-_cEEAAAAJ", "vaSdahkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:-R8zdRKcOnQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCloser%2BLook%2BAt%2BDeep%2BLearning%2BHeuristics:%2BLearning%2BRate%2BRestarts,%2BWarmup%2BAnd%2BDistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-R8zdRKcOnQJ&ei=7WBeYpSUO86E6rQPz8uiuAc&json=", "num_citations": 120, "citedby_url": "/scholar?cites=8375178060138487801&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-R8zdRKcOnQJ:scholar.google.com/&scioq=A+Closer+Look+At+Deep+Learning+Heuristics:+Learning+Rate+Restarts,+Warmup+And+Distillation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.13243?ref=https://githubhelp.com"}, "Feed-forward Propagation In Probabilistic Neural Networks With Categorical And Max Layers": {"container_type": "Publication", "bib": {"title": "Feed-forward propagation in probabilistic neural networks with categorical and max layers", "author": ["A Shekhovtsov", "B Flach"], "pub_year": "2018", "venue": "International conference on learning \u2026", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc. In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance wrt all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty. Methods propagating also the variance have been proposed by several authors in different context"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkMuPjRcKQ", "author_id": ["6Ty5Md4AAAAJ", "14xctsUAAAAJ"], "url_scholarbib": "/scholar?q=info:3_NCxo6JIg4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeed-forward%2BPropagation%2BIn%2BProbabilistic%2BNeural%2BNetworks%2BWith%2BCategorical%2BAnd%2BMax%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3_NCxo6JIg4J&ei=8WBeYqjdKpHKsQKNt6-YAw&json=", "num_citations": 13, "citedby_url": "/scholar?cites=1018527712043791327&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3_NCxo6JIg4J:scholar.google.com/&scioq=Feed-forward+Propagation+In+Probabilistic+Neural+Networks+With+Categorical+And+Max+Layers&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkMuPjRcKQ"}, "Context-adaptive Entropy Model For End-to-end Optimized Image Compression": {"container_type": "Publication", "bib": {"title": "Context-adaptive entropy model for end-to-end optimized image compression", "author": ["J Lee", "S Cho", "SK Beack"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.10452", "abstract": "We propose a context-adaptive entropy model for use in end-to-end optimized image compression. Our model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required. Based on these contexts, we allow the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an enhanced compression performance. Based on the experimental"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10452", "author_id": ["", "KBHLt-8AAAAJ", ""], "url_scholarbib": "/scholar?q=info:bZUYdtZUSPIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContext-adaptive%2BEntropy%2BModel%2BFor%2BEnd-to-end%2BOptimized%2BImage%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bZUYdtZUSPIJ&ei=9WBeYtKPKpHKsQKNt6-YAw&json=", "num_citations": 189, "citedby_url": "/scholar?cites=17458297235582784877&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bZUYdtZUSPIJ:scholar.google.com/&scioq=Context-adaptive+Entropy+Model+For+End-to-end+Optimized+Image+Compression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10452"}, "Learning Factorized Representations For Open-set Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Learning factorized representations for open-set domain adaptation", "author": ["M Baktashmotlagh", "M Faraki", "T Drummond"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Domain adaptation for visual recognition has undergone great progress in the past few years. Nevertheless, most existing methods work in the so-called closed-set scenario, assuming that the classes depicted by the target images are exactly the same as those of the source domain. In this paper, we tackle the more challenging, yet more realistic case of open-set domain adaptation, where new, unknown classes can be present in the target data. While, in the unsupervised scenario, one cannot expect to be able to identify each specific"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.12277", "author_id": ["3kaiBBYAAAAJ", "zEVWJu0AAAAJ", "6sWGL5wAAAAJ"], "url_scholarbib": "/scholar?q=info:iQiY2eP5wQ0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFactorized%2BRepresentations%2BFor%2BOpen-set%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iQiY2eP5wQ0J&ei=-WBeYqKnE82Ny9YPqPyUgAs&json=", "num_citations": 41, "citedby_url": "/scholar?cites=991348150001731721&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iQiY2eP5wQ0J:scholar.google.com/&scioq=Learning+Factorized+Representations+For+Open-set+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.12277"}, "What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Representations": {"container_type": "Publication", "bib": {"title": "What do you learn from context? probing for sentence structure in contextualized word representations", "author": ["I Tenney", "P Xia", "B Chen", "A Wang", "A Poliak"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.06316", "author_id": ["7WntHrAAAAAJ", "k9IJYg8AAAAJ", "", "7lSuRloAAAAJ", "emOpjxAAAAAJ"], "url_scholarbib": "/scholar?q=info:AQTptnqoMwYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhat%2BDo%2BYou%2BLearn%2BFrom%2BContext%253F%2BProbing%2BFor%2BSentence%2BStructure%2BIn%2BContextualized%2BWord%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AQTptnqoMwYJ&ei=_WBeYrWIDIvMsQK69Y7ABg&json=", "num_citations": 437, "citedby_url": "/scholar?cites=446886033048011777&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AQTptnqoMwYJ:scholar.google.com/&scioq=What+Do+You+Learn+From+Context%3F+Probing+For+Sentence+Structure+In+Contextualized+Word+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.06316"}, "Meta-learning With Latent Embedding Optimization": {"container_type": "Publication", "bib": {"title": "Meta-learning with latent embedding optimization", "author": ["AA Rusu", "D Rao", "J Sygnowski", "O Vinyals"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.05960", "author_id": ["qcZB864AAAAJ", "1t3Fi2AAAAAJ", "_Iz9Z0sAAAAJ", "NkzyCvUAAAAJ"], "url_scholarbib": "/scholar?q=info:nkbyiKjaUqAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BWith%2BLatent%2BEmbedding%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nkbyiKjaUqAJ&ei=EWFeYtOPMN-Vy9YPs66ekAk&json=", "num_citations": 811, "citedby_url": "/scholar?cites=11552536411545683614&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nkbyiKjaUqAJ:scholar.google.com/&scioq=Meta-learning+With+Latent+Embedding+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.05960.pdf?ref=https://githubhelp.com"}, "Hierarchical Generative Modeling For Controllable Speech Synthesis": {"container_type": "Publication", "bib": {"title": "Hierarchical generative modeling for controllable speech synthesis", "author": ["WN Hsu", "Y Zhang", "RJ Weiss", "H Zen", "Y Wu"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper proposes a neural sequence-to-sequence text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model based on the variational autoencoder (VAE) framework, with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (eg clean/noisy) and provides"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.07217", "author_id": ["N5HDmqoAAAAJ", "EilVnKwAAAAJ", "_VhMIOIAAAAJ", "z3IRvDwAAAAJ", "55FnA9wAAAAJ"], "url_scholarbib": "/scholar?q=info:YQVXMuPWXmsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BGenerative%2BModeling%2BFor%2BControllable%2BSpeech%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YQVXMuPWXmsJ&ei=FGFeYtG2EZGJmwGIxre4DA&json=", "num_citations": 155, "citedby_url": "/scholar?cites=7736857481159574881&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YQVXMuPWXmsJ:scholar.google.com/&scioq=Hierarchical+Generative+Modeling+For+Controllable+Speech+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.07217"}, "M^3rl: Mind-aware Multi-agent Management Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "MRL: Mind-aware Multi-agent Management Reinforcement Learning", "author": ["T Shu", "Y Tian"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00147", "abstract": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly controlling the agents to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (ie, worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not wish to do. For achieving optimal coordination among these agents, we train a super agent (ie"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00147", "author_id": ["YT_ffdwAAAAJ", "0mgEF28AAAAJ"], "url_scholarbib": "/scholar?q=info:V2Hs8E2vrgsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DM%255E3rl:%2BMind-aware%2BMulti-agent%2BManagement%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=V2Hs8E2vrgsJ&ei=FmFeYuKXLcLZmQHc1ovQAg&json=", "num_citations": 26, "citedby_url": "/scholar?cites=841802929654227287&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:V2Hs8E2vrgsJ:scholar.google.com/&scioq=M%5E3rl:+Mind-aware+Multi-agent+Management+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00147"}, "Unsupervised Speech Recognition Via Segmental Empirical Output Distribution Matching": {"container_type": "Publication", "bib": {"title": "Unsupervised speech recognition via segmental empirical output distribution matching", "author": ["CK Yeh", "J Chen", "C Yu", "D Yu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.09323", "abstract": "We consider the problem of training speech recognition systems without using any labeled data, under the assumption that the learner can only access to the input utterances and a phoneme language model estimated from a non-overlapping corpus. We propose a fully unsupervised learning algorithm that alternates between solving two sub-problems:(i) learn a phoneme classifier for a given set of phoneme segmentation boundaries, and (ii) refining the phoneme boundaries based on a given classifier. To solve the first sub-problem, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09323", "author_id": ["tikMtMsAAAAJ", "jQeFWdoAAAAJ", "bwjcElUAAAAJ", "tMY31_gAAAAJ"], "url_scholarbib": "/scholar?q=info:lKfcDYBnvToJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BSpeech%2BRecognition%2BVia%2BSegmental%2BEmpirical%2BOutput%2BDistribution%2BMatching%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lKfcDYBnvToJ&ei=G2FeYufLGciBy9YP18Gi8As&json=", "num_citations": 24, "citedby_url": "/scholar?cites=4232653024484173716&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lKfcDYBnvToJ:scholar.google.com/&scioq=Unsupervised+Speech+Recognition+Via+Segmental+Empirical+Output+Distribution+Matching&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09323"}, "Plan Online, Learn Offline: Efficient Learning And Exploration Via Model-based Control": {"container_type": "Publication", "bib": {"title": "Plan online, learn offline: Efficient learning and exploration via model-based control", "author": ["K Lowrey", "A Rajeswaran", "S Kakade", "E Todorov"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.01848", "author_id": ["ejsX7D0AAAAJ", "_EJrRVAAAAAJ", "wb-DKCIAAAAJ", "QCBdB7AAAAAJ"], "url_scholarbib": "/scholar?q=info:JIJs0HIq6bAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPlan%2BOnline,%2BLearn%2BOffline:%2BEfficient%2BLearning%2BAnd%2BExploration%2BVia%2BModel-based%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JIJs0HIq6bAJ&ei=H2FeYr75I4vMsQK69Y7ABg&json=", "num_citations": 143, "citedby_url": "/scholar?cites=12747766892860310052&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JIJs0HIq6bAJ:scholar.google.com/&scioq=Plan+Online,+Learn+Offline:+Efficient+Learning+And+Exploration+Via+Model-based+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.01848"}, "Excessive Invariance Causes Adversarial Vulnerability": {"container_type": "Publication", "bib": {"title": "Excessive invariance causes adversarial vulnerability", "author": ["JH Jacobsen", "J Behrmann", "R Zemel"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.00401", "author_id": ["c1FYGAQAAAAJ", "aiGPcFUAAAAJ", "iBeDoRAAAAAJ"], "url_scholarbib": "/scholar?q=info:wLARmdNWUysJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExcessive%2BInvariance%2BCauses%2BAdversarial%2BVulnerability%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wLARmdNWUysJ&ei=ImFeYvT6MJGJmwGIxre4DA&json=", "num_citations": 105, "citedby_url": "/scholar?cites=3121934433504047296&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wLARmdNWUysJ:scholar.google.com/&scioq=Excessive+Invariance+Causes+Adversarial+Vulnerability&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.00401"}, "Whitening And Coloring Batch Transform For Gans": {"container_type": "Publication", "bib": {"title": "Whitening and coloring batch transform for gans", "author": ["A Siarohin", "E Sangineto", "N Sebe"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.00420", "abstract": "Batch Normalization (BN) is a common technique used to speed-up and stabilize training. On the other hand, the learnable parameters of BN are commonly used in conditional Generative Adversarial Networks (cGANs) for representing class-specific information using conditional Batch Normalization (cBN). In this paper we propose to generalize both BN and cBN using a Whitening and Coloring based batch normalization. We show that our conditional Coloring can represent categorical conditioning information which largely helps"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.00420", "author_id": ["uMl5-k4AAAAJ", "eJZlvlAAAAAJ", "tNtjSewAAAAJ"], "url_scholarbib": "/scholar?q=info:WQ0oUAENy3MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhitening%2BAnd%2BColoring%2BBatch%2BTransform%2BFor%2BGans%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WQ0oUAENy3MJ&ei=JmFeYu3LCN-Vy9YPs66ekAk&json=", "num_citations": 32, "citedby_url": "/scholar?cites=8343777033924906329&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WQ0oUAENy3MJ:scholar.google.com/&scioq=Whitening+And+Coloring+Batch+Transform+For+Gans&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.00420"}, "Signsgd With Majority Vote Is Communication Efficient And Fault Tolerant": {"container_type": "Publication", "bib": {"title": "signSGD with majority vote is communication efficient and fault tolerant", "author": ["J Bernstein", "J Zhao", "K Azizzadenesheli"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05291", "author_id": ["Kz5C0p0AAAAJ", "VN4CkY8AAAAJ", "Odek140AAAAJ"], "url_scholarbib": "/scholar?q=info:AmhH4ckcrt0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSignsgd%2BWith%2BMajority%2BVote%2BIs%2BCommunication%2BEfficient%2BAnd%2BFault%2BTolerant%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AmhH4ckcrt0J&ei=KWFeYr_VEpHKsQKNt6-YAw&json=", "num_citations": 92, "citedby_url": "/scholar?cites=15973736581723285506&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AmhH4ckcrt0J:scholar.google.com/&scioq=Signsgd+With+Majority+Vote+Is+Communication+Efficient+And+Fault+Tolerant&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05291"}, "Auxiliary Variational Mcmc": {"container_type": "Publication", "bib": {"title": "Auxiliary variational MCMC", "author": ["R Habib", "D Barber"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "2.1 MIXTURE PROPOSAL MCMC To develop a valid MCMC algorithm we need to construct   This proposal can be naturally combined with the auxiliary variational method which we will"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1NJqsRctX", "author_id": ["S1y3RpYAAAAJ", "dqJPZHEAAAAJ"], "url_scholarbib": "/scholar?q=info:u1uEI7dlkGcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAuxiliary%2BVariational%2BMcmc%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=u1uEI7dlkGcJ&ei=LmFeYsGXJciBy9YP18Gi8As&json=", "num_citations": 20, "citedby_url": "/scholar?cites=7462576419802209211&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:u1uEI7dlkGcJ:scholar.google.com/&scioq=Auxiliary+Variational+Mcmc&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1NJqsRctX"}, "Relational Forward Models For Multi-agent Learning": {"container_type": "Publication", "bib": {"title": "Relational forward models for multi-agent learning", "author": ["A Tacchetti", "HF Song", "PAM Mediano"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them. Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments. Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.11044", "author_id": ["HKybSogAAAAJ", "oVF9D6EAAAAJ", "I9-416MAAAAJ"], "url_scholarbib": "/scholar?q=info:j90MY4EiIhQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelational%2BForward%2BModels%2BFor%2BMulti-agent%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j90MY4EiIhQJ&ei=NWFeYuXJNY-bmAGmiqCIBw&json=", "num_citations": 52, "citedby_url": "/scholar?cites=1450759969074634127&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:j90MY4EiIhQJ:scholar.google.com/&scioq=Relational+Forward+Models+For+Multi-agent+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.11044"}, "Kernel Rnn Learning (kernl)": {"container_type": "Publication", "bib": {"title": "Kernel rnn learning (kernl)", "author": ["C Roth", "I Kanitscheider", "I Fiete"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Finally, we show that KeRNL can learn long time-scales more efficiently than BPTT in an   the importance of learning the kernel timescales, we implemented KeRNL without training the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryGfnoC5KQ", "author_id": ["cM8wDgQAAAAJ", "tf-wLXoAAAAJ", "uE-CihIAAAAJ"], "url_scholarbib": "/scholar?q=info:j7zaT7OcwG4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKernel%2BRnn%2BLearning%2B(kernl)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j7zaT7OcwG4J&ei=QmFeYqLvCc2Ny9YPqPyUgAs&json=", "num_citations": 13, "citedby_url": "/scholar?cites=7980550833653333135&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:j7zaT7OcwG4J:scholar.google.com/&scioq=Kernel+Rnn+Learning+(kernl)&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryGfnoC5KQ"}, "Decoupled Weight Decay Regularization": {"container_type": "Publication", "bib": {"title": "Decoupled weight decay regularization", "author": ["I Loshchilov", "F Hutter"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.05101", "abstract": "improve regularization in Adam by decoupling the weight decay  better with decoupled weight  decay than with L2 regularization,  We also demonstrate that our decoupled weight decay"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.05101", "author_id": ["GladWQwAAAAJ", "YUrxwrkAAAAJ"], "url_scholarbib": "/scholar?q=info:SR-mH6TpwE0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecoupled%2BWeight%2BDecay%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SR-mH6TpwE0J&ei=RWFeYryBD82Ny9YPqPyUgAs&json=", "num_citations": 3017, "citedby_url": "/scholar?cites=5602734827563786057&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SR-mH6TpwE0J:scholar.google.com/&scioq=Decoupled+Weight+Decay+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.05101.pdf]"}, "Benchmarking Neural Network Robustness To Common Corruptions And Perturbations": {"container_type": "Publication", "bib": {"title": "Benchmarking neural network robustness to common corruptions and perturbations", "author": ["D Hendrycks", "T Dietterich"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.12261", "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.12261", "author_id": ["czyretsAAAAJ", "09kJn28AAAAJ"], "url_scholarbib": "/scholar?q=info:HkCyUN0soT0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBenchmarking%2BNeural%2BNetwork%2BRobustness%2BTo%2BCommon%2BCorruptions%2BAnd%2BPerturbations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HkCyUN0soT0J&ei=TWFeYo_uBJLeyQTms5KQBg&json=", "num_citations": 1101, "citedby_url": "/scholar?cites=4440880036617273374&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HkCyUN0soT0J:scholar.google.com/&scioq=Benchmarking+Neural+Network+Robustness+To+Common+Corruptions+And+Perturbations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.12261.pdf?ref=https://githubhelp.com"}, "Multi-agent Dual Learning": {"container_type": "Publication", "bib": {"title": "Multi-agent dual learning", "author": ["Y Wang", "Y Xia", "T He", "F Tian", "T Qin", "CX Zhai"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "dual learning framework forms a system with two agents (one primal model and one dual   multiple primal and dual models, and propose the multi-agent dual learning framework."}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/biblio/10172973", "author_id": ["wd3FbFMAAAAJ", "GS5wRxYAAAAJ", "P08KU1YAAAAJ", "SZbCPDEAAAAJ", "Bl4SRU0AAAAJ", "YU-baPIAAAAJ"], "url_scholarbib": "/scholar?q=info:N_yLEItx4eUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-agent%2BDual%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N_yLEItx4eUJ&ei=U2FeYuG_AZGJmwGIxre4DA&json=", "num_citations": 49, "citedby_url": "/scholar?cites=16564645746537397303&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:N_yLEItx4eUJ:scholar.google.com/&scioq=Multi-agent+Dual+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10172973"}, "No Training Required: Exploring Random Encoders For Sentence Classification": {"container_type": "Publication", "bib": {"title": "No training required: Exploring random encoders for sentence classification", "author": ["J Wieting", "D Kiela"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.10444", "abstract": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, ie, using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.10444", "author_id": ["Z0dGdDUAAAAJ", "Q0piorUAAAAJ"], "url_scholarbib": "/scholar?q=info:spJfOCtndbEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNo%2BTraining%2BRequired:%2BExploring%2BRandom%2BEncoders%2BFor%2BSentence%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=spJfOCtndbEJ&ei=VWFeYoCnL5GJmwGIxre4DA&json=", "num_citations": 88, "citedby_url": "/scholar?cites=12787240152315433650&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:spJfOCtndbEJ:scholar.google.com/&scioq=No+Training+Required:+Exploring+Random+Encoders+For+Sentence+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.10444.pdf?ref=https://githubhelp.com"}, "Non-vacuous Generalization Bounds At The Imagenet Scale: A Pac-bayesian Compression Approach": {"container_type": "Publication", "bib": {"title": "Non-vacuous generalization bounds at the imagenet scale: a PAC-bayesian compression approach", "author": ["W Zhou", "V Veitch", "M Austern", "RP Adams"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Modern neural networks are highly overparameterized, with capacity to substantially overfit to training data. Nevertheless, these networks often generalize well in practice. It has also been observed that trained networks can often be\" compressed\" to much smaller representations. The purpose of this paper is to connect these two empirical observations. Our main technical result is a generalization bound for compressed networks based on the compressed size. Combined with off-the-shelf compression algorithms, the bound leads to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.05862", "author_id": ["CdrwG8AAAAAJ", "xkn_XZgAAAAJ", "", "grQ_GBgAAAAJ"], "url_scholarbib": "/scholar?q=info:awMvEgQDCqkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNon-vacuous%2BGeneralization%2BBounds%2BAt%2BThe%2BImagenet%2BScale:%2BA%2BPac-bayesian%2BCompression%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=awMvEgQDCqkJ&ei=V2FeYvTQO82Ny9YPqPyUgAs&json=", "num_citations": 107, "citedby_url": "/scholar?cites=12180551458196751211&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:awMvEgQDCqkJ:scholar.google.com/&scioq=Non-vacuous+Generalization+Bounds+At+The+Imagenet+Scale:+A+Pac-bayesian+Compression+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.05862?ref=https://githubhelp.com"}, "A Rotation-equivariant Convolutional Neural Network Model Of Primary Visual Cortex": {"container_type": "Publication", "bib": {"title": "A rotation-equivariant convolutional neural network model of primary visual cortex", "author": ["AS Ecker", "FH Sinz", "E Froudarakis", "PG Fahey"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Classical models describe primary visual cortex (V1) as a filter bank of orientation-selective linear-nonlinear (LN) or energy models, but these models fail to predict neural responses to natural stimuli accurately. Recent work shows that models based on convolutional neural networks (CNNs) lead to much more accurate predictions, but it remains unclear which features are extracted by V1 neurons beyond orientation selectivity and phase invariance. Here we work towards systematically studying V1 computations by categorizing neurons into"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10504", "author_id": ["VgYU_m8AAAAJ", "xpwMxy8AAAAJ", "peGMCr8AAAAJ", ""], "url_scholarbib": "/scholar?q=info:tEWhOQ5bz-gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BRotation-equivariant%2BConvolutional%2BNeural%2BNetwork%2BModel%2BOf%2BPrimary%2BVisual%2BCortex%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tEWhOQ5bz-gJ&ei=WmFeYsOeF5GJmwGIxre4DA&json=", "num_citations": 26, "citedby_url": "/scholar?cites=16775727253632927156&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tEWhOQ5bz-gJ:scholar.google.com/&scioq=A+Rotation-equivariant+Convolutional+Neural+Network+Model+Of+Primary+Visual+Cortex&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10504"}, "Random Mesh Projectors For Inverse Problems": {"container_type": "Publication", "bib": {"title": "Random mesh projectors for inverse problems", "author": ["S Gupta", "K Kothari", "MV de Hoop", "I Dokmani\u0107"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "to solve ill-posed inverse problems using random low-dimensional projections and  inverse  problem into a collection of simpler learning problems of estimating projections into random ("}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11718", "author_id": ["32Qg__0AAAAJ", "d3GvXScAAAAJ", "El9s8sIAAAAJ", "0SQnwL4AAAAJ"], "url_scholarbib": "/scholar?q=info:aLSI6lM89A8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRandom%2BMesh%2BProjectors%2BFor%2BInverse%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aLSI6lM89A8J&ei=XWFeYrDTLJHKsQKNt6-YAw&json=", "num_citations": 5, "citedby_url": "/scholar?cites=1149610136001098856&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aLSI6lM89A8J:scholar.google.com/&scioq=Random+Mesh+Projectors+For+Inverse+Problems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11718"}, "Adashift: Decorrelation And Convergence Of Adaptive Learning Rate Methods": {"container_type": "Publication", "bib": {"title": "Adashift: Decorrelation and convergence of adaptive learning rate methods", "author": ["Z Zhou", "Q Zhang", "G Lu", "H Wang", "W Zhang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient $ g_t $ and the second-moment term $ v_t $ in Adam ($ t $ is the timestep), which results in that a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00143", "author_id": ["b8YJ1EMAAAAJ", "7YM-faYAAAAJ", "YIt8thUAAAAJ", "3C__4wsAAAAJ", "Qzss0GEAAAAJ"], "url_scholarbib": "/scholar?q=info:XyFP8tuuu4AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdashift:%2BDecorrelation%2BAnd%2BConvergence%2BOf%2BAdaptive%2BLearning%2BRate%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XyFP8tuuu4AJ&ei=YGFeYvvcLZWMy9YPt8OamA0&json=", "num_citations": 43, "citedby_url": "/scholar?cites=9276200117186011487&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XyFP8tuuu4AJ:scholar.google.com/&scioq=Adashift:+Decorrelation+And+Convergence+Of+Adaptive+Learning+Rate+Methods&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00143"}, "A Closer Look At Few-shot Classification": {"container_type": "Publication", "bib": {"title": "A closer look at few-shot classification", "author": ["WY Chen", "YC Liu", "Z Kira", "YCF Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We show that current few-shot classification algorithms fail to address such domain shifts   , highlighting the importance of learning to adapt to domain differences in few-shot learning."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.04232", "author_id": ["XV4ZUc8AAAAJ", "yeAeAhsAAAAJ", "2a5XgNAAAAAJ", "HSGvdtoAAAAJ"], "url_scholarbib": "/scholar?q=info:X7mFJDK-1pAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCloser%2BLook%2BAt%2BFew-shot%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=X7mFJDK-1pAJ&ei=ZGFeYsPbBIvMsQK69Y7ABg&json=", "num_citations": 899, "citedby_url": "/scholar?cites=10436738309048088927&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:X7mFJDK-1pAJ:scholar.google.com/&scioq=A+Closer+Look+At+Few-shot+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.04232.pdf?ref=https://githubhelp.com"}, "Disjoint Mapping Network For Cross-modal Matching Of Voices And Faces": {"container_type": "Publication", "bib": {"title": "Disjoint mapping network for cross-modal matching of voices and faces", "author": ["Y Wen", "MA Ismail", "W Liu", "B Raj", "R Singh"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1807.04836", "abstract": "We propose a novel framework, called Disjoint Mapping Network (DIMNet), for cross-modal biometric matching, in particular of voices and faces. Different from the existing methods, DIMNet does not explicitly learn the joint relationship between the modalities. Instead, DIMNet learns a shared representation for different modalities by mapping them individually to their common covariates. These shared representations can then be used to find the correspondences between the modalities. We show empirically that DIMNet is able to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.04836", "author_id": ["hq0jqhgAAAAJ", "4F-sPfEAAAAJ", "DMjROf0AAAAJ", "IWcGY98AAAAJ", ""], "url_scholarbib": "/scholar?q=info:txU_YacbPUcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDisjoint%2BMapping%2BNetwork%2BFor%2BCross-modal%2BMatching%2BOf%2BVoices%2BAnd%2BFaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=txU_YacbPUcJ&ei=Z2FeYsPFApyO6rQP-viEEA&json=", "num_citations": 38, "citedby_url": "/scholar?cites=5133289555977246135&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:txU_YacbPUcJ:scholar.google.com/&scioq=Disjoint+Mapping+Network+For+Cross-modal+Matching+Of+Voices+And+Faces&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.04836"}, "Graph Wavelet Neural Network": {"container_type": "Publication", "bib": {"title": "Graph wavelet neural network", "author": ["B Xu", "H Shen", "Q Cao", "Y Qiu", "X Cheng"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.07785", "abstract": "convolutional neural network (CNN), leveraging graph wavelet  graph CNN methods  that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.07785", "author_id": ["PSx2drgAAAAJ", "", "FflWb1gAAAAJ", "nRRBlVYAAAAJ", "hY8aLqAAAAAJ"], "url_scholarbib": "/scholar?q=info:XOqV4atIIJAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BWavelet%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XOqV4atIIJAJ&ei=amFeYrj6B5LeyQTms5KQBg&json=", "num_citations": 171, "citedby_url": "/scholar?cites=10385380643777669724&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XOqV4atIIJAJ:scholar.google.com/&scioq=Graph+Wavelet+Neural+Network&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.07785.pdf?ref=https://githubhelp.com"}, "Understanding Straight-through Estimator In Training Activation Quantized Neural Nets": {"container_type": "Publication", "bib": {"title": "Understanding straight-through estimator in training activation quantized neural nets", "author": ["P Yin", "J Lyu", "S Zhang", "S Osher", "Y Qi", "J Xin"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Training activation quantized neural networks involves minimizing a piecewise constant function whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE)(Bengio et al., 2013) in the backward pass only, so that the\" gradient\" through the modified chain rule becomes non-trivial. Since this unusual\" gradient\" is certainly not the gradient of loss function, the following question arises: why searching in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.05662", "author_id": ["PY1Cb7MAAAAJ", "lrX5z6QAAAAJ", "QxEgRpUAAAAJ", "d3UtiX8AAAAJ", "", "b5r5ldYAAAAJ"], "url_scholarbib": "/scholar?q=info:FJTTJYqVLzQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BStraight-through%2BEstimator%2BIn%2BTraining%2BActivation%2BQuantized%2BNeural%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FJTTJYqVLzQJ&ei=bmFeYsazBovMsQK69Y7ABg&json=", "num_citations": 118, "citedby_url": "/scholar?cites=3760388634450301972&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FJTTJYqVLzQJ:scholar.google.com/&scioq=Understanding+Straight-through+Estimator+In+Training+Activation+Quantized+Neural+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.05662"}, "Dpsnet: End-to-end Deep Plane Sweep Stereo": {"container_type": "Publication", "bib": {"title": "Dpsnet: End-to-end deep plane sweep stereo", "author": ["S Im", "HG Jeon", "S Lin", "IS Kweon"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.00538", "abstract": "Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.00538", "author_id": ["37fSLtAAAAAJ", "Ei00xroAAAAJ", "c3PYmxUAAAAJ", "XA8EOlEAAAAJ"], "url_scholarbib": "/scholar?q=info:OU1zZqFzL5oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDpsnet:%2BEnd-to-end%2BDeep%2BPlane%2BSweep%2BStereo%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OU1zZqFzL5oJ&ei=cWFeYofwKd-Vy9YPs66ekAk&json=", "num_citations": 120, "citedby_url": "/scholar?cites=11110225942792064313&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OU1zZqFzL5oJ:scholar.google.com/&scioq=Dpsnet:+End-to-end+Deep+Plane+Sweep+Stereo&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.00538"}, "Learning Actionable Representations With Goal Conditioned Policies": {"container_type": "Publication", "bib": {"title": "Learning actionable representations with goal-conditioned policies", "author": ["D Ghosh", "A Gupta", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.07819", "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.07819", "author_id": ["znnl0kwAAAAJ", "1wLVDP4AAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:3W_TfIXGIPYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BActionable%2BRepresentations%2BWith%2BGoal%2BConditioned%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3W_TfIXGIPYJ&ei=dmFeYp-dEIvMsQK69Y7ABg&json=", "num_citations": 71, "citedby_url": "/scholar?cites=17735393609212194781&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3W_TfIXGIPYJ:scholar.google.com/&scioq=Learning+Actionable+Representations+With+Goal+Conditioned+Policies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.07819"}, "Generating Multiple Objects At Spatially Distinct Locations": {"container_type": "Publication", "bib": {"title": "Generating multiple objects at spatially distinct locations", "author": ["T Hinz", "S Heinrich", "S Wermter"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.00686", "abstract": "Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, ie where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.00686", "author_id": ["SugTw28AAAAJ", "hHx7vgoAAAAJ", "uIeaxuAAAAAJ"], "url_scholarbib": "/scholar?q=info:_MlVFJ6uY7wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BMultiple%2BObjects%2BAt%2BSpatially%2BDistinct%2BLocations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_MlVFJ6uY7wJ&ei=emFeYsjHCd-Vy9YPs66ekAk&json=", "num_citations": 74, "citedby_url": "/scholar?cites=13574885695794039292&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_MlVFJ6uY7wJ:scholar.google.com/&scioq=Generating+Multiple+Objects+At+Spatially+Distinct+Locations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.00686"}, "Spigan: Privileged Adversarial Learning From Simulation": {"container_type": "Publication", "bib": {"title": "Spigan: Privileged adversarial learning from simulation", "author": ["KH Lee", "G Ros", "J Li", "A Gaidon"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.03756", "abstract": "Deep Learning for Computer Vision depends mainly on the source of supervision. Photo-realistic simulators can generate large-scale automatically labeled syntheticdata, but introduce a domain gap negatively impacting performance. We propose anew unsupervised domain adaptation algorithm, called SPIGAN, relying on Sim-ulator Privileged Information (PI) and Generative Adversarial Networks (GAN). We use internal data from the simulator as PI during the training of a target tasknetwork. We experimentally evaluate our approach on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03756", "author_id": ["u8K7nOwAAAAJ", "uDFb6OcAAAAJ", "_I3COxAAAAAJ", "2StUgf4AAAAJ"], "url_scholarbib": "/scholar?q=info:kXoDwo5kV0AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpigan:%2BPrivileged%2BAdversarial%2BLearning%2BFrom%2BSimulation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kXoDwo5kV0AJ&ei=gGFeYqWXCpyO6rQP-viEEA&json=", "num_citations": 56, "citedby_url": "/scholar?cites=4636284905704356497&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kXoDwo5kV0AJ:scholar.google.com/&scioq=Spigan:+Privileged+Adversarial+Learning+From+Simulation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03756"}, "Deep Lagrangian Networks: Using Physics As Model Prior For Deep Learning": {"container_type": "Publication", "bib": {"title": "Deep lagrangian networks: Using physics as model prior for deep learning", "author": ["M Lutter", "C Ritter", "J Peters"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.04490", "abstract": "Deep learning has achieved astonishing results on many tasks with large amounts of data and generalization within the proximity of training data. For many important real-world applications, these requirements are unfeasible and additional prior knowledge on the task domain is required to overcome the resulting problems. In particular, learning physics models for model-based control requires robust extrapolation from fewer samples-often collected online in real-time-and model errors may lead to drastic damages of the system"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.04490", "author_id": ["Wvdo5bYAAAAJ", "", "-kIVAcAAAAAJ"], "url_scholarbib": "/scholar?q=info:Hi6sgG1g92UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLagrangian%2BNetworks:%2BUsing%2BPhysics%2BAs%2BModel%2BPrior%2BFor%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Hi6sgG1g92UJ&ei=h2FeYtnbBZHKsQKNt6-YAw&json=", "num_citations": 196, "citedby_url": "/scholar?cites=7347447340504722974&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Hi6sgG1g92UJ:scholar.google.com/&scioq=Deep+Lagrangian+Networks:+Using+Physics+As+Model+Prior+For+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.04490"}, "Adaptive Input Representations For Neural Language Modeling": {"container_type": "Publication", "bib": {"title": "Adaptive input representations for neural language modeling", "author": ["A Baevski", "M Auli"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.10853", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al.(2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10853", "author_id": ["i7sxIX8AAAAJ", "KMcwQtcAAAAJ"], "url_scholarbib": "/scholar?q=info:C75_gAD814kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BInput%2BRepresentations%2BFor%2BNeural%2BLanguage%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=C75_gAD814kJ&ei=imFeYuX1NJGJmwGIxre4DA&json=", "num_citations": 207, "citedby_url": "/scholar?cites=9932684582274973195&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:C75_gAD814kJ:scholar.google.com/&scioq=Adaptive+Input+Representations+For+Neural+Language+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10853"}, "Value Propagation Networks": {"container_type": "Publication", "bib": {"title": "Value propagation networks", "author": ["N Nardelli", "G Synnaeve", "Z Lin", "P Kohli"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning  modules built on Value Iteration which can successfully be trained using reinforcement"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11199", "author_id": ["iSI9-1oAAAAJ", "wN9rBkcAAAAJ", "ZDjmMuwAAAAJ", "3pyzQQ8AAAAJ"], "url_scholarbib": "/scholar?q=info:gVNlULy6Zn8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DValue%2BPropagation%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gVNlULy6Zn8J&ei=jmFeYtSnGs2Ny9YPqPyUgAs&json=", "num_citations": 21, "citedby_url": "/scholar?cites=9180230208406770561&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gVNlULy6Zn8J:scholar.google.com/&scioq=Value+Propagation+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11199"}, "Learning Two-layer Neural Networks With Symmetric Inputs": {"container_type": "Publication", "bib": {"title": "Learning two-layer neural networks with symmetric inputs", "author": ["R Ge", "R Kuditipudi", "Z Li", "X Wang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.06793", "abstract": "We give a new algorithm for learning a two-layer neural network under a general class of input distributions. Assuming there is a ground-truth two-layer network $$ y= A\\sigma (Wx)+\\xi, $$ where $ A, W $ are weight matrices, $\\xi $ represents noise, and the number of neurons in the hidden layer is no larger than the input or output, our algorithm is guaranteed to recover the parameters $ A, W $ of the ground-truth network. The only requirement on the input $ x $ is that it is symmetric, which still allows highly complicated and structured input"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06793", "author_id": ["MVxcjEoAAAAJ", "8fIzL-4AAAAJ", "uAFPPigAAAAJ", "dHjYcrgAAAAJ"], "url_scholarbib": "/scholar?q=info:yL1TPTGO2wIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTwo-layer%2BNeural%2BNetworks%2BWith%2BSymmetric%2BInputs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yL1TPTGO2wIJ&ei=kWFeYqnsJ4-bmAGmiqCIBw&json=", "num_citations": 38, "citedby_url": "/scholar?cites=205914550108929480&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yL1TPTGO2wIJ:scholar.google.com/&scioq=Learning+Two-layer+Neural+Networks+With+Symmetric+Inputs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06793"}, "Learning To Propagate Labels: Transductive Propagation Network For Few-shot Learning": {"container_type": "Publication", "bib": {"title": "Learning to propagate labels: Transductive propagation network for few-shot learning", "author": ["Y Liu", "J Lee", "M Park", "S Kim", "E Yang", "SJ Hwang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.10002", "author_id": ["znb27R8AAAAJ", "Py4URJUAAAAJ", "hZRH7hoAAAAJ", "_ZfueMIAAAAJ", "UWO1mloAAAAJ", "RP4Qx3QAAAAJ"], "url_scholarbib": "/scholar?q=info:hTeb_kHn5VUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BPropagate%2BLabels:%2BTransductive%2BPropagation%2BNetwork%2BFor%2BFew-shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hTeb_kHn5VUJ&ei=lGFeYpjyCIySyATlkbrQCA&json=", "num_citations": 433, "citedby_url": "/scholar?cites=6189607533521090437&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hTeb_kHn5VUJ:scholar.google.com/&scioq=Learning+To+Propagate+Labels:+Transductive+Propagation+Network+For+Few-shot+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.10002"}, "A Convergence Analysis Of Gradient Descent For Deep Linear Neural Networks": {"container_type": "Publication", "bib": {"title": "A convergence analysis of gradient descent for deep linear neural networks", "author": ["S Arora", "N Cohen", "N Golowich", "W Hu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.02281", "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as $ x\\mapsto W_N W_ {N-1}\\cdots W_1 x $) by minimizing the $\\ell_2 $ loss over whitened data. Convergence at a linear rate is guaranteed when the following hold:(i) dimensions of hidden layers are at least the minimum of the input and output dimensions;(ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02281", "author_id": ["RUP4S68AAAAJ", "DmzoCRMAAAAJ", "roUlyWcAAAAJ", "ZybgAqkAAAAJ"], "url_scholarbib": "/scholar?q=info:r8ffBYMAH30J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BConvergence%2BAnalysis%2BOf%2BGradient%2BDescent%2BFor%2BDeep%2BLinear%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r8ffBYMAH30J&ei=nmFeYrnLN5HKsQKNt6-YAw&json=", "num_citations": 155, "citedby_url": "/scholar?cites=9015925541758289839&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r8ffBYMAH30J:scholar.google.com/&scioq=A+Convergence+Analysis+Of+Gradient+Descent+For+Deep+Linear+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02281"}, "Wizard Of Wikipedia: Knowledge-powered Conversational Agents": {"container_type": "Publication", "bib": {"title": "Wizard of wikipedia: Knowledge-powered conversational agents", "author": ["E Dinan", "S Roller", "K Shuster", "A Fan", "M Auli"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically\" generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance (s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.01241", "author_id": ["pfqzHqUAAAAJ", "22TE5qkAAAAJ", "k8eeP8EAAAAJ", "TLZR9zgAAAAJ", "KMcwQtcAAAAJ"], "url_scholarbib": "/scholar?q=info:KS77w8jxxYwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWizard%2BOf%2BWikipedia:%2BKnowledge-powered%2BConversational%2BAgents%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KS77w8jxxYwJ&ei=omFeYovjKZGJmwGIxre4DA&json=", "num_citations": 391, "citedby_url": "/scholar?cites=10143779580305681961&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KS77w8jxxYwJ:scholar.google.com/&scioq=Wizard+Of+Wikipedia:+Knowledge-powered+Conversational+Agents&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.01241.pdf?fbclid=IwAR1pKzukQ8jXk48Jbe-x4HzC4je0Pboo6Dew-CMEP7NF4C6gdGnc_yc6Xts"}, "Energy-constrained Compression For Deep Neural Networks Via Weighted Sparse Projection And Layer Input Masking": {"container_type": "Publication", "bib": {"title": "Energy-constrained compression for deep neural networks via weighted sparse projection and layer input masking", "author": ["H Yang", "Y Zhu", "J Liu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.04321", "abstract": "Deep Neural Networks (DNNs) are increasingly deployed in highly energy-constrained environments such as autonomous drones and wearable devices while at the same time must operate in real-time. Therefore, reducing the energy consumption has become a major design consideration in DNN training. This paper proposes the first end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the DNN training as an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.04321", "author_id": ["4KNoCFIAAAAJ", "gyvpAroAAAAJ", "RRzVwKkAAAAJ"], "url_scholarbib": "/scholar?q=info:zkgJc9acjlYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnergy-constrained%2BCompression%2BFor%2BDeep%2BNeural%2BNetworks%2BVia%2BWeighted%2BSparse%2BProjection%2BAnd%2BLayer%2BInput%2BMasking%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zkgJc9acjlYJ&ei=pmFeYqC0CYvMsQK69Y7ABg&json=", "num_citations": 28, "citedby_url": "/scholar?cites=6237094978821638350&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zkgJc9acjlYJ:scholar.google.com/&scioq=Energy-constrained+Compression+For+Deep+Neural+Networks+Via+Weighted+Sparse+Projection+And+Layer+Input+Masking&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.04321?ref=https://githubhelp.com"}, "Improving Sequence-to-sequence Learning Via Optimal Transport": {"container_type": "Publication", "bib": {"title": "Improving sequence-to-sequence learning via optimal transport", "author": ["L Chen", "Y Zhang", "R Zhang", "C Tao", "Z Gan"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Sequence-to-sequence models are commonly trained via maximum likelihood estimation (MLE). However, standard MLE training considers a word-level objective, predicting the next word given the previous ground-truth partial sentence. This procedure focuses on modeling local syntactic patterns, and may fail to capture long-range semantic structure. We present a novel solution to alleviate these issues. Our approach imposes global sequence-level guidance via new supervision based on optimal transport, enabling the overall"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.06283", "author_id": ["3IH9iokAAAAJ", "WDVMfggAAAAJ", "-seCWbAAAAAJ", "qyzhQgIAAAAJ", "E64XWyMAAAAJ"], "url_scholarbib": "/scholar?q=info:wcm3wvTpaoIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BSequence-to-sequence%2BLearning%2BVia%2BOptimal%2BTransport%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wcm3wvTpaoIJ&ei=qWFeYqKHN5WMy9YPt8OamA0&json=", "num_citations": 52, "citedby_url": "/scholar?cites=9397580809910077889&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wcm3wvTpaoIJ:scholar.google.com/&scioq=Improving+Sequence-to-sequence+Learning+Via+Optimal+Transport&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.06283"}, "Dher: Hindsight Experience Replay For Dynamic Goals": {"container_type": "Publication", "bib": {"title": "DHER: Hindsight experience replay for dynamic goals", "author": ["M Fang", "C Zhou", "B Shi", "B Gong", "J Xu"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Dealing with sparse rewards is one of the most important challenges in reinforcement learning (RL), especially when a goal is dynamic (eg, to grasp a moving object). Hindsight experience replay (HER) has been shown an effective solution to handling sparse rewards with fixed goals. However, it does not account for dynamic goals in its vanilla form and, as a result, even degrades the performance of existing off-policy RL algorithms when the goal is changing over time. In this paper, we present Dynamic Hindsight Experience Replay"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byf5-30qFX", "author_id": ["IcNYP1oAAAAJ", "", "UJ_tgMYAAAAJ", "lv9ZeVUAAAAJ", "ayUyj9YAAAAJ"], "url_scholarbib": "/scholar?q=info:_na8kgAtYGUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDher:%2BHindsight%2BExperience%2BReplay%2BFor%2BDynamic%2BGoals%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_na8kgAtYGUJ&ei=rWFeYuvPG9-Vy9YPs66ekAk&json=", "num_citations": 47, "citedby_url": "/scholar?cites=7304888076080019198&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_na8kgAtYGUJ:scholar.google.com/&scioq=Dher:+Hindsight+Experience+Replay+For+Dynamic+Goals&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byf5-30qFX"}, "Adef: An Iterative Algorithm To Construct Adversarial Deformations": {"container_type": "Publication", "bib": {"title": "ADef: an iterative algorithm to construct adversarial deformations", "author": ["R Alaifari", "GS Alberti", "T Gauksson"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.07729", "abstract": "While deep neural networks have proven to be a powerful tool for many recognition and classification tasks, their stability properties are still not well understood. In the past, image classifiers have been shown to be vulnerable to so-called adversarial attacks, which are created by additively perturbing the correctly classified image. In this paper, we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image, found through a gradient descent step. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.07729", "author_id": ["WiX5uI4AAAAJ", "boBf5cgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:CwUTcqkv2j8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdef:%2BAn%2BIterative%2BAlgorithm%2BTo%2BConstruct%2BAdversarial%2BDeformations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CwUTcqkv2j8J&ei=r2FeYoTMMI-bmAGmiqCIBw&json=", "num_citations": 73, "citedby_url": "/scholar?cites=4601042374122210571&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CwUTcqkv2j8J:scholar.google.com/&scioq=Adef:+An+Iterative+Algorithm+To+Construct+Adversarial+Deformations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.07729"}, "Learning Latent Superstructures In Variational Autoencoders For Deep Multidimensional Clustering": {"container_type": "Publication", "bib": {"title": "Learning latent superstructures in variational autoencoders for deep multidimensional clustering", "author": ["X Li", "Z Chen", "LKM Poon", "NL Zhang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.05206", "abstract": "We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features. In general, our superstructure is a tree structure of multiple super latent variables and it is automatically learned from data. When there is only one latent variable in the superstructure, our model reduces to one that assumes the latent features to be generated from a Gaussian mixture model. We call our model the latent tree variational autoencoder (LTVAE). Whereas previous deep learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.05206", "author_id": ["vUZu9msAAAAJ", "wIV5UK4AAAAJ", "", "18_xlPUAAAAJ"], "url_scholarbib": "/scholar?q=info:YDVVH2kERFIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLatent%2BSuperstructures%2BIn%2BVariational%2BAutoencoders%2BFor%2BDeep%2BMultidimensional%2BClustering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YDVVH2kERFIJ&ei=tWFeYsi8MY-bmAGmiqCIBw&json=", "num_citations": 32, "citedby_url": "/scholar?cites=5927867859070170464&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YDVVH2kERFIJ:scholar.google.com/&scioq=Learning+Latent+Superstructures+In+Variational+Autoencoders+For+Deep+Multidimensional+Clustering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.05206"}, "Top-down Neural Model For Formulae": {"container_type": "Publication", "bib": {"title": "Top-down neural model for formulae", "author": ["K Chvalovsk\u00fd"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "a simple neural model that given a formula and a property tries to answer the question  whether the formula has the given property, for example whether a propositional formula is"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byg5QhR5FQ", "author_id": [""], "url_scholarbib": "/scholar?q=info:w-Ko4-d3aEYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTop-down%2BNeural%2BModel%2BFor%2BFormulae%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w-Ko4-d3aEYJ&ei=vGFeYpSyOY-bmAGmiqCIBw&json=", "num_citations": 13, "citedby_url": "/scholar?cites=5073436818073510595&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w-Ko4-d3aEYJ:scholar.google.com/&scioq=Top-down+Neural+Model+For+Formulae&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byg5QhR5FQ"}, "Caveats For Information Bottleneck In Deterministic Scenarios": {"container_type": "Publication", "bib": {"title": "Caveats for information bottleneck in deterministic scenarios", "author": ["A Kolchinsky", "BD Tracey", "S Van Kuyk"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1808.07593", "abstract": "Information bottleneck (IB) is a method for extracting information from one random variable $ X $ that is relevant for predicting another random variable $ Y $. To do so, IB identifies an intermediate\" bottleneck\" variable $ T $ that has low mutual information $ I (X; T) $ and high mutual information $ I (Y; T) $. The\" IB curve\" characterizes the set of bottleneck variables that achieve maximal $ I (Y; T) $ for a given $ I (X; T) $, and is typically explored by maximizing the\" IB Lagrangian\", $ I (Y; T)-\\beta I (X; T) $. In some cases, $ Y $ is a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.07593", "author_id": ["RmRwJJIAAAAJ", "bYqAaqYAAAAJ", "AKfravoAAAAJ"], "url_scholarbib": "/scholar?q=info:UcSrgTQd0HYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCaveats%2BFor%2BInformation%2BBottleneck%2BIn%2BDeterministic%2BScenarios%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UcSrgTQd0HYJ&ei=v2FeYpXZIsLZmQHc1ovQAg&json=", "num_citations": 47, "citedby_url": "/scholar?cites=8561375002982335569&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UcSrgTQd0HYJ:scholar.google.com/&scioq=Caveats+For+Information+Bottleneck+In+Deterministic+Scenarios&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.07593"}, "Towards Understanding Regularization In Batch Normalization": {"container_type": "Publication", "bib": {"title": "Towards understanding regularization in batch normalization", "author": ["P Luo", "X Wang", "W Shao", "Z Peng"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.00846", "abstract": "Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.00846", "author_id": ["aXdjxb4AAAAJ", "q4lnWaoAAAAJ", "Bs9mrwwAAAAJ", "78fKbCkAAAAJ"], "url_scholarbib": "/scholar?q=info:ujE1Xxqz2WkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BUnderstanding%2BRegularization%2BIn%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ujE1Xxqz2WkJ&ei=w2FeYpSbGd-Vy9YPs66ekAk&json=", "num_citations": 138, "citedby_url": "/scholar?cites=7627324369776488890&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ujE1Xxqz2WkJ:scholar.google.com/&scioq=Towards+Understanding+Regularization+In+Batch+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.00846"}, "Discriminator Rejection Sampling": {"container_type": "Publication", "bib": {"title": "Discriminator rejection sampling", "author": ["S Azadi", "C Olsson", "T Darrell", "I Goodfellow"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a rejection sampling scheme using the discriminator of a GAN to approximately  correct errors in the GAN generator distribution. We show that under quite strict assumptions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06758", "author_id": ["X0EXfT8AAAAJ", "TvdMDhwAAAAJ", "bh-uRFMAAAAJ", "iYN86KEAAAAJ"], "url_scholarbib": "/scholar?q=info:AA2V1hELmfQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscriminator%2BRejection%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AA2V1hELmfQJ&ei=zGFeYsOFFMLZmQHc1ovQAg&json=", "num_citations": 96, "citedby_url": "/scholar?cites=17625130787933588736&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AA2V1hELmfQJ:scholar.google.com/&scioq=Discriminator+Rejection+Sampling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06758.pdf)"}, "Maximal Divergence Sequential Autoencoder For Binary Software Vulnerability Detection": {"container_type": "Publication", "bib": {"title": "Maximal divergence sequential autoencoder for binary software vulnerability detection", "author": ["T Le", "T Nguyen", "T Le", "D Phung"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security. However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts. In this paper, we attempt to alleviate this severe binary vulnerability detection"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByloIiCqYQ", "author_id": ["", "DPsUJFgAAAAJ", "gysdMxwAAAAJ", "OtA9SwIAAAAJ"], "url_scholarbib": "/scholar?q=info:aXeLbklsW6QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaximal%2BDivergence%2BSequential%2BAutoencoder%2BFor%2BBinary%2BSoftware%2BVulnerability%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aXeLbklsW6QJ&ei=0GFeYtT0Ac2Ny9YPqPyUgAs&json=", "num_citations": 19, "citedby_url": "/scholar?cites=11843178707743897449&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aXeLbklsW6QJ:scholar.google.com/&scioq=Maximal+Divergence+Sequential+Autoencoder+For+Binary+Software+Vulnerability+Detection&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByloIiCqYQ"}, "Information-directed Exploration For Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Information-directed exploration for deep reinforcement learning", "author": ["N Nikolov", "J Kirschner", "F Berkenkamp"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.07544", "author_id": ["UvNjyt8AAAAJ", "IgO2ThIAAAAJ", "N_tCEl8AAAAJ"], "url_scholarbib": "/scholar?q=info:aepDnqehXKwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation-directed%2BExploration%2BFor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aepDnqehXKwJ&ei=0mFeYqy4JoyuyASD3KfABw&json=", "num_citations": 42, "citedby_url": "/scholar?cites=12419979613667846761&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aepDnqehXKwJ:scholar.google.com/&scioq=Information-directed+Exploration+For+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.07544"}, "Approximating Cnns With Bag-of-local-features Models Works Surprisingly Well On Imagenet": {"container_type": "Publication", "bib": {"title": "Approximating cnns with bag-of-local-features models works surprisingly well on imagenet", "author": ["W Brendel", "M Bethge"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.00760", "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.00760", "author_id": ["v-JL-hsAAAAJ", "0z0fNxUAAAAJ"], "url_scholarbib": "/scholar?q=info:eLYQrFfnQboJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DApproximating%2BCnns%2BWith%2BBag-of-local-features%2BModels%2BWorks%2BSurprisingly%2BWell%2BOn%2BImagenet%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eLYQrFfnQboJ&ei=1mFeYrGWAcLZmQHc1ovQAg&json=", "num_citations": 363, "citedby_url": "/scholar?cites=13421262728275736184&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eLYQrFfnQboJ:scholar.google.com/&scioq=Approximating+Cnns+With+Bag-of-local-features+Models+Works+Surprisingly+Well+On+Imagenet&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.00760"}, "Adversarial Imitation Via Variational Inverse Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Adversarial imitation via variational inverse reinforcement learning", "author": ["AH Qureshi", "B Boots", "MC Yip"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.06404", "abstract": "We consider a problem of learning the reward and policy from expert examples under unknown dynamics. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.06404", "author_id": ["Lkrx2SkAAAAJ", "kXB8FBoAAAAJ", "gSYxbCYAAAAJ"], "url_scholarbib": "/scholar?q=info:5mR5qDGNI-wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BImitation%2BVia%2BVariational%2BInverse%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5mR5qDGNI-wJ&ei=22FeYtusLMiBy9YP18Gi8As&json=", "num_citations": 49, "citedby_url": "/scholar?cites=17015599061555307750&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5mR5qDGNI-wJ:scholar.google.com/&scioq=Adversarial+Imitation+Via+Variational+Inverse+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.06404"}, "A Statistical Approach To Assessing Neural Network Robustness": {"container_type": "Publication", "bib": {"title": "A statistical approach to assessing neural network robustness", "author": ["S Webb", "T Rainforth", "YW Teh", "MP Kumar"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.07209", "abstract": "We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.07209", "author_id": ["71p7DlAAAAAJ", "ieLRNKMAAAAJ", "y-nUzMwAAAAJ", "BfmcfEAAAAAJ"], "url_scholarbib": "/scholar?q=info:lC2VCoxhmm0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BStatistical%2BApproach%2BTo%2BAssessing%2BNeural%2BNetwork%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lC2VCoxhmm0J&ei=4GFeYu2hM5HKsQKNt6-YAw&json=", "num_citations": 38, "citedby_url": "/scholar?cites=7897732150648450452&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lC2VCoxhmm0J:scholar.google.com/&scioq=A+Statistical+Approach+To+Assessing+Neural+Network+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.07209"}, "Minimum Divergence Vs. Maximum Margin: An Empirical Comparison On Seq2seq Models": {"container_type": "Publication", "bib": {"title": "Minimum divergence vs. maximum margin: an empirical comparison on seq2seq models", "author": ["H Zhang", "H Zhao"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a)"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1xD9sR5Fm", "author_id": ["", "H6vHKJoAAAAJ"], "url_scholarbib": "/scholar?q=info:rFjn1suiLBgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimum%2BDivergence%2BVs.%2BMaximum%2BMargin:%2BAn%2BEmpirical%2BComparison%2BOn%2BSeq2seq%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rFjn1suiLBgJ&ei=5GFeYqehI4vMsQK69Y7ABg&json=", "num_citations": 8, "citedby_url": "/scholar?cites=1741946152253085868&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rFjn1suiLBgJ:scholar.google.com/&scioq=Minimum+Divergence+Vs.+Maximum+Margin:+An+Empirical+Comparison+On+Seq2seq+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1xD9sR5Fm"}, "Two-timescale Networks For Nonlinear Value Function Approximation": {"container_type": "Publication", "bib": {"title": "Two-timescale networks for nonlinear value function approximation", "author": ["W Chung", "S Nath", "A Joseph", "M White"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control. Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation. Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJleN20qK7", "author_id": ["y5e1qjQAAAAJ", "fNjl1AwAAAAJ", "", "t5zdD_IAAAAJ"], "url_scholarbib": "/scholar?q=info:57ulw71UD_kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTwo-timescale%2BNetworks%2BFor%2BNonlinear%2BValue%2BFunction%2BApproximation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=57ulw71UD_kJ&ei=52FeYpvWNJHKsQKNt6-YAw&json=", "num_citations": 26, "citedby_url": "/scholar?cites=17946656214102686695&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:57ulw71UD_kJ:scholar.google.com/&scioq=Two-timescale+Networks+For+Nonlinear+Value+Function+Approximation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJleN20qK7"}, "The Deep Weight Prior": {"container_type": "Publication", "bib": {"title": "The deep weight prior", "author": ["A Atanov", "A Ashukha", "K Struminsky", "D Vetrov"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose deep weight prior, a framework that approximates the source kernel distribution  and incorporates prior knowledge about the structure of convolutional filters into the prior dis"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06943", "author_id": ["XriU_R8AAAAJ", "IU-kuP8AAAAJ", "q69zIO0AAAAJ", "7HU0UoUAAAAJ"], "url_scholarbib": "/scholar?q=info:u__CWs23B9YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BDeep%2BWeight%2BPrior%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=u__CWs23B9YJ&ei=6mFeYvzjNo6pywSdh6agAg&json=", "num_citations": 27, "citedby_url": "/scholar?cites=15422497541572460475&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:u__CWs23B9YJ:scholar.google.com/&scioq=The+Deep+Weight+Prior&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06943"}, "Learning To Learn With Conditional Class Dependencies": {"container_type": "Publication", "bib": {"title": "Learning to learn with conditional class dependencies", "author": ["X Jiang", "M Havaei", "F Varno", "G Chartrand"], "pub_year": "2018", "venue": "international \u2026", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies. We propose a meta-learning framework, Conditional class-Aware"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJfOXnActQ", "author_id": ["9K7gwtoAAAAJ", "LAoMyyoAAAAJ", "YWjULa8AAAAJ", "k2yQEOkAAAAJ"], "url_scholarbib": "/scholar?q=info:gecxLlfXCE0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BLearn%2BWith%2BConditional%2BClass%2BDependencies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gecxLlfXCE0J&ei=72FeYuPSGo-bmAGmiqCIBw&json=", "num_citations": 45, "citedby_url": "/scholar?cites=5550923310171285377&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gecxLlfXCE0J:scholar.google.com/&scioq=Learning+To+Learn+With+Conditional+Class+Dependencies&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJfOXnActQ"}, "Measuring Compositionality In Representation Learning": {"container_type": "Publication", "bib": {"title": "Measuring compositionality in representation learning", "author": ["J Andreas"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.07181", "abstract": "Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (eg objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.07181", "author_id": ["dnZ8udEAAAAJ"], "url_scholarbib": "/scholar?q=info:EYVWKBwKgwAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2BCompositionality%2BIn%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EYVWKBwKgwAJ&ei=82FeYsy4A9-Vy9YPs66ekAk&json=", "num_citations": 78, "citedby_url": "/scholar?cites=36884338001216785&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EYVWKBwKgwAJ:scholar.google.com/&scioq=Measuring+Compositionality+In+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.07181"}, "The Relativistic Discriminator: A Key Element Missing From Standard Gan": {"container_type": "Publication", "bib": {"title": "The relativistic discriminator: a key element missing from standard GAN", "author": ["A Jolicoeur-Martineau"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1807.00734", "abstract": "In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.00734", "author_id": ["0qytQ1oAAAAJ"], "url_scholarbib": "/scholar?q=info:UR0Rf9ehu4EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BRelativistic%2BDiscriminator:%2BA%2BKey%2BElement%2BMissing%2BFrom%2BStandard%2BGan%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UR0Rf9ehu4EJ&ei=_WFeYtm5OZHKsQKNt6-YAw&json=", "num_citations": 654, "citedby_url": "/scholar?cites=9348243398459465041&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UR0Rf9ehu4EJ:scholar.google.com/&scioq=The+Relativistic+Discriminator:+A+Key+Element+Missing+From+Standard+Gan&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.00734.pdf?ref=https://githubhelp.com"}, "Poincare Glove: Hyperbolic Word Embeddings": {"container_type": "Publication", "bib": {"title": "Poincar\\'e glove: Hyperbolic word embeddings", "author": ["A Tifrea", "G B\u00e9cigneul", "OE Ganea"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.06546", "abstract": "Words are not created equal. In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal. In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry. This connection allows us to introduce a novel principled hypernymy score for word embeddings. Moreover"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06546", "author_id": ["i7T1FUsAAAAJ", "_tehCwgAAAAJ", "0dYS0sMAAAAJ"], "url_scholarbib": "/scholar?q=info:Py1F08iO6ToJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPoincare%2BGlove:%2BHyperbolic%2BWord%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Py1F08iO6ToJ&ei=AWJeYrq-JsLZmQHc1ovQAg&json=", "num_citations": 144, "citedby_url": "/scholar?cites=4245081116962532671&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Py1F08iO6ToJ:scholar.google.com/&scioq=Poincare+Glove:+Hyperbolic+Word+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06546"}, "Adaptive Posterior Learning: Few-shot Learning With A Surprise-based Memory Module": {"container_type": "Publication", "bib": {"title": "Adaptive posterior learning: few-shot learning with a surprise-based memory module", "author": ["T Ramalho", "M Garnelo"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.02527", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.02527", "author_id": ["2H5lbrAAAAAJ", "Hr3zNQUAAAAJ"], "url_scholarbib": "/scholar?q=info:S2FWAXstzjUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BPosterior%2BLearning:%2BFew-shot%2BLearning%2BWith%2BA%2BSurprise-based%2BMemory%2BModule%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S2FWAXstzjUJ&ei=BWJeYqLpBIyuyASD3KfABw&json=", "num_citations": 41, "citedby_url": "/scholar?cites=3877086335539241291&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:S2FWAXstzjUJ:scholar.google.com/&scioq=Adaptive+Posterior+Learning:+Few-shot+Learning+With+A+Surprise-based+Memory+Module&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.02527"}, "Generative Predecessor Models For Sample-efficient Imitation Learning": {"container_type": "Publication", "bib": {"title": "Generative predecessor models for sample-efficient imitation learning", "author": ["Y Schroecker", "M Vecerik", "J Scholz"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.01139", "abstract": "We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.01139", "author_id": ["dNqsv5MAAAAJ", "Jvi_XPAAAAAJ", "bwORIKIAAAAJ"], "url_scholarbib": "/scholar?q=info:5MNTkc8obXoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BPredecessor%2BModels%2BFor%2BSample-efficient%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5MNTkc8obXoJ&ei=B2JeYqiYN4yuyASD3KfABw&json=", "num_citations": 25, "citedby_url": "/scholar?cites=8821752117050196964&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5MNTkc8obXoJ:scholar.google.com/&scioq=Generative+Predecessor+Models+For+Sample-efficient+Imitation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.01139"}, "Gan Dissection: Visualizing And Understanding Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Gan dissection: Visualizing and understanding generative adversarial networks", "author": ["D Bau", "JY Zhu", "H Strobelt", "B Zhou"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.10597", "author_id": ["CYI6cKgAAAAJ", "UdpacsMAAAAJ", "H4vEe_oAAAAJ", "9D4aG8AAAAAJ"], "url_scholarbib": "/scholar?q=info:69IszW4svwIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGan%2BDissection:%2BVisualizing%2BAnd%2BUnderstanding%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=69IszW4svwIJ&ei=CmJeYuKuOIvMsQK69Y7ABg&json=", "num_citations": 308, "citedby_url": "/scholar?cites=197925763027882731&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:69IszW4svwIJ:scholar.google.com/&scioq=Gan+Dissection:+Visualizing+And+Understanding+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.10597"}, "Gansynth: Adversarial Neural Audio Synthesis": {"container_type": "Publication", "bib": {"title": "Gansynth: Adversarial neural audio synthesis", "author": ["J Engel", "KK Agrawal", "S Chen", "I Gulrajani"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.08710", "author_id": ["Sc7qOfcAAAAJ", "Wd8_fOcAAAAJ", "", "E2SLBwIAAAAJ"], "url_scholarbib": "/scholar?q=info:gOg-uNXe2A8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGansynth:%2BAdversarial%2BNeural%2BAudio%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gOg-uNXe2A8J&ei=FWJeYrazLZGJmwGIxre4DA&json=", "num_citations": 295, "citedby_url": "/scholar?cites=1141907515038951552&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gOg-uNXe2A8J:scholar.google.com/&scioq=Gansynth:+Adversarial+Neural+Audio+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.08710"}, "Initialized Equilibrium Propagation For Backprop-free Training": {"container_type": "Publication", "bib": {"title": "Initialized equilibrium propagation for backprop-free training", "author": ["P O'Connor", "E Gavves", "M Welling"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Deep neural networks are almost universally trained with reverse-mode automatic differentiation (aka backpropagation). Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way. In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation-a method for gradient-based train-ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1GMDsR5tm", "author_id": ["", "QqfCvsgAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:TI_YrCylFt0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInitialized%2BEquilibrium%2BPropagation%2BFor%2BBackprop-free%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TI_YrCylFt0J&ei=G2JeYsqfNI-bmAGmiqCIBw&json=", "num_citations": 8, "citedby_url": "/scholar?cites=15931102343166725964&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TI_YrCylFt0J:scholar.google.com/&scioq=Initialized+Equilibrium+Propagation+For+Backprop-free+Training&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1GMDsR5tm"}, "Modeling The Long Term Future In Model-based Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Modeling the long term future in model-based reinforcement learning", "author": ["NR Ke", "A Singh", "A Touati", "A Goyal"], "pub_year": "2018", "venue": "International \u2026", "abstract": "In model-based reinforcement learning, the agent interleaves between model learning and planning. These two components are inextricably intertwined. If the model is not able to provide sensible long-term prediction, the executed planer would exploit model flaws, which can yield catastrophic failures. This paper focuses on building a model that reasons about the long-term future and demonstrates how to use this for efficient planning and exploration. To this end, we build a latent-variable autoregressive model by leveraging recent ideas in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkgQBn0cF7", "author_id": ["dxwPYhQAAAAJ", "aWiMKLsAAAAJ", "D4LT5xAAAAAJ", "krrh6OUAAAAJ"], "url_scholarbib": "/scholar?q=info:cQOg7MZlNMoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModeling%2BThe%2BLong%2BTerm%2BFuture%2BIn%2BModel-based%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cQOg7MZlNMoJ&ei=HmJeYpDWKYyuyASD3KfABw&json=", "num_citations": 18, "citedby_url": "/scholar?cites=14570382599498236785&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cQOg7MZlNMoJ:scholar.google.com/&scioq=Modeling+The+Long+Term+Future+In+Model-based+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkgQBn0cF7"}, "Deep Online Learning Via Meta-learning: Continual Adaptation For Model-based Rl": {"container_type": "Publication", "bib": {"title": "Deep online learning via meta-learning: Continual adaptation for model-based RL", "author": ["A Nagabandi", "C Finn", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.07671", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.07671", "author_id": ["DkUUhXEAAAAJ", "vfPE6hgAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:kcwuXUrVsJkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BOnline%2BLearning%2BVia%2BMeta-learning:%2BContinual%2BAdaptation%2BFor%2BModel-based%2BRl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kcwuXUrVsJkJ&ei=KWJeYtXvKI6pywSdh6agAg&json=", "num_citations": 135, "citedby_url": "/scholar?cites=11074585999071693969&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kcwuXUrVsJkJ:scholar.google.com/&scioq=Deep+Online+Learning+Via+Meta-learning:+Continual+Adaptation+For+Model-based+Rl&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.07671"}, "A Universal Music Translation Network": {"container_type": "Publication", "bib": {"title": "A universal music translation network", "author": ["N Mor", "L Wolf", "A Polyak", "Y Taigman"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.07848", "abstract": "We present a method for translating music across musical instruments, genres, and styles.  This method is based on a multi-domain wavenet autoencoder, with a shared encoder and a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.07848", "author_id": ["GqspICAAAAAJ", "UbFrXTsAAAAJ", "CP62OTMAAAAJ", "mbB3MRIAAAAJ"], "url_scholarbib": "/scholar?q=info:fv5PHZdRmlUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BUniversal%2BMusic%2BTranslation%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fv5PHZdRmlUJ&ei=LGJeYqOVEpGJmwGIxre4DA&json=", "num_citations": 117, "citedby_url": "/scholar?cites=6168332349111008894&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fv5PHZdRmlUJ:scholar.google.com/&scioq=A+Universal+Music+Translation+Network&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.07848"}, "Do Deep Generative Models Know What They Don't Know?": {"container_type": "Publication", "bib": {"title": "Do deep generative models know what they don't know?", "author": ["E Nalisnick", "A Matsukawa", "YW Teh", "D Gorur"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we investigate if modern deep generative models can be used for anomaly   model to assign higher density to the training data than to some other data set. However, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.09136", "author_id": ["-hM1oqwAAAAJ", "", "y-nUzMwAAAAJ", "NwaIVdUAAAAJ"], "url_scholarbib": "/scholar?q=info:vH1OaiwJ8XUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDo%2BDeep%2BGenerative%2BModels%2BKnow%2BWhat%2BThey%2BDon%2527t%2BKnow%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vH1OaiwJ8XUJ&ei=N2JeYrv0JYvMsQK69Y7ABg&json=", "num_citations": 372, "citedby_url": "/scholar?cites=8498584058191576508&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vH1OaiwJ8XUJ:scholar.google.com/&scioq=Do+Deep+Generative+Models+Know+What+They+Don%27t+Know%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.09136?ref=https://githubhelp.com"}, "Variational Autoencoder With Arbitrary Conditioning": {"container_type": "Publication", "bib": {"title": "Variational autoencoder with arbitrary conditioning", "author": ["O Ivanov", "M Figurnov", "D Vetrov"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.02382", "abstract": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in\" one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.02382", "author_id": ["avqzx4IAAAAJ", "qd0tOpQAAAAJ", "7HU0UoUAAAAJ"], "url_scholarbib": "/scholar?q=info:-jNWgnH6pYsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BAutoencoder%2BWith%2BArbitrary%2BConditioning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-jNWgnH6pYsJ&ei=OmJeYtaMLJGJmwGIxre4DA&json=", "num_citations": 87, "citedby_url": "/scholar?cites=10062724307854177274&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-jNWgnH6pYsJ:scholar.google.com/&scioq=Variational+Autoencoder+With+Arbitrary+Conditioning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.02382.pdf?ref=https://githubhelp.com"}, "Trellis Networks For Sequence Modeling": {"container_type": "Publication", "bib": {"title": "Trellis networks for sequence modeling", "author": ["S Bai", "JZ Kolter", "V Koltun"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.06682", "abstract": "We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06682", "author_id": ["DLVP3PcAAAAJ", "UXh1I6UAAAAJ", "kg4bCpgAAAAJ"], "url_scholarbib": "/scholar?q=info:lyTWbBXXRr8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTrellis%2BNetworks%2BFor%2BSequence%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lyTWbBXXRr8J&ei=PWJeYoqCG5WMy9YPt8OamA0&json=", "num_citations": 78, "citedby_url": "/scholar?cites=13782940196634240151&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lyTWbBXXRr8J:scholar.google.com/&scioq=Trellis+Networks+For+Sequence+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06682"}, "Generalizable Adversarial Training Via Spectral Normalization": {"container_type": "Publication", "bib": {"title": "Generalizable adversarial training via spectral normalization", "author": ["F Farnia", "JM Zhang", "D Tse"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.07457", "abstract": "Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which undermines their true practicality. Recent works have increased the robustness of DNNs by fitting networks using adversarially-perturbed training samples, but the improved performance can still be far below the performance seen in non-adversarial settings. A significant portion of this gap can be attributed to the decrease in generalization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.07457", "author_id": ["GYPCqcYAAAAJ", "4jViXZgAAAAJ", "MzD8rjoAAAAJ"], "url_scholarbib": "/scholar?q=info:GUtmDA33W-sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizable%2BAdversarial%2BTraining%2BVia%2BSpectral%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GUtmDA33W-sJ&ei=QGJeYvDyKo-bmAGmiqCIBw&json=", "num_citations": 76, "citedby_url": "/scholar?cites=16959420457208400665&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GUtmDA33W-sJ:scholar.google.com/&scioq=Generalizable+Adversarial+Training+Via+Spectral+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.07457"}, "An Empirical Study Of Example Forgetting During Deep Neural Network Learning": {"container_type": "Publication", "bib": {"title": "An empirical study of example forgetting during deep neural network learning", "author": ["M Toneva", "A Sordoni", "RT Combes", "A Trischler"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define aforgetting event'to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that:(i) certain examples are forgotten with high"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.05159", "author_id": ["a61sk-4AAAAJ", "DJon7w4AAAAJ", "1MZF70cAAAAJ", "EvUM6UUAAAAJ"], "url_scholarbib": "/scholar?q=info:y9GqneI18s4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BEmpirical%2BStudy%2BOf%2BExample%2BForgetting%2BDuring%2BDeep%2BNeural%2BNetwork%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y9GqneI18s4J&ei=RGJeYum6CJGJmwGIxre4DA&json=", "num_citations": 151, "citedby_url": "/scholar?cites=14912040563601232331&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y9GqneI18s4J:scholar.google.com/&scioq=An+Empirical+Study+Of+Example+Forgetting+During+Deep+Neural+Network+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.05159"}, "Distributional Concavity Regularization For Gans": {"container_type": "Publication", "bib": {"title": "Distributional concavity regularization for GANs", "author": ["S Yamaguchi", "M Koyama"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "We propose Distributional Concavity (DC) regularization for Generative Adversarial Networks (GANs), a functional gradient-based method that promotes the entropy of the generator distribution and works against mode collapse. Our DC regularization is an easy-to-implement method that can be used in combination with the current state of the art methods like Spectral Normalization and Wasserstein GAN with gradient penalty to further improve the performance. We will not only show that our DC regularization can achieve highly"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SklEEnC5tQ", "author_id": ["YuBhA1wAAAAJ", "oY1gA10AAAAJ"], "url_scholarbib": "/scholar?q=info:ZAhCx6sADpAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributional%2BConcavity%2BRegularization%2BFor%2BGans%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZAhCx6sADpAJ&ei=R2JeYuCNKo6pywSdh6agAg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=10380234928917973092&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZAhCx6sADpAJ:scholar.google.com/&scioq=Distributional+Concavity+Regularization+For+Gans&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SklEEnC5tQ"}, "Deterministic Variational Inference For Robust Bayesian Neural Networks": {"container_type": "Publication", "bib": {"title": "Deterministic variational inference for robust bayesian neural networks", "author": ["A Wu", "S Nowozin", "E Meeds", "RE Turner"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03958", "author_id": ["ptGYJiEAAAAJ", "7-B7aQkAAAAJ", "oxrYi1cAAAAJ", "DgLEyZgAAAAJ"], "url_scholarbib": "/scholar?q=info:TCaLLZYmgAIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeterministic%2BVariational%2BInference%2BFor%2BRobust%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TCaLLZYmgAIJ&ei=SWJeYvGPPM2Ny9YPqPyUgAs&json=", "num_citations": 124, "citedby_url": "/scholar?cites=180186411545863756&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TCaLLZYmgAIJ:scholar.google.com/&scioq=Deterministic+Variational+Inference+For+Robust+Bayesian+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03958"}, "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy And Robustness": {"container_type": "Publication", "bib": {"title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "author": ["R Geirhos", "P Rubisch", "C Michaelis", "M Bethge"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.12231", "author_id": ["w3kGtMIAAAAJ", "p82yqqIAAAAJ", "ORqsx1YAAAAJ", "0z0fNxUAAAAJ"], "url_scholarbib": "/scholar?q=info:H_KJQ7ef7sQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImagenet-trained%2BCnns%2BAre%2BBiased%2BTowards%2BTexture%253B%2BIncreasing%2BShape%2BBias%2BImproves%2BAccuracy%2BAnd%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H_KJQ7ef7sQJ&ei=TmJeYtHBDM2Ny9YPqPyUgAs&json=", "num_citations": 1268, "citedby_url": "/scholar?cites=14190455085351957023&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:H_KJQ7ef7sQJ:scholar.google.com/&scioq=Imagenet-trained+Cnns+Are+Biased+Towards+Texture%3B+Increasing+Shape+Bias+Improves+Accuracy+And+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.12231"}, "Per-tensor Fixed-point Quantization Of The Back-propagation Algorithm": {"container_type": "Publication", "bib": {"title": "Per-tensor fixed-point quantization of the back-propagation algorithm", "author": ["C Sakr", "N Shanbhag"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.11732", "abstract": "The high computational and parameter complexity of neural networks makes their training very slow and difficult to deploy on energy and storage-constrained computing systems. Many network complexity reduction techniques have been proposed including fixed-point implementation. However, a systematic approach for designing full fixed-point training and inference of deep neural networks remains elusive. We describe a precision assignment methodology for neural network training in which all network parameters, ie, activations and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.11732", "author_id": ["Ks1WOEUAAAAJ", "u6GT7eoAAAAJ"], "url_scholarbib": "/scholar?q=info:p2cfB4ayFM0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPer-tensor%2BFixed-point%2BQuantization%2BOf%2BThe%2BBack-propagation%2BAlgorithm%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p2cfB4ayFM0J&ei=UGJeYtS8OpWMy9YPt8OamA0&json=", "num_citations": 34, "citedby_url": "/scholar?cites=14777632566024300455&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p2cfB4ayFM0J:scholar.google.com/&scioq=Per-tensor+Fixed-point+Quantization+Of+The+Back-propagation+Algorithm&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.11732?ref=https://githubhelp.com"}, "Visual Explanation By Interpretation: Improving Visual Feedback Capabilities Of Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Visual explanation by interpretation: Improving visual feedback capabilities of deep neural networks", "author": ["J Oramas", "K Wang", "T Tuytelaars"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.06302", "abstract": "visuallydescriptive predictions and propose means to improve the quality of the visual feedback  capabilities  between methods aiming at model interpretation, ie, understanding what a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.06302", "author_id": ["FurBYlUAAAAJ", "4MP_OecAAAAJ", "EuFF9kUAAAAJ"], "url_scholarbib": "/scholar?q=info:1KBwQXCeVE4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisual%2BExplanation%2BBy%2BInterpretation:%2BImproving%2BVisual%2BFeedback%2BCapabilities%2BOf%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1KBwQXCeVE4J&ei=ZGJeYo7XKYyuyASD3KfABw&json=", "num_citations": 43, "citedby_url": "/scholar?cites=5644310437973500116&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1KBwQXCeVE4J:scholar.google.com/&scioq=Visual+Explanation+By+Interpretation:+Improving+Visual+Feedback+Capabilities+Of+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.06302"}, "Sliced Wasserstein Auto-encoders": {"container_type": "Publication", "bib": {"title": "Sliced Wasserstein auto-encoders", "author": ["S Kolouri", "PE Pope", "CE Martin"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "auto-encoders for generative modeling (Algorithm 1), which we call Sliced-Wasserstein  auto-encoders  In this paper we also used the sliced Wasserstein distance as a measure of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1xaJn05FQ", "author_id": ["yREBSy0AAAAJ", "w_Y1qcwAAAAJ", "Osyo7s0AAAAJ"], "url_scholarbib": "/scholar?q=info:ZhvPE-eApUYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSliced%2BWasserstein%2BAuto-encoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZhvPE-eApUYJ&ei=aGJeYtfKD9-Vy9YPs66ekAk&json=", "num_citations": 79, "citedby_url": "/scholar?cites=5090616683770354534&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZhvPE-eApUYJ:scholar.google.com/&scioq=Sliced+Wasserstein+Auto-encoders&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1xaJn05FQ"}, "Differentiable Learning-to-normalize Via Switchable Normalization": {"container_type": "Publication", "bib": {"title": "Differentiable learning-to-normalize via switchable normalization", "author": ["P Luo", "J Ren", "Z Peng", "R Zhang", "J Li"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.10779", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig. 1). Second, it is robust to a wide range of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.10779", "author_id": ["aXdjxb4AAAAJ", "CEj_NxUAAAAJ", "78fKbCkAAAAJ", "ZJwZdtgAAAAJ", "yye11UQAAAAJ"], "url_scholarbib": "/scholar?q=info:jXwUMoog1eEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BLearning-to-normalize%2BVia%2BSwitchable%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jXwUMoog1eEJ&ei=cWJeYqdc35XL1g-zrp6QCQ&json=", "num_citations": 134, "citedby_url": "/scholar?cites=16272948606490934413&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jXwUMoog1eEJ:scholar.google.com/&scioq=Differentiable+Learning-to-normalize+Via+Switchable+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.10779.pdf?ref=https://githubhelp.com"}, "Theoretical Analysis Of Auto Rate-tuning By Batch Normalization": {"container_type": "Publication", "bib": {"title": "Theoretical analysis of auto rate-tuning by batch normalization", "author": ["S Arora", "Z Li", "K Lyu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.03981", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (eg, weights of each layer with BN) to a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.03981", "author_id": ["RUP4S68AAAAJ", "5vVjpBsAAAAJ", "843JJtgAAAAJ"], "url_scholarbib": "/scholar?q=info:6MRDUlQk7LEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTheoretical%2BAnalysis%2BOf%2BAuto%2BRate-tuning%2BBy%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6MRDUlQk7LEJ&ei=fmJeYqa9DN-Vy9YPs66ekAk&json=", "num_citations": 72, "citedby_url": "/scholar?cites=12820662183792985320&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6MRDUlQk7LEJ:scholar.google.com/&scioq=Theoretical+Analysis+Of+Auto+Rate-tuning+By+Batch+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.03981"}, "Overcoming Catastrophic Forgetting For Continual Learning Via Model Adaptation": {"container_type": "Publication", "bib": {"title": "Overcoming catastrophic forgetting for continual learning via model adaptation", "author": ["W Hu", "Z Lin", "B Liu", "C Tao", "Z Tao", "J Ma"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Learning multiple tasks sequentially is important for the development of AI and lifelong learning systems. However, standard neural network architectures suffer from catastrophic forgetting which makes it difficult for them to learn a sequence of tasks. Several continual learning methods have been proposed to address the problem. In this paper, we propose a very different approach, called Parameter Generation and Model Adaptation (PGMA), to dealing with the problem. The proposed approach learns to build a model, called the solver"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryGvcoA5YX", "author_id": ["", "", "Kt1bjZoAAAAJ", "x_cOKuwAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:T_m38ttVme4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOvercoming%2BCatastrophic%2BForgetting%2BFor%2BContinual%2BLearning%2BVia%2BModel%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=T_m38ttVme4J&ei=gWJeYsXKGsiBy9YP18Gi8As&json=", "num_citations": 80, "citedby_url": "/scholar?cites=17192867455621921103&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:T_m38ttVme4J:scholar.google.com/&scioq=Overcoming+Catastrophic+Forgetting+For+Continual+Learning+Via+Model+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryGvcoA5YX"}, "Learning Protein Structure With A Differentiable Simulator": {"container_type": "Publication", "bib": {"title": "Learning protein structure with a differentiable simulator", "author": ["J Ingraham", "A Riesselman", "C Sander"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "The Boltzmann distribution is a natural model for many systems, from brains to materials and biomolecules, but is often of limited utility for fitting data because Monte Carlo algorithms are unable to simulate it in available time. This gap between the expressive capabilities and sampling practicalities of energy-based models is exemplified by the protein folding problem, since energy landscapes underlie contemporary knowledge of protein biophysics but computer simulations are challenged to fold all but the smallest proteins from first"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byg3y3C9Km", "author_id": ["qBarioAAAAAJ", "SurU7qsAAAAJ", "4R7_wW8AAAAJ"], "url_scholarbib": "/scholar?q=info:308kNh-XdakJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BProtein%2BStructure%2BWith%2BA%2BDifferentiable%2BSimulator%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=308kNh-XdakJ&ei=hmJeYs7UB5GJmwGIxre4DA&json=", "num_citations": 89, "citedby_url": "/scholar?cites=12210832124993097695&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:308kNh-XdakJ:scholar.google.com/&scioq=Learning+Protein+Structure+With+A+Differentiable+Simulator&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byg3y3C9Km"}, "Biologically-plausible Learning Algorithms Can Scale To Large Datasets": {"container_type": "Publication", "bib": {"title": "Biologically-plausible learning algorithms can scale to large datasets", "author": ["W Xiao", "H Chen", "Q Liao", "T Poggio"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.03567", "abstract": "The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this\" weight transport problem\"(Grossberg, 1987), two more biologically plausible algorithms, proposed by Liao et al.(2016) and Lillicrap et al.(2016), relax BP's weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.03567", "author_id": ["", "", "", "WgAGy7wAAAAJ"], "url_scholarbib": "/scholar?q=info:xdVv1jLz_5cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBiologically-plausible%2BLearning%2BAlgorithms%2BCan%2BScale%2BTo%2BLarge%2BDatasets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xdVv1jLz_5cJ&ei=imJeYvWDGoyuyASD3KfABw&json=", "num_citations": 45, "citedby_url": "/scholar?cites=10952740218459903429&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xdVv1jLz_5cJ:scholar.google.com/&scioq=Biologically-plausible+Learning+Algorithms+Can+Scale+To+Large+Datasets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.03567"}, "On The Convergence Of A Class Of Adam-type Algorithms For Non-convex Optimization": {"container_type": "Publication", "bib": {"title": "On the convergence of a class of adam-type algorithms for non-convex optimization", "author": ["X Chen", "S Liu", "R Sun", "M Hong"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1808.02941", "abstract": "This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the\" Adam-type\", includes the popular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their popularity in training deep neural networks, the convergence of these algorithms for solving nonconvex problems remains an open question. This paper provides a set of mild sufficient conditions that guarantee the convergence for the Adam-type"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.02941", "author_id": ["M0ki5ZgAAAAJ", "C7dO_UgAAAAJ", "PsfzbCMAAAAJ", "qRnP-p0AAAAJ"], "url_scholarbib": "/scholar?q=info:uJN0zPUFzOIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BConvergence%2BOf%2BA%2BClass%2BOf%2BAdam-type%2BAlgorithms%2BFor%2BNon-convex%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uJN0zPUFzOIJ&ei=j2JeYpvmOpGJmwGIxre4DA&json=", "num_citations": 201, "citedby_url": "/scholar?cites=16342443701076005816&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uJN0zPUFzOIJ:scholar.google.com/&scioq=On+The+Convergence+Of+A+Class+Of+Adam-type+Algorithms+For+Non-convex+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.02941"}, "A Max-affine Spline Perspective Of Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "A max-affine spline perspective of recurrent neural networks", "author": ["Z Wang", "R Balestriero", "R Baraniuk"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs). We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator. The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper. First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time. Second, we show that the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJej72AqF7", "author_id": ["IbCALKcAAAAJ", "S1x_xqcAAAAJ", "N-BBA20AAAAJ"], "url_scholarbib": "/scholar?q=info:YTm5yWh0hHUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BMax-affine%2BSpline%2BPerspective%2BOf%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YTm5yWh0hHUJ&ei=mmJeYra3LpyO6rQP-viEEA&json=", "num_citations": 10, "citedby_url": "/scholar?cites=8468021192773155169&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YTm5yWh0hHUJ:scholar.google.com/&scioq=A+Max-affine+Spline+Perspective+Of+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJej72AqF7"}, "Adaptivity Of Deep Relu Network For Learning In Besov And Mixed Smooth Besov Spaces: Optimal Rate And Curse Of Dimensionality": {"container_type": "Publication", "bib": {"title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality", "author": ["T Suzuki"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.08033", "abstract": "Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Holder space and Sobolev space, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.08033", "author_id": ["x8osrBsAAAAJ"], "url_scholarbib": "/scholar?q=info:aRDKMDIQ-04J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptivity%2BOf%2BDeep%2BRelu%2BNetwork%2BFor%2BLearning%2BIn%2BBesov%2BAnd%2BMixed%2BSmooth%2BBesov%2BSpaces:%2BOptimal%2BRate%2BAnd%2BCurse%2BOf%2BDimensionality%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aRDKMDIQ-04J&ei=nmJeYs_fOYryyASen4NI&json=", "num_citations": 108, "citedby_url": "/scholar?cites=5691160361865711721&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aRDKMDIQ-04J:scholar.google.com/&scioq=Adaptivity+Of+Deep+Relu+Network+For+Learning+In+Besov+And+Mixed+Smooth+Besov+Spaces:+Optimal+Rate+And+Curse+Of+Dimensionality&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.08033"}, "L-shapley And C-shapley: Efficient Model Interpretation For Structured Data": {"container_type": "Publication", "bib": {"title": "L-shapley and c-shapley: Efficient model interpretation for structured data", "author": ["J Chen", "L Song", "MJ Wainwright", "MI Jordan"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study instancewise feature importance scoring as a method for model interpretation. Any such method yields, for each predicted instance, a vector of importance scores associated with the feature vector. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions of this kind, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of the Shapley value and prevents these methods from being scalable to large data sets and complex"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.02610", "author_id": ["3bwC_f4AAAAJ", "Xl4E0CsAAAAJ", "J5Rvh6gAAAAJ", "yxUduqMAAAAJ"], "url_scholarbib": "/scholar?q=info:2KhGnwY1DLsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DL-shapley%2BAnd%2BC-shapley:%2BEfficient%2BModel%2BInterpretation%2BFor%2BStructured%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2KhGnwY1DLsJ&ei=omJeYtTbH5HKsQKNt6-YAw&json=", "num_citations": 117, "citedby_url": "/scholar?cites=13478206087371335896&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2KhGnwY1DLsJ:scholar.google.com/&scioq=L-shapley+And+C-shapley:+Efficient+Model+Interpretation+For+Structured+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.02610"}, "Learning To Remember More With Less Memorization": {"container_type": "Publication", "bib": {"title": "Learning to remember more with less memorization", "author": ["H Le", "T Tran", "S Venkatesh"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.01347", "abstract": "Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.01347", "author_id": ["q2HbxngAAAAJ", "zvspVLwAAAAJ", "AEkRUQcAAAAJ"], "url_scholarbib": "/scholar?q=info:8qgvyB2SMJYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BRemember%2BMore%2BWith%2BLess%2BMemorization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8qgvyB2SMJYJ&ei=p2JeYquWDciBy9YP18Gi8As&json=", "num_citations": 30, "citedby_url": "/scholar?cites=10822310561181575410&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8qgvyB2SMJYJ:scholar.google.com/&scioq=Learning+To+Remember+More+With+Less+Memorization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.01347"}, "Learning To Make Analogies By Contrasting Abstract Relational Structure": {"container_type": "Publication", "bib": {"title": "Learning to make analogies by contrasting abstract relational structure", "author": ["F Hill", "A Santoro", "DGT Barrett", "AS Morcos"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Analogical reasoning has been a principal focus of various waves of AI research. Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience. Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data. We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.00120", "author_id": ["4HLUnhIAAAAJ", "evIkDWoAAAAJ", "Whh_d2EAAAAJ", "v-A_7UsAAAAJ"], "url_scholarbib": "/scholar?q=info:ctDeTnG0Z9cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BMake%2BAnalogies%2BBy%2BContrasting%2BAbstract%2BRelational%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ctDeTnG0Z9cJ&ei=q2JeYt7xBo-bmAGmiqCIBw&json=", "num_citations": 46, "citedby_url": "/scholar?cites=15521573039503233138&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ctDeTnG0Z9cJ:scholar.google.com/&scioq=Learning+To+Make+Analogies+By+Contrasting+Abstract+Relational+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.00120"}, "Neural Tts Stylization With Adversarial And Collaborative Games": {"container_type": "Publication", "bib": {"title": "Neural TTS stylization with adversarial and collaborative games", "author": ["S Ma", "D Mcduff", "Y Song"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "The modeling of style when synthesizing natural human speech from text has been the focus of significant attention. Some state-of-the-art approaches train an encoder-decoder network on paired text and audio samples (x_txt, x_aud) by encouraging its output to reconstruct x_aud. The synthesized audio waveform is expected to contain the verbal content of x_txt and the auditory style of x_aud. Unfortunately, modeling style in TTS is somewhat under-determined and training models with a reconstruction loss alone is insufficient to disentangle"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByzcS3AcYX", "author_id": ["IHPRZuMAAAAJ", "m7Jr-b4AAAAJ", "dNHNpxoAAAAJ"], "url_scholarbib": "/scholar?q=info:64KcODntOcYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BTts%2BStylization%2BWith%2BAdversarial%2BAnd%2BCollaborative%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=64KcODntOcYJ&ei=r2JeYojyEZHKsQKNt6-YAw&json=", "num_citations": 18, "citedby_url": "/scholar?cites=14283708523200938731&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:64KcODntOcYJ:scholar.google.com/&scioq=Neural+Tts+Stylization+With+Adversarial+And+Collaborative+Games&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByzcS3AcYX"}, "Soft Q-learning With Mutual-information Regularization": {"container_type": "Publication", "bib": {"title": "Soft q-learning with mutual-information regularization", "author": ["J Grau-Moya", "F Leibfried", "P Vrancx"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyEtjoCqFX", "author_id": ["u8ccN8sAAAAJ", "-sxdNd0AAAAJ", "ql5dOYoAAAAJ"], "url_scholarbib": "/scholar?q=info:QwG4EZuLSb4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSoft%2BQ-learning%2BWith%2BMutual-information%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QwG4EZuLSb4J&ei=s2JeYrvvI5HKsQKNt6-YAw&json=", "num_citations": 38, "citedby_url": "/scholar?cites=13711644038639649091&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QwG4EZuLSb4J:scholar.google.com/&scioq=Soft+Q-learning+With+Mutual-information+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyEtjoCqFX"}, "Generative Code Modeling With Graphs": {"container_type": "Publication", "bib": {"title": "Generative code modeling with graphs", "author": ["M Brockschmidt", "M Allamanis", "AL Gaunt"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. The generative procedure interleaves grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.08490", "author_id": ["pF27eLMAAAAJ", "rYsjwZgAAAAJ", "IzwvqPIAAAAJ"], "url_scholarbib": "/scholar?q=info:J4vayIxh-yAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BCode%2BModeling%2BWith%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J4vayIxh-yAJ&ei=tmJeYqyhE4yuyASD3KfABw&json=", "num_citations": 109, "citedby_url": "/scholar?cites=2376600485661149991&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:J4vayIxh-yAJ:scholar.google.com/&scioq=Generative+Code+Modeling+With+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.08490"}, "Universal Transformers": {"container_type": "Publication", "bib": {"title": "Universal transformers", "author": ["M Dehghani", "S Gouws", "O Vinyals", "J Uszkoreit"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "to build a computationally universal machine, we equipped the Universal Transformer with  the ability  , which shows stronger results compared to the fixed-depth Universal Transformer."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.03819", "author_id": ["MiHOX3QAAAAJ", "lLTdYUYAAAAJ", "NkzyCvUAAAAJ", "mOG0bwsAAAAJ"], "url_scholarbib": "/scholar?q=info:qsWuKDnmLHUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BTransformers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qsWuKDnmLHUJ&ei=uGJeYr-QLpWMy9YPt8OamA0&json=", "num_citations": 455, "citedby_url": "/scholar?cites=8443376534582904234&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qsWuKDnmLHUJ:scholar.google.com/&scioq=Universal+Transformers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.03819"}, "Multilingual Neural Machine Translation With Knowledge Distillation": {"container_type": "Publication", "bib": {"title": "Multilingual neural machine translation with knowledge distillation", "author": ["X Tan", "Y Ren", "D He", "T Qin", "Z Zhao", "TY Liu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.10461", "author_id": ["tob-U1oAAAAJ", "4FA6C0AAAAAJ", "orVoz4IAAAAJ", "Bl4SRU0AAAAJ", "IIoFY90AAAAJ", "Nh832fgAAAAJ"], "url_scholarbib": "/scholar?q=info:pQzxw_352E8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultilingual%2BNeural%2BMachine%2BTranslation%2BWith%2BKnowledge%2BDistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pQzxw_352E8J&ei=vGJeYoS6FN-Vy9YPs66ekAk&json=", "num_citations": 149, "citedby_url": "/scholar?cites=5753623392275205285&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pQzxw_352E8J:scholar.google.com/&scioq=Multilingual+Neural+Machine+Translation+With+Knowledge+Distillation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.10461"}, "Texttovec: Deep Contextualized Neural Autoregressive Topic Models Of Language With Distributed Compositional Prior": {"container_type": "Publication", "bib": {"title": "Texttovec: Deep contextualized neural autoregressive topic models of language with distributed compositional prior", "author": ["P Gupta", "Y Chaudhary", "F Buettner"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, ie, P (word| context):(1) No Language Structure in Context: Probabilistic topic models ignore word order by summarizing a given context as a\" bag-of-word\" and consequently the semantics of words in the context is lost. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns and models complex characteristics of language (eg, syntax and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03947", "author_id": ["_YjIJF0AAAAJ", "g4m2iUAAAAAJ", "AaPKbPAAAAAJ"], "url_scholarbib": "/scholar?q=info:uXJE1bMDcOYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTexttovec:%2BDeep%2BContextualized%2BNeural%2BAutoregressive%2BTopic%2BModels%2BOf%2BLanguage%2BWith%2BDistributed%2BCompositional%2BPrior%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uXJE1bMDcOYJ&ei=v2JeYvaIKciBy9YP18Gi8As&json=", "num_citations": 8, "citedby_url": "/scholar?cites=16604775897027080889&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uXJE1bMDcOYJ:scholar.google.com/&scioq=Texttovec:+Deep+Contextualized+Neural+Autoregressive+Topic+Models+Of+Language+With+Distributed+Compositional+Prior&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03947"}, "Learning Exploration Policies For Navigation": {"container_type": "Publication", "bib": {"title": "Learning exploration policies for navigation", "author": ["T Chen", "S Gupta", "A Gupta"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.01959", "abstract": "Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that the use of policies with spatial memory that are bootstrapped with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.01959", "author_id": ["gdUv1PIAAAAJ", "1HO5UacAAAAJ", "bqL73OkAAAAJ"], "url_scholarbib": "/scholar?q=info:evIHNiaxLxUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BExploration%2BPolicies%2BFor%2BNavigation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=evIHNiaxLxUJ&ei=wmJeYvuuIpGJmwGIxre4DA&json=", "num_citations": 107, "citedby_url": "/scholar?cites=1526633576375251578&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:evIHNiaxLxUJ:scholar.google.com/&scioq=Learning+Exploration+Policies+For+Navigation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.01959"}, "Feature-wise Bias Amplification": {"container_type": "Publication", "bib": {"title": "Feature-wise bias amplification", "author": ["K Leino", "E Black", "M Fredrikson", "S Sen"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the phenomenon of bias amplification in classifiers, wherein a machine learning   We demonstrate that bias amplification can arise via an inductive bias in gradient descent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.08999", "author_id": ["", "dBkGY6gAAAAJ", "tMYCvLAAAAAJ", "vdiw0xMAAAAJ"], "url_scholarbib": "/scholar?q=info:lFP5FxEA5fUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeature-wise%2BBias%2BAmplification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lFP5FxEA5fUJ&ei=02JeYuDAMYyuyASD3KfABw&json=", "num_citations": 20, "citedby_url": "/scholar?cites=17718568382375744404&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lFP5FxEA5fUJ:scholar.google.com/&scioq=Feature-wise+Bias+Amplification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.08999"}, "Wasserstein Barycenter Model Ensembling": {"container_type": "Publication", "bib": {"title": "Wasserstein barycenter model ensembling", "author": ["P Dognin", "I Melnyk", "Y Mroueh", "J Ross"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "propose to perform model ensembling in a multiclass or a multilabel learning setting using  Wasserstein (W.) barycenters. Optimal transport metrics, such as the Wasserstein distance,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.04999", "author_id": ["goQ8S1YAAAAJ", "", "6F90JHgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:ZhQMPqdj8oUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWasserstein%2BBarycenter%2BModel%2BEnsembling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZhQMPqdj8oUJ&ei=1mJeYpmnJovMsQK69Y7ABg&json=", "num_citations": 19, "citedby_url": "/scholar?cites=9651886521360061542&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZhQMPqdj8oUJ:scholar.google.com/&scioq=Wasserstein+Barycenter+Model+Ensembling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.04999"}, "Slimmable Neural Networks": {"container_type": "Publication", "bib": {"title": "Slimmable neural networks", "author": ["J Yu", "L Yang", "N Xu", "J Yang", "T Huang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.08928", "abstract": "Our trained networks, named slimmable neural networks,  We also demonstrate better  performance of slimmable models  cuss the learned features of slimmable networks. Code and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.08928", "author_id": ["-CLCMk4AAAAJ", "XptEO8oAAAAJ", "dRDZBoEAAAAJ", "HWFvq_wAAAAJ", "rGF6-WkAAAAJ"], "url_scholarbib": "/scholar?q=info:yMSSGr9-HNMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSlimmable%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yMSSGr9-HNMJ&ei=2WJeYtb1N4-bmAGmiqCIBw&json=", "num_citations": 286, "citedby_url": "/scholar?cites=15212173000600372424&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yMSSGr9-HNMJ:scholar.google.com/&scioq=Slimmable+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.08928"}, "Generating High Fidelity Images With Subscale Pixel Networks And Multidimensional Upscaling": {"container_type": "Publication", "bib": {"title": "Generating high fidelity images with subscale pixel networks and multidimensional upscaling", "author": ["J Menick", "N Kalchbrenner"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.01608", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.01608", "author_id": ["te_gpaEAAAAJ", "LFyg0tAAAAAJ"], "url_scholarbib": "/scholar?q=info:MJeUJSBcSFgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BHigh%2BFidelity%2BImages%2BWith%2BSubscale%2BPixel%2BNetworks%2BAnd%2BMultidimensional%2BUpscaling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MJeUJSBcSFgJ&ei=3WJeYvy0KMiBy9YP18Gi8As&json=", "num_citations": 90, "citedby_url": "/scholar?cites=6361435766800029488&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MJeUJSBcSFgJ:scholar.google.com/&scioq=Generating+High+Fidelity+Images+With+Subscale+Pixel+Networks+And+Multidimensional+Upscaling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.01608"}, "Relgan: Relational Generative Adversarial Networks For Text Generation": {"container_type": "Publication", "bib": {"title": "Relgan: Relational generative adversarial networks for text generation", "author": ["W Nie", "N Narodytska", "A Patel"], "pub_year": "2018", "venue": "International conference on learning \u2026", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJedV3R5tm", "author_id": ["zW7BH7oAAAAJ", "pQw6xK4AAAAJ", "Gbe5UncAAAAJ"], "url_scholarbib": "/scholar?q=info:S5vqyVJ4SnYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelgan:%2BRelational%2BGenerative%2BAdversarial%2BNetworks%2BFor%2BText%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S5vqyVJ4SnYJ&ei=4GJeYuPRNd-Vy9YPs66ekAk&json=", "num_citations": 115, "citedby_url": "/scholar?cites=8523757541722331979&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:S5vqyVJ4SnYJ:scholar.google.com/&scioq=Relgan:+Relational+Generative+Adversarial+Networks+For+Text+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJedV3R5tm"}, "Adaptive Estimators Show Information Compression In Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Adaptive estimators show information compression in deep neural networks", "author": ["I Chelombiev", "C Houghton", "C O'Donnell"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09037", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09037", "author_id": ["ughmF9IAAAAJ", "VoP4kDQAAAAJ", "KGKHB2QAAAAJ"], "url_scholarbib": "/scholar?q=info:RV03iArt1SQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BEstimators%2BShow%2BInformation%2BCompression%2BIn%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RV03iArt1SQJ&ei=52JeYsaLIMiBy9YP18Gi8As&json=", "num_citations": 27, "citedby_url": "/scholar?cites=2654288184895561029&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RV03iArt1SQJ:scholar.google.com/&scioq=Adaptive+Estimators+Show+Information+Compression+In+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09037"}, "Unsupervised Discovery Of Parts, Structure, And Dynamics": {"container_type": "Publication", "bib": {"title": "Unsupervised discovery of parts, structure, and dynamics", "author": ["Z Xu", "Z Liu", "C Sun", "K Murphy", "WT Freeman"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.05136", "author_id": ["QE8cLMEAAAAJ", "3coYSTUAAAAJ", "vQa7heEAAAAJ", "MxxZkEcAAAAJ", "0zZnyMEAAAAJ"], "url_scholarbib": "/scholar?q=info:3viGTGEOmVsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BDiscovery%2BOf%2BParts,%2BStructure,%2BAnd%2BDynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3viGTGEOmVsJ&ei=6mJeYr1XkcqxAo23r5gD&json=", "num_citations": 48, "citedby_url": "/scholar?cites=6600322539946703070&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3viGTGEOmVsJ:scholar.google.com/&scioq=Unsupervised+Discovery+Of+Parts,+Structure,+And+Dynamics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.05136"}, "Adversarial Domain Adaptation For Stable Brain-machine Interfaces": {"container_type": "Publication", "bib": {"title": "Adversarial domain adaptation for stable brain-machine interfaces", "author": ["A Farshchian", "JA Gallego", "JP Cohen", "Y Bengio"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option to restore voluntary movements after paralysis. These devices are based on the ability to extract information about movement intent from neural signals recorded using multi-electrode arrays chronically implanted in the motor cortices of the brain. However, the inherent loss and turnover of recorded neurons requires repeated recalibrations of the interface, which can potentially alter the day-to-day user experience. The resulting need for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00045", "author_id": ["aet613kAAAAJ", "8ss6egkAAAAJ", "slS0dvUAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:oDahEd6gnRAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BDomain%2BAdaptation%2BFor%2BStable%2BBrain-machine%2BInterfaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oDahEd6gnRAJ&ei=7WJeYsWCH8iBy9YP18Gi8As&json=", "num_citations": 32, "citedby_url": "/scholar?cites=1197289951589381792&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oDahEd6gnRAJ:scholar.google.com/&scioq=Adversarial+Domain+Adaptation+For+Stable+Brain-machine+Interfaces&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00045"}, "Variational Autoencoders With Jointly Optimized Latent Dependency Structure": {"container_type": "Publication", "bib": {"title": "Variational autoencoders with jointly optimized latent dependency structure", "author": ["J He", "Y Gong", "J Marino", "G Mori"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "We propose a method for learning the dependency structure between latent variables in deep latent variable models. Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJgsCjCqt7", "author_id": ["", "MQUGM8IAAAAJ", "LTprTF0AAAAJ", "Bl9FSL0AAAAJ"], "url_scholarbib": "/scholar?q=info:ecJu5CMKL94J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BAutoencoders%2BWith%2BJointly%2BOptimized%2BLatent%2BDependency%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ecJu5CMKL94J&ei=8GJeYqrfK82Ny9YPqPyUgAs&json=", "num_citations": 8, "citedby_url": "/scholar?cites=16010026349597999737&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ecJu5CMKL94J:scholar.google.com/&scioq=Variational+Autoencoders+With+Jointly+Optimized+Latent+Dependency+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJgsCjCqt7"}, "Overcoming The Disentanglement Vs Reconstruction Trade-off Via Jacobian Supervision": {"container_type": "Publication", "bib": {"title": "Overcoming the disentanglement vs reconstruction trade-off via Jacobian supervision", "author": ["J Lezama"], "pub_year": "2018", "venue": "International Conference on Learning Representations", "abstract": "A major challenge in learning image representations is the disentangling of the factors of variation underlying the image formation. This is typically achieved with an autoencoder architecture where a subset of the latent variables is constrained to correspond to specific factors, and the rest of them are considered nuisance variables. This approach has an important drawback: as the dimension of the nuisance variables is increased, image reconstruction is improved, but the decoder has the flexibility to ignore the specified factors"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hkg4W2AcFm", "author_id": ["iDP84cQAAAAJ"], "url_scholarbib": "/scholar?q=info:8MyBJrXjkIkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOvercoming%2BThe%2BDisentanglement%2BVs%2BReconstruction%2BTrade-off%2BVia%2BJacobian%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8MyBJrXjkIkJ&ei=9WJeYuOyCt-Vy9YPs66ekAk&json=", "num_citations": 14, "citedby_url": "/scholar?cites=9912673147017088240&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8MyBJrXjkIkJ:scholar.google.com/&scioq=Overcoming+The+Disentanglement+Vs+Reconstruction+Trade-off+Via+Jacobian+Supervision&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hkg4W2AcFm"}, "Improving The Generalization Of Adversarial Training With Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Improving the generalization of adversarial training with domain adaptation", "author": ["C Song", "K He", "L Wang", "JE Hopcroft"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00740", "abstract": "By injecting adversarial examples into training data, adversarial training is promising for improving the robustness of deep learning models. However, most existing adversarial training approaches are based on a specific type of adversarial attack. It may not provide sufficiently representative samples from the adversarial domain, leading to a weak generalization ability on adversarial examples from other attacks. Moreover, during the adversarial training, adversarial perturbations on inputs are usually crafted by fast single"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00740", "author_id": ["el17bJoAAAAJ", "YTQnGJsAAAAJ", "VZHxoh8AAAAJ", "4Z6vo5QAAAAJ"], "url_scholarbib": "/scholar?q=info:-1W_I7vj8a0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BThe%2BGeneralization%2BOf%2BAdversarial%2BTraining%2BWith%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-1W_I7vj8a0J&ei=-GJeYozEJZHKsQKNt6-YAw&json=", "num_citations": 89, "citedby_url": "/scholar?cites=12534049630846932475&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-1W_I7vj8a0J:scholar.google.com/&scioq=Improving+The+Generalization+Of+Adversarial+Training+With+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00740.pdf?ref=https://githubhelp.com"}, "Learning To Schedule Communication In Multi-agent Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning to schedule communication in multi-agent reinforcement learning", "author": ["D Kim", "S Moon", "D Hostallero", "WJ Kang", "T Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents' interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.01554", "author_id": ["vMDdUQgAAAAJ", "", "vk6eM5QAAAAJ", "YXqc3VkAAAAJ", ""], "url_scholarbib": "/scholar?q=info:eJR0dnWauyEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BSchedule%2BCommunication%2BIn%2BMulti-agent%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eJR0dnWauyEJ&ei=-2JeYqzONZGJmwGIxre4DA&json=", "num_citations": 92, "citedby_url": "/scholar?cites=2430706253185717368&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eJR0dnWauyEJ:scholar.google.com/&scioq=Learning+To+Schedule+Communication+In+Multi-agent+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.01554"}, "Learning To Understand Goal Specifications By Modelling Reward": {"container_type": "Publication", "bib": {"title": "Learning to understand goal specifications by modelling reward", "author": ["D Bahdanau", "F Hill", "J Leike", "E Hughes"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.01946", "author_id": ["Nq0dVMcAAAAJ", "4HLUnhIAAAAJ", "beiWcokAAAAJ", "3tj5358AAAAJ"], "url_scholarbib": "/scholar?q=info:UCNLddoCBa4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BUnderstand%2BGoal%2BSpecifications%2BBy%2BModelling%2BReward%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UCNLddoCBa4J&ei=_mJeYoSULovMsQK69Y7ABg&json=", "num_citations": 83, "citedby_url": "/scholar?cites=12539431874776998736&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UCNLddoCBa4J:scholar.google.com/&scioq=Learning+To+Understand+Goal+Specifications+By+Modelling+Reward&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.01946"}, "Model-predictive Policy Learning With Uncertainty Regularization For Driving In Dense Traffic": {"container_type": "Publication", "bib": {"title": "Model-predictive policy learning with uncertainty regularization for driving in dense traffic", "author": ["M Henaff", "A Canziani", "Y LeCun"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.02705", "abstract": "Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. We propose to train a policy by unrolling a learned model of the environment dynamics over multiple time steps while explicitly penalizing two costs: the original cost the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We measure this second cost by using the uncertainty of the dynamics model about its"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.02705", "author_id": ["bX__wkYAAAAJ", "dU5bJOMAAAAJ", "WLN3QrAAAAAJ"], "url_scholarbib": "/scholar?q=info:zMBfpuySD0YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel-predictive%2BPolicy%2BLearning%2BWith%2BUncertainty%2BRegularization%2BFor%2BDriving%2BIn%2BDense%2BTraffic%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zMBfpuySD0YJ&ei=AWNeYt3xJ4vMsQK69Y7ABg&json=", "num_citations": 83, "citedby_url": "/scholar?cites=5048415252406845644&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zMBfpuySD0YJ:scholar.google.com/&scioq=Model-predictive+Policy+Learning+With+Uncertainty+Regularization+For+Driving+In+Dense+Traffic&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.02705"}, "Execution-guided Neural Program Synthesis": {"container_type": "Publication", "bib": {"title": "Execution-guided neural program synthesis", "author": ["X Chen", "C Liu", "D Song"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Note that our execution-guided synthesis algorithm can be applied to any neural synthesizer  \u0393,  In our evaluation, we demonstrate that our execution-guided synthesis technique helps"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1gfOiAqYm", "author_id": ["d4W1UT0AAAAJ", "Zrbs8hIAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:V1BeeE83wfYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExecution-guided%2BNeural%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=V1BeeE83wfYJ&ei=BWNeYvWcLovMsQK69Y7ABg&json=", "num_citations": 68, "citedby_url": "/scholar?cites=17780553618296819799&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:V1BeeE83wfYJ:scholar.google.com/&scioq=Execution-guided+Neural+Program+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1gfOiAqYm"}, "Adversarial Audio Synthesis": {"container_type": "Publication", "bib": {"title": "Adversarial audio synthesis", "author": ["C Donahue", "J McAuley", "M Puckette"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.04208", "abstract": ", but they have seen little application to audio generation. In this paper we introduce   synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.04208", "author_id": ["MgzHAPQAAAAJ", "icbo4M0AAAAJ", "MvRffMcAAAAJ"], "url_scholarbib": "/scholar?q=info:Mo1T5oAgI1IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BAudio%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Mo1T5oAgI1IJ&ei=C2NeYrPfGsiBy9YP18Gi8As&json=", "num_citations": 396, "citedby_url": "/scholar?cites=5918610073287101746&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Mo1T5oAgI1IJ:scholar.google.com/&scioq=Adversarial+Audio+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.04208"}, "Aggregated Momentum: Stability Through Passive Damping": {"container_type": "Publication", "bib": {"title": "Aggregated momentum: Stability through passive damping", "author": ["J Lucas", "S Sun", "R Zemel", "R Grosse"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.00325", "abstract": "Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions. Its performance depends crucially on a damping coefficient $\\beta $. Large $\\beta $ values can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9. We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different $\\beta $ parameters. AggMo is trivial to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.00325", "author_id": ["AYaHBAQAAAAJ", "", "iBeDoRAAAAAJ", "xgQd1qgAAAAJ"], "url_scholarbib": "/scholar?q=info:MiMnKAiyzjUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAggregated%2BMomentum:%2BStability%2BThrough%2BPassive%2BDamping%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MiMnKAiyzjUJ&ei=D2NeYqeLIM2Ny9YPqPyUgAs&json=", "num_citations": 43, "citedby_url": "/scholar?cites=3877232077315711794&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MiMnKAiyzjUJ:scholar.google.com/&scioq=Aggregated+Momentum:+Stability+Through+Passive+Damping&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.00325"}, "Multi-step Retriever-reader Interaction For Scalable Open-domain Question Answering": {"container_type": "Publication", "bib": {"title": "Multi-step retriever-reader interaction for scalable open-domain question answering", "author": ["R Das", "S Dhuliawala", "M Zaheer"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "for open-domain question answering in which the retriever and the reader iteratively interact  with  We conduct analysis and show that iterative interaction helps in retrieving informative"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.05733", "author_id": ["FKoKAwIAAAAJ", "7O33ij4AAAAJ", "A33FhJMAAAAJ"], "url_scholarbib": "/scholar?q=info:lZZNbZIK8PcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-step%2BRetriever-reader%2BInteraction%2BFor%2BScalable%2BOpen-domain%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lZZNbZIK8PcJ&ei=E2NeYo71G4-bmAGmiqCIBw&json=", "num_citations": 135, "citedby_url": "/scholar?cites=17865791345794061973&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lZZNbZIK8PcJ:scholar.google.com/&scioq=Multi-step+Retriever-reader+Interaction+For+Scalable+Open-domain+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.05733"}, "Time-agnostic Prediction: Predicting Predictable Video Frames": {"container_type": "Publication", "bib": {"title": "Time-agnostic prediction: Predicting predictable video frames", "author": ["D Jayaraman", "F Ebert", "AA Efros", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.07784", "author_id": ["QxLpghAAAAAJ", "OFlBL2kAAAAJ", "d97bGd8AAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:lRIN10QMdUgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTime-agnostic%2BPrediction:%2BPredicting%2BPredictable%2BVideo%2BFrames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lRIN10QMdUgJ&ei=GGNeYoeKDsLZmQHc1ovQAg&json=", "num_citations": 61, "citedby_url": "/scholar?cites=5221092832811225749&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lRIN10QMdUgJ:scholar.google.com/&scioq=Time-agnostic+Prediction:+Predicting+Predictable+Video+Frames&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.07784"}, "Knowledge Flow: Improve Upon Your Teachers": {"container_type": "Publication", "bib": {"title": "Knowledge flow: Improve upon your teachers", "author": ["IJ Liu", "J Peng", "AG Schwing"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.05878", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves' knowledge'from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.05878", "author_id": ["rwIHKw4AAAAJ", "H2JX-RQAAAAJ", "3B2c31wAAAAJ"], "url_scholarbib": "/scholar?q=info:NQVN8MXSZagJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKnowledge%2BFlow:%2BImprove%2BUpon%2BYour%2BTeachers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NQVN8MXSZagJ&ei=G2NeYt64Od-Vy9YPs66ekAk&json=", "num_citations": 34, "citedby_url": "/scholar?cites=12134336518601639221&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NQVN8MXSZagJ:scholar.google.com/&scioq=Knowledge+Flow:+Improve+Upon+Your+Teachers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.05878"}, "Learning Factorized Multimodal Representations": {"container_type": "Publication", "bib": {"title": "Learning factorized multimodal representations", "author": ["YHH Tsai", "PP Liang", "A Zadeh", "LP Morency"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.06176", "author_id": ["TMimDRoAAAAJ", "pKf5LtQAAAAJ", "FfQXY34AAAAJ", "APgaFK0AAAAJ"], "url_scholarbib": "/scholar?q=info:3QbM7jRadCQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFactorized%2BMultimodal%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3QbM7jRadCQJ&ei=HmNeYsm5Os6E6rQPz8uiuAc&json=", "num_citations": 144, "citedby_url": "/scholar?cites=2626823666054989533&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3QbM7jRadCQJ:scholar.google.com/&scioq=Learning+Factorized+Multimodal+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.06176"}, "Learning Robust Representations By Projecting Superficial Statistics Out": {"container_type": "Publication", "bib": {"title": "Learning robust representations by projecting superficial statistics out", "author": ["H Wang", "Z He", "ZC Lipton", "EP Xing"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.06256", "abstract": "Despite impressive performance as evaluated on iid holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.06256", "author_id": ["nZxJGeUAAAAJ", "-JrCM0AAAAAJ", "MN9Kfg8AAAAJ", "5pKTRxEAAAAJ"], "url_scholarbib": "/scholar?q=info:W8zvSGZDG5UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRobust%2BRepresentations%2BBy%2BProjecting%2BSuperficial%2BStatistics%2BOut%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W8zvSGZDG5UJ&ei=ImNeYtqSBIvMsQK69Y7ABg&json=", "num_citations": 108, "citedby_url": "/scholar?cites=10744255442611850331&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W8zvSGZDG5UJ:scholar.google.com/&scioq=Learning+Robust+Representations+By+Projecting+Superficial+Statistics+Out&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.06256"}, "Deep Convolutional Networks As Shallow Gaussian Processes": {"container_type": "Publication", "bib": {"title": "Deep convolutional networks as shallow gaussian processes", "author": ["A Garriga-Alonso", "CE Rasmussen"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We show that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike\" deep kernels\", has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.05587", "author_id": ["OtnThiMAAAAJ", "aqgFQqMAAAAJ"], "url_scholarbib": "/scholar?q=info:STuAa2zCETYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BConvolutional%2BNetworks%2BAs%2BShallow%2BGaussian%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=STuAa2zCETYJ&ei=KWNeYpDHMIvMsQK69Y7ABg&json=", "num_citations": 163, "citedby_url": "/scholar?cites=3896108923568012105&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:STuAa2zCETYJ:scholar.google.com/&scioq=Deep+Convolutional+Networks+As+Shallow+Gaussian+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.05587"}, "Multi-domain Adversarial Learning": {"container_type": "Publication", "bib": {"title": "Multi-domain adversarial learning", "author": ["A Schoenauer-Sebag", "L Heinrich"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Multi-domain learning (MDL) aims at obtaining a model with minimal average risk across  multiple domains. Our empirical motivation is automated microscopy data, where cultured cells"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.09239", "author_id": ["HHCrGDMAAAAJ", "ag8K4zoAAAAJ"], "url_scholarbib": "/scholar?q=info:aVcuwaA8SLMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-domain%2BAdversarial%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aVcuwaA8SLMJ&ei=LGNeYvLtN9-Vy9YPs66ekAk&json=", "num_citations": 48, "citedby_url": "/scholar?cites=12918642192245741417&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aVcuwaA8SLMJ:scholar.google.com/&scioq=Multi-domain+Adversarial+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.09239"}, "Diversity Is All You Need: Learning Skills Without A Reward Function": {"container_type": "Publication", "bib": {"title": "Diversity is all you need: Learning skills without a reward function", "author": ["B Eysenbach", "A Gupta", "J Ibarz", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.06070", "abstract": "Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.06070", "author_id": ["DRnOvU8AAAAJ", "1wLVDP4AAAAJ", "l-la0GQAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:NhHOeZM0CasJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiversity%2BIs%2BAll%2BYou%2BNeed:%2BLearning%2BSkills%2BWithout%2BA%2BReward%2BFunction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NhHOeZM0CasJ&ei=OGNeYuD4ForyyASen4NI&json=", "num_citations": 510, "citedby_url": "/scholar?cites=12324439663284457782&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NhHOeZM0CasJ:scholar.google.com/&scioq=Diversity+Is+All+You+Need:+Learning+Skills+Without+A+Reward+Function&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.06070.pdf?ref=https://githubhelp.com"}, "Phase-aware Speech Enhancement With Deep Complex U-net": {"container_type": "Publication", "bib": {"title": "Phase-aware speech enhancement with deep complex u-net", "author": ["HS Choi", "JH Kim", "J Huh", "A Kim", "JW Ha"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Most deep learning-based models for speech enhancement have mainly focused on estimating the magnitude of spectrogram while reusing the phase from noisy speech for reconstruction. This is due to the difficulty of estimating the phase of clean speech. To improve speech enhancement performance, we tackle the phase estimation problem in three ways. First, we propose Deep Complex U-Net, an advanced U-Net structured model incorporating well-defined complex-valued building blocks to deal with complex-valued"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkeRTsAcYm", "author_id": ["YhtK34kAAAAJ", "8JKsHJcAAAAJ", "VDMZ-pQAAAAJ", "l6lDgpgAAAAJ", "eGj3ay4AAAAJ"], "url_scholarbib": "/scholar?q=info:aBM-D_jc858J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPhase-aware%2BSpeech%2BEnhancement%2BWith%2BDeep%2BComplex%2BU-net%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aBM-D_jc858J&ei=-GBeYu-pIovMsQK69Y7ABg&json=", "num_citations": 140, "citedby_url": "/scholar?cites=11525798829336957800&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aBM-D_jc858J:scholar.google.com/&scioq=Phase-aware+Speech+Enhancement+With+Deep+Complex+U-net&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkeRTsAcYm"}, "Go Gradient For Expectation-based Objectives": {"container_type": "Publication", "bib": {"title": "GO gradient for expectation-based objectives", "author": ["Y Cong", "M Zhao", "K Bai", "L Carin"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.06020", "abstract": "Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\\gammav $ for expectation-based objectives $\\Ebb_ {q_ {\\gammav}(\\yv)}[f (\\yv)] $. Most existing methods either (i) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or (ii) they only apply to reparameterizable continuous random variables and employ a reparameterization trick. To address these limitations, we propose a General and One"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.06020", "author_id": ["i_XfCIkAAAAJ", "hXm2uDEAAAAJ", "CbbhlMoAAAAJ", "ZhGL6WcAAAAJ"], "url_scholarbib": "/scholar?q=info:727R0nGCg7gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGo%2BGradient%2BFor%2BExpectation-based%2BObjectives%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=727R0nGCg7gJ&ei=_GBeYoOpJJHKsQKNt6-YAw&json=", "num_citations": 14, "citedby_url": "/scholar?cites=13295613950307692271&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:727R0nGCg7gJ:scholar.google.com/&scioq=Go+Gradient+For+Expectation-based+Objectives&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.06020"}, "The Unusual Effectiveness Of Averaging In Gan Training": {"container_type": "Publication", "bib": {"title": "The unusual effectiveness of averaging in GAN training", "author": ["Y Yaz", "CS Foo", "S Winkler", "KH Yap"], "pub_year": "2018", "venue": "\u2026 on Learning \u2026", "abstract": "We examine two different techniques for parameter averaging in GAN training. Moving  Average (MA) computes the time-average of parameters, whereas Exponential Moving Average ("}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJgw_sRqFQ&source=post_page---------------------------", "author_id": ["", "AgbeqGkAAAAJ", "R8XE-aoAAAAJ", "nr86m98AAAAJ"], "url_scholarbib": "/scholar?q=info:F6vn4RKOTUIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BUnusual%2BEffectiveness%2BOf%2BAveraging%2BIn%2BGan%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F6vn4RKOTUIJ&ei=_2BeYr2aB5GJmwGIxre4DA&json=", "num_citations": 49, "citedby_url": "/scholar?cites=4777630991460576023&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F6vn4RKOTUIJ:scholar.google.com/&scioq=The+Unusual+Effectiveness+Of+Averaging+In+Gan+Training&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJgw_sRqFQ"}, "Sample Efficient Imitation Learning For Continuous Control": {"container_type": "Publication", "bib": {"title": "Sample efficient imitation learning for continuous control", "author": ["F Sasaki", "T Yohira", "A Kawaguchi"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkN5UoAqF7", "author_id": ["", "", "JuJGnvcAAAAJ"], "url_scholarbib": "/scholar?q=info:RGAAUVxdK_8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSample%2BEfficient%2BImitation%2BLearning%2BFor%2BContinuous%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RGAAUVxdK_8J&ei=A2FeYtPlC8iBy9YP18Gi8As&json=", "num_citations": 47, "citedby_url": "/scholar?cites=18386892554747535428&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RGAAUVxdK_8J:scholar.google.com/&scioq=Sample+Efficient+Imitation+Learning+For+Continuous+Control&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkN5UoAqF7"}, "Analysing Mathematical Reasoning Abilities Of Neural Models": {"container_type": "Publication", "bib": {"title": "Analysing mathematical reasoning abilities of neural models", "author": ["D Saxton", "E Grefenstette", "F Hill", "P Kohli"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.01557", "abstract": "Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.01557", "author_id": ["ekterPoAAAAJ", "ezllEwMAAAAJ", "4HLUnhIAAAAJ", "3pyzQQ8AAAAJ"], "url_scholarbib": "/scholar?q=info:MPFNdLNQ20cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalysing%2BMathematical%2BReasoning%2BAbilities%2BOf%2BNeural%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MPFNdLNQ20cJ&ei=B2FeYpX8BN-Vy9YPs66ekAk&json=", "num_citations": 160, "citedby_url": "/scholar?cites=5177820928273150256&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MPFNdLNQ20cJ:scholar.google.com/&scioq=Analysing+Mathematical+Reasoning+Abilities+Of+Neural+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.01557"}, "Bayesian Policy Optimization For Model Uncertainty": {"container_type": "Publication", "bib": {"title": "Bayesian policy optimization for model uncertainty", "author": ["G Lee", "B Hou", "A Mandalika", "J Lee", "S Choudhury"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world. We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01014", "author_id": ["-lONjNgAAAAJ", "ucSLToQAAAAJ", "of_W1_MAAAAJ", "DJ6mzSUAAAAJ", "5lB_d78AAAAJ"], "url_scholarbib": "/scholar?q=info:Bg2ZAgT4thMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2BPolicy%2BOptimization%2BFor%2BModel%2BUncertainty%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Bg2ZAgT4thMJ&ei=CmFeYv-fDo-bmAGmiqCIBw&json=", "num_citations": 27, "citedby_url": "/scholar?cites=1420595428589112582&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Bg2ZAgT4thMJ:scholar.google.com/&scioq=Bayesian+Policy+Optimization+For+Model+Uncertainty&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01014"}, "Supervised Policy Update For Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Supervised policy update for deep reinforcement learning", "author": ["Q Vuong", "Y Zhang", "KW Ross"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.11706", "abstract": "We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11706", "author_id": ["NSWI3OwAAAAJ", "A7-xkvcAAAAJ", "RhUcYmQAAAAJ"], "url_scholarbib": "/scholar?q=info:iG5qXKF0MYYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSupervised%2BPolicy%2BUpdate%2BFor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iG5qXKF0MYYJ&ei=DWFeYr2wPJGJmwGIxre4DA&json=", "num_citations": 15, "citedby_url": "/scholar?cites=9669638111330201224&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iG5qXKF0MYYJ:scholar.google.com/&scioq=Supervised+Policy+Update+For+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11706"}, "Robust Conditional Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Robust conditional generative adversarial networks", "author": ["GG Chrysos", "J Kossaifi", "S Zafeiriou"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.08657", "abstract": "Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has been on performance improvement, while there has been little effort in making cGAN more robust to noise. The regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGAN unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.08657", "author_id": ["1bU041kAAAAJ", "hJS2TXwAAAAJ", "QKOH5iYAAAAJ"], "url_scholarbib": "/scholar?q=info:oqaOGNAzIdwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BConditional%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oqaOGNAzIdwJ&ei=EWFeYpmGEc2Ny9YPqPyUgAs&json=", "num_citations": 48, "citedby_url": "/scholar?cites=15862016331433813666&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oqaOGNAzIdwJ:scholar.google.com/&scioq=Robust+Conditional+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.08657"}, "Accumulation Bit-width Scaling For Ultra-low Precision Training Of Deep Networks": {"container_type": "Publication", "bib": {"title": "Accumulation bit-width scaling for ultra-low precision training of deep networks", "author": ["C Sakr", "N Wang", "CY Chen", "J Choi", "A Agrawal"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, yet employ wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. The absence of any framework to analyze the precision requirements of partial sum accumulations results in conservative design choices. This imposes an upper-bound on the reduction of complexity of multiply-accumulate units. We present a statistical"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.06588", "author_id": ["Ks1WOEUAAAAJ", "DRuE88cAAAAJ", "", "YPT98zwAAAAJ", "VZHyAQcAAAAJ"], "url_scholarbib": "/scholar?q=info:E6MxbsJBh-UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccumulation%2BBit-width%2BScaling%2BFor%2BUltra-low%2BPrecision%2BTraining%2BOf%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=E6MxbsJBh-UJ&ei=FWFeYrSdCI-bmAGmiqCIBw&json=", "num_citations": 18, "citedby_url": "/scholar?cites=16539260459869643539&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:E6MxbsJBh-UJ:scholar.google.com/&scioq=Accumulation+Bit-width+Scaling+For+Ultra-low+Precision+Training+Of+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.06588"}, "Learning Concise Representations For Regression By Evolving Networks Of Trees": {"container_type": "Publication", "bib": {"title": "Learning concise representations for regression by evolving networks of trees", "author": ["W La Cava", "TR Singh", "J Taggart", "S Suri"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.00981", "author_id": ["iZB7inEAAAAJ", "", "", ""], "url_scholarbib": "/scholar?q=info:OABkNHONo3cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BConcise%2BRepresentations%2BFor%2BRegression%2BBy%2BEvolving%2BNetworks%2BOf%2BTrees%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OABkNHONo3cJ&ei=GGFeYuesLo-bmAGmiqCIBw&json=", "num_citations": 12, "citedby_url": "/scholar?cites=8620889637656985656&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OABkNHONo3cJ:scholar.google.com/&scioq=Learning+Concise+Representations+For+Regression+By+Evolving+Networks+Of+Trees&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.00981"}, "Query-efficient Hard-label Black-box Attack: An Optimization-based Approach": {"container_type": "Publication", "bib": {"title": "Query-efficient hard-label black-box attack: An optimization-based approach", "author": ["M Cheng", "T Le", "PY Chen", "J Yi", "H Zhang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the problem of attacking a machine learning model in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (eg, CW or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only current approach is based on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.04457", "author_id": ["_LkC1yoAAAAJ", "pR5IazYAAAAJ", "jxwlCUUAAAAJ", "lZxRZ84AAAAJ", "LTa3GzEAAAAJ"], "url_scholarbib": "/scholar?q=info:Ubu6loFQAUcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuery-efficient%2BHard-label%2BBlack-box%2BAttack:%2BAn%2BOptimization-based%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ubu6loFQAUcJ&ei=G2FeYtWfJ5WMy9YPt8OamA0&json=", "num_citations": 231, "citedby_url": "/scholar?cites=5116459169179417425&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ubu6loFQAUcJ:scholar.google.com/&scioq=Query-efficient+Hard-label+Black-box+Attack:+An+Optimization-based+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.04457.pdf)."}, "Variance Reduction For Reinforcement Learning In Input-driven Environments": {"container_type": "Publication", "bib": {"title": "Variance reduction for reinforcement learning in input-driven environments", "author": ["H Mao", "SB Venkatakrishnan", "M Schwarzkopf"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.02264", "author_id": ["zQbwxcwAAAAJ", "9wLpwbsAAAAJ", "Lf1hSacAAAAJ"], "url_scholarbib": "/scholar?q=info:ELUvT-zZXaUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance%2BReduction%2BFor%2BReinforcement%2BLearning%2BIn%2BInput-driven%2BEnvironments%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ELUvT-zZXaUJ&ei=HmFeYtigIN-Vy9YPs66ekAk&json=", "num_citations": 58, "citedby_url": "/scholar?cites=11915919798056236304&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ELUvT-zZXaUJ:scholar.google.com/&scioq=Variance+Reduction+For+Reinforcement+Learning+In+Input-driven+Environments&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.02264"}, "Conditional Network Embeddings": {"container_type": "Publication", "bib": {"title": "Conditional network embeddings", "author": ["B Kang", "J Lijffijt", "T De Bie"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.07544", "abstract": "Conditional Network Embedding (CNE)\u2014that allows optimizing embeddings wrt certain prior  knowledge about the network NE conditional on certain prior knowledge about the network."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.07544", "author_id": ["_DxP-KUAAAAJ", "cBSEeSMAAAAJ", "eH_c4R4AAAAJ"], "url_scholarbib": "/scholar?q=info:1HLBDx44aJsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConditional%2BNetwork%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1HLBDx44aJsJ&ei=J2FeYoDFMt-Vy9YPs66ekAk&json=", "num_citations": 33, "citedby_url": "/scholar?cites=11198262175221248724&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1HLBDx44aJsJ:scholar.google.com/&scioq=Conditional+Network+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.07544"}, "On The Relation Between The Sharpest Directions Of Dnn Loss And The Sgd Step Length": {"container_type": "Publication", "bib": {"title": "On the relation between the sharpest directions of DNN loss and the SGD step length", "author": ["S Jastrz\u0119bski", "Z Kenton", "N Ballas", "A Fischer"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Stochastic Gradient Descent (SGD) based training of neural networks with a large learning rate or a small batch-size typically ends in well-generalizing, flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. However, the curvature along the SGD trajectory is poorly understood. An empirical investigation shows that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. When studying the SGD dynamics in relation to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.05031", "author_id": ["wbJxGQ8AAAAJ", "bp6Ah4EAAAAJ", "euUV4iUAAAAJ", "FyZbyIUAAAAJ"], "url_scholarbib": "/scholar?q=info:Zk7kd9IViDUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BRelation%2BBetween%2BThe%2BSharpest%2BDirections%2BOf%2BDnn%2BLoss%2BAnd%2BThe%2BSgd%2BStep%2BLength%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Zk7kd9IViDUJ&ei=M2FeYsKRLt-Vy9YPs66ekAk&json=", "num_citations": 47, "citedby_url": "/scholar?cites=3857357074541596262&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Zk7kd9IViDUJ:scholar.google.com/&scioq=On+The+Relation+Between+The+Sharpest+Directions+Of+Dnn+Loss+And+The+Sgd+Step+Length&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.05031"}, "Accelerating Nonconvex Learning Via Replica Exchange Langevin Diffusion": {"container_type": "Publication", "bib": {"title": "Accelerating nonconvex learning via replica exchange Langevin diffusion", "author": ["Y Chen", "J Chen", "J Dong", "J Peng", "Z Wang"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between``global exploration''and``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2007.01990", "author_id": ["", "", "o9g6YPMAAAAJ", "H2JX-RQAAAAJ", "HSx0BgQAAAAJ"], "url_scholarbib": "/scholar?q=info:BWbhCpjhOJUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccelerating%2BNonconvex%2BLearning%2BVia%2BReplica%2BExchange%2BLangevin%2BDiffusion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BWbhCpjhOJUJ&ei=OGFeYsLhJY6pywSdh6agAg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=10752592153480881669&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BWbhCpjhOJUJ:scholar.google.com/&scioq=Accelerating+Nonconvex+Learning+Via+Replica+Exchange+Langevin+Diffusion&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2007.01990"}, "Self-monitoring Navigation Agent Via Auxiliary Progress Estimation": {"container_type": "Publication", "bib": {"title": "Self-monitoring navigation agent via auxiliary progress estimation", "author": ["CY Ma", "J Lu", "Z Wu", "G AlRegib", "Z Kira", "R Socher"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components:(1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.03035", "author_id": ["HrrtgKkAAAAJ", "zP9K32EAAAAJ", "7t12hVkAAAAJ", "7k5QSdoAAAAJ", "2a5XgNAAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:1qakmQ7UYUsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-monitoring%2BNavigation%2BAgent%2BVia%2BAuxiliary%2BProgress%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1qakmQ7UYUsJ&ei=O2FeYqyuLt-Vy9YPs66ekAk&json=", "num_citations": 144, "citedby_url": "/scholar?cites=5431855784757864150&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1qakmQ7UYUsJ:scholar.google.com/&scioq=Self-monitoring+Navigation+Agent+Via+Auxiliary+Progress+Estimation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.03035"}, "Efficient Multi-objective Neural Architecture Search Via Lamarckian Evolution": {"container_type": "Publication", "bib": {"title": "Efficient multi-objective neural architecture search via lamarckian evolution", "author": ["T Elsken", "JH Metzen", "F Hutter"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1804.09081", "abstract": "Neural Architecture Search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons:(1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption,(2) most architecture search methods require vast computational resources"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.09081", "author_id": ["tzDC8FQAAAAJ", "w047VfEAAAAJ", "YUrxwrkAAAAJ"], "url_scholarbib": "/scholar?q=info:v-vyPyfwHGUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BMulti-objective%2BNeural%2BArchitecture%2BSearch%2BVia%2BLamarckian%2BEvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v-vyPyfwHGUJ&ei=QGFeYuKGN4-bmAGmiqCIBw&json=", "num_citations": 341, "citedby_url": "/scholar?cites=7285962348545895359&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v-vyPyfwHGUJ:scholar.google.com/&scioq=Efficient+Multi-objective+Neural+Architecture+Search+Via+Lamarckian+Evolution&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.09081"}, "Learning Deep Representations By Mutual Information Estimation And Maximization": {"container_type": "Publication", "bib": {"title": "Learning deep representations by mutual information estimation and maximization", "author": ["RD Hjelm", "A Fedorov", "S Lavoie-Marchildon"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.06670", "author_id": ["68c5HfwAAAAJ", "Jm3jBOEAAAAJ", "pnZiH_0AAAAJ"], "url_scholarbib": "/scholar?q=info:dLxx8MzAU34J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDeep%2BRepresentations%2BBy%2BMutual%2BInformation%2BEstimation%2BAnd%2BMaximization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dLxx8MzAU34J&ei=RGFeYtjeH5WMy9YPt8OamA0&json=", "num_citations": 1305, "citedby_url": "/scholar?cites=9102831258285751412&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dLxx8MzAU34J:scholar.google.com/&scioq=Learning+Deep+Representations+By+Mutual+Information+Estimation+And+Maximization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.06670.pdf?source=post_page---------------------------"}, "Spectral Inference Networks: Unifying Deep And Spectral Learning": {"container_type": "Publication", "bib": {"title": "Spectral inference networks: Unifying deep and spectral learning", "author": ["D Pfau", "S Petersen", "A Agarwal", "DGT Barrett"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.02215", "author_id": ["rgRibJwAAAAJ", "RS3IjvIAAAAJ", "Uz_sAYYAAAAJ", "Whh_d2EAAAAJ"], "url_scholarbib": "/scholar?q=info:36EKy7VENucJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpectral%2BInference%2BNetworks:%2BUnifying%2BDeep%2BAnd%2BSpectral%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=36EKy7VENucJ&ei=R2FeYorbCciBy9YP18Gi8As&json=", "num_citations": 12, "citedby_url": "/scholar?cites=16660579419089969631&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:36EKy7VENucJ:scholar.google.com/&scioq=Spectral+Inference+Networks:+Unifying+Deep+And+Spectral+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.02215"}, "Bounce And Learn: Modeling Scene Dynamics With Real-world Bounces": {"container_type": "Publication", "bib": {"title": "Bounce and learn: Modeling scene dynamics with real-world bounces", "author": ["S Purushwalkam", "A Gupta", "DM Kaufman"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing-restitution and effective collision normals. Our model, Bounce and Learn, comprises two modules--a Physics Inference Module (PIM) and a Visual Inference Module (VIM). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM learns to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.06827", "author_id": ["T3Tt0S8AAAAJ", "bqL73OkAAAAJ", "wLRwwYcAAAAJ"], "url_scholarbib": "/scholar?q=info:I_VLYmXxDCoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBounce%2BAnd%2BLearn:%2BModeling%2BScene%2BDynamics%2BWith%2BReal-world%2BBounces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=I_VLYmXxDCoJ&ei=SmFeYuvQEpHKsQKNt6-YAw&json=", "num_citations": 17, "citedby_url": "/scholar?cites=3030062067056637219&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:I_VLYmXxDCoJ:scholar.google.com/&scioq=Bounce+And+Learn:+Modeling+Scene+Dynamics+With+Real-world+Bounces&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.06827"}, "Integer Networks For Data Compression With Latent-variable Models": {"container_type": "Publication", "bib": {"title": "Integer networks for data compression with latent-variable models", "author": ["J Ball\u00e9", "N Johnston", "D Minnen"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "We consider the problem of using variational latent-variable models for data compression. For such models to produce a compressed binary sequence, which is the universal data representation in a digital world, the latent representation needs to be subjected to entropy coding. Range coding as an entropy coding technique is optimal, but it can fail catastrophically if the computation of the prior differs even slightly between the sending and the receiving side. Unfortunately, this is a common scenario when floating point math is used"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1zz2i0cY7", "author_id": ["uKDe38UAAAAJ", "9QR0uE4AAAAJ", "Pa8qzYQAAAAJ"], "url_scholarbib": "/scholar?q=info:o2DmlOKegm8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInteger%2BNetworks%2BFor%2BData%2BCompression%2BWith%2BLatent-variable%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o2DmlOKegm8J&ei=TWFeYpbaCN-Vy9YPs66ekAk&json=", "num_citations": 19, "citedby_url": "/scholar?cites=8035159381180309667&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o2DmlOKegm8J:scholar.google.com/&scioq=Integer+Networks+For+Data+Compression+With+Latent-variable+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1zz2i0cY7"}, "Woulda, Coulda, Shoulda: Counterfactually-guided Policy Search": {"container_type": "Publication", "bib": {"title": "Woulda, coulda, shoulda: Counterfactually-guided policy search", "author": ["L Buesing", "T Weber", "Y Zwols", "S Racaniere"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, actions that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.06272", "author_id": ["1h_mxPMAAAAJ", "LZxqcX4AAAAJ", "2EvUQlcAAAAJ", "o-h0vrQAAAAJ"], "url_scholarbib": "/scholar?q=info:eAYUeLMaKLQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWoulda,%2BCoulda,%2BShoulda:%2BCounterfactually-guided%2BPolicy%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eAYUeLMaKLQJ&ei=VmFeYpWsIpyO6rQP-viEEA&json=", "num_citations": 80, "citedby_url": "/scholar?cites=12981655284011501176&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eAYUeLMaKLQJ:scholar.google.com/&scioq=Woulda,+Coulda,+Shoulda:+Counterfactually-guided+Policy+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.06272"}, "Flowqa: Grasping Flow In History For Conversational Machine Comprehension": {"container_type": "Publication", "bib": {"title": "Flowqa: Grasping flow in history for conversational machine comprehension", "author": ["HY Huang", "E Choi", "W Yih"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.06683", "abstract": "Conversational machine comprehension requires the understanding of the conversation history, such as previous question/answer pairs, the document context, and the current question. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to approaches that concatenate previous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06683", "author_id": ["2y5YF-gAAAAJ", "aczkJSgAAAAJ", "8rDNIMsAAAAJ"], "url_scholarbib": "/scholar?q=info:m9d10YA4tLQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFlowqa:%2BGrasping%2BFlow%2BIn%2BHistory%2BFor%2BConversational%2BMachine%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m9d10YA4tLQJ&ei=WWFeYu7vLpWMy9YPt8OamA0&json=", "num_citations": 85, "citedby_url": "/scholar?cites=13021094548556076955&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m9d10YA4tLQJ:scholar.google.com/&scioq=Flowqa:+Grasping+Flow+In+History+For+Conversational+Machine+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06683.pdf?ref=https://githubhelp.com"}, "Von Mises-fisher Loss For Training Sequence To Sequence Models With Continuous Outputs": {"container_type": "Publication", "bib": {"title": "Von mises-fisher loss for training sequence to sequence models with continuous outputs", "author": ["S Kumar", "Y Tsvetkov"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.04616", "abstract": "The Softmax function is used in the final layer of nearly all existing sequence-to-sequence models for language generation. However, it is usually the slowest layer to compute which limits the vocabulary size to a subset of most frequent types; and it has a large memory footprint. We propose a general technique for replacing the softmax layer with a continuous embedding layer. Our primary innovations are a novel probabilistic loss, and a training and inference procedure in which we generate a probability distribution over pre-trained word"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.04616", "author_id": ["qO38fRIAAAAJ", "SEDPkrsAAAAJ"], "url_scholarbib": "/scholar?q=info:hN-7lZw_ShkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVon%2BMises-fisher%2BLoss%2BFor%2BTraining%2BSequence%2BTo%2BSequence%2BModels%2BWith%2BContinuous%2BOutputs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hN-7lZw_ShkJ&ei=XGFeYuC-NY-bmAGmiqCIBw&json=", "num_citations": 51, "citedby_url": "/scholar?cites=1822338940984352644&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hN-7lZw_ShkJ:scholar.google.com/&scioq=Von+Mises-fisher+Loss+For+Training+Sequence+To+Sequence+Models+With+Continuous+Outputs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.04616"}, "Towards The First Adversarially Robust Neural Network Model On Mnist": {"container_type": "Publication", "bib": {"title": "Towards the first adversarially robust neural network model on MNIST", "author": ["L Schott", "J Rauber", "M Bethge", "W Brendel"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.09190", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al.(1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations),(2) classifies unrecognizable images with high certainty,(3)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.09190", "author_id": ["I4mXKS4AAAAJ", "1ujJpuMAAAAJ", "0z0fNxUAAAAJ", "v-JL-hsAAAAJ"], "url_scholarbib": "/scholar?q=info:kDaSL_wZBWsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BThe%2BFirst%2BAdversarially%2BRobust%2BNeural%2BNetwork%2BModel%2BOn%2BMnist%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kDaSL_wZBWsJ&ei=YWFeYuXgLY-bmAGmiqCIBw&json=", "num_citations": 242, "citedby_url": "/scholar?cites=7711598507862406800&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kDaSL_wZBWsJ:scholar.google.com/&scioq=Towards+The+First+Adversarially+Robust+Neural+Network+Model+On+Mnist&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.09190.pdf?ref=https://githubhelp.com"}, "Off-policy Evaluation And Learning From Logged Bandit Feedback: Error Reduction Via Surrogate Policy": {"container_type": "Publication", "bib": {"title": "Off-policy evaluation and learning from logged bandit feedback: Error reduction via surrogate policy", "author": ["Y Xie", "B Liu", "Q Liu", "Z Wang", "Y Zhou", "J Peng"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "When learning from a batch of logged bandit feedback, the discrepancy between the policy to be learned and the off-policy training data imposes statistical and computational challenges. Unlike classical supervised learning and online learning settings, in batch contextual bandit learning, one only has access to a collection of logged feedback from the actions taken by a historical policy, and expect to learn a policy that takes good actions in possibly unseen contexts. Such a batch learning setting is ubiquitous in online and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.00232", "author_id": ["dK2ZuDcAAAAJ", "1G8RH_YAAAAJ", "XEx1fZkAAAAJ", "HSx0BgQAAAAJ", "j4Fshz0AAAAJ", "H2JX-RQAAAAJ"], "url_scholarbib": "/scholar?q=info:pYAfJnAfpqIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOff-policy%2BEvaluation%2BAnd%2BLearning%2BFrom%2BLogged%2BBandit%2BFeedback:%2BError%2BReduction%2BVia%2BSurrogate%2BPolicy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pYAfJnAfpqIJ&ei=ZWFeYpGbNMiBy9YP18Gi8As&json=", "num_citations": 12, "citedby_url": "/scholar?cites=11720089646814691493&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pYAfJnAfpqIJ:scholar.google.com/&scioq=Off-policy+Evaluation+And+Learning+From+Logged+Bandit+Feedback:+Error+Reduction+Via+Surrogate+Policy&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.00232"}, "Peernets: Exploiting Peer Wisdom Against Adversarial Attacks": {"container_type": "Publication", "bib": {"title": "Peernets: Exploiting peer wisdom against adversarial attacks", "author": ["J Svoboda", "J Masci", "F Monti", "MM Bronstein"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful uses. Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (eg autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.00088", "author_id": ["HS1H46sAAAAJ", "HwDTzQEAAAAJ", "NUdNFucAAAAJ", "UU3N6-UAAAAJ"], "url_scholarbib": "/scholar?q=info:1O1fS1xVy0MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPeernets:%2BExploiting%2BPeer%2BWisdom%2BAgainst%2BAdversarial%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1O1fS1xVy0MJ&ei=aWFeYoaLBJGJmwGIxre4DA&json=", "num_citations": 36, "citedby_url": "/scholar?cites=4885092075703365076&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1O1fS1xVy0MJ:scholar.google.com/&scioq=Peernets:+Exploiting+Peer+Wisdom+Against+Adversarial+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.00088"}, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks": {"container_type": "Publication", "bib": {"title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "author": ["J Frankle", "M Carbin"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.03635", "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.03635", "author_id": ["MlLJapIAAAAJ", "mtejbKYAAAAJ"], "url_scholarbib": "/scholar?q=info:g345uM6lAMYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLottery%2BTicket%2BHypothesis:%2BFinding%2BSparse,%2BTrainable%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g345uM6lAMYJ&ei=dGFeYs2WJYvMsQK69Y7ABg&json=", "num_citations": 1604, "citedby_url": "/scholar?cites=14267585926782353027&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:g345uM6lAMYJ:scholar.google.com/&scioq=The+Lottery+Ticket+Hypothesis:+Finding+Sparse,+Trainable+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.03635?ref=https://githubhelp.com"}, "A Comprehensive, Application-oriented Study Of Catastrophic Forgetting In Dnns": {"container_type": "Publication", "bib": {"title": "A comprehensive, application-oriented study of catastrophic forgetting in dnns", "author": ["B Pf\u00fclb", "A Gepperth"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.08101", "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.08101", "author_id": ["n3FRZgoAAAAJ", "QR2zb3IAAAAJ"], "url_scholarbib": "/scholar?q=info:z-las8cuhDwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BComprehensive,%2BApplication-oriented%2BStudy%2BOf%2BCatastrophic%2BForgetting%2BIn%2BDnns%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=z-las8cuhDwJ&ei=eGFeYqD2DZGJmwGIxre4DA&json=", "num_citations": 52, "citedby_url": "/scholar?cites=4360661774443932111&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:z-las8cuhDwJ:scholar.google.com/&scioq=A+Comprehensive,+Application-oriented+Study+Of+Catastrophic+Forgetting+In+Dnns&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.08101"}, "Algorithmic Framework For Model-based Deep Reinforcement Learning With Theoretical Guarantees": {"container_type": "Publication", "bib": {"title": "Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees", "author": ["Y Luo", "H Xu", "Y Li", "Y Tian", "T Darrell", "T Ma"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1807.03858", "abstract": "Model-based reinforcement learning (RL) is considered to be a promising approach to reduce the sample complexity that hinders model-free RL. However, the theoretical understanding of such methods has been rather limited. This paper introduces a novel algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. We design a meta-algorithm with a theoretical guarantee of monotone improvement to a local maximum of the expected reward. The meta-algorithm"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.03858", "author_id": ["r1Fn_YsAAAAJ", "t9HPFawAAAAJ", "aHtfItQAAAAJ", "0mgEF28AAAAJ", "bh-uRFMAAAAJ", "i38QlUwAAAAJ"], "url_scholarbib": "/scholar?q=info:VRq-njNXEiwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAlgorithmic%2BFramework%2BFor%2BModel-based%2BDeep%2BReinforcement%2BLearning%2BWith%2BTheoretical%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VRq-njNXEiwJ&ei=e2FeYoOYJJHKsQKNt6-YAw&json=", "num_citations": 131, "citedby_url": "/scholar?cites=3175696566467828309&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VRq-njNXEiwJ:scholar.google.com/&scioq=Algorithmic+Framework+For+Model-based+Deep+Reinforcement+Learning+With+Theoretical+Guarantees&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.03858"}, "Understanding Composition Of Word Embeddings Via Tensor Decomposition": {"container_type": "Publication", "bib": {"title": "Understanding composition of word embeddings via tensor decomposition", "author": ["A Frandsen", "R Ge"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.00613", "abstract": "Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition\\---given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.00613", "author_id": ["Ekimkw0AAAAJ", "MVxcjEoAAAAJ"], "url_scholarbib": "/scholar?q=info:Whu75rDE530J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BComposition%2BOf%2BWord%2BEmbeddings%2BVia%2BTensor%2BDecomposition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Whu75rDE530J&ei=f2FeYtK9AZWMy9YPt8OamA0&json=", "num_citations": 5, "citedby_url": "/scholar?cites=9072436238425463642&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Whu75rDE530J:scholar.google.com/&scioq=Understanding+Composition+Of+Word+Embeddings+Via+Tensor+Decomposition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.00613"}, "Learning Embeddings Into Entropic Wasserstein Spaces": {"container_type": "Publication", "bib": {"title": "Learning embeddings into entropic wasserstein spaces", "author": ["C Frogner", "F Mirzazadeh", "J Solomon"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.03329", "abstract": "Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions. Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric. Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures. We exploit this flexibility by learning an embedding that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.03329", "author_id": ["inrZ06oAAAAJ", "7CDyT3IAAAAJ", "pImSVwoAAAAJ"], "url_scholarbib": "/scholar?q=info:w9PdWtNmlC4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BEmbeddings%2BInto%2BEntropic%2BWasserstein%2BSpaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w9PdWtNmlC4J&ei=k2FeYv-jPMLZmQHc1ovQAg&json=", "num_citations": 12, "citedby_url": "/scholar?cites=3356420680246481859&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w9PdWtNmlC4J:scholar.google.com/&scioq=Learning+Embeddings+Into+Entropic+Wasserstein+Spaces&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.03329?ref=https://githubhelp.com"}, "Strokenet: A Neural Painting Environment": {"container_type": "Publication", "bib": {"title": "Strokenet: A neural painting environment", "author": ["N Zheng", "Y Jiang", "D Huang"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "We've seen tremendous success of image generating models these years. Generating images through a neural network is usually pixel-based, which is fundamentally different from how humans create artwork using brushes. To imitate human drawing, interactions between the environment and the agent is required to allow trials. However, the environment is usually non-differentiable, leading to slow convergence and massive computation. In this paper we try to address the discrete nature of software environment with an intermediate"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJxwDiActX", "author_id": ["", "", ""], "url_scholarbib": "/scholar?q=info:3R7GgR-QLeQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStrokenet:%2BA%2BNeural%2BPainting%2BEnvironment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3R7GgR-QLeQJ&ei=l2FeYufrMt-Vy9YPs66ekAk&json=", "num_citations": 49, "citedby_url": "/scholar?cites=16441956279595179741&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3R7GgR-QLeQJ:scholar.google.com/&scioq=Strokenet:+A+Neural+Painting+Environment&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJxwDiActX"}, "Learning To Simulate": {"container_type": "Publication", "bib": {"title": "Learning to simulate", "author": ["N Ruiz", "S Schulter", "M Chandraker"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.02513", "abstract": "Simulation is a useful tool in situations where training data for machine learning models is  costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02513", "author_id": ["CiOmcSIAAAAJ", "VQ6dsFEAAAAJ", "oPFCNk4AAAAJ"], "url_scholarbib": "/scholar?q=info:Vv7cTG-mqf8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BSimulate%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Vv7cTG-mqf8J&ei=mmFeYo-FHc6E6rQPz8uiuAc&json=", "num_citations": 72, "citedby_url": "/scholar?cites=18422438747696856662&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Vv7cTG-mqf8J:scholar.google.com/&scioq=Learning+To+Simulate&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02513"}, "Learning Neural Pde Solvers With Convergence Guarantees": {"container_type": "Publication", "bib": {"title": "Learning neural PDE solvers with convergence guarantees", "author": ["JT Hsieh", "S Zhao", "S Eismann", "L Mirabella"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Partial differential equations (PDEs) are widely used across the physical and computational sciences. Decades of research and engineering went into designing fast iterative solution methods. Existing solvers are general purpose, but may be sub-optimal for specific classes of problems. In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain. We achieve this goal by learning to modify the updates of an existing solver using a deep neural network. Crucially, our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.01200", "author_id": ["zYhq-BEAAAAJ", "bMoauM4AAAAJ", "NctrKSIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:yTjaOJG-BLAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BNeural%2BPde%2BSolvers%2BWith%2BConvergence%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yTjaOJG-BLAJ&ei=oWFeYrXeHJHKsQKNt6-YAw&json=", "num_citations": 61, "citedby_url": "/scholar?cites=12683471981515520201&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yTjaOJG-BLAJ:scholar.google.com/&scioq=Learning+Neural+Pde+Solvers+With+Convergence+Guarantees&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.01200"}, "Hierarchical Reinforcement Learning Via Advantage-weighted Information Maximization": {"container_type": "Publication", "bib": {"title": "Hierarchical reinforcement learning via advantage-weighted information maximization", "author": ["T Osa", "V Tangkaratt", "M Sugiyama"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.01365", "abstract": "Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.01365", "author_id": ["LqVev6MAAAAJ", "cj9n5hwAAAAJ", "GkYIrlIAAAAJ"], "url_scholarbib": "/scholar?q=info:RSfEZ2VGLHQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BReinforcement%2BLearning%2BVia%2BAdvantage-weighted%2BInformation%2BMaximization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RSfEZ2VGLHQJ&ei=qGFeYqjqA8LZmQHc1ovQAg&json=", "num_citations": 29, "citedby_url": "/scholar?cites=8371143208721459013&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RSfEZ2VGLHQJ:scholar.google.com/&scioq=Hierarchical+Reinforcement+Learning+Via+Advantage-weighted+Information+Maximization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.01365"}, "Autoloss: Learning Discrete Schedule For Alternate Optimization": {"container_type": "Publication", "bib": {"title": "Autoloss: Learning discrete schedules for alternate optimization", "author": ["H Xu", "H Zhang", "Z Hu", "X Liang", "R Salakhutdinov"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02442", "author_id": ["eNJkUEIAAAAJ", "H1d4BS8AAAAJ", "N7_xhHoAAAAJ", "voxznZAAAAAJ", "ITZ1e7MAAAAJ"], "url_scholarbib": "/scholar?q=info:8Em8txd7DNUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutoloss:%2BLearning%2BDiscrete%2BSchedule%2BFor%2BAlternate%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8Em8txd7DNUJ&ei=q2FeYteMI5GJmwGIxre4DA&json=", "num_citations": 27, "citedby_url": "/scholar?cites=15351780571596212720&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8Em8txd7DNUJ:scholar.google.com/&scioq=Autoloss:+Learning+Discrete+Schedule+For+Alternate+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02442"}, "Synthetic Datasets For Neural Program Synthesis": {"container_type": "Publication", "bib": {"title": "Synthetic datasets for neural program synthesis", "author": ["R Shin", "N Kant", "K Gupta", "C Bender", "B Trabucco"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The goal of program synthesis is to automatically generate programs in a particular language from corresponding specifications, eg input-output behavior. Many current approaches achieve impressive results after training on randomly generated I/O examples in limited domain-specific languages (DSLs), as with string transformations in RobustFill. However, we empirically discover that applying test input generation techniques for languages with control flow and rich input space causes deep networks to generalize poorly"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.12345", "author_id": ["xPnkc80AAAAJ", "eSgXTkkAAAAJ", "OqL9swoAAAAJ", "lU2XslAAAAAJ", "aLquhd4AAAAJ"], "url_scholarbib": "/scholar?q=info:Ewt-639W5m4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSynthetic%2BDatasets%2BFor%2BNeural%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ewt-639W5m4J&ei=rmFeYpPTIpHKsQKNt6-YAw&json=", "num_citations": 21, "citedby_url": "/scholar?cites=7991169696227265299&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ewt-639W5m4J:scholar.google.com/&scioq=Synthetic+Datasets+For+Neural+Program+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.12345"}, "Stochastic Prediction Of Multi-agent Interactions From Partial Observations": {"container_type": "Publication", "bib": {"title": "Stochastic prediction of multi-agent interactions from partial observations", "author": ["C Sun", "P Karlsson", "J Wu", "JB Tenenbaum"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a method that learns to integrate temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09641", "author_id": ["vQa7heEAAAAJ", "", "2efgcS0AAAAJ", "rRJ9wTJMUB8C"], "url_scholarbib": "/scholar?q=info:UPyEhaPMFKIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BPrediction%2BOf%2BMulti-agent%2BInteractions%2BFrom%2BPartial%2BObservations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UPyEhaPMFKIJ&ei=smFeYqSxNJHKsQKNt6-YAw&json=", "num_citations": 59, "citedby_url": "/scholar?cites=11679184736370359376&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UPyEhaPMFKIJ:scholar.google.com/&scioq=Stochastic+Prediction+Of+Multi-agent+Interactions+From+Partial+Observations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09641"}, "Neural Persistence: A Complexity Measure For Deep Neural Networks Using Algebraic Topology": {"container_type": "Publication", "bib": {"title": "Neural persistence: A complexity measure for deep neural networks using algebraic topology", "author": ["B Rieck", "M Togninalli", "C Bock", "M Moor", "M Horn"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09764", "author_id": ["La7zuKQAAAAJ", "LnyaZjQAAAAJ", "dwg4raMAAAAJ", "Q_Y5csgAAAAJ", "60cGPvIAAAAJ"], "url_scholarbib": "/scholar?q=info:V6_bE1svhKoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BPersistence:%2BA%2BComplexity%2BMeasure%2BFor%2BDeep%2BNeural%2BNetworks%2BUsing%2BAlgebraic%2BTopology%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=V6_bE1svhKoJ&ei=tWFeYsiNI5WMy9YPt8OamA0&json=", "num_citations": 66, "citedby_url": "/scholar?cites=12286997751595249495&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:V6_bE1svhKoJ:scholar.google.com/&scioq=Neural+Persistence:+A+Complexity+Measure+For+Deep+Neural+Networks+Using+Algebraic+Topology&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09764"}, "Learning From Positive And Unlabeled Data With A Selection Bias": {"container_type": "Publication", "bib": {"title": "Learning from positive and unlabeled data with a selection bias", "author": ["M Kato", "T Teshima", "J Honda"], "pub_year": "2018", "venue": "International conference on learning \u2026", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJzLciCqKm", "author_id": ["SAsgCP0AAAAJ", "JZoQoDsAAAAJ", "Aw8OrxQAAAAJ"], "url_scholarbib": "/scholar?q=info:Vae_98Go2QQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFrom%2BPositive%2BAnd%2BUnlabeled%2BData%2BWith%2BA%2BSelection%2BBias%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Vae_98Go2QQJ&ei=umFeYsH2FJHKsQKNt6-YAw&json=", "num_citations": 42, "citedby_url": "/scholar?cites=349495997136611157&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Vae_98Go2QQJ:scholar.google.com/&scioq=Learning+From+Positive+And+Unlabeled+Data+With+A+Selection+Bias&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJzLciCqKm"}, "Double Viterbi: Weight Encoding For High Compression Ratio And Fast On-chip Reconstruction For Deep Neural Network": {"container_type": "Publication", "bib": {"title": "Double Viterbi: Weight encoding for high compression ratio and fast on-chip reconstruction for deep neural network", "author": ["D Ahn", "D Lee", "T Kim", "JJ Kim"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Weight pruning has been introduced as an efficient model compression technique. Even though pruning removes significant amount of weights in a network, memory requirement reduction was limited since conventional sparse matrix formats require significant amount of memory to store index-related information. Moreover, computations associated with such sparse matrix formats are slow because sequential sparse matrix decoding process does not utilize highly parallel computing systems efficiently. As an attempt to compress index"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkfYOoCcYX", "author_id": ["a4e-yE4AAAAJ", "ALiieEkAAAAJ", "zzII2gsAAAAJ", "Ee994T0AAAAJ"], "url_scholarbib": "/scholar?q=info:Bv5-7jhDqd4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDouble%2BViterbi:%2BWeight%2BEncoding%2BFor%2BHigh%2BCompression%2BRatio%2BAnd%2BFast%2BOn-chip%2BReconstruction%2BFor%2BDeep%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Bv5-7jhDqd4J&ei=vmFeYqOcFZWMy9YPt8OamA0&json=", "num_citations": 9, "citedby_url": "/scholar?cites=16044429059282632198&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Bv5-7jhDqd4J:scholar.google.com/&scioq=Double+Viterbi:+Weight+Encoding+For+High+Compression+Ratio+And+Fast+On-chip+Reconstruction+For+Deep+Neural+Network&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkfYOoCcYX"}, "Visual Reasoning By Progressive Module Networks": {"container_type": "Publication", "bib": {"title": "Visual reasoning by progressive module networks", "author": ["SW Kim", "M Tapaswi", "S Fidler"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.02453", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn-most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.02453", "author_id": ["bGNcndcAAAAJ", "rJotb-YAAAAJ", "CUlqK5EAAAAJ"], "url_scholarbib": "/scholar?q=info:YZ8nLZcDl68J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisual%2BReasoning%2BBy%2BProgressive%2BModule%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YZ8nLZcDl68J&ei=xmFeYpbjCd-Vy9YPs66ekAk&json=", "num_citations": 8, "citedby_url": "/scholar?cites=12652585625953214305&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YZ8nLZcDl68J:scholar.google.com/&scioq=Visual+Reasoning+By+Progressive+Module+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.02453"}, "Delta: Deep Learning Transfer Using Feature Map With Attention For Convolutional Networks": {"container_type": "Publication", "bib": {"title": "Delta: Deep learning transfer using feature map with attention for convolutional networks", "author": ["X Li", "H Xiong", "H Wang", "Y Rao", "L Liu", "Z Chen"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this paper, we propose a novel regularized transfer learning framework DELTA, namely DEep Learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.09229", "author_id": ["f9V0NZkAAAAJ", "f_Kcie0AAAAJ", "", "", "", "LCzu9MEAAAAJ"], "url_scholarbib": "/scholar?q=info:XHX-iU2Oyg4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDelta:%2BDeep%2BLearning%2BTransfer%2BUsing%2BFeature%2BMap%2BWith%2BAttention%2BFor%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XHX-iU2Oyg4J&ei=zGFeYt_wBZGJmwGIxre4DA&json=", "num_citations": 75, "citedby_url": "/scholar?cites=1065820725505324380&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XHX-iU2Oyg4J:scholar.google.com/&scioq=Delta:+Deep+Learning+Transfer+Using+Feature+Map+With+Attention+For+Convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.09229"}, "Prior Convictions: Black-box Adversarial Attacks With Bandits And Priors": {"container_type": "Publication", "bib": {"title": "Prior convictions: Black-box adversarial attacks with bandits and priors", "author": ["A Ilyas", "L Engstrom", "A Madry"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1807.07978", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and we demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.07978", "author_id": ["Dtw3YBoAAAAJ", "IWPWNxkAAAAJ", "SupjsEUAAAAJ"], "url_scholarbib": "/scholar?q=info:ePwlCGFrAUAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPrior%2BConvictions:%2BBlack-box%2BAdversarial%2BAttacks%2BWith%2BBandits%2BAnd%2BPriors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ePwlCGFrAUAJ&ei=zmFeYvLCLPmQ6rQPzKCxuAY&json=", "num_citations": 200, "citedby_url": "/scholar?cites=4612085557896805496&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ePwlCGFrAUAJ:scholar.google.com/&scioq=Prior+Convictions:+Black-box+Adversarial+Attacks+With+Bandits+And+Priors&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.07978.pdf)."}, "Capsule Graph Neural Network": {"container_type": "Publication", "bib": {"title": "Capsule graph neural network", "author": ["Z Xinyi", "L Chen"], "pub_year": "2018", "venue": "International conference on learning \u2026", "abstract": "capsule (a group of neurons in neural network), so we follow the same notation in our work.  Introducing capsules  low-level capsules and routes them to the closest high-level capsules."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byl8BnRcYm", "author_id": ["", "XqNeXssAAAAJ"], "url_scholarbib": "/scholar?q=info:QPUCvtH67zcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCapsule%2BGraph%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QPUCvtH67zcJ&ei=02FeYueGEt-Vy9YPs66ekAk&json=", "num_citations": 145, "citedby_url": "/scholar?cites=4030715970262857024&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QPUCvtH67zcJ:scholar.google.com/&scioq=Capsule+Graph+Neural+Network&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byl8BnRcYm"}, "Knockoffgan: Generating Knockoffs For Feature Selection Using Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "KnockoffGAN: Generating knockoffs for feature selection using generative adversarial networks", "author": ["J Jordon", "J Yoon", "M van der Schaar"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Feature selection is a pervasive problem. The discovery of relevant features can be as important for performing a particular task (such as to avoid overfitting in prediction) as it can be for understanding the underlying processes governing the true label (such as discovering relevant genetic factors for a disease). Machine learning driven feature selection can enable discovery from large, high-dimensional, non-linear observational datasets by creating a subset of features for experts to focus on. In order to use expert time most efficiently, we"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByeZ5jC5YQ&source=post_page---------------------------", "author_id": ["jiyonF0AAAAJ", "kiFd6A8AAAAJ", "DZ3S--MAAAAJ"], "url_scholarbib": "/scholar?q=info:KOaNshLjsR8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKnockoffgan:%2BGenerating%2BKnockoffs%2BFor%2BFeature%2BSelection%2BUsing%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KOaNshLjsR8J&ei=22FeYpfrBciBy9YP18Gi8As&json=", "num_citations": 51, "citedby_url": "/scholar?cites=2283856155498112552&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KOaNshLjsR8J:scholar.google.com/&scioq=Knockoffgan:+Generating+Knockoffs+For+Feature+Selection+Using+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByeZ5jC5YQ"}, "Optimal Transport Maps For Distribution Preserving Operations On Latent Spaces Of Generative Models": {"container_type": "Publication", "bib": {"title": "Optimal transport maps for distribution preserving operations on latent spaces of generative models", "author": ["E Agustsson", "A Sage", "R Timofte", "L Van Gool"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative models such as Variational Auto Encoders (VAEs) and Generative Adversarial Networks (GANs) are typically trained for a fixed prior distribution in the latent space, such as uniform or Gaussian. After a trained model is obtained, one can sample the Generator in various forms for exploration and understanding, such as interpolating between two samples, sampling in the vicinity of a sample or exploring differences between a pair of samples applied to a third sample. In this paper, we show that the latent space operations"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.01970", "author_id": ["Uhvyua4AAAAJ", "", "u3MwH5kAAAAJ", "TwMib_QAAAAJ"], "url_scholarbib": "/scholar?q=info:QnOXh0aIaqEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BTransport%2BMaps%2BFor%2BDistribution%2BPreserving%2BOperations%2BOn%2BLatent%2BSpaces%2BOf%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QnOXh0aIaqEJ&ei=3WFeYsGuKcLZmQHc1ovQAg&json=", "num_citations": 24, "citedby_url": "/scholar?cites=11631258824141665090&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QnOXh0aIaqEJ:scholar.google.com/&scioq=Optimal+Transport+Maps+For+Distribution+Preserving+Operations+On+Latent+Spaces+Of+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.01970"}, "Timbretron: A Wavenet(cyclegan(cqt(audio))) Pipeline For Musical Timbre Transfer": {"container_type": "Publication", "bib": {"title": "Timbretron: A wavenet (cyclegan (cqt (audio))) pipeline for musical timbre transfer", "author": ["S Huang", "Q Li", "C Anil", "X Bao", "S Oore"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.09620", "author_id": ["Qp2pme4AAAAJ", "qlwwdfEAAAAJ", "1VDV6ZEAAAAJ", "gnBwBZ4AAAAJ", "cI0dYX4AAAAJ"], "url_scholarbib": "/scholar?q=info:DlZb9PhCYJsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTimbretron:%2BA%2BWavenet(cyclegan(cqt(audio)))%2BPipeline%2BFor%2BMusical%2BTimbre%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DlZb9PhCYJsJ&ei=4mFeYqPdDpGJmwGIxre4DA&json=", "num_citations": 70, "citedby_url": "/scholar?cites=11196022310662002190&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DlZb9PhCYJsJ:scholar.google.com/&scioq=Timbretron:+A+Wavenet(cyclegan(cqt(audio)))+Pipeline+For+Musical+Timbre+Transfer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.09620"}, "Coarse-grain Fine-grain Coattention Network For Multi-evidence Question Answering": {"container_type": "Publication", "bib": {"title": "Coarse-grain fine-grain coattention network for multi-evidence question answering", "author": ["V Zhong", "C Xiong", "NS Keskar", "R Socher"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.00603", "abstract": "End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.00603", "author_id": ["lT3YoNkAAAAJ", "vaSdahkAAAAJ", "CJ-_cEEAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:q5ZINs-I_1MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCoarse-grain%2BFine-grain%2BCoattention%2BNetwork%2BFor%2BMulti-evidence%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=q5ZINs-I_1MJ&ei=5GFeYru_NJHKsQKNt6-YAw&json=", "num_citations": 49, "citedby_url": "/scholar?cites=6052706847759570603&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:q5ZINs-I_1MJ:scholar.google.com/&scioq=Coarse-grain+Fine-grain+Coattention+Network+For+Multi-evidence+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.00603.pdf?ref=https://githubhelp.com"}, "Practical Lossless Compression With Latent Variables Using Bits Back Coding": {"container_type": "Publication", "bib": {"title": "Practical lossless compression with latent variables using bits back coding", "author": ["J Townsend", "T Bird", "D Barber"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.04866", "abstract": "Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We presentBits Back with ANS'(BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational auto-encoder model (VAE), achieving compression rates superior to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.04866", "author_id": ["fhYBZTcAAAAJ", "2dkA4KwAAAAJ", "dqJPZHEAAAAJ"], "url_scholarbib": "/scholar?q=info:iLvzzl_ABhQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPractical%2BLossless%2BCompression%2BWith%2BLatent%2BVariables%2BUsing%2BBits%2BBack%2BCoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iLvzzl_ABhQJ&ei=8GFeYpLGCt-Vy9YPs66ekAk&json=", "num_citations": 70, "citedby_url": "/scholar?cites=1443052248345328520&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iLvzzl_ABhQJ:scholar.google.com/&scioq=Practical+Lossless+Compression+With+Latent+Variables+Using+Bits+Back+Coding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.04866"}, "Analysis Of Quantized Models": {"container_type": "Publication", "bib": {"title": "Analysis of quantized models", "author": ["L Hou", "R Zhang", "JT Kwok"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "of both weight and gradient quantization affects convergence. We show that (i) weight-quantized  models converge to an error related to the weight quantization resolution and weight"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryM_IoAqYX", "author_id": ["rnjoL5cAAAAJ", "Qc__e5YAAAAJ", "-oTraZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:6ZZAEM1aPQQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalysis%2BOf%2BQuantized%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6ZZAEM1aPQQJ&ei=92FeYqv4CYryyASen4NI&json=", "num_citations": 23, "citedby_url": "/scholar?cites=305500186518525673&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6ZZAEM1aPQQJ:scholar.google.com/&scioq=Analysis+Of+Quantized+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryM_IoAqYX"}, "Unsupervised Learning Of The Set Of Local Maxima": {"container_type": "Publication", "bib": {"title": "Unsupervised learning of the set of local maxima", "author": ["L Wolf", "S Benaim", "T Galanti"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.05026", "abstract": "This paper describes a new form of unsupervised learning, whose input is a set of unlabeled points that are assumed to be local maxima of an unknown value function v in an unknown subset of the vector space. Two functions are learned:(i) a set indicator c, which is a binary classifier, and (ii) a comparator function h that given two nearby samples, predicts which sample has the higher value of the unknown function v. Loss terms are used to ensure that all training samples x are a local maxima of v, according to h and satisfy c (x)= 1. Therefore"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.05026", "author_id": ["UbFrXTsAAAAJ", "-zSM2I8AAAAJ", "ut_ISVIAAAAJ"], "url_scholarbib": "/scholar?q=info:1k5Iwr0dyUQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2BOf%2BThe%2BSet%2BOf%2BLocal%2BMaxima%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1k5Iwr0dyUQJ&ei=-2FeYt2DEM2Ny9YPqPyUgAs&json=", "num_citations": 7, "citedby_url": "/scholar?cites=4956525565743484630&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1k5Iwr0dyUQJ:scholar.google.com/&scioq=Unsupervised+Learning+Of+The+Set+Of+Local+Maxima&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.05026"}, "Identifying And Controlling Important Neurons In Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Identifying and controlling important neurons in neural machine translation", "author": ["A Bau", "Y Belinkov", "H Sajjad", "N Durrani", "F Dalvi"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.01157", "author_id": ["", "K-6ujU4AAAAJ", "t3BH6NkAAAAJ", "K6uisFAAAAAJ", "uQGCv10AAAAJ"], "url_scholarbib": "/scholar?q=info:7TzWfeI9FJQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIdentifying%2BAnd%2BControlling%2BImportant%2BNeurons%2BIn%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7TzWfeI9FJQJ&ei=_mFeYs26Dt-Vy9YPs66ekAk&json=", "num_citations": 85, "citedby_url": "/scholar?cites=10670221460130643181&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7TzWfeI9FJQJ:scholar.google.com/&scioq=Identifying+And+Controlling+Important+Neurons+In+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.01157"}, "Graph Hypernetworks For Neural Architecture Search": {"container_type": "Publication", "bib": {"title": "Graph hypernetworks for neural architecture search", "author": ["C Zhang", "M Ren", "R Urtasun"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.05749", "abstract": "Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05749", "author_id": ["d0BhFY0AAAAJ", "XcQ9WqMAAAAJ", "jyxO2akAAAAJ"], "url_scholarbib": "/scholar?q=info:tc8EMP1tePwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BHypernetworks%2BFor%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tc8EMP1tePwJ&ei=AWJeYty9FZWMy9YPt8OamA0&json=", "num_citations": 155, "citedby_url": "/scholar?cites=18192411628962893749&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tc8EMP1tePwJ:scholar.google.com/&scioq=Graph+Hypernetworks+For+Neural+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05749.pdf?ref=https://githubhelp.com"}, "Invariant And Equivariant Graph Networks": {"container_type": "Publication", "bib": {"title": "Invariant and equivariant graph networks", "author": ["H Maron", "H Ben-Hamu", "N Shamir", "Y Lipman"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant linear layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. In this paper we provide a characterization of all permutation invariant and equivariant linear layers"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09902", "author_id": ["4v8uJrIAAAAJ", "DL6Gk3AAAAAJ", "", "vyteiT4AAAAJ"], "url_scholarbib": "/scholar?q=info:kCaoqZR3cvcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInvariant%2BAnd%2BEquivariant%2BGraph%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kCaoqZR3cvcJ&ei=BWJeYq_TA5HKsQKNt6-YAw&json=", "num_citations": 182, "citedby_url": "/scholar?cites=17830445355098449552&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kCaoqZR3cvcJ:scholar.google.com/&scioq=Invariant+And+Equivariant+Graph+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09902"}, "Robustness May Be At Odds With Accuracy": {"container_type": "Publication", "bib": {"title": "Robustness may be at odds with accuracy", "author": ["D Tsipras", "S Santurkar", "L Engstrom", "A Turner"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.12152", "author_id": ["26eh1jAAAAAJ", "QMkbFp8AAAAJ", "IWPWNxkAAAAJ", "S6ImO74AAAAJ"], "url_scholarbib": "/scholar?q=info:IOam6o67MlEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobustness%2BMay%2BBe%2BAt%2BOdds%2BWith%2BAccuracy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IOam6o67MlEJ&ei=B2JeYt7DO4vMsQK69Y7ABg&json=", "num_citations": 890, "citedby_url": "/scholar?cites=5850945088404252192&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IOam6o67MlEJ:scholar.google.com/&scioq=Robustness+May+Be+At+Odds+With+Accuracy&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.12152.pdf,"}, "Fixup Initialization: Residual Learning Without Normalization": {"container_type": "Publication", "bib": {"title": "Fixup initialization: Residual learning without normalization", "author": ["H Zhang", "YN Dauphin", "T Ma"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.09321", "abstract": "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.09321", "author_id": ["6Wg-hF4AAAAJ", "XSforroAAAAJ", "i38QlUwAAAAJ"], "url_scholarbib": "/scholar?q=info:AW0EAJYNh48J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFixup%2BInitialization:%2BResidual%2BLearning%2BWithout%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AW0EAJYNh48J&ei=CmJeYryuIc2Ny9YPqPyUgAs&json=", "num_citations": 195, "citedby_url": "/scholar?cites=10342250007176178945&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AW0EAJYNh48J:scholar.google.com/&scioq=Fixup+Initialization:+Residual+Learning+Without+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.09321.pdf)"}, "Nadpex: An On-policy Temporally Consistent Exploration Method For Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning", "author": ["S Xie", "J Huang", "L Lei", "C Liu", "Z Ma", "W Zhang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy-Neural Adaptive Dropout Policy Exploration (NADPEx)-for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09028", "author_id": ["9GJn5FIAAAAJ", "kaSP3zIAAAAJ", "", "4m061tYAAAAJ", "", "5GtyVooAAAAJ"], "url_scholarbib": "/scholar?q=info:IXnxd9TxpOUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNadpex:%2BAn%2BOn-policy%2BTemporally%2BConsistent%2BExploration%2BMethod%2BFor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IXnxd9TxpOUJ&ei=F2JeYr2hDMLZmQHc1ovQAg&json=", "num_citations": 5, "citedby_url": "/scholar?cites=16547616825713719585&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IXnxd9TxpOUJ:scholar.google.com/&scioq=Nadpex:+An+On-policy+Temporally+Consistent+Exploration+Method+For+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09028"}, "A Data-driven And Distributed Approach To Sparse Signal Representation And Recovery": {"container_type": "Publication", "bib": {"title": "A data-driven and distributed approach to sparse signal representation and recovery", "author": ["A Mousavi", "G Dasarathy", "RG Baraniuk"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "In this paper, we focus on two challenges which offset the promise of sparse signal representation, sensing, and recovery. First, real-world signals can seldom be described as perfectly sparse vectors in a known basis, and traditionally used random measurement schemes are seldom optimal for sensing them. Second, existing signal recovery algorithms are usually not fast enough to make them applicable to real-time problems. In this paper, we address these two challenges by presenting a novel framework based on deep learning. For"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1xVTjCqKQ", "author_id": ["Yuxd6uYAAAAJ", "iSL1cKsAAAAJ", "N-BBA20AAAAJ"], "url_scholarbib": "/scholar?q=info:lLZKO_WQcIEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BData-driven%2BAnd%2BDistributed%2BApproach%2BTo%2BSparse%2BSignal%2BRepresentation%2BAnd%2BRecovery%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lLZKO_WQcIEJ&ei=G2JeYpPkMovMsQK69Y7ABg&json=", "num_citations": 17, "citedby_url": "/scholar?cites=9327114211220436628&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lLZKO_WQcIEJ:scholar.google.com/&scioq=A+Data-driven+And+Distributed+Approach+To+Sparse+Signal+Representation+And+Recovery&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1xVTjCqKQ"}, "Efficient Lifelong Learning With A-gem": {"container_type": "Publication", "bib": {"title": "Efficient lifelong learning with a-gem", "author": ["A Chaudhry", "MA Ranzato", "M Rohrbach"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.00420", "author_id": ["FO8vjQMAAAAJ", "NbXF7T8AAAAJ", "3kDtybgAAAAJ"], "url_scholarbib": "/scholar?q=info:ZGxIEhjK88QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BLifelong%2BLearning%2BWith%2BA-gem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZGxIEhjK88QJ&ei=H2JeYrO5G4-bmAGmiqCIBw&json=", "num_citations": 485, "citedby_url": "/scholar?cites=14191909055509326948&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZGxIEhjK88QJ:scholar.google.com/&scioq=Efficient+Lifelong+Learning+With+A-gem&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.00420"}, "Stcn: Stochastic Temporal Convolutional Networks": {"container_type": "Publication", "bib": {"title": "STCN: Stochastic temporal convolutional networks", "author": ["E Aksan", "O Hilliges"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.06568", "abstract": "Convolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs), while providing computational and modeling advantages due to inherent parallelism. However, currently there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.06568", "author_id": ["0P_684sAAAAJ", "-epU9OsAAAAJ"], "url_scholarbib": "/scholar?q=info:5Hvh1OpUL9oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStcn:%2BStochastic%2BTemporal%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5Hvh1OpUL9oJ&ei=I2JeYtzmAovMsQK69Y7ABg&json=", "num_citations": 38, "citedby_url": "/scholar?cites=15721878191744318436&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5Hvh1OpUL9oJ:scholar.google.com/&scioq=Stcn:+Stochastic+Temporal+Convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.06568"}, "Learning Multimodal Graph-to-graph Translation For Molecule Optimization": {"container_type": "Publication", "bib": {"title": "Learning multimodal graph-to-graph translation for molecular optimization", "author": ["W Jin", "K Yang", "R Barzilay", "T Jaakkola"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.01070", "abstract": "We view molecular optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.01070", "author_id": ["IE5D8_QAAAAJ", "sRpY9TIAAAAJ", "", "Ao4gtsYAAAAJ"], "url_scholarbib": "/scholar?q=info:ja5mrTGId08J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BMultimodal%2BGraph-to-graph%2BTranslation%2BFor%2BMolecule%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ja5mrTGId08J&ei=L2JeYsnzJ5HKsQKNt6-YAw&json=", "num_citations": 150, "citedby_url": "/scholar?cites=5726195198168837773&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ja5mrTGId08J:scholar.google.com/&scioq=Learning+Multimodal+Graph-to-graph+Translation+For+Molecule+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.01070"}, "Representation Degeneration Problem In Training Natural Language Generation Models": {"container_type": "Publication", "bib": {"title": "Representation degeneration problem in training natural language generation models", "author": ["J Gao", "D He", "X Tan", "T Qin", "L Wang", "TY Liu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study an interesting problem in training neural network-based models for natural language generation tasks, which we call the\\emph {representation degeneration problem}. We observe that when training a model for natural language generation tasks through likelihood maximization with the weight tying trick, especially with big training datasets, most of the learnt word embeddings tend to degenerate and be distributed into a narrow cone, which largely limits the representation power of word embeddings. We analyze the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.12009", "author_id": ["jTdkr10AAAAJ", "orVoz4IAAAAJ", "tob-U1oAAAAJ", "Bl4SRU0AAAAJ", "VZHxoh8AAAAJ", "Nh832fgAAAAJ"], "url_scholarbib": "/scholar?q=info:bdfGZDEn0jUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRepresentation%2BDegeneration%2BProblem%2BIn%2BTraining%2BNatural%2BLanguage%2BGeneration%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bdfGZDEn0jUJ&ei=MmJeYqr3JZLeyQTms5KQBg&json=", "num_citations": 58, "citedby_url": "/scholar?cites=3878205322217052013&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bdfGZDEn0jUJ:scholar.google.com/&scioq=Representation+Degeneration+Problem+In+Training+Natural+Language+Generation+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.12009"}, "Universal Successor Features Approximators": {"container_type": "Publication", "bib": {"title": "Universal successor features approximators", "author": ["D Borsa", "A Barreto", "J Quan", "D Mankowitz"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "approximator that is given the task description as input; one of its most common form are  universal value function approximators (UVFAs through successor features (SFs). Our proposed"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.07626", "author_id": ["LK_CV24AAAAJ", "H-xtdV4AAAAJ", "", "v84tWxsAAAAJ"], "url_scholarbib": "/scholar?q=info:HgfUwfz0MckJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BSuccessor%2BFeatures%2BApproximators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HgfUwfz0MckJ&ei=NmJeYvrfEJGJmwGIxre4DA&json=", "num_citations": 60, "citedby_url": "/scholar?cites=14497638041903171358&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HgfUwfz0MckJ:scholar.google.com/&scioq=Universal+Successor+Features+Approximators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.07626"}, "Augmented Cyclic Adversarial Learning For Low Resource Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Augmented cyclic adversarial learning for low resource domain adaptation", "author": ["E Hosseini-Asl", "Y Zhou", "C Xiong", "R Socher"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied. However, it is often the case that data are abundant in some domains but scarce in others. Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain. In general, this requires learning plausible mappings between domains. CycleGAN is a powerful framework that efficiently learns to map inputs from one"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.00374", "author_id": ["I9w3ON4AAAAJ", "H_6RQ7oAAAAJ", "vaSdahkAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:cWKIe7cUx54J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAugmented%2BCyclic%2BAdversarial%2BLearning%2BFor%2BLow%2BResource%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cWKIe7cUx54J&ei=OmJeYpWLGo-bmAGmiqCIBw&json=", "num_citations": 37, "citedby_url": "/scholar?cites=11441136156642140785&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cWKIe7cUx54J:scholar.google.com/&scioq=Augmented+Cyclic+Adversarial+Learning+For+Low+Resource+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.00374"}, "Post Selection Inference With Incomplete Maximum Mean Discrepancy Estimator": {"container_type": "Publication", "bib": {"title": "Post selection inference with incomplete maximum mean discrepancy estimator", "author": ["M Yamada", "D Wu", "YHH Tsai", "I Takeuchi"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Measuring divergence between two distributions is essential in machine learning and statistics and has various applications including binary classification, change point detection, and two-sample test. Furthermore, in the era of big data, designing divergence measure that is interpretable and can handle high-dimensional and complex data becomes extremely important. In the paper, we propose a post selection inference (PSI) framework for divergence measure, which can select a set of statistically significant features that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.06226", "author_id": ["1cKNu1gAAAAJ", "x7Q3zj4AAAAJ", "TMimDRoAAAAJ", "IwBHa3gAAAAJ"], "url_scholarbib": "/scholar?q=info:ctt6Dw8MyywJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPost%2BSelection%2BInference%2BWith%2BIncomplete%2BMaximum%2BMean%2BDiscrepancy%2BEstimator%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ctt6Dw8MyywJ&ei=PmJeYtnrLpGJmwGIxre4DA&json=", "num_citations": 10, "citedby_url": "/scholar?cites=3227686816764844914&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ctt6Dw8MyywJ:scholar.google.com/&scioq=Post+Selection+Inference+With+Incomplete+Maximum+Mean+Discrepancy+Estimator&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.06226"}, "Neural Graph Evolution: Towards Efficient Automatic Robot Design": {"container_type": "Publication", "bib": {"title": "Neural graph evolution: Towards efficient automatic robot design", "author": ["T Wang", "Y Zhou", "S Fidler", "J Ba"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.05370", "abstract": "Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering. Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates. To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space. We propose Neural Graph Evolution (NGE), which performs selection on current"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.05370", "author_id": ["HvJj-pEAAAAJ", "9-6xvKYAAAAJ", "CUlqK5EAAAAJ", "ymzxRhAAAAAJ"], "url_scholarbib": "/scholar?q=info:AZ5lha_NQB8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BGraph%2BEvolution:%2BTowards%2BEfficient%2BAutomatic%2BRobot%2BDesign%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AZ5lha_NQB8J&ei=QmJeYtm5F8iBy9YP18Gi8As&json=", "num_citations": 27, "citedby_url": "/scholar?cites=2252025967426248193&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AZ5lha_NQB8J:scholar.google.com/&scioq=Neural+Graph+Evolution:+Towards+Efficient+Automatic+Robot+Design&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.05370"}, "Critical Learning Periods In Deep Networks": {"container_type": "Publication", "bib": {"title": "Critical learning periods in deep networks", "author": ["A Achille", "M Rovere", "S Soatto"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkeStsCcKQ", "author_id": ["LiwtZz4AAAAJ", "Ih29F_QAAAAJ", "lH1PdF8AAAAJ"], "url_scholarbib": "/scholar?q=info:CCZ0dFSXrP0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCritical%2BLearning%2BPeriods%2BIn%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CCZ0dFSXrP0J&ei=RmJeYvW_D5WMy9YPt8OamA0&json=", "num_citations": 75, "citedby_url": "/scholar?cites=18279151376576816648&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CCZ0dFSXrP0J:scholar.google.com/&scioq=Critical+Learning+Periods+In+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkeStsCcKQ"}, "Snip: Single-shot Network Pruning Based On Connection Sensitivity": {"container_type": "Publication", "bib": {"title": "Snip: Single-shot network pruning based on connection sensitivity", "author": ["N Lee", "T Ajanthan", "PHS Torr"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.02340", "abstract": "Pruning large neural networks while maintaining their performance is often desirable due to the reduced space and time complexity. In existing methods, pruning is done within an iterative optimization procedure with either heuristically designed pruning schedules or additional hyperparameters, undermining their utility. In this work, we present a new approach that prunes a given network once at initialization prior to training. To achieve this, we introduce a saliency criterion based on connection sensitivity that identifies structurally"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02340", "author_id": ["wi9q5T8AAAAJ", "Rza8c10AAAAJ", "kPxa2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:2ILPsJnHR4gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSnip:%2BSingle-shot%2BNetwork%2BPruning%2BBased%2BOn%2BConnection%2BSensitivity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2ILPsJnHR4gJ&ei=SGJeYun6ON-Vy9YPs66ekAk&json=", "num_citations": 446, "citedby_url": "/scholar?cites=9820036975414969048&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2ILPsJnHR4gJ:scholar.google.com/&scioq=Snip:+Single-shot+Network+Pruning+Based+On+Connection+Sensitivity&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02340"}, "Optimal Completion Distillation For Sequence Learning": {"container_type": "Publication", "bib": {"title": "Optimal completion distillation for sequence learning", "author": ["S Sabour", "W Chan", "M Norouzi"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.01398", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pretraining or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm. Then, for each position of the generated sequence, we use a target distribution"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01398", "author_id": ["l8wQ39EAAAAJ", "Nla9qfUAAAAJ", "Lncr-VoAAAAJ"], "url_scholarbib": "/scholar?q=info:x1rwXk5U0Q4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BCompletion%2BDistillation%2BFor%2BSequence%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x1rwXk5U0Q4J&ei=S2JeYu-CNovMsQK69Y7ABg&json=", "num_citations": 36, "citedby_url": "/scholar?cites=1067727282240510663&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x1rwXk5U0Q4J:scholar.google.com/&scioq=Optimal+Completion+Distillation+For+Sequence+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01398"}, "Stochastic Gradient/mirror Descent: Minimax Optimality And Implicit Regularization": {"container_type": "Publication", "bib": {"title": "Stochastic gradient/mirror descent: Minimax optimality and implicit regularization", "author": ["N Azizan", "B Hassibi"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.00952", "abstract": "Stochastic descent methods (of the gradient and mirror varieties) have become increasingly popular in optimization. In fact, it is now widely recognized that the success of deep learning is not only due to the special deep architecture of the models, but also due to the behavior of the stochastic descent methods used, which play a key role in reaching\" good\" solutions that generalize well to unseen data. In an attempt to shed some light on why this is the case, we revisit some minimax properties of stochastic gradient descent (SGD) for the square loss of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.00952", "author_id": ["3gMiILkAAAAJ", "1XoZPhEAAAAJ"], "url_scholarbib": "/scholar?q=info:mj6z-GuyTaYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BGradient/mirror%2BDescent:%2BMinimax%2BOptimality%2BAnd%2BImplicit%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mj6z-GuyTaYJ&ei=T2JeYqH6LIvMsQK69Y7ABg&json=", "num_citations": 39, "citedby_url": "/scholar?cites=11983430360306499226&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mj6z-GuyTaYJ:scholar.google.com/&scioq=Stochastic+Gradient/mirror+Descent:+Minimax+Optimality+And+Implicit+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.00952"}, "Three Mechanisms Of Weight Decay Regularization": {"container_type": "Publication", "bib": {"title": "Three mechanisms of weight decay regularization", "author": ["G Zhang", "C Wang", "B Xu", "R Grosse"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.12281", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $ L_2 $ regularization. Literal weight decay has been shown to outperform $ L_2 $ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.12281", "author_id": ["B_TZBtwAAAAJ", "yN2iRpwAAAAJ", "", "xgQd1qgAAAAJ"], "url_scholarbib": "/scholar?q=info:aTrAcJlt7twJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThree%2BMechanisms%2BOf%2BWeight%2BDecay%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aTrAcJlt7twJ&ei=VGJeYvK-GovMsQK69Y7ABg&json=", "num_citations": 127, "citedby_url": "/scholar?cites=15919782238590351977&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aTrAcJlt7twJ:scholar.google.com/&scioq=Three+Mechanisms+Of+Weight+Decay+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.12281"}, "Approximability Of Discriminators Implies Diversity In Gans": {"container_type": "Publication", "bib": {"title": "Approximability of discriminators implies diversity in GANs", "author": ["Y Bai", "T Ma", "A Risteski"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.10586", "abstract": "While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. suggests a dilemma about GANs' statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse. By contrast, we show in this paper that GANs can in principle learn distributions in Wasserstein distance (or KL"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.10586", "author_id": ["owqhKD8AAAAJ", "i38QlUwAAAAJ", "NWPDSEsAAAAJ"], "url_scholarbib": "/scholar?q=info:LSr9Pmvta4cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DApproximability%2BOf%2BDiscriminators%2BImplies%2BDiversity%2BIn%2BGans%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LSr9Pmvta4cJ&ei=V2JeYpqUHZWMy9YPt8OamA0&json=", "num_citations": 55, "citedby_url": "/scholar?cites=9758154062502373933&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LSr9Pmvta4cJ:scholar.google.com/&scioq=Approximability+Of+Discriminators+Implies+Diversity+In+Gans&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.10586"}, "Probgan: Towards Probabilistic Gan With Theoretical Guarantees": {"container_type": "Publication", "bib": {"title": "Probgan: Towards probabilistic gan with theoretical guarantees", "author": ["H He", "H Wang", "GH Lee", "Y Tian"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN). In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1l7bnR5Ym", "author_id": ["v1sUoqwAAAAJ", "NrOA9QoAAAAJ", "1mdLkSMAAAAJ", "OsP7JHAAAAAJ"], "url_scholarbib": "/scholar?q=info:Nw4Oul6J5wcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbgan:%2BTowards%2BProbabilistic%2BGan%2BWith%2BTheoretical%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Nw4Oul6J5wcJ&ei=WmJeYvbfJo-bmAGmiqCIBw&json=", "num_citations": 25, "citedby_url": "/scholar?cites=569574917827071543&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Nw4Oul6J5wcJ:scholar.google.com/&scioq=Probgan:+Towards+Probabilistic+Gan+With+Theoretical+Guarantees&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1l7bnR5Ym"}, "Systematic Generalization: What Is Required And Can It Be Learned?": {"container_type": "Publication", "bib": {"title": "Systematic generalization: what is required and can it be learned?", "author": ["D Bahdanau", "S Murty", "M Noukhovitch"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.12889", "author_id": ["Nq0dVMcAAAAJ", "ubAcojQAAAAJ", "hxKUkRUAAAAJ"], "url_scholarbib": "/scholar?q=info:FFTUKnA1OwUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSystematic%2BGeneralization:%2BWhat%2BIs%2BRequired%2BAnd%2BCan%2BIt%2BBe%2BLearned%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FFTUKnA1OwUJ&ei=XWJeYr2nGd-Vy9YPs66ekAk&json=", "num_citations": 102, "citedby_url": "/scholar?cites=376953749686735892&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FFTUKnA1OwUJ:scholar.google.com/&scioq=Systematic+Generalization:+What+Is+Required+And+Can+It+Be+Learned%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.12889"}, "Information Theoretic Lower Bounds On Negative Log Likelihood": {"container_type": "Publication", "bib": {"title": "Information theoretic lower bounds on negative log likelihood", "author": ["LA Lastras"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.06395", "abstract": "In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model? One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model. The core of our contribution is to formally show"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.06395", "author_id": ["IxBeLg8AAAAJ"], "url_scholarbib": "/scholar?q=info:qz9q1K2N17QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation%2BTheoretic%2BLower%2BBounds%2BOn%2BNegative%2BLog%2BLikelihood%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qz9q1K2N17QJ&ei=YmJeYsfJMoryyASen4NI&json=", "num_citations": 2, "citedby_url": "/scholar?cites=13031039824552411051&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qz9q1K2N17QJ:scholar.google.com/&scioq=Information+Theoretic+Lower+Bounds+On+Negative+Log+Likelihood&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.06395"}, "Structured Neural Summarization": {"container_type": "Publication", "bib": {"title": "A hierarchical structured self-attentive model for extractive document summarization (HSSAS)", "author": ["K Al-Sabahi", "Z Zuping", "M Nadher"], "pub_year": "2018", "venue": "IEEE Access", "abstract": "In this work, we use a hierarchical structured self-attention  neural network-based approach  for summarization that extracts sentences from a document by treating the summarization"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/8344797/", "author_id": ["nTd254oAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:oMX72u0r_gEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructured%2BNeural%2BSummarization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oMX72u0r_gEJ&ei=ZWJeYr74OJWMy9YPt8OamA0&json=", "num_citations": 93, "citedby_url": "/scholar?cites=143600538703611296&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oMX72u0r_gEJ:scholar.google.com/&scioq=Structured+Neural+Summarization&hl=en&as_sdt=0,33", "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/08344797.pdf"}, "Rotate: Knowledge Graph Embedding By Relational Rotation In Complex Space": {"container_type": "Publication", "bib": {"title": "Rotate: Knowledge graph embedding by relational rotation in complex space", "author": ["Z Sun", "ZH Deng", "JY Nie", "J Tang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.10197", "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.10197", "author_id": ["xkH30GgAAAAJ", "tRoAxlsAAAAJ", "W7uYg0UAAAAJ", "1ir6WUEAAAAJ"], "url_scholarbib": "/scholar?q=info:9kpjVH4ISYgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRotate:%2BKnowledge%2BGraph%2BEmbedding%2BBy%2BRelational%2BRotation%2BIn%2BComplex%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9kpjVH4ISYgJ&ei=aWJeYpjIMIvMsQK69Y7ABg&json=", "num_citations": 724, "citedby_url": "/scholar?cites=9820389801132772086&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9kpjVH4ISYgJ:scholar.google.com/&scioq=Rotate:+Knowledge+Graph+Embedding+By+Relational+Rotation+In+Complex+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.10197?ref=https://githubhelp.com"}, "Human-level Protein Localization With Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Human-level protein localization with convolutional neural networks", "author": ["E Rumetshofer", "M Hofmarcher", "C R\u00f6hrl"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Localizing a specific protein in a human cell is essential for understanding cellular functions and biological processes of underlying diseases. A promising, low-cost, and time-efficient biotechnology for localizing proteins is high-throughput fluorescence microscopy imaging (HTI). This imaging technique stains the protein of interest in a cell with fluorescent antibodies and subsequently takes a microscopic image. Together with images of other stained proteins or cell organelles and the annotation by the Human Protein Atlas project"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryl5khRcKm&source=post_page---------------------------", "author_id": ["0AUdatYAAAAJ", "FD27EMIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:j8WVUt101xAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHuman-level%2BProtein%2BLocalization%2BWith%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j8WVUt101xAJ&ei=dmJeYprvDpHKsQKNt6-YAw&json=", "num_citations": 17, "citedby_url": "/scholar?cites=1213567118521779599&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:j8WVUt101xAJ:scholar.google.com/&scioq=Human-level+Protein+Localization+With+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryl5khRcKm"}, "Composing Complex Skills By Learning Transition Policies": {"container_type": "Publication", "bib": {"title": "Composing complex skills by learning transition policies", "author": ["Y Lee", "SH Sun", "S Somasundaram", "ES Hu"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a set of complex continuous control tasks in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rygrBhC5tQ", "author_id": ["CDPa3AgAAAAJ", "uXsfnaQAAAAJ", "ih2R6qsAAAAJ", "e35eTwYAAAAJ"], "url_scholarbib": "/scholar?q=info:qplXTFgsYpoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComposing%2BComplex%2BSkills%2BBy%2BLearning%2BTransition%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qplXTFgsYpoJ&ei=eWJeYuz8DI-bmAGmiqCIBw&json=", "num_citations": 31, "citedby_url": "/scholar?cites=11124502787308100010&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qplXTFgsYpoJ:scholar.google.com/&scioq=Composing+Complex+Skills+By+Learning+Transition+Policies&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rygrBhC5tQ"}, "Learning Procedural Abstractions And Evaluating Discrete Latent Temporal Structure": {"container_type": "Publication", "bib": {"title": "Learning procedural abstractions and evaluating discrete latent temporal structure", "author": ["K Goel", "E Brunskill"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Clustering methods and latent variable models are often used as tools for pattern mining and discovery of latent structure in time-series data. In this work, we consider the problem of learning procedural abstractions from possibly high-dimensional observational sequences, such as video demonstrations. Given a dataset of time-series, the goal is to identify the latent sequence of steps common to them and label each time-series with the temporal extent of these procedural steps. We introduce a hierarchical Bayesian model called Prism that"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByleB2CcKm", "author_id": ["1i3X2GgAAAAJ", "HaN8b2YAAAAJ"], "url_scholarbib": "/scholar?q=info:c11fSorzhkIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BProcedural%2BAbstractions%2BAnd%2BEvaluating%2BDiscrete%2BLatent%2BTemporal%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c11fSorzhkIJ&ei=hWJeYqmXFZWMy9YPt8OamA0&json=", "num_citations": 5, "citedby_url": "/scholar?cites=4793786628661271923&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c11fSorzhkIJ:scholar.google.com/&scioq=Learning+Procedural+Abstractions+And+Evaluating+Discrete+Latent+Temporal+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByleB2CcKm"}, "Pate-gan: Generating Synthetic Data With Differential Privacy Guarantees": {"container_type": "Publication", "bib": {"title": "PATE-GAN: Generating synthetic data with differential privacy guarantees", "author": ["J Jordon", "J Yoon", "M Van Der Schaar"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1zk9iRqF7", "author_id": ["jiyonF0AAAAJ", "kiFd6A8AAAAJ", "DZ3S--MAAAAJ"], "url_scholarbib": "/scholar?q=info:TIPT8tQWC38J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPate-gan:%2BGenerating%2BSynthetic%2BData%2BWith%2BDifferential%2BPrivacy%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TIPT8tQWC38J&ei=kWJeYoLHFZWMy9YPt8OamA0&json=", "num_citations": 235, "citedby_url": "/scholar?cites=9154435771423490892&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TIPT8tQWC38J:scholar.google.com/&scioq=Pate-gan:+Generating+Synthetic+Data+With+Differential+Privacy+Guarantees&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1zk9iRqF7"}, "An Empirical Study Of Binary Neural Networks' Optimisation": {"container_type": "Publication", "bib": {"title": "An empirical study of binary neural networks' optimisation", "author": ["M Alizadeh", "J Fern\u00e1ndez-Marqu\u00e9s"], "pub_year": "2018", "venue": "\u2026 conference on learning \u2026", "abstract": "Binary neural networks using the Straight-Through-Estimator (STE) have been shown to achieve state-of-the-art results, but their training process is not well-founded. This is due to the discrepancy between the evaluated function in the forward path, and the weight updates in the back-propagation, updates which do not correspond to gradients of the forward path. Efficient convergence and accuracy of binary models often rely on careful fine-tuning and various ad-hoc techniques. In this work, we empirically identify and study the effectiveness of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJfUCoR5KX", "author_id": ["YmGyDhcAAAAJ", "Htu1YhIAAAAJ"], "url_scholarbib": "/scholar?q=info:ozcVjfjGyJsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BEmpirical%2BStudy%2BOf%2BBinary%2BNeural%2BNetworks%2527%2BOptimisation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ozcVjfjGyJsJ&ei=nGJeYtTPOd-Vy9YPs66ekAk&json=", "num_citations": 53, "citedby_url": "/scholar?cites=11225440842042128291&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ozcVjfjGyJsJ:scholar.google.com/&scioq=An+Empirical+Study+Of+Binary+Neural+Networks%27+Optimisation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJfUCoR5KX"}, "Verification Of Non-linear Specifications For Neural Networks": {"container_type": "Publication", "bib": {"title": "Verification of non-linear specifications for neural networks", "author": ["C Qin", "B O'Donoghue", "R Bunel", "R Stanforth"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Prior work on neural network verification has focused on specifications that are linear functions of the output of the network, eg, invariance of the classifier output under adversarial perturbations of the input. In this paper, we extend verification algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable specifications, which constitute nonlinear specifications that can be verified using a convex relaxation. We show that a number of important properties of interest"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09592", "author_id": ["D5ut5cMAAAAJ", "0Pzjj-cAAAAJ", "7cqQFSoAAAAJ", ""], "url_scholarbib": "/scholar?q=info:1s96oPFAIfMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVerification%2BOf%2BNon-linear%2BSpecifications%2BFor%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1s96oPFAIfMJ&ei=oGJeYr-NNZWMy9YPt8OamA0&json=", "num_citations": 30, "citedby_url": "/scholar?cites=17519355431971639254&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1s96oPFAIfMJ:scholar.google.com/&scioq=Verification+Of+Non-linear+Specifications+For+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09592"}, "Rigorous Agent Evaluation: An Adversarial Approach To Uncover Catastrophic Failures": {"container_type": "Publication", "bib": {"title": "Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures", "author": ["J Uesato", "A Kumar", "C Szepesvari", "T Erez"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.01647", "author_id": ["E_-4S6wAAAAJ", "tP5IBFkAAAAJ", "zvC19mQAAAAJ", "gVFnjOcAAAAJ"], "url_scholarbib": "/scholar?q=info:M0QaOE26T7UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRigorous%2BAgent%2BEvaluation:%2BAn%2BAdversarial%2BApproach%2BTo%2BUncover%2BCatastrophic%2BFailures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M0QaOE26T7UJ&ei=pGJeYtX4Hd-Vy9YPs66ekAk&json=", "num_citations": 48, "citedby_url": "/scholar?cites=13064865884841591859&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:M0QaOE26T7UJ:scholar.google.com/&scioq=Rigorous+Agent+Evaluation:+An+Adversarial+Approach+To+Uncover+Catastrophic+Failures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.01647"}, "Learning To Infer And Execute 3d Shape Programs": {"container_type": "Publication", "bib": {"title": "Learning to infer and execute 3d shape programs", "author": ["Y Tian", "A Luo", "X Sun", "K Ellis", "WT Freeman"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Human perception of 3D shapes goes beyond reconstructing them as a set of points or a composition of geometric primitives: we also effortlessly understand higher-level shape structure such as the repetition and reflective symmetry of object parts. In contrast, recent advances in 3D shape sensing focus more on low-level geometry but less on these higher-level relationships. In this paper, we propose 3D shape programs, integrating bottom-up recognition systems with top-down, symbolic program structure to capture both low-level"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.02875", "author_id": ["OsP7JHAAAAAJ", "bWYvvkUAAAAJ", "lRLvr5EAAAAJ", "tVjxANMAAAAJ", "0zZnyMEAAAAJ"], "url_scholarbib": "/scholar?q=info:PjrKVScxiaYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BInfer%2BAnd%2BExecute%2B3d%2BShape%2BPrograms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PjrKVScxiaYJ&ei=rGJeYsrrFZLeyQTms5KQBg&json=", "num_citations": 85, "citedby_url": "/scholar?cites=12000176727118199358&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PjrKVScxiaYJ:scholar.google.com/&scioq=Learning+To+Infer+And+Execute+3d+Shape+Programs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.02875"}, "Learning Implicitly Recurrent Cnns Through Parameter Sharing": {"container_type": "Publication", "bib": {"title": "Learning implicitly recurrent CNNs through parameter sharing", "author": ["P Savarese", "M Maire"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09701", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy. Our simple parameter sharing scheme, though defined via soft"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09701", "author_id": ["PwQHMlwAAAAJ", "HXowq5YAAAAJ"], "url_scholarbib": "/scholar?q=info:ZItyri1M4tEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BImplicitly%2BRecurrent%2BCnns%2BThrough%2BParameter%2BSharing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZItyri1M4tEJ&ei=sWJeYtHmCMiBy9YP18Gi8As&json=", "num_citations": 43, "citedby_url": "/scholar?cites=15123734257747528548&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZItyri1M4tEJ:scholar.google.com/&scioq=Learning+Implicitly+Recurrent+Cnns+Through+Parameter+Sharing&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09701"}, "Deep Anomaly Detection With Outlier Exposure": {"container_type": "Publication", "bib": {"title": "Deep anomaly detection with outlier exposure", "author": ["D Hendrycks", "M Mazeika", "T Dietterich"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.04606", "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.04606", "author_id": ["czyretsAAAAJ", "", "09kJn28AAAAJ"], "url_scholarbib": "/scholar?q=info:uaJzUNAAHcEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BAnomaly%2BDetection%2BWith%2BOutlier%2BExposure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uaJzUNAAHcEJ&ei=tGJeYsyzB5HKsQKNt6-YAw&json=", "num_citations": 605, "citedby_url": "/scholar?cites=13915279318347653817&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uaJzUNAAHcEJ:scholar.google.com/&scioq=Deep+Anomaly+Detection+With+Outlier+Exposure&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.04606"}, "Imposing Category Trees Onto Word-embeddings Using A Geometric Construction": {"container_type": "Publication", "bib": {"title": "Imposing category trees onto word-embeddings using a geometric construction", "author": ["T Dong", "C Bauckhage", "H Jin", "J Li"], "pub_year": "2018", "venue": "International \u2026", "abstract": "We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short). Inclusion relations among N-balls implicitly encode subordinate relations among categories. The similarity measurement in terms of the cosine function is enriched by category information. Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions:(1) category trees are"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJlWOj0qF7", "author_id": ["6vT7cLQAAAAJ", "f9iP-80AAAAJ", "", "SgNB-ioAAAAJ"], "url_scholarbib": "/scholar?q=info:kKQu13JqPLkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImposing%2BCategory%2BTrees%2BOnto%2BWord-embeddings%2BUsing%2BA%2BGeometric%2BConstruction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kKQu13JqPLkJ&ei=t2JeYvnyIsiBy9YP18Gi8As&json=", "num_citations": 17, "citedby_url": "/scholar?cites=13347660437088281744&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kKQu13JqPLkJ:scholar.google.com/&scioq=Imposing+Category+Trees+Onto+Word-embeddings+Using+A+Geometric+Construction&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJlWOj0qF7"}, "Transfer Learning For Sequences Via Learning To Collocate": {"container_type": "Publication", "bib": {"title": "Transfer learning for sequences via learning to collocate", "author": ["W Cui", "G Zheng", "Z Shen", "S Jiang", "W Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Transfer learning aims to solve the data sparsity for a target domain by applying information of the source domain. Given a sequence (eg a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represents the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply represents the whole sentence by a single vector, which is unfeasible for seq2seq and sequence labeling"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09092", "author_id": ["Pw_jF8sAAAAJ", "", "DGr0fVoAAAAJ", "ZGTHnpAAAAAJ", "UedS9LQAAAAJ"], "url_scholarbib": "/scholar?q=info:kR6h1nv7uYkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransfer%2BLearning%2BFor%2BSequences%2BVia%2BLearning%2BTo%2BCollocate%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kR6h1nv7uYkJ&ei=vGJeYpW6FpGJmwGIxre4DA&json=", "num_citations": 18, "citedby_url": "/scholar?cites=9924239763188031121&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kR6h1nv7uYkJ:scholar.google.com/&scioq=Transfer+Learning+For+Sequences+Via+Learning+To+Collocate&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09092"}, "Preconditioner On Matrix Lie Group For Sgd": {"container_type": "Publication", "bib": {"title": "Preconditioner on Matrix Lie Group for SGD", "author": ["XL Li"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.10232", "abstract": "We study two types of preconditioners and preconditioned stochastic gradient descent (SGD) methods in a unified framework. We call the first one the Newton type due to its close relationship to the Newton method, and the second one the Fisher type as its preconditioner is closely related to the inverse of Fisher information matrix. Both preconditioners can be derived from one framework, and efficiently estimated on any matrix Lie groups designated by the user using natural or relative gradient descent minimizing certain preconditioner"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10232", "author_id": [""], "url_scholarbib": "/scholar?q=info:5ZGRUMcMxAgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPreconditioner%2BOn%2BMatrix%2BLie%2BGroup%2BFor%2BSgd%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5ZGRUMcMxAgJ&ei=wGJeYumTGI6pywSdh6agAg&json=", "num_citations": 2, "citedby_url": "/scholar?cites=631643897928454629&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5ZGRUMcMxAgJ:scholar.google.com/&scioq=Preconditioner+On+Matrix+Lie+Group+For+Sgd&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10232"}, "Guiding Policies With Language Via Meta-learning": {"container_type": "Publication", "bib": {"title": "Guiding policies with language via meta-learning", "author": ["JD Co-Reyes", "A Gupta", "S Sanjeev", "N Altieri"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.07882", "author_id": ["xBH73TYAAAAJ", "1wLVDP4AAAAJ", "onuj8XQAAAAJ", ""], "url_scholarbib": "/scholar?q=info:Ft4585b5o_oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGuiding%2BPolicies%2BWith%2BLanguage%2BVia%2BMeta-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ft4585b5o_oJ&ei=x2JeYvaWF4vMsQK69Y7ABg&json=", "num_citations": 33, "citedby_url": "/scholar?cites=18060553357406887446&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ft4585b5o_oJ:scholar.google.com/&scioq=Guiding+Policies+With+Language+Via+Meta-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.07882"}, "Temporal Difference Variational Auto-encoder": {"container_type": "Publication", "bib": {"title": "Temporal difference variational auto-encoder", "author": ["K Gregor", "G Papamakarios", "F Besse", "L Buesing"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics:(a) it should build an abstract state representing the condition of the world;(b) it should form a belief which represents uncertainty on the world;(c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.03107", "author_id": ["", "wHcpf58AAAAJ", "gMdUxasAAAAJ", "1h_mxPMAAAAJ"], "url_scholarbib": "/scholar?q=info:2KI3r8ZWSx4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTemporal%2BDifference%2BVariational%2BAuto-encoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2KI3r8ZWSx4J&ei=1mJeYsefMovMsQK69Y7ABg&json=", "num_citations": 98, "citedby_url": "/scholar?cites=2182933855734309592&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2KI3r8ZWSx4J:scholar.google.com/&scioq=Temporal+Difference+Variational+Auto-encoder&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.03107"}, "Analyzing Inverse Problems With Invertible Neural Networks": {"container_type": "Publication", "bib": {"title": "Analyzing inverse problems with invertible neural networks", "author": ["L Ardizzone", "J Kruse", "S Wirkert", "D Rahner"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In many tasks, in particular in natural science, the goal is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter-to measurement-space is a well-defined function, whereas the inverse problem is ambiguous: one measurement may map to multiple different sets of parameters. In this setting, the posterior parameter distribution, conditioned on an input measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task--so"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.04730", "author_id": ["DsAf47EAAAAJ", "PDuep_IAAAAJ", "fl77GZMAAAAJ", ""], "url_scholarbib": "/scholar?q=info:sE8GZuq2u6IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalyzing%2BInverse%2BProblems%2BWith%2BInvertible%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sE8GZuq2u6IJ&ei=2WJeYtyIIIyuyASD3KfABw&json=", "num_citations": 248, "citedby_url": "/scholar?cites=11726167172639510448&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sE8GZuq2u6IJ:scholar.google.com/&scioq=Analyzing+Inverse+Problems+With+Invertible+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.04730"}, "Riemannian Adaptive Optimization Methods": {"container_type": "Publication", "bib": {"title": "Riemannian adaptive optimization methods", "author": ["G B\u00e9cigneul", "OE Ganea"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00760", "abstract": "Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the  most agnostic Riemannian  case of a product of Riemannian manifolds, in which adaptivity"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00760", "author_id": ["_tehCwgAAAAJ", "0dYS0sMAAAAJ"], "url_scholarbib": "/scholar?q=info:n4T8ZKQHYxMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRiemannian%2BAdaptive%2BOptimization%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n4T8ZKQHYxMJ&ei=3GJeYsmPLZGJmwGIxre4DA&json=", "num_citations": 108, "citedby_url": "/scholar?cites=1396968712065287327&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n4T8ZKQHYxMJ:scholar.google.com/&scioq=Riemannian+Adaptive+Optimization+Methods&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00760"}, "Information Asymmetry In Kl-regularized Rl": {"container_type": "Publication", "bib": {"title": "Information asymmetry in KL-regularized RL", "author": ["A Galashov", "SM Jayakumar", "L Hasenclever"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.01240", "author_id": ["kIpoNtcAAAAJ", "rJUAY8QAAAAJ", "dD-3S4QAAAAJ"], "url_scholarbib": "/scholar?q=info:bQBLwGmcJqgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation%2BAsymmetry%2BIn%2BKl-regularized%2BRl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bQBLwGmcJqgJ&ei=32JeYuetK9-Vy9YPs66ekAk&json=", "num_citations": 56, "citedby_url": "/scholar?cites=12116543825498538093&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bQBLwGmcJqgJ:scholar.google.com/&scioq=Information+Asymmetry+In+Kl-regularized+Rl&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.01240"}, "Lagging Inference Networks And Posterior Collapse In Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "Lagging inference networks and posterior collapse in variational autoencoders", "author": ["J He", "D Spokoyny", "G Neubig"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as\" posterior collapse\" where the model learns to ignore the latent variable and the approximate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.05534", "author_id": ["BIFGeoUAAAAJ", "", "wlosgkoAAAAJ"], "url_scholarbib": "/scholar?q=info:eu0ffO9XXkkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLagging%2BInference%2BNetworks%2BAnd%2BPosterior%2BCollapse%2BIn%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eu0ffO9XXkkJ&ei=42JeYqeSHM2Ny9YPqPyUgAs&json=", "num_citations": 203, "citedby_url": "/scholar?cites=5286759698670808442&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eu0ffO9XXkkJ:scholar.google.com/&scioq=Lagging+Inference+Networks+And+Posterior+Collapse+In+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.05534"}, "Doubly Reparameterized Gradient Estimators For Monte Carlo Objectives": {"container_type": "Publication", "bib": {"title": "Doubly reparameterized gradient estimators for monte carlo objectives", "author": ["G Tucker", "D Lawson", "S Gu", "CJ Maddison"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.04152", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling, 2013; Rezende et al., 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al.(2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.04152", "author_id": ["-gJkPHIAAAAJ", "8xSYX9IAAAAJ", "B8wslVsAAAAJ", "WjCG3owAAAAJ"], "url_scholarbib": "/scholar?q=info:EcXAxFbmktoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDoubly%2BReparameterized%2BGradient%2BEstimators%2BFor%2BMonte%2BCarlo%2BObjectives%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EcXAxFbmktoJ&ei=7mJeYqOBLo-bmAGmiqCIBw&json=", "num_citations": 71, "citedby_url": "/scholar?cites=15749904107210589457&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EcXAxFbmktoJ:scholar.google.com/&scioq=Doubly+Reparameterized+Gradient+Estimators+For+Monte+Carlo+Objectives&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.04152"}, "A Unified Theory Of Early Visual Representations From Retina To Cortex Through Anatomically Constrained Deep Cnns": {"container_type": "Publication", "bib": {"title": "A unified theory of early visual representations from retina to cortex through anatomically constrained deep CNNs", "author": ["J Lindsey", "SA Ocko", "S Ganguli", "S Deny"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.00945", "abstract": "The visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex, typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.00945", "author_id": ["CNrQvh4AAAAJ", "S-WI588AAAAJ", "rF2VvOgAAAAJ", "Z3jU6mQAAAAJ"], "url_scholarbib": "/scholar?q=info:j3AVKIlxxhwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BUnified%2BTheory%2BOf%2BEarly%2BVisual%2BRepresentations%2BFrom%2BRetina%2BTo%2BCortex%2BThrough%2BAnatomically%2BConstrained%2BDeep%2BCnns%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j3AVKIlxxhwJ&ei=-WJeYuyrGY6pywSdh6agAg&json=", "num_citations": 53, "citedby_url": "/scholar?cites=2073469512347644047&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:j3AVKIlxxhwJ:scholar.google.com/&scioq=A+Unified+Theory+Of+Early+Visual+Representations+From+Retina+To+Cortex+Through+Anatomically+Constrained+Deep+Cnns&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.00945.pdf?ref=https://githubhelp.com"}, "Latent Convolutional Models": {"container_type": "Publication", "bib": {"title": "Latent convolutional models", "author": ["SR Athar", "E Burnaev", "V Lempitsky"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.06284", "abstract": "In the experiments, we compare the learned latent models with latent models  latent  modeling to high-resolution images, we consider latent models with tens of thousands of latent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.06284", "author_id": ["mdUv8wcAAAAJ", "pCRdcOwAAAAJ", "gYYVokYAAAAJ"], "url_scholarbib": "/scholar?q=info:BLTorWrbqhAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BConvolutional%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BLTorWrbqhAJ&ei=_GJeYr3DK5GJmwGIxre4DA&json=", "num_citations": 23, "citedby_url": "/scholar?cites=1201013501878383620&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BLTorWrbqhAJ:scholar.google.com/&scioq=Latent+Convolutional+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.06284"}, "On Computation And Generalization Of Generative Adversarial Networks Under Spectrum Control": {"container_type": "Publication", "bib": {"title": "On computation and generalization of generative adversarial networks under spectrum control", "author": ["H Jiang", "Z Chen", "M Chen", "F Liu", "D Wang"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Generative Adversarial Networks (GANs), though powerful, is hard to train. Several recent works (Brock et al., 2016; Miyato et al., 2018) suggest that controlling the spectra of weight matrices in the discriminator can significantly improve the training of GANs. Motivated by their discovery, we propose a new framework for training GANs, which allows more flexible spectrum control (eg, making the weight matrices of the discriminator have slow singular value decays). Specifically, we propose a new reparameterization approach for the weight"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJNH6sAqY7", "author_id": ["XaFhuG8AAAAJ", "2lvIrNAAAAAJ", "qU9WvTgAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:2BBxYAbFt58J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BComputation%2BAnd%2BGeneralization%2BOf%2BGenerative%2BAdversarial%2BNetworks%2BUnder%2BSpectrum%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2BBxYAbFt58J&ei=AGNeYua3NZGJmwGIxre4DA&json=", "num_citations": 17, "citedby_url": "/scholar?cites=11508884003947090136&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2BBxYAbFt58J:scholar.google.com/&scioq=On+Computation+And+Generalization+Of+Generative+Adversarial+Networks+Under+Spectrum+Control&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJNH6sAqY7"}, "Automatically Composing Representation Transformations As A Means For Generalization": {"container_type": "Publication", "bib": {"title": "Automatically composing representation transformations as a means for generalization", "author": ["MB Chang", "A Gupta", "S Levine", "TL Griffiths"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning--either training a separate learner per task or training a single learner for all tasks--both have difficulty with such generalization because they do not leverage the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.04640", "author_id": ["vgfGtykAAAAJ", "1wLVDP4AAAAJ", "8R35rCwAAAAJ", "UAwKvEsAAAAJ"], "url_scholarbib": "/scholar?q=info:hQtlb5su8h8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomatically%2BComposing%2BRepresentation%2BTransformations%2BAs%2BA%2BMeans%2BFor%2BGeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hQtlb5su8h8J&ei=A2NeYv7qLN-Vy9YPs66ekAk&json=", "num_citations": 55, "citedby_url": "/scholar?cites=2301953604663446405&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hQtlb5su8h8J:scholar.google.com/&scioq=Automatically+Composing+Representation+Transformations+As+A+Means+For+Generalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.04640"}, "Learning Mixed-curvature Representations In Product Spaces": {"container_type": "Publication", "bib": {"title": "Learning mixed-curvature representations in product spaces", "author": ["A Gu", "F Sala", "B Gunel", "C R\u00e9"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data. Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly. We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJxeWnCcF7", "author_id": ["DVCHv1kAAAAJ", "9KhIkNkAAAAJ", "FckK6vUAAAAJ", "DnnCWN0AAAAJ"], "url_scholarbib": "/scholar?q=info:ATKV83s-nRIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BMixed-curvature%2BRepresentations%2BIn%2BProduct%2BSpaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ATKV83s-nRIJ&ei=BmNeYuPLL82Ny9YPqPyUgAs&json=", "num_citations": 97, "citedby_url": "/scholar?cites=1341296966114816513&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ATKV83s-nRIJ:scholar.google.com/&scioq=Learning+Mixed-curvature+Representations+In+Product+Spaces&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJxeWnCcF7"}, "Neural Program Repair By Jointly Learning To Localize And Repair": {"container_type": "Publication", "bib": {"title": "Neural program repair by jointly learning to localize and repair", "author": ["M Vasic", "A Kanade", "P Maniatis", "D Bieber"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.01720", "author_id": ["cC0hHK0AAAAJ", "FCCMbWYAAAAJ", "cTGO2HoAAAAJ", "KVXW75wAAAAJ"], "url_scholarbib": "/scholar?q=info:CSDWvV_2LCQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BProgram%2BRepair%2BBy%2BJointly%2BLearning%2BTo%2BLocalize%2BAnd%2BRepair%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CSDWvV_2LCQJ&ei=CWNeYtXNApGJmwGIxre4DA&json=", "num_citations": 71, "citedby_url": "/scholar?cites=2606729175407927305&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CSDWvV_2LCQJ:scholar.google.com/&scioq=Neural+Program+Repair+By+Jointly+Learning+To+Localize+And+Repair&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.01720"}, "Tree-structured Recurrent Switching Linear Dynamical Systems For Multi-scale Modeling": {"container_type": "Publication", "bib": {"title": "Tree-structured recurrent switching linear dynamical systems for multi-scale modeling", "author": ["J Nassar", "SW Linderman", "M Bugallo"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.12386", "author_id": ["a5RNqTYAAAAJ", "6mD3I24AAAAJ", ""], "url_scholarbib": "/scholar?q=info:r_Qblnnd5pcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTree-structured%2BRecurrent%2BSwitching%2BLinear%2BDynamical%2BSystems%2BFor%2BMulti-scale%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r_Qblnnd5pcJ&ei=C2NeYp7VNpHKsQKNt6-YAw&json=", "num_citations": 43, "citedby_url": "/scholar?cites=10945679458649765039&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r_Qblnnd5pcJ:scholar.google.com/&scioq=Tree-structured+Recurrent+Switching+Linear+Dynamical+Systems+For+Multi-scale+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.12386"}, "Boosting Robustness Certification Of Neural Networks": {"container_type": "Publication", "bib": {"title": "Boosting robustness certification of neural networks", "author": ["G Singh", "T Gehr", "M P\u00fcschel"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming. This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJgeEh09KQ", "author_id": ["m4b2ruEAAAAJ", "HcL76tsAAAAJ", "az9ZryAAAAAJ"], "url_scholarbib": "/scholar?q=info:SZ2sx8Aqk4QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosting%2BRobustness%2BCertification%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SZ2sx8Aqk4QJ&ei=GGNeYquwBfmQ6rQPzKCxuAY&json=", "num_citations": 109, "citedby_url": "/scholar?cites=9553026242055019849&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SZ2sx8Aqk4QJ:scholar.google.com/&scioq=Boosting+Robustness+Certification+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJgeEh09KQ"}, "Instagan: Instance-aware Image-to-image Translation": {"container_type": "Publication", "bib": {"title": "Instagan: Instance-aware image-to-image translation", "author": ["S Mo", "M Cho", "J Shin"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.10889", "abstract": "Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs). However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, eg, translating pants to skirts in fashion images. To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (eg, object"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.10889", "author_id": ["Sq9y3NMAAAAJ", "5TyoF5QAAAAJ", "m3eDp7kAAAAJ"], "url_scholarbib": "/scholar?q=info:KbwEYPLX3sIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInstagan:%2BInstance-aware%2BImage-to-image%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KbwEYPLX3sIJ&ei=GmNeYvSnOvmQ6rQPzKCxuAY&json=", "num_citations": 137, "citedby_url": "/scholar?cites=14041898124180765737&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KbwEYPLX3sIJ:scholar.google.com/&scioq=Instagan:+Instance-aware+Image-to-image+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.10889.pdf&xid=17259,15700023,15700186,15700191,15700256,15700259,15700262,15700264"}, "Defensive Quantization: When Efficiency Meets Robustness": {"container_type": "Publication", "bib": {"title": "Defensive quantization: When efficiency meets robustness", "author": ["J Lin", "C Gan", "S Han"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.08444", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.08444", "author_id": ["dVtzVVAAAAAJ", "PTeSCbIAAAAJ", "E0iCaa4AAAAJ"], "url_scholarbib": "/scholar?q=info:IHs2penuvgQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDefensive%2BQuantization:%2BWhen%2BEfficiency%2BMeets%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IHs2penuvgQJ&ei=HWNeYtTJHpyO6rQP-viEEA&json=", "num_citations": 134, "citedby_url": "/scholar?cites=341973308993338144&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IHs2penuvgQJ:scholar.google.com/&scioq=Defensive+Quantization:+When+Efficiency+Meets+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.08444.pdf?ref=https://githubhelp.com"}, "Arm: Augment-reinforce-merge Gradient For Stochastic Binary Networks": {"container_type": "Publication", "bib": {"title": "ARM: Augment-REINFORCE-merge gradient for stochastic binary networks", "author": ["M Yin", "M Zhou"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1807.11143", "abstract": "To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.11143", "author_id": ["oAEsILEAAAAJ", "LXwCIisAAAAJ"], "url_scholarbib": "/scholar?q=info:qmUL7_5jpRAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DArm:%2BAugment-reinforce-merge%2BGradient%2BFor%2BStochastic%2BBinary%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qmUL7_5jpRAJ&ei=IGNeYojjCciBy9YP18Gi8As&json=", "num_citations": 42, "citedby_url": "/scholar?cites=1199474822347449770&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qmUL7_5jpRAJ:scholar.google.com/&scioq=Arm:+Augment-reinforce-merge+Gradient+For+Stochastic+Binary+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.11143"}, "Bayesian Deep Convolutional Networks With Many Channels Are Gaussian Processes": {"container_type": "Publication", "bib": {"title": "Bayesian deep convolutional networks with many channels are gaussian processes", "author": ["R Novak", "L Xiao", "J Lee", "Y Bahri", "G Yang", "J Hron"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05148", "author_id": ["syG6krEAAAAJ", "fvwzUnIAAAAJ", "d3YhiooAAAAJ", "p2_vHmAAAAAJ", "Xz4RAJkAAAAJ", "Jp7hKlAAAAAJ"], "url_scholarbib": "/scholar?q=info:xsVN2vwd8PsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2BDeep%2BConvolutional%2BNetworks%2BWith%2BMany%2BChannels%2BAre%2BGaussian%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xsVN2vwd8PsJ&ei=ImNeYv6OMJGJmwGIxre4DA&json=", "num_citations": 200, "citedby_url": "/scholar?cites=18154043069761963462&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xsVN2vwd8PsJ:scholar.google.com/&scioq=Bayesian+Deep+Convolutional+Networks+With+Many+Channels+Are+Gaussian+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05148"}, "Hierarchical Interpretations For Neural Network Predictions": {"container_type": "Publication", "bib": {"title": "Hierarchical interpretations for neural network predictions", "author": ["C Singh", "WJ Murdoch", "B Yu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.05337", "abstract": "Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.05337", "author_id": ["LPPeYm4AAAAJ", "3JNHcMwAAAAJ", "xT19Jc0AAAAJ"], "url_scholarbib": "/scholar?q=info:RztHFLxMjskJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BInterpretations%2BFor%2BNeural%2BNetwork%2BPredictions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RztHFLxMjskJ&ei=JmNeYo21DM2Ny9YPqPyUgAs&json=", "num_citations": 84, "citedby_url": "/scholar?cites=14523630218994203463&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RztHFLxMjskJ:scholar.google.com/&scioq=Hierarchical+Interpretations+For+Neural+Network+Predictions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.05337?ref=https://githubhelp.com"}, "Modeling Uncertainty With Hedged Instance Embeddings": {"container_type": "Publication", "bib": {"title": "Modeling uncertainty with hedged instance embedding", "author": ["SJ Oh", "K Murphy", "J Pan", "J Roth", "F Schroff"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods represent the input as a single point in the embedding space. Often the distance between points is used as a proxy for match confidence. However, this can fail to represent uncertainty arising when the input is ambiguous, eg, due to occlusion or blurriness. This work addresses this issue and explicitly models the uncertainty by hedging the location of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00319", "author_id": ["kmXOOdsAAAAJ", "MxxZkEcAAAAJ", "vXbS7XIAAAAJ", "87I45JsAAAAJ", "eWbZJlMAAAAJ"], "url_scholarbib": "/scholar?q=info:idhCjWHiY9YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModeling%2BUncertainty%2BWith%2BHedged%2BInstance%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=idhCjWHiY9YJ&ei=KWNeYohPyIHL1g_XwaLwCw&json=", "num_citations": 39, "citedby_url": "/scholar?cites=15448440055420606601&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:idhCjWHiY9YJ:scholar.google.com/&scioq=Modeling+Uncertainty+With+Hedged+Instance+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00319"}, "Directed-info Gail: Learning Hierarchical Policies From Unsegmented Demonstrations Using Directed Information": {"container_type": "Publication", "bib": {"title": "Directed-info gail: Learning hierarchical policies from unsegmented demonstrations using directed information", "author": ["A Sharma", "M Sharma", "N Rhinehart"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting state-action trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01266", "author_id": ["uOSmj2MAAAAJ", "FBVwO5wAAAAJ", "xUGZX_MAAAAJ"], "url_scholarbib": "/scholar?q=info:pugvoGcMYkoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDirected-info%2BGail:%2BLearning%2BHierarchical%2BPolicies%2BFrom%2BUnsegmented%2BDemonstrations%2BUsing%2BDirected%2BInformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pugvoGcMYkoJ&ei=NWNeYv6DDJHKsQKNt6-YAw&json=", "num_citations": 34, "citedby_url": "/scholar?cites=5359860145732970662&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pugvoGcMYkoJ:scholar.google.com/&scioq=Directed-info+Gail:+Learning+Hierarchical+Policies+From+Unsegmented+Demonstrations+Using+Directed+Information&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01266"}, "Dynamic Channel Pruning: Feature Boosting And Suppression": {"container_type": "Publication", "bib": {"title": "Dynamic channel pruning: Feature boosting and suppression", "author": ["X Gao", "Y Zhao", "\u0141 Dudziak", "R Mullins", "C Xu"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05331", "author_id": ["-YIUCL8AAAAJ", "lOOmgEgAAAAJ", "R47NvpoAAAAJ", "zjXO2HMAAAAJ", "XsBBTUgAAAAJ"], "url_scholarbib": "/scholar?q=info:XeU9ZTTDTBoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BChannel%2BPruning:%2BFeature%2BBoosting%2BAnd%2BSuppression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XeU9ZTTDTBoJ&ei=OGNeYuaHBcLZmQHc1ovQAg&json=", "num_citations": 172, "citedby_url": "/scholar?cites=1895104173020407133&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XeU9ZTTDTBoJ:scholar.google.com/&scioq=Dynamic+Channel+Pruning:+Feature+Boosting+And+Suppression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05331?ref=https://githubhelp.com"}, "Learning Programmatically Structured Representations With Perceptor Gradients": {"container_type": "Publication", "bib": {"title": "Learning programmatically structured representations with perceptor gradients", "author": ["S Penkov", "S Ramamoorthy"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.00956", "abstract": "We present the perceptor gradients algorithm--a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.00956", "author_id": ["zuoAPEgAAAAJ", "K_v3RvMAAAAJ"], "url_scholarbib": "/scholar?q=info:XdhxHrUMzpEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BProgrammatically%2BStructured%2BRepresentations%2BWith%2BPerceptor%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XdhxHrUMzpEJ&ei=O2NeYrjoJJWMy9YPt8OamA0&json=", "num_citations": 8, "citedby_url": "/scholar?cites=10506348952741337181&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XdhxHrUMzpEJ:scholar.google.com/&scioq=Learning+Programmatically+Structured+Representations+With+Perceptor+Gradients&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.00956"}, "Local Sgd Converges Fast And Communicates Little": {"container_type": "Publication", "bib": {"title": "Local SGD converges fast and communicates little", "author": ["SU Stich"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.09767", "abstract": "Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speedup with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.09767", "author_id": ["8l-mDfQAAAAJ"], "url_scholarbib": "/scholar?q=info:J_cQqflPjfEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLocal%2BSgd%2BConverges%2BFast%2BAnd%2BCommunicates%2BLittle%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J_cQqflPjfEJ&ei=PWNeYtm2OY-bmAGmiqCIBw&json=", "num_citations": 477, "citedby_url": "/scholar?cites=17405656068558747431&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:J_cQqflPjfEJ:scholar.google.com/&scioq=Local+Sgd+Converges+Fast+And+Communicates+Little&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.09767.pdf?ref=https://githubhelp.com"}, "From Language To Goals: Inverse Reinforcement Learning For Vision-based Instruction Following": {"container_type": "Publication", "bib": {"title": "From language to goals: Inverse reinforcement learning for vision-based instruction following", "author": ["J Fu", "A Korattikara", "S Levine", "S Guadarrama"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.07742", "author_id": ["T9To2C0AAAAJ", "20_pofsAAAAJ", "8R35rCwAAAAJ", "gYiCq88AAAAJ"], "url_scholarbib": "/scholar?q=info:B9bpxfROrn4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrom%2BLanguage%2BTo%2BGoals:%2BInverse%2BReinforcement%2BLearning%2BFor%2BVision-based%2BInstruction%2BFollowing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=B9bpxfROrn4J&ei=QGNeYqDWOOHDywTjooCQBQ&json=", "num_citations": 73, "citedby_url": "/scholar?cites=9128320307925997063&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:B9bpxfROrn4J:scholar.google.com/&scioq=From+Language+To+Goals:+Inverse+Reinforcement+Learning+For+Vision-based+Instruction+Following&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.07742"}, "Neural Logic Machines": {"container_type": "Publication", "bib": {"title": "Neural logic machines", "author": ["H Dong", "J Mao", "T Lin", "C Wang", "L Li", "D Zhou"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we propose a novel neural-symbolic architecture called Neural Logic Machines  (NLMs) which can conduct first-order logic deduction. Our model is fully differentiable, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.11694", "author_id": ["MrGN4oMAAAAJ", "-xaOIZIAAAAJ", "w34PyQ8AAAAJ", "vRI2blsAAAAJ", "Rqy5KDEAAAAJ", "UwLsYw8AAAAJ"], "url_scholarbib": "/scholar?q=info:90KE0SeuzD4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BLogic%2BMachines%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=90KE0SeuzD4J&ei=RmNeYvfmJI-bmAGmiqCIBw&json=", "num_citations": 146, "citedby_url": "/scholar?cites=4525183211642569463&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:90KE0SeuzD4J:scholar.google.com/&scioq=Neural+Logic+Machines&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.11694"}, "There Are Many Consistent Explanations Of Unlabeled Data: Why You Should Average": {"container_type": "Publication", "bib": {"title": "There are many consistent explanations of unlabeled data: Why you should average", "author": ["B Athiwaratkun", "M Finzi", "P Izmailov"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.05594", "author_id": ["KZpZTTQAAAAJ", "ysMAhlwAAAAJ", "AXxTpGUAAAAJ"], "url_scholarbib": "/scholar?q=info:I4GKauSU5N8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThere%2BAre%2BMany%2BConsistent%2BExplanations%2BOf%2BUnlabeled%2BData:%2BWhy%2BYou%2BShould%2BAverage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=I4GKauSU5N8J&ei=TGNeYvboJpGJmwGIxre4DA&json=", "num_citations": 165, "citedby_url": "/scholar?cites=16133183473908875555&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:I4GKauSU5N8J:scholar.google.com/&scioq=There+Are+Many+Consistent+Explanations+Of+Unlabeled+Data:+Why+You+Should+Average&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.05594"}, "Policy Transfer With Strategy Optimization": {"container_type": "Publication", "bib": {"title": "Policy transfer with strategy optimization", "author": ["W Yu", "CK Liu", "G Turk"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.05751", "abstract": "Computer simulation provides an automatic and safe way for training robotic control policies to achieve complex tasks such as locomotion. However, a policy trained in simulation usually does not transfer directly to the real hardware due to the differences between the two environments. Transfer learning using domain randomization is a promising approach, but it usually assumes that the target environment is close to the distribution of the training environments, thus relying heavily on accurate system identification. In this paper, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05751", "author_id": ["1bF2s2kAAAAJ", "i28fU0MAAAAJ", "Q_4d9N0AAAAJ"], "url_scholarbib": "/scholar?q=info:_WYHyEVqBCsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPolicy%2BTransfer%2BWith%2BStrategy%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_WYHyEVqBCsJ&ei=UGNeYvzVMMiBy9YP18Gi8As&json=", "num_citations": 51, "citedby_url": "/scholar?cites=3099719291478959869&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_WYHyEVqBCsJ:scholar.google.com/&scioq=Policy+Transfer+With+Strategy+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05751"}, "Learning Recurrent Binary/ternary Weights": {"container_type": "Publication", "bib": {"title": "Learning recurrent binary/ternary weights", "author": ["A Ardakani", "Z Ji", "SC Smithson", "BH Meyer"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent neural networks (RNNs) have shown excellent performance in processing sequence data. However, they are both complex and memory intensive due to their recursive nature. These limitations make RNNs difficult to embed on mobile devices requiring real-time processes with limited hardware resources. To address the above issues, we introduce a method that can learn binary and ternary weights during the training phase to facilitate hardware implementations of RNNs. As a result, using this approach replaces all"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.11086", "author_id": ["QFXIpuIAAAAJ", "DejwpvMAAAAJ", "", "UyAeYDwAAAAJ"], "url_scholarbib": "/scholar?q=info:ltheGG-TzMYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRecurrent%2BBinary/ternary%2BWeights%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ltheGG-TzMYJ&ei=U2NeYt3fIZGJmwGIxre4DA&json=", "num_citations": 22, "citedby_url": "/scholar?cites=14324986620118227094&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ltheGG-TzMYJ:scholar.google.com/&scioq=Learning+Recurrent+Binary/ternary+Weights&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.11086"}, "Robust Estimation Via Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Robust estimation via generative adversarial networks", "author": ["G Chao", "Y Yuan", "Z Weizhi"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Robust estimation under Huber's $\\epsilon $-contamination model has become an important topic in statistics and theoretical computer science. Rate-optimal procedures such as Tukey's median and other estimators based on statistical depth functions are impractical because of their computational intractability. In this paper, we establish an intriguing connection between f-GANs and various depth functions through the lens of f-Learning. Similar to the derivation of f-GAN, we show that these depth functions that lead to rate"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJgRDjR9tQ", "author_id": ["", "", ""], "url_scholarbib": "/scholar?q=info:-cjMyjqKPtMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BEstimation%2BVia%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-cjMyjqKPtMJ&ei=WGNeYubmA4yuyASD3KfABw&json=", "num_citations": 6, "citedby_url": "/scholar?cites=15221755775674009849&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-cjMyjqKPtMJ:scholar.google.com/&scioq=Robust+Estimation+Via+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJgRDjR9tQ"}, "Efficiently Testing Local Optimality And Escaping Saddles For Relu Networks": {"container_type": "Publication", "bib": {"title": "Efficiently testing local optimality and escaping saddles for ReLU networks", "author": ["C Yun", "S Sra", "A Jadbabaie"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.10858", "abstract": "We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks. Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction. The presence of $ M $ data points on the nondifferentiability of the ReLU divides the parameter space into at most $2^ M $ regions, which makes analysis difficult. By exploiting polyhedral geometry, we reduce the total computation down to one"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10858", "author_id": ["Ukl64ggAAAAJ", "eyCw9goAAAAJ", "ZBc_WwYAAAAJ"], "url_scholarbib": "/scholar?q=info:DCkR2e40O-4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficiently%2BTesting%2BLocal%2BOptimality%2BAnd%2BEscaping%2BSaddles%2BFor%2BRelu%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DCkR2e40O-4J&ei=W2NeYsHoPI-bmAGmiqCIBw&json=", "num_citations": 7, "citedby_url": "/scholar?cites=17166372605101418764&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DCkR2e40O-4J:scholar.google.com/&scioq=Efficiently+Testing+Local+Optimality+And+Escaping+Saddles+For+Relu+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10858"}, "Learnable Embedding Space For Efficient Neural Architecture Compression": {"container_type": "Publication", "bib": {"title": "Learnable embedding space for efficient neural architecture compression", "author": ["S Cao", "X Wang", "KM Kitani"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.00383", "abstract": "We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.00383", "author_id": ["yMYTz3AAAAAJ", "YQomDVsAAAAJ", "yv3sH74AAAAJ"], "url_scholarbib": "/scholar?q=info:VPk9-4dfoAEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearnable%2BEmbedding%2BSpace%2BFor%2BEfficient%2BNeural%2BArchitecture%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VPk9-4dfoAEJ&ei=X2NeYsbLCpWMy9YPt8OamA0&json=", "num_citations": 39, "citedby_url": "/scholar?cites=117198627951999316&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VPk9-4dfoAEJ:scholar.google.com/&scioq=Learnable+Embedding+Space+For+Efficient+Neural+Architecture+Compression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.00383"}, "Large-scale Answerer In Questioner's Mind For Visual Dialog Question Generation": {"container_type": "Publication", "bib": {"title": "Large-scale answerer in questioner's mind for visual dialog question generation", "author": ["SW Lee", "T Gao", "S Yang", "J Yoo", "JW Ha"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.08355", "abstract": "Answerer in Questioner's Mind (AQM) is an information-theoretic framework that has been recently proposed for task-oriented dialog systems. AQM benefits from asking a question that would maximize the information gain when it is asked. However, due to its intrinsic nature of explicitly calculating the information gain, AQM has a limitation when the solution space is very large. To address this, we propose AQM+ that can deal with a large-scale problem and ask a question that is more coherent to the current context of the dialog. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.08355", "author_id": ["TMTTMuQAAAAJ", "M4AKMC4AAAAJ", "jh547hEAAAAJ", "7NBlQw4AAAAJ", "eGj3ay4AAAAJ"], "url_scholarbib": "/scholar?q=info:PXO57CtbDGYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge-scale%2BAnswerer%2BIn%2BQuestioner%2527s%2BMind%2BFor%2BVisual%2BDialog%2BQuestion%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PXO57CtbDGYJ&ei=GWFeYsKYLN-Vy9YPs66ekAk&json=", "num_citations": 8, "citedby_url": "/scholar?cites=7353352535802475325&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PXO57CtbDGYJ:scholar.google.com/&scioq=Large-scale+Answerer+In+Questioner%27s+Mind+For+Visual+Dialog+Question+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.08355"}, "Learning To Solve Circuit-sat: An Unsupervised Differentiable Approach": {"container_type": "Publication", "bib": {"title": "Learning to solve circuit-SAT: An unsupervised differentiable approach", "author": ["S Amizadeh", "S Matusevych"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Recent efforts to combine Representation Learning with Formal Methods, commonly known as the Neuro-Symbolic Methods, have given rise to a new trend of applying rich neural architectures to solve classical combinatorial optimization problems. In this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem. Our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure and an end-to-end differentiable training procedure that"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJxgz2R9t7", "author_id": ["", "QJy0rQwAAAAJ"], "url_scholarbib": "/scholar?q=info:_QoH36tXsnsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BSolve%2BCircuit-sat:%2BAn%2BUnsupervised%2BDifferentiable%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_QoH36tXsnsJ&ei=HWFeYtTtD4vMsQK69Y7ABg&json=", "num_citations": 45, "citedby_url": "/scholar?cites=8913283008212437757&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_QoH36tXsnsJ:scholar.google.com/&scioq=Learning+To+Solve+Circuit-sat:+An+Unsupervised+Differentiable+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJxgz2R9t7"}, "Cem-rl: Combining Evolutionary And Gradient-based Methods For Policy Search": {"container_type": "Publication", "bib": {"title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search", "author": ["A Pourchot", "O Sigaud"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.01222", "abstract": "Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01222", "author_id": ["9ehNv-4AAAAJ", "elLfDv0AAAAJ"], "url_scholarbib": "/scholar?q=info:UtmEJ0bTRqYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCem-rl:%2BCombining%2BEvolutionary%2BAnd%2BGradient-based%2BMethods%2BFor%2BPolicy%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UtmEJ0bTRqYJ&ei=KWFeYpaiKIvMsQK69Y7ABg&json=", "num_citations": 80, "citedby_url": "/scholar?cites=11981496156929972562&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UtmEJ0bTRqYJ:scholar.google.com/&scioq=Cem-rl:+Combining+Evolutionary+And+Gradient-based+Methods+For+Policy+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01222.pdf?ref=https://githubhelp.com"}, "Harmonizing Maximum Likelihood With Gans For Multimodal Conditional Generation": {"container_type": "Publication", "bib": {"title": "Harmonizing maximum likelihood with gans for multimodal conditional generation", "author": ["S Lee", "J Ha", "G Kim"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09225", "abstract": "Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09225", "author_id": ["8O3MKJkAAAAJ", "LfC0tYAAAAAJ", "CiSdOV0AAAAJ"], "url_scholarbib": "/scholar?q=info:z_rYhqmhjwwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHarmonizing%2BMaximum%2BLikelihood%2BWith%2BGans%2BFor%2BMultimodal%2BConditional%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=z_rYhqmhjwwJ&ei=MmFeYoDXBsLZmQHc1ovQAg&json=", "num_citations": 20, "citedby_url": "/scholar?cites=905119799608670927&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:z_rYhqmhjwwJ:scholar.google.com/&scioq=Harmonizing+Maximum+Likelihood+With+Gans+For+Multimodal+Conditional+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09225"}, "Regularized Learning For Domain Adaptation Under Label Shifts": {"container_type": "Publication", "bib": {"title": "Regularized learning for domain adaptation under label shifts", "author": ["K Azizzadenesheli", "A Liu", "F Yang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain. We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples. We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.09734", "author_id": ["Odek140AAAAJ", "Q8yp6zQAAAAJ", "BfDKicQAAAAJ"], "url_scholarbib": "/scholar?q=info:G7-t2-6KxocJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegularized%2BLearning%2BFor%2BDomain%2BAdaptation%2BUnder%2BLabel%2BShifts%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G7-t2-6KxocJ&ei=NWFeYoveNMiBy9YP18Gi8As&json=", "num_citations": 99, "citedby_url": "/scholar?cites=9783659999001427739&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:G7-t2-6KxocJ:scholar.google.com/&scioq=Regularized+Learning+For+Domain+Adaptation+Under+Label+Shifts&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.09734"}, "A Generative Model For Electron Paths": {"container_type": "Publication", "bib": {"title": "A generative model for electron paths", "author": ["J Bradshaw", "MJ Kusner", "B Paige", "MHS Segler"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted usingarrow-pushing'diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret,(b) incorporating"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.10970", "author_id": ["", "57KRSu8AAAAJ", "JrFJmx0AAAAJ", "imsL94QAAAAJ"], "url_scholarbib": "/scholar?q=info:Mx5x7AOec-sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BGenerative%2BModel%2BFor%2BElectron%2BPaths%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Mx5x7AOec-sJ&ei=OGFeYturI4-bmAGmiqCIBw&json=", "num_citations": 34, "citedby_url": "/scholar?cites=16966077960923717171&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Mx5x7AOec-sJ:scholar.google.com/&scioq=A+Generative+Model+For+Electron+Paths&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.10970"}, "Neural Probabilistic Motor Primitives For Humanoid Control": {"container_type": "Publication", "bib": {"title": "Neural probabilistic motor primitives for humanoid control", "author": ["J Merel", "L Hasenclever", "A Galashov", "A Ahuja"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. To do this, we propose a motor architecture that has the general structure of an inverse model with a latent-variable bottleneck. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.11711", "author_id": ["K4OcFXUAAAAJ", "dD-3S4QAAAAJ", "kIpoNtcAAAAJ", "HFV9GmMAAAAJ"], "url_scholarbib": "/scholar?q=info:avdpR2OPC5sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BProbabilistic%2BMotor%2BPrimitives%2BFor%2BHumanoid%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=avdpR2OPC5sJ&ei=O2FeYvqgIY-bmAGmiqCIBw&json=", "num_citations": 69, "citedby_url": "/scholar?cites=11172180957185308522&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:avdpR2OPC5sJ:scholar.google.com/&scioq=Neural+Probabilistic+Motor+Primitives+For+Humanoid+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.11711"}, "Marginalized Average Attentional Network For Weakly-supervised Learning": {"container_type": "Publication", "bib": {"title": "Marginalized average attentional network for weakly-supervised learning", "author": ["Y Yuan", "Y Lyu", "X Shen", "IW Tsang", "DY Yeung"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.08586", "author_id": ["9tI89HMAAAAJ", "uQXB6-oAAAAJ", "nKSXus4AAAAJ", "rJMOlVsAAAAJ", "nEsOOx8AAAAJ"], "url_scholarbib": "/scholar?q=info:HOJ0Slyqzi4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMarginalized%2BAverage%2BAttentional%2BNetwork%2BFor%2BWeakly-supervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HOJ0Slyqzi4J&ei=PmFeYsmKOpHKsQKNt6-YAw&json=", "num_citations": 55, "citedby_url": "/scholar?cites=3372820484309967388&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HOJ0Slyqzi4J:scholar.google.com/&scioq=Marginalized+Average+Attentional+Network+For+Weakly-supervised+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.08586.pdf?ref=https://codemonkey.link"}, "The Laplacian In Rl: Learning Representations With Efficient Approximations": {"container_type": "Publication", "bib": {"title": "The laplacian in rl: Learning representations with efficient approximations", "author": ["Y Wu", "G Tucker", "O Nachum"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.04586", "abstract": "The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.04586", "author_id": ["vJQgYfsAAAAJ", "-gJkPHIAAAAJ", "C-ZlBWMAAAAJ"], "url_scholarbib": "/scholar?q=info:CEPPIGP1AVMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLaplacian%2BIn%2BRl:%2BLearning%2BRepresentations%2BWith%2BEfficient%2BApproximations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CEPPIGP1AVMJ&ei=QmFeYu6pGc6E6rQPz8uiuAc&json=", "num_citations": 27, "citedby_url": "/scholar?cites=5981331586225750792&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CEPPIGP1AVMJ:scholar.google.com/&scioq=The+Laplacian+In+Rl:+Learning+Representations+With+Efficient+Approximations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.04586"}, "Learning When To Communicate At Scale In Multiagent Cooperative And Competitive Tasks": {"container_type": "Publication", "bib": {"title": "Learning when to communicate at scale in multiagent cooperative and competitive tasks", "author": ["A Singh", "T Jain", "S Sukhbaatar"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.09755", "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09755", "author_id": ["aWiMKLsAAAAJ", "fQoKPwkAAAAJ", "ri1sE34AAAAJ"], "url_scholarbib": "/scholar?q=info:ZVYYwU6trKoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BWhen%2BTo%2BCommunicate%2BAt%2BScale%2BIn%2BMultiagent%2BCooperative%2BAnd%2BCompetitive%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZVYYwU6trKoJ&ei=RWFeYomYOsiBy9YP18Gi8As&json=", "num_citations": 100, "citedby_url": "/scholar?cites=12298395236200633957&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZVYYwU6trKoJ:scholar.google.com/&scioq=Learning+When+To+Communicate+At+Scale+In+Multiagent+Cooperative+And+Competitive+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09755"}, "Learning-based Frequency Estimation Algorithms": {"container_type": "Publication", "bib": {"title": "Learning-Based Frequency Estimation Algorithms.", "author": ["CY Hsu", "P Indyk", "D Katabi", "A Vakilian"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Estimating the frequencies of elements in a data stream is a fundamental task in data analysis and machine learning. The problem is typically addressed using streaming algorithms which can process very large data using limited storage. Today's streaming algorithms, however, cannot exploit patterns in their input to improve performance. We propose a new class of algorithms that automatically learn relevant patterns in the input data and use them to improve its frequency estimates. The proposed algorithms combine the"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/servlets/purl/10112257", "author_id": ["zDax7zYAAAAJ", "oOwNKsAAAAAJ", "nst5fHgAAAAJ", "uXZaVaAAAAAJ"], "url_scholarbib": "/scholar?q=info:-XfWFWylH0YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning-based%2BFrequency%2BEstimation%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-XfWFWylH0YJ&ei=SWFeYviyLM2Ny9YPqPyUgAs&json=", "num_citations": 79, "citedby_url": "/scholar?cites=5052939190574413817&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-XfWFWylH0YJ:scholar.google.com/&scioq=Learning-based+Frequency+Estimation+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10112257"}, "Clarinet: Parallel Wave Generation In End-to-end Text-to-speech": {"container_type": "Publication", "bib": {"title": "Clarinet: Parallel wave generation in end-to-end text-to-speech", "author": ["W Ping", "K Peng", "J Chen"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1807.07281", "abstract": "In this work, we propose a new solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (van den Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we introduce the first text-to-wave neural architecture for speech synthesis, which is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.07281", "author_id": ["6gKEYRgAAAAJ", "Fhg8dSwAAAAJ", "3cUlApYAAAAJ"], "url_scholarbib": "/scholar?q=info:o5p2jYOXQBcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DClarinet:%2BParallel%2BWave%2BGeneration%2BIn%2BEnd-to-end%2BText-to-speech%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o5p2jYOXQBcJ&ei=TWFeYqvGBIvMsQK69Y7ABg&json=", "num_citations": 260, "citedby_url": "/scholar?cites=1675505652651694755&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o5p2jYOXQBcJ:scholar.google.com/&scioq=Clarinet:+Parallel+Wave+Generation+In+End-to-end+Text-to-speech&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.07281"}, "Preferences Implicit In The State Of The World": {"container_type": "Publication", "bib": {"title": "Preferences implicit in the state of the world", "author": ["R Shah", "D Krasheninnikov", "J Alexander"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.04198", "author_id": ["odFQXSYAAAAJ", "BIQflKQAAAAJ", ""], "url_scholarbib": "/scholar?q=info:MuABtgXRDIYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPreferences%2BImplicit%2BIn%2BThe%2BState%2BOf%2BThe%2BWorld%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MuABtgXRDIYJ&ei=V2FeYqDRKY-bmAGmiqCIBw&json=", "num_citations": 37, "citedby_url": "/scholar?cites=9659325123261489202&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MuABtgXRDIYJ:scholar.google.com/&scioq=Preferences+Implicit+In+The+State+Of+The+World&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.04198"}, "Relaxed Quantization For Discretized Neural Networks": {"container_type": "Publication", "bib": {"title": "Relaxed quantization for discretized neural networks", "author": ["C Louizos", "M Reisser", "T Blankevoort", "E Gavves"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01875", "author_id": ["xrSUChoAAAAJ", "zOQWUZoAAAAJ", "OGEyrG8AAAAJ", "QqfCvsgAAAAJ"], "url_scholarbib": "/scholar?q=info:cBd8vV7agzMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelaxed%2BQuantization%2BFor%2BDiscretized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cBd8vV7agzMJ&ei=W2FeYuT8J9-Vy9YPs66ekAk&json=", "num_citations": 124, "citedby_url": "/scholar?cites=3712050618324227952&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cBd8vV7agzMJ:scholar.google.com/&scioq=Relaxed+Quantization+For+Discretized+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01875?ref=https://githubhelp.com"}, "Deep, Skinny Neural Networks Are Not Universal Approximators": {"container_type": "Publication", "bib": {"title": "Deep, skinny neural networks are not universal approximators", "author": ["J Johnson"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00393", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00393", "author_id": [""], "url_scholarbib": "/scholar?q=info:gc7T2XUJaawJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep,%2BSkinny%2BNeural%2BNetworks%2BAre%2BNot%2BUniversal%2BApproximators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gc7T2XUJaawJ&ei=YWFeYplVwtmZAdzWi9AC&json=", "num_citations": 38, "citedby_url": "/scholar?cites=12423471448848584321&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gc7T2XUJaawJ:scholar.google.com/&scioq=Deep,+Skinny+Neural+Networks+Are+Not+Universal+Approximators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00393"}, "Selfless Sequential Learning": {"container_type": "Publication", "bib": {"title": "Selfless sequential learning", "author": ["R Aljundi", "M Rohrbach", "T Tuytelaars"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.05421", "abstract": "Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.05421", "author_id": ["YLh7yrwAAAAJ", "3kDtybgAAAAJ", "EuFF9kUAAAAJ"], "url_scholarbib": "/scholar?q=info:c_OztyC-2p8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelfless%2BSequential%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c_OztyC-2p8J&ei=Y2FeYtfdNpGJmwGIxre4DA&json=", "num_citations": 74, "citedby_url": "/scholar?cites=11518728044683719539&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c_OztyC-2p8J:scholar.google.com/&scioq=Selfless+Sequential+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.05421"}, "Proxquant: Quantized Neural Networks Via Proximal Operators": {"container_type": "Publication", "bib": {"title": "Proxquant: Quantized neural networks via proximal operators", "author": ["Y Bai", "YX Wang", "E Liberty"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00861", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works. Building upon a novel observation that the straight-through gradient method is in fact identical to the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00861", "author_id": ["owqhKD8AAAAJ", "HGNZ1fkAAAAJ", "QHS_pZAAAAAJ"], "url_scholarbib": "/scholar?q=info:NUPBHwaXr74J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProxquant:%2BQuantized%2BNeural%2BNetworks%2BVia%2BProximal%2BOperators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NUPBHwaXr74J&ei=ZmFeYqPdMYvMsQK69Y7ABg&json=", "num_citations": 65, "citedby_url": "/scholar?cites=13740367040689029941&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NUPBHwaXr74J:scholar.google.com/&scioq=Proxquant:+Quantized+Neural+Networks+Via+Proximal+Operators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00861?ref=https://githubhelp.com"}, "A Mean Field Theory Of Batch Normalization": {"container_type": "Publication", "bib": {"title": "A mean field theory of batch normalization", "author": ["G Yang", "J Pennington", "V Rao", "J Sohl-Dickstein"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.08129", "author_id": ["Xz4RAJkAAAAJ", "cn_FoswAAAAJ", "uyQHjhYAAAAJ", "-3zYIjQAAAAJ"], "url_scholarbib": "/scholar?q=info:dSnK_UWUKfUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BMean%2BField%2BTheory%2BOf%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dSnK_UWUKfUJ&ei=aWFeYu_HMJHKsQKNt6-YAw&json=", "num_citations": 124, "citedby_url": "/scholar?cites=17665814041669020021&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dSnK_UWUKfUJ:scholar.google.com/&scioq=A+Mean+Field+Theory+Of+Batch+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.08129"}, "Predicting The Generalization Gap In Deep Networks With Margin Distributions": {"container_type": "Publication", "bib": {"title": "Predicting the generalization gap in deep networks with margin distributions", "author": ["Y Jiang", "D Krishnan", "H Mobahi", "S Bengio"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00113", "author_id": ["x9qzWg8AAAAJ", "_MEuWIMAAAAJ", "GSHmKZkAAAAJ", "Vs-MdPcAAAAJ"], "url_scholarbib": "/scholar?q=info:Zz4X2FtYM70J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPredicting%2BThe%2BGeneralization%2BGap%2BIn%2BDeep%2BNetworks%2BWith%2BMargin%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Zz4X2FtYM70J&ei=cmFeYpOMK46pywSdh6agAg&json=", "num_citations": 120, "citedby_url": "/scholar?cites=13633337648471293543&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Zz4X2FtYM70J:scholar.google.com/&scioq=Predicting+The+Generalization+Gap+In+Deep+Networks+With+Margin+Distributions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00113"}, "A2bcd: Asynchronous Acceleration With Optimal Complexity": {"container_type": "Publication", "bib": {"title": "A2BCD: Asynchronous acceleration with optimal complexity", "author": ["R Hannah", "F Feng", "W Yin"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "In this paper, we propose the Asynchronous Accelerated Nonuniform Randomized Block Coordinate Descent algorithm (A2BCD). We prove A2BCD converges linearly to a solution of the convex minimization problem at the same rate as NU_ACDM, so long as the maximum delay is not too large. This is the first asynchronous Nesterov-accelerated algorithm that attains any provable speedup. Moreover, we then prove that these algorithms both have optimal complexity. Asynchronous algorithms complete much faster iterations, and A2BCD"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rylIAsCqYm", "author_id": ["w0srO9sAAAAJ", "2HJu9wgAAAAJ", "kpQGGFUAAAAJ"], "url_scholarbib": "/scholar?q=info:F0tlAnjOFL8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA2bcd:%2BAsynchronous%2BAcceleration%2BWith%2BOptimal%2BComplexity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F0tlAnjOFL8J&ei=dmFeYuurGMiBy9YP18Gi8As&json=", "num_citations": 12, "citedby_url": "/scholar?cites=13768856975610039063&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F0tlAnjOFL8J:scholar.google.com/&scioq=A2bcd:+Asynchronous+Acceleration+With+Optimal+Complexity&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rylIAsCqYm"}, "Solving The Rubik's Cube With Approximate Policy Iteration": {"container_type": "Publication", "bib": {"title": "Solving the rubik's cube with approximate policy iteration", "author": ["S McAleer", "F Agostinelli", "A Shmakov"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data. These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network). In these two-player games, a reward is always received at the end of the game. However, the Rubik's Cube has only a single solved state, and episodes are not guaranteed to terminate. This poses a major problem for these API algorithms since"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hyfn2jCcKm", "author_id": ["iEFL4-YAAAAJ", "R3ru5X8AAAAJ", "tVPWAKIAAAAJ"], "url_scholarbib": "/scholar?q=info:OuT8gkq549EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSolving%2BThe%2BRubik%2527s%2BCube%2BWith%2BApproximate%2BPolicy%2BIteration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OuT8gkq549EJ&ei=emFeYt2BHMLZmQHc1ovQAg&json=", "num_citations": 27, "citedby_url": "/scholar?cites=15124135703316587578&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OuT8gkq549EJ:scholar.google.com/&scioq=Solving+The+Rubik%27s+Cube+With+Approximate+Policy+Iteration&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hyfn2jCcKm"}, "Learning To Describe Scenes With Programs": {"container_type": "Publication", "bib": {"title": "Learning to describe scenes with programs", "author": ["Y Liu", "Z Wu"], "pub_year": "2019", "venue": "International conference on learning representations", "abstract": "People spend a large percentage of their lives indoors\u2014in bedrooms, living rooms, kitchens, etc. As computer graphics reproduces the real world in increasing fidelity, the demand for virtual versions of such spaces also grows. Virtual and augmented reality experiences often take place in such environments. Online virtual interior design tools are available to help people redesign their own spaces [Planner5d 2017; RoomSketcher 2017]. Some furniture design companies now primarily advertise their products by rendering virtual scenes, as it is"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/servlets/purl/10155955", "author_id": ["Fkpf8FsAAAAJ", "Lx_cK2YAAAAJ"], "url_scholarbib": "/scholar?q=info:vZ3F-Ntig-0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BDescribe%2BScenes%2BWith%2BPrograms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vZ3F-Ntig-0J&ei=fmFeYpOeJs6E6rQPz8uiuAc&json=", "num_citations": 36, "citedby_url": "/scholar?cites=17114631705849077181&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vZ3F-Ntig-0J:scholar.google.com/&scioq=Learning+To+Describe+Scenes+With+Programs&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10155955"}, "Emergent Coordination Through Competition": {"container_type": "Publication", "bib": {"title": "Emergent coordination through competition", "author": ["S Liu", "G Lever", "J Merel", "S Tunyasuvunakool"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.07151", "author_id": ["7U_OA0oAAAAJ", "1XgR518AAAAJ", "K4OcFXUAAAAJ", "9dAjSlYAAAAJ"], "url_scholarbib": "/scholar?q=info:7Us6isMaY30J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BCoordination%2BThrough%2BCompetition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7Us6isMaY30J&ei=gmFeYt-bBMLZmQHc1ovQAg&json=", "num_citations": 98, "citedby_url": "/scholar?cites=9035094704575368173&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7Us6isMaY30J:scholar.google.com/&scioq=Emergent+Coordination+Through+Competition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.07151"}, "Ffjord: Free-form Continuous Dynamics For Scalable Reversible Generative Models": {"container_type": "Publication", "bib": {"title": "Ffjord: Free-form continuous dynamics for scalable reversible generative models", "author": ["W Grathwohl", "RTQ Chen", "J Bettencourt"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01367", "author_id": ["ZbClz98AAAAJ", "7MxQd6UAAAAJ", "sWkxswQAAAAJ"], "url_scholarbib": "/scholar?q=info:GRrnHiupUbIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFfjord:%2BFree-form%2BContinuous%2BDynamics%2BFor%2BScalable%2BReversible%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GRrnHiupUbIJ&ei=hmFeYoizLt-Vy9YPs66ekAk&json=", "num_citations": 430, "citedby_url": "/scholar?cites=12849237214531885593&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GRrnHiupUbIJ:scholar.google.com/&scioq=Ffjord:+Free-form+Continuous+Dynamics+For+Scalable+Reversible+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01367"}, "Emerging Disentanglement In Auto-encoder Based Unsupervised Image Content Transfer": {"container_type": "Publication", "bib": {"title": "Emerging disentanglement in auto-encoder based unsupervised image content transfer", "author": ["O Press", "T Galanti", "S Benaim", "L Wolf"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.05017", "abstract": "We study the problem of learning to map, in an unsupervised way, between domains A and B, such that the samples b in B contain all the information that exists in samples a in A and some additional information. For example, ignoring occlusions, B can be people with glasses, A people without, and the glasses, would be the added information. When mapping a sample a from the first domain to the other domain, the missing information is replicated from an independent reference sample b in B. Thus, in the above example, we can create"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.05017", "author_id": ["vDNPTAkAAAAJ", "ut_ISVIAAAAJ", "-zSM2I8AAAAJ", "UbFrXTsAAAAJ"], "url_scholarbib": "/scholar?q=info:JkI2Q4VRsA8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmerging%2BDisentanglement%2BIn%2BAuto-encoder%2BBased%2BUnsupervised%2BImage%2BContent%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JkI2Q4VRsA8J&ei=iWFeYpH-L8LZmQHc1ovQAg&json=", "num_citations": 28, "citedby_url": "/scholar?cites=1130493139270124070&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JkI2Q4VRsA8J:scholar.google.com/&scioq=Emerging+Disentanglement+In+Auto-encoder+Based+Unsupervised+Image+Content+Transfer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.05017"}, "Multi-class Classification Without Multi-class Labels": {"container_type": "Publication", "bib": {"title": "Multi-class classification without multi-class labels", "author": ["YC Hsu", "Z Lv", "J Schlosser", "P Odom", "Z Kira"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.00544", "author_id": ["7QWAiigAAAAJ", "fSb94nAAAAAJ", "5DMIxyUAAAAJ", "b8sBkqUAAAAJ", "2a5XgNAAAAAJ"], "url_scholarbib": "/scholar?q=info:X35IUNS0U9kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-class%2BClassification%2BWithout%2BMulti-class%2BLabels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=X35IUNS0U9kJ&ei=jWFeYteVGcLZmQHc1ovQAg&json=", "num_citations": 59, "citedby_url": "/scholar?cites=15660059153270341215&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:X35IUNS0U9kJ:scholar.google.com/&scioq=Multi-class+Classification+Without+Multi-class+Labels&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.00544"}, "Slalom: Fast, Verifiable And Private Execution Of Neural Networks In Trusted Hardware": {"container_type": "Publication", "bib": {"title": "Slalom: Fast, verifiable and private execution of neural networks in trusted hardware", "author": ["F Tramer", "D Boneh"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.03287", "abstract": "As Machine Learning (ML) gets applied to security-critical or sensitive domains, there is a growing need for integrity and privacy for outsourced ML computations. A pragmatic solution comes from Trusted Execution Environments (TEEs), which use hardware and software protections to isolate sensitive computations from the untrusted software stack. However, these isolation guarantees come at a price in performance, compared to untrusted alternatives. This paper initiates the study of high performance execution of Deep Neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.03287", "author_id": ["ijH0-a8AAAAJ", "MwLqCs4AAAAJ"], "url_scholarbib": "/scholar?q=info:3oBp2UuvjGcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSlalom:%2BFast,%2BVerifiable%2BAnd%2BPrivate%2BExecution%2BOf%2BNeural%2BNetworks%2BIn%2BTrusted%2BHardware%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3oBp2UuvjGcJ&ei=kWFeYqO4Is2Ny9YPqPyUgAs&json=", "num_citations": 204, "citedby_url": "/scholar?cites=7461531422951047390&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3oBp2UuvjGcJ:scholar.google.com/&scioq=Slalom:+Fast,+Verifiable+And+Private+Execution+Of+Neural+Networks+In+Trusted+Hardware&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.03287"}, "Characterizing Audio Adversarial Examples Using Temporal Dependency": {"container_type": "Publication", "bib": {"title": "Characterizing audio adversarial examples using temporal dependency", "author": ["Z Yang", "B Li", "PY Chen", "D Song"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.10875", "abstract": "Recent studies have highlighted adversarial examples as a ubiquitous threat to different neural network models and many downstream applications. Nonetheless, as unique data properties have inspired distinct and powerful learning principles, this paper aims to explore their potentials towards mitigating adversarial inputs. In particular, our results reveal the importance of using the temporal dependency in audio data to gain discriminate power against adversarial examples. Tested on the automatic speech recognition (ASR) tasks and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10875", "author_id": ["BvSv-C0AAAAJ", "K8vJkTcAAAAJ", "jxwlCUUAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:tjid5vK44bYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCharacterizing%2BAudio%2BAdversarial%2BExamples%2BUsing%2BTemporal%2BDependency%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tjid5vK44bYJ&ei=nmFeYuLREZHKsQKNt6-YAw&json=", "num_citations": 91, "citedby_url": "/scholar?cites=13178017338053441718&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tjid5vK44bYJ:scholar.google.com/&scioq=Characterizing+Audio+Adversarial+Examples+Using+Temporal+Dependency&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10875"}, "Marginal Policy Gradients: A Unified Family Of Estimators For Bounded Action Spaces With Applications": {"container_type": "Publication", "bib": {"title": "Marginal policy gradients: A unified family of estimators for bounded action spaces with applications", "author": ["C Eisenach", "H Yang", "J Liu", "H Liu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.05134", "abstract": "Many complex domains, such as robotics control and real-time strategy (RTS) games, require an agent to learn a continuous control. In the former, an agent learns a policy over $\\mathbb {R}^ d $ and in the latter, over a discrete set of actions each of which is parametrized by a continuous parameter. Such problems are naturally solved using policy based reinforcement learning (RL) methods, but unfortunately these often suffer from high variance leading to instability and slow convergence. Unnecessary variance is introduced"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.05134", "author_id": ["Jx269u0AAAAJ", "4KNoCFIAAAAJ", "RRzVwKkAAAAJ", "XaFT1o4AAAAJ"], "url_scholarbib": "/scholar?q=info:15sOlrg7vs0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMarginal%2BPolicy%2BGradients:%2BA%2BUnified%2BFamily%2BOf%2BEstimators%2BFor%2BBounded%2BAction%2BSpaces%2BWith%2BApplications%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=15sOlrg7vs0J&ei=oWFeYovjFI6pywSdh6agAg&json=", "num_citations": 9, "citedby_url": "/scholar?cites=14825352687327812567&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:15sOlrg7vs0J:scholar.google.com/&scioq=Marginal+Policy+Gradients:+A+Unified+Family+Of+Estimators+For+Bounded+Action+Spaces+With+Applications&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.05134"}, "Probabilistic Planning With Sequential Monte Carlo Methods": {"container_type": "Publication", "bib": {"title": "Probabilistic planning with sequential monte carlo methods", "author": ["A Pich\u00e9", "V Thomas", "C Ibrahim", "Y Bengio"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "In this work, we propose a novel formulation of planning which views it as a probabilistic inference problem over future optimal trajectories. This enables us to use sampling methods, and thus, tackle planning in continuous domains using a fixed computational budget. We design a new algorithm, Sequential Monte Carlo Planning, by leveraging classical methods in Sequential Monte Carlo and Bayesian smoothing in the context of control as inference. Furthermore, we show that Sequential Monte Carlo Planning can capture multimodal"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByetGn0cYX", "author_id": ["RJos_EEAAAAJ", "XRhKEGMAAAAJ", "ORU2iekAAAAJ", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:m_4LgySS0RkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbabilistic%2BPlanning%2BWith%2BSequential%2BMonte%2BCarlo%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m_4LgySS0RkJ&ei=o2FeYsO9NsiBy9YP18Gi8As&json=", "num_citations": 30, "citedby_url": "/scholar?cites=1860428806595804827&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m_4LgySS0RkJ:scholar.google.com/&scioq=Probabilistic+Planning+With+Sequential+Monte+Carlo+Methods&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByetGn0cYX"}, "Learning What You Can Do Before Doing Anything": {"container_type": "Publication", "bib": {"title": "Learning what you can do before doing anything", "author": ["O Rybkin", "K Pertsch", "KG Derpanis", "K Daniilidis"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent's action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. We introduce a loss"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.09655", "author_id": ["CQEyVPMAAAAJ", "3oe0I0QAAAAJ", "3Br8x_gAAAAJ", "dGs2BcIAAAAJ"], "url_scholarbib": "/scholar?q=info:18_X9bKmBWEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BWhat%2BYou%2BCan%2BDo%2BBefore%2BDoing%2BAnything%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=18_X9bKmBWEJ&ei=p2FeYuWGNIryyASen4NI&json=", "num_citations": 10, "citedby_url": "/scholar?cites=6991177284121513943&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:18_X9bKmBWEJ:scholar.google.com/&scioq=Learning+What+You+Can+Do+Before+Doing+Anything&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.09655"}, "A New Dog Learns Old Tricks: Rl Finds Classic Optimization Algorithms": {"container_type": "Publication", "bib": {"title": "A new dog learns old tricks: RL finds classic optimization algorithms", "author": ["W Kong", "C Liaw", "A Mehta"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "This paper introduces a novel framework for learning algorithms to solve online combinatorial optimization problems. Towards this goal, we introduce a number of key ideas from traditional algorithms and complexity theory. First, we draw a new connection between primal-dual methods and reinforcement learning. Next, we introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst case. We test our"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkluJ2R9KQ", "author_id": ["2YtN42MAAAAJ", "05WRGRsAAAAJ", "S39CcbQAAAAJ"], "url_scholarbib": "/scholar?q=info:L1Z4l9s3RaoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BNew%2BDog%2BLearns%2BOld%2BTricks:%2BRl%2BFinds%2BClassic%2BOptimization%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=L1Z4l9s3RaoJ&ei=q2FeYpDkK4vMsQK69Y7ABg&json=", "num_citations": 25, "citedby_url": "/scholar?cites=12269274176119395887&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:L1Z4l9s3RaoJ:scholar.google.com/&scioq=A+New+Dog+Learns+Old+Tricks:+Rl+Finds+Classic+Optimization+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkluJ2R9KQ"}, "Measuring And Regularizing Networks In Function Space": {"container_type": "Publication", "bib": {"title": "Measuring and regularizing networks in function space", "author": ["AS Benjamin", "D Rolnick", "K Kording"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.08289", "abstract": "To optimize a neural network one often thinks of optimizing its parameters, but it is ultimately a matter of optimizing the function that maps inputs to outputs. Since a change in the parameters might serve as a poor proxy for the change in the function, it is of some concern that primacy is given to parameters but that the correspondence has not been tested. Here, we show that it is simple and computationally feasible to calculate distances between functions in a $ L^ 2$ Hilbert space. We examine how typical networks behave in this space"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.08289", "author_id": ["GW6D4ZIAAAAJ", "P_luG3cAAAAJ", "MiFqJGcAAAAJ"], "url_scholarbib": "/scholar?q=info:PhwUAzLhNdUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2BAnd%2BRegularizing%2BNetworks%2BIn%2BFunction%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PhwUAzLhNdUJ&ei=rmFeYseyNciBy9YP18Gi8As&json=", "num_citations": 39, "citedby_url": "/scholar?cites=15363433308760579134&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PhwUAzLhNdUJ:scholar.google.com/&scioq=Measuring+And+Regularizing+Networks+In+Function+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.08289"}, "Supervised Community Detection With Line Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Supervised community detection with line graph neural networks", "author": ["Z Chen", "X Li", "J Bruna"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.08415", "abstract": "Traditionally, community detection in graphs can be solved using spectral methods or posterior inference under probabilistic graphical models. Focusing on random graph families such as the stochastic block model, recent research has unified both approaches and identified both statistical and computational detection thresholds in terms of the signal-to-noise ratio. By recasting community detection as a node-wise classification problem on graphs, we can also study it from a learning perspective. We present a novel family of Graph"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.08415", "author_id": ["sNCfPikAAAAJ", "UR7uLq8AAAAJ", "L4bNmsMAAAAJ"], "url_scholarbib": "/scholar?q=info:FSnFe8K7gEUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSupervised%2BCommunity%2BDetection%2BWith%2BLine%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FSnFe8K7gEUJ&ei=smFeYpbGKI-bmAGmiqCIBw&json=", "num_citations": 162, "citedby_url": "/scholar?cites=5008209229610559765&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FSnFe8K7gEUJ:scholar.google.com/&scioq=Supervised+Community+Detection+With+Line+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.08415"}, "Learning To Navigate The Web": {"container_type": "Publication", "bib": {"title": "Learning to navigate the web", "author": ["I Gur", "U Rueckert", "A Faust", "D Hakkani-Tur"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09195", "author_id": ["qS_ugJAAAAAJ", "", "RK72t68AAAAJ", "GMcL_9kAAAAJ"], "url_scholarbib": "/scholar?q=info:ULRSrhF_ZmQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BNavigate%2BThe%2BWeb%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ULRSrhF_ZmQJ&ei=tmFeYvG4IM2Ny9YPqPyUgAs&json=", "num_citations": 20, "citedby_url": "/scholar?cites=7234609565333107792&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ULRSrhF_ZmQJ:scholar.google.com/&scioq=Learning+To+Navigate+The+Web&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09195"}, "Functional Variational Bayesian Neural Networks": {"container_type": "Publication", "bib": {"title": "Functional variational bayesian neural networks", "author": ["S Sun", "G Zhang", "J Shi", "R Grosse"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.05779", "abstract": "Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, ie distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.05779", "author_id": ["", "B_TZBtwAAAAJ", "juZXbFoAAAAJ", "xgQd1qgAAAAJ"], "url_scholarbib": "/scholar?q=info:QY7ZoQvpc50J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFunctional%2BVariational%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QY7ZoQvpc50J&ei=umFeYum_LYvMsQK69Y7ABg&json=", "num_citations": 148, "citedby_url": "/scholar?cites=11345668122445712961&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QY7ZoQvpc50J:scholar.google.com/&scioq=Functional+Variational+Bayesian+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.05779"}, "Residual Non-local Attention Networks For Image Restoration": {"container_type": "Publication", "bib": {"title": "Residual non-local attention networks for image restoration", "author": ["Y Zhang", "K Li", "K Li", "B Zhong", "Y Fu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.10082", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial-and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.10082", "author_id": ["ORmLjWoAAAAJ", "0TOLw9YAAAAJ", "YsROc4UAAAAJ", "hvRBydsAAAAJ", "h-JEcQ8AAAAJ"], "url_scholarbib": "/scholar?q=info:D21qWb7TSksJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DResidual%2BNon-local%2BAttention%2BNetworks%2BFor%2BImage%2BRestoration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D21qWb7TSksJ&ei=vmFeYonGAZHKsQKNt6-YAw&json=", "num_citations": 341, "citedby_url": "/scholar?cites=5425381515618577679&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D21qWb7TSksJ:scholar.google.com/&scioq=Residual+Non-local+Attention+Networks+For+Image+Restoration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.10082.pdf?ref=https://codemonkey.link"}, "Anytime Minibatch: Exploiting Stragglers In Online Distributed Optimization": {"container_type": "Publication", "bib": {"title": "Anytime minibatch: Exploiting stragglers in online distributed optimization", "author": ["N Ferdinand", "H Al-Lawati", "SC Draper"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2006.05752", "author_id": ["YrvS16kAAAAJ", "H5B2vcYAAAAJ", "xbXtit8AAAAJ"], "url_scholarbib": "/scholar?q=info:2V9914ykkhUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnytime%2BMinibatch:%2BExploiting%2BStragglers%2BIn%2BOnline%2BDistributed%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2V9914ykkhUJ&ei=wWFeYvKQKI-bmAGmiqCIBw&json=", "num_citations": 24, "citedby_url": "/scholar?cites=1554485746213937113&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2V9914ykkhUJ:scholar.google.com/&scioq=Anytime+Minibatch:+Exploiting+Stragglers+In+Online+Distributed+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2006.05752"}, "Deep Layers As Stochastic Solvers": {"container_type": "Publication", "bib": {"title": "Deep layers as stochastic solvers", "author": ["A Bibi", "B Ghanem", "V Koltun", "R Ranftl"], "pub_year": "2019", "venue": "NA", "abstract": "We provide a novel perspective on the forward pass through a block of layers in a deep network. In particular, we show that a forward pass through a standard dropout layer"}, "filled": false, "gsrank": 1, "pub_url": "https://repository.kaust.edu.sa/handle/10754/662270", "author_id": ["Q4j2laYAAAAJ", "rVsGTeEAAAAJ", "kg4bCpgAAAAJ", "cwKg158AAAAJ"], "url_scholarbib": "/scholar?q=info:JdGLfm1kAMgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLayers%2BAs%2BStochastic%2BSolvers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JdGLfm1kAMgJ&ei=xGFeYoTKG9-Vy9YPs66ekAk&json=", "num_citations": 19, "citedby_url": "/scholar?cites=14411629229022892325&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JdGLfm1kAMgJ:scholar.google.com/&scioq=Deep+Layers+As+Stochastic+Solvers&hl=en&as_sdt=0,33", "eprint_url": "https://repository.kaust.edu.sa/bitstream/handle/10754/662270/deep_layers_as_stochastic_solvers.pdf?sequence=1&isAllowed=y"}, "Efficient Training On Very Large Corpora Via Gramian Estimation": {"container_type": "Publication", "bib": {"title": "Efficient training on very large corpora via gramian estimation", "author": ["W Krichene", "N Mayoraz", "S Rendle", "L Zhang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the problem of learning similarity functions over very large corpora using neural network embedding models. These models are typically trained using SGD with sampling of random observed and unobserved pairs, with a number of samples that grows quadratically with the corpus size, making it expensive to scale to very large corpora. We propose new efficient methods to train these models without having to sample unobserved pairs. Inspired by matrix factorization, our approach relies on adding a global quadratic penalty to all pairs"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.07187", "author_id": ["", "", "yR-ugIoAAAAJ", "wFEJvJUAAAAJ"], "url_scholarbib": "/scholar?q=info:B3sl5XPmvs8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BTraining%2BOn%2BVery%2BLarge%2BCorpora%2BVia%2BGramian%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=B3sl5XPmvs8J&ei=x2FeYqCjO4yuyASD3KfABw&json=", "num_citations": 23, "citedby_url": "/scholar?cites=14969655596866173703&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:B3sl5XPmvs8J:scholar.google.com/&scioq=Efficient+Training+On+Very+Large+Corpora+Via+Gramian+Estimation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.07187"}, "Revealing Interpretable Object Representations From Human Behavior": {"container_type": "Publication", "bib": {"title": "Revealing interpretable object representations from human behavior", "author": ["CY Zheng", "F Pereira", "CI Baker", "MN Hebart"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories. These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments. Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.02915", "author_id": ["tRKUk_YAAAAJ", "HpbSzssAAAAJ", "RTtshYYAAAAJ", "Q-n9_FgAAAAJ"], "url_scholarbib": "/scholar?q=info:aIfn7tj5oukJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRevealing%2BInterpretable%2BObject%2BRepresentations%2BFrom%2BHuman%2BBehavior%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aIfn7tj5oukJ&ei=y2FeYrKLDY-bmAGmiqCIBw&json=", "num_citations": 15, "citedby_url": "/scholar?cites=16835293067180738408&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aIfn7tj5oukJ:scholar.google.com/&scioq=Revealing+Interpretable+Object+Representations+From+Human+Behavior&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.02915"}, "Deterministic Pac-bayesian Generalization Bounds For Deep Networks Via Generalizing Noise-resilience": {"container_type": "Publication", "bib": {"title": "Deterministic PAC-bayesian generalization bounds for deep networks via generalizing noise-resilience", "author": ["V Nagarajan", "JZ Kolter"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.13344", "abstract": "The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss--minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either\\textit {stochastic} or\\textit {compressed}. In this work, we present a general PAC-Bayesian framework that leverages"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.13344", "author_id": ["", "UXh1I6UAAAAJ"], "url_scholarbib": "/scholar?q=info:BhuJL8sOaSEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeterministic%2BPac-bayesian%2BGeneralization%2BBounds%2BFor%2BDeep%2BNetworks%2BVia%2BGeneralizing%2BNoise-resilience%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BhuJL8sOaSEJ&ei=0GFeYt_YPMiBy9YP18Gi8As&json=", "num_citations": 61, "citedby_url": "/scholar?cites=2407471741644905222&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BhuJL8sOaSEJ:scholar.google.com/&scioq=Deterministic+Pac-bayesian+Generalization+Bounds+For+Deep+Networks+Via+Generalizing+Noise-resilience&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.13344"}, "Ordered Neurons: Integrating Tree Structures Into Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Ordered neurons: Integrating tree structures into recurrent neural networks", "author": ["Y Shen", "S Tan", "A Sordoni", "A Courville"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.09536", "abstract": "Natural language is hierarchically structured: smaller units (eg, phrases) are nested within larger units (eg, clauses). When a larger constituent ends, all of the smaller constituents that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.09536", "author_id": ["qff5rRYAAAAJ", "57Nf7EYAAAAJ", "DJon7w4AAAAJ", "km6CP8cAAAAJ"], "url_scholarbib": "/scholar?q=info:3jq6t2yp-PkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOrdered%2BNeurons:%2BIntegrating%2BTree%2BStructures%2BInto%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3jq6t2yp-PkJ&ei=1WFeYu6lOoryyASen4NI&json=", "num_citations": 246, "citedby_url": "/scholar?cites=18012332994072296158&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3jq6t2yp-PkJ:scholar.google.com/&scioq=Ordered+Neurons:+Integrating+Tree+Structures+Into+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.09536.pdf?ref=https://githubhelp.com"}, "Layoutgan: Generating Graphic Layouts With Wireframe Discriminators": {"container_type": "Publication", "bib": {"title": "Layoutgan: Generating graphic layouts with wireframe discriminators", "author": ["J Li", "J Yang", "A Hertzmann", "J Zhang", "T Xu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.06767", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.06767", "author_id": ["sQ_nP0ZaMn0C", "GwKF9rMAAAAJ", "ZcWO2AEAAAAJ", "TkVHKDgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:GY9FzRZfGwEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLayoutgan:%2BGenerating%2BGraphic%2BLayouts%2BWith%2BWireframe%2BDiscriminators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GY9FzRZfGwEJ&ei=2WFeYuH1FZWMy9YPt8OamA0&json=", "num_citations": 96, "citedby_url": "/scholar?cites=79761969946922777&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GY9FzRZfGwEJ:scholar.google.com/&scioq=Layoutgan:+Generating+Graphic+Layouts+With+Wireframe+Discriminators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.06767"}, "Reasoning About Physical Interactions With Object-oriented Prediction And Planning": {"container_type": "Publication", "bib": {"title": "Reasoning about physical interactions with object-oriented prediction and planning", "author": ["M Janner", "S Levine", "WT Freeman"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.10972", "author_id": ["zvz6LIYAAAAJ", "8R35rCwAAAAJ", "0zZnyMEAAAAJ"], "url_scholarbib": "/scholar?q=info:EK1HdG_yLNYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReasoning%2BAbout%2BPhysical%2BInteractions%2BWith%2BObject-oriented%2BPrediction%2BAnd%2BPlanning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EK1HdG_yLNYJ&ei=3GFeYvi-Lo-bmAGmiqCIBw&json=", "num_citations": 82, "citedby_url": "/scholar?cites=15432976583597993232&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EK1HdG_yLNYJ:scholar.google.com/&scioq=Reasoning+About+Physical+Interactions+With+Object-oriented+Prediction+And+Planning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.10972.pdf?ref=https://githubhelp.com"}, "Learning To Adapt In Dynamic, Real-world Environments Through Meta-reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning to adapt in dynamic, real-world environments through meta-reinforcement learning", "author": ["A Nagabandi", "I Clavera", "S Liu", "RS Fearing"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.11347", "author_id": ["DkUUhXEAAAAJ", "yABlzrsAAAAJ", "", "uA0kNBUAAAAJ"], "url_scholarbib": "/scholar?q=info:UU0vznf_JtwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BAdapt%2BIn%2BDynamic,%2BReal-world%2BEnvironments%2BThrough%2BMeta-reinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UU0vznf_JtwJ&ei=4GFeYp2ZAsLZmQHc1ovQAg&json=", "num_citations": 283, "citedby_url": "/scholar?cites=15863647627484548433&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UU0vznf_JtwJ:scholar.google.com/&scioq=Learning+To+Adapt+In+Dynamic,+Real-world+Environments+Through+Meta-reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.11347"}, "The Neuro-symbolic Concept Learner: Interpreting Scenes, Words, And Sentences From Natural Supervision": {"container_type": "Publication", "bib": {"title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision", "author": ["J Mao", "C Gan", "P Kohli", "JB Tenenbaum", "J Wu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.12584", "author_id": ["-xaOIZIAAAAJ", "PTeSCbIAAAAJ", "3pyzQQ8AAAAJ", "rRJ9wTJMUB8C", "2efgcS0AAAAJ"], "url_scholarbib": "/scholar?q=info:xzL-RErJo3oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BNeuro-symbolic%2BConcept%2BLearner:%2BInterpreting%2BScenes,%2BWords,%2BAnd%2BSentences%2BFrom%2BNatural%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xzL-RErJo3oJ&ei=42FeYpvcE82Ny9YPqPyUgAs&json=", "num_citations": 352, "citedby_url": "/scholar?cites=8837128214653317831&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xzL-RErJo3oJ:scholar.google.com/&scioq=The+Neuro-symbolic+Concept+Learner:+Interpreting+Scenes,+Words,+And+Sentences+From+Natural+Supervision&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.12584"}, "Mode Normalization": {"container_type": "Publication", "bib": {"title": "Mode normalization", "author": ["L Deecke", "I Murray", "H Bilen"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.05466", "abstract": "Normalization methods are a central building block in the deep learning toolbox. They accelerate and stabilize training, while decreasing the dependence on manually tuned"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05466", "author_id": ["6-x0_AsAAAAJ", "4BEvaw8AAAAJ", "PtBtfawAAAAJ"], "url_scholarbib": "/scholar?q=info:lUw9xqjxe5IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMode%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lUw9xqjxe5IJ&ei=5mFeYt6XCZWMy9YPt8OamA0&json=", "num_citations": 24, "citedby_url": "/scholar?cites=10555295858855595157&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lUw9xqjxe5IJ:scholar.google.com/&scioq=Mode+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05466"}, "Deep Reinforcement Learning With Relational Inductive Biases": {"container_type": "Publication", "bib": {"title": "Deep reinforcement learning with relational inductive biases", "author": ["V Zambaldi", "D Raposo", "A Santoro", "V Bapst"], "pub_year": "2018", "venue": "International \u2026", "abstract": "We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkxaFoC9KQ", "author_id": ["Iyc-xXkAAAAJ", "4iINIzkAAAAJ", "evIkDWoAAAAJ", "95mnc80AAAAJ"], "url_scholarbib": "/scholar?q=info:jgdLzuXiZuUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BReinforcement%2BLearning%2BWith%2BRelational%2BInductive%2BBiases%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jgdLzuXiZuUJ&ei=6WFeYrnLFYryyASen4NI&json=", "num_citations": 126, "citedby_url": "/scholar?cites=16530148958946396046&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jgdLzuXiZuUJ:scholar.google.com/&scioq=Deep+Reinforcement+Learning+With+Relational+Inductive+Biases&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkxaFoC9KQ"}, "Janossy Pooling: Learning Deep Permutation-invariant Functions For Variable-size Inputs": {"container_type": "Publication", "bib": {"title": "Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs", "author": ["RL Murphy", "B Srinivasan", "V Rao", "B Ribeiro"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or multiset functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.01900", "author_id": ["cmAFPxYAAAAJ", "uM4EhgEAAAAJ", "IQibv4UAAAAJ", "KIEleCsAAAAJ"], "url_scholarbib": "/scholar?q=info:FEg3b0G6lXwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DJanossy%2BPooling:%2BLearning%2BDeep%2BPermutation-invariant%2BFunctions%2BFor%2BVariable-size%2BInputs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FEg3b0G6lXwJ&ei=62FeYuXdOoyuyASD3KfABw&json=", "num_citations": 107, "citedby_url": "/scholar?cites=8977286222434486292&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FEg3b0G6lXwJ:scholar.google.com/&scioq=Janossy+Pooling:+Learning+Deep+Permutation-invariant+Functions+For+Variable-size+Inputs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.01900"}, "Enabling Factorized Piano Music Modeling And Generation With The Maestro Dataset": {"container_type": "Publication", "bib": {"title": "Enabling factorized piano music modeling and generation with the MAESTRO dataset", "author": ["C Hawthorne", "A Stasyuk", "A Roberts", "I Simon"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.12247", "author_id": ["9ziPoxAAAAAJ", "", "U5UpKq8AAAAJ", "pKqwl3wAAAAJ"], "url_scholarbib": "/scholar?q=info:QoPGSAW8er0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnabling%2BFactorized%2BPiano%2BMusic%2BModeling%2BAnd%2BGeneration%2BWith%2BThe%2BMaestro%2BDataset%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QoPGSAW8er0J&ei=72FeYqGHLJGJmwGIxre4DA&json=", "num_citations": 223, "citedby_url": "/scholar?cites=13653431951208907586&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QoPGSAW8er0J:scholar.google.com/&scioq=Enabling+Factorized+Piano+Music+Modeling+And+Generation+With+The+Maestro+Dataset&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.12247.pdf?ref=https://githubhelp.com"}, "Opportunistic Learning: Budgeted Cost-sensitive Learning From Data Streams": {"container_type": "Publication", "bib": {"title": "Opportunistic learning: Budgeted cost-sensitive learning from data streams", "author": ["M Kachuee", "O Goldstein", "K Karkkainen"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In many real-world learning scenarios, features are only acquirable at a cost constrained under a budget. In this paper, we propose a novel approach for cost-sensitive feature acquisition at the prediction-time. The suggested method acquires features incrementally based on a context-aware feature-value function. We formulate the problem in the reinforcement learning paradigm, and introduce a reward function based on the utility of each feature. Specifically, MC dropout sampling is used to measure expected variations of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.00243", "author_id": ["GJ1sDlEAAAAJ", "leh0V20AAAAJ", "PIPoDrsAAAAJ"], "url_scholarbib": "/scholar?q=info:KT44f0Cl3AwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOpportunistic%2BLearning:%2BBudgeted%2BCost-sensitive%2BLearning%2BFrom%2BData%2BStreams%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KT44f0Cl3AwJ&ei=8mFeYvXvN8LZmQHc1ovQAg&json=", "num_citations": 21, "citedby_url": "/scholar?cites=926797319762361897&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KT44f0Cl3AwJ:scholar.google.com/&scioq=Opportunistic+Learning:+Budgeted+Cost-sensitive+Learning+From+Data+Streams&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.00243"}, "Small Nonlinearities In Activation Functions Create Bad Local Minima In Neural Networks": {"container_type": "Publication", "bib": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "author": ["C Yun", "S Sra", "A Jadbabaie"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.03487", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with\" slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general\" no spurious local minima\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU (-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.03487", "author_id": ["Ukl64ggAAAAJ", "eyCw9goAAAAJ", "ZBc_WwYAAAAJ"], "url_scholarbib": "/scholar?q=info:D3BS2xvXPqoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmall%2BNonlinearities%2BIn%2BActivation%2BFunctions%2BCreate%2BBad%2BLocal%2BMinima%2BIn%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D3BS2xvXPqoJ&ei=92FeYvth35XL1g-zrp6QCQ&json=", "num_citations": 71, "citedby_url": "/scholar?cites=12267478949647511567&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D3BS2xvXPqoJ:scholar.google.com/&scioq=Small+Nonlinearities+In+Activation+Functions+Create+Bad+Local+Minima+In+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.03487"}, "Attention, Learn To Solve Routing Problems!": {"container_type": "Publication", "bib": {"title": "Attention, learn to solve routing problems!", "author": ["W Kool", "H Van Hoof", "M Welling"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1803.08475", "abstract": "The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.08475", "author_id": ["DLCKZqUAAAAJ", "9owUkLYAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:o4hm_smkK8sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttention,%2BLearn%2BTo%2BSolve%2BRouting%2BProblems!%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o4hm_smkK8sJ&ei=-WFeYqSCKMLZmQHc1ovQAg&json=", "num_citations": 455, "citedby_url": "/scholar?cites=14639976201161443491&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o4hm_smkK8sJ:scholar.google.com/&scioq=Attention,+Learn+To+Solve+Routing+Problems!&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.08475.pdf?source=post_page---------------------------"}, "On Random Deep Weight-tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, And Implications To Training": {"container_type": "Publication", "bib": {"title": "On random deep weight-tied autoencoders: Exact asymptotic analysis, phase transitions, and implications to training", "author": ["P Li", "PM Nguyen"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature. Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs \u201capproximate inference\u201d as posed by Scellier"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJx54i05tX", "author_id": ["", "lPG5fAIAAAAJ"], "url_scholarbib": "/scholar?q=info:uaiimPaVlUAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BRandom%2BDeep%2BWeight-tied%2BAutoencoders:%2BExact%2BAsymptotic%2BAnalysis,%2BPhase%2BTransitions,%2BAnd%2BImplications%2BTo%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uaiimPaVlUAJ&ei=_GFeYsieMd-Vy9YPs66ekAk&json=", "num_citations": 27, "citedby_url": "/scholar?cites=4653790676312565945&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uaiimPaVlUAJ:scholar.google.com/&scioq=On+Random+Deep+Weight-tied+Autoencoders:+Exact+Asymptotic+Analysis,+Phase+Transitions,+And+Implications+To+Training&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJx54i05tX"}, "Data-dependent Coresets For Compressing Neural Networks With Applications To Generalization Bounds": {"container_type": "Publication", "bib": {"title": "Data-dependent coresets for compressing neural networks with applications to generalization bounds", "author": ["C Baykal", "L Liebenwein", "I Gilitschenski"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present an efficient coresets-based neural network compression algorithm that sparsifies the parameters of a trained fully-connected neural network in a manner that provably approximates the network's output. Our approach is based on an importance sampling scheme that judiciously defines a sampling distribution over the neural network parameters, and as a result, retains parameters of high importance while discarding redundant ones. We leverage a novel, empirical notion of sensitivity and extend traditional coreset constructions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.05345", "author_id": ["lRxoOlwAAAAJ", "e7ab8u4AAAAJ", "Nuw1Y4oAAAAJ"], "url_scholarbib": "/scholar?q=info:g9Hd6tOpBtEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DData-dependent%2BCoresets%2BFor%2BCompressing%2BNeural%2BNetworks%2BWith%2BApplications%2BTo%2BGeneralization%2BBounds%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g9Hd6tOpBtEJ&ei=AGJeYtP1Ec6E6rQPz8uiuAc&json=", "num_citations": 55, "citedby_url": "/scholar?cites=15061912731430801795&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:g9Hd6tOpBtEJ:scholar.google.com/&scioq=Data-dependent+Coresets+For+Compressing+Neural+Networks+With+Applications+To+Generalization+Bounds&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.05345.pdf?ref=https://githubhelp.com"}, "Distribution-interpolation Trade Off In Generative Models": {"container_type": "Publication", "bib": {"title": "Distribution-interpolation trade off in generative models", "author": ["D Le\u015bniak", "I Sieradzki", "I Podolak"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors--regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SyMhLo0qKQ&source=post_page---------------------------", "author_id": ["", "1391wvMAAAAJ", "30LH850AAAAJ"], "url_scholarbib": "/scholar?q=info:Mb72PvCiAEoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistribution-interpolation%2BTrade%2BOff%2BIn%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Mb72PvCiAEoJ&ei=C2JeYuy1Jo-bmAGmiqCIBw&json=", "num_citations": 12, "citedby_url": "/scholar?cites=5332441111538875953&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Mb72PvCiAEoJ:scholar.google.com/&scioq=Distribution-interpolation+Trade+Off+In+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SyMhLo0qKQ"}, "Learning What And Where To Attend": {"container_type": "Publication", "bib": {"title": "Learning what and where to attend", "author": ["D Linsley", "D Shiebler", "S Eberhardt", "T Serre"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs). Because these networks are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.08819", "author_id": ["cXZlAuQAAAAJ", "J1nlG6EAAAAJ", "jZYdAv8AAAAJ", "kZlPW4wAAAAJ"], "url_scholarbib": "/scholar?q=info:1Lfa7Ujg71kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BWhat%2BAnd%2BWhere%2BTo%2BAttend%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1Lfa7Ujg71kJ&ei=D2JeYsGJGI-bmAGmiqCIBw&json=", "num_citations": 55, "citedby_url": "/scholar?cites=6480644992642234324&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1Lfa7Ujg71kJ:scholar.google.com/&scioq=Learning+What+And+Where+To+Attend&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.08819"}, "Recall Traces: Backtracking Models For Efficient Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Recall traces: Backtracking models for efficient reinforcement learning", "author": ["A Goyal", "P Brakel", "W Fedus", "S Singhal"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a backtracking model that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.00379", "author_id": ["krrh6OUAAAAJ", "Q6UMpRYAAAAJ", "-ZfwQOkAAAAJ", "PapKp_cAAAAJ"], "url_scholarbib": "/scholar?q=info:MiZ0liIruz0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecall%2BTraces:%2BBacktracking%2BModels%2BFor%2BEfficient%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MiZ0liIruz0J&ei=E2JeYpTFGsLZmQHc1ovQAg&json=", "num_citations": 50, "citedby_url": "/scholar?cites=4448196484511573554&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MiZ0liIruz0J:scholar.google.com/&scioq=Recall+Traces:+Backtracking+Models+For+Efficient+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.00379"}, "Dom-q-net: Grounded Rl On Structured Language": {"container_type": "Publication", "bib": {"title": "Dom-q-net: Grounded rl on structured language", "author": ["S Jia", "J Kiros", "J Ba"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.07257", "abstract": "Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.07257", "author_id": ["Hx22Vn8AAAAJ", "W8zwlYQAAAAJ", "ymzxRhAAAAAJ"], "url_scholarbib": "/scholar?q=info:Qmnhx2A5iYwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDom-q-net:%2BGrounded%2BRl%2BOn%2BStructured%2BLanguage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Qmnhx2A5iYwJ&ei=F2JeYvikG4vMsQK69Y7ABg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=10126688324952353090&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Qmnhx2A5iYwJ:scholar.google.com/&scioq=Dom-q-net:+Grounded+Rl+On+Structured+Language&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.07257"}, "Diagnosing And Enhancing Vae Models": {"container_type": "Publication", "bib": {"title": "Diagnosing and enhancing VAE models", "author": ["B Dai", "D Wipf"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.05789", "abstract": "Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.05789", "author_id": ["tYzD6G4AAAAJ", ""], "url_scholarbib": "/scholar?q=info:lA0csuOLZ9UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiagnosing%2BAnd%2BEnhancing%2BVae%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lA0csuOLZ9UJ&ei=GmJeYrfNDsLZmQHc1ovQAg&json=", "num_citations": 220, "citedby_url": "/scholar?cites=15377413262741867924&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lA0csuOLZ9UJ:scholar.google.com/&scioq=Diagnosing+And+Enhancing+Vae+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.05789"}, "Rethinking The Value Of Network Pruning": {"container_type": "Publication", "bib": {"title": "Rethinking the value of network pruning", "author": ["Z Liu", "M Sun", "T Zhou", "G Huang", "T Darrell"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.05270", "abstract": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, ie, training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05270", "author_id": ["7OTD-LEAAAAJ", "wCZbouUAAAAJ", "yA4rb60AAAAJ", "-P9LwcgAAAAJ", "bh-uRFMAAAAJ"], "url_scholarbib": "/scholar?q=info:0f_MU0ND_DEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2BThe%2BValue%2BOf%2BNetwork%2BPruning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0f_MU0ND_DEJ&ei=I2JeYrnnIM2Ny9YPqPyUgAs&json=", "num_citations": 854, "citedby_url": "/scholar?cites=3601827758437367761&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0f_MU0ND_DEJ:scholar.google.com/&scioq=Rethinking+The+Value+Of+Network+Pruning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05270.pdf?ref=https://githubhelp.com"}, "Snas: Stochastic Neural Architecture Search": {"container_type": "Publication", "bib": {"title": "SNAS: stochastic neural architecture search", "author": ["S Xie", "H Zheng", "C Liu", "L Lin"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.09926", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09926", "author_id": ["9GJn5FIAAAAJ", "qTMA5BQAAAAJ", "4m061tYAAAAJ", "Nav8m8gAAAAJ"], "url_scholarbib": "/scholar?q=info:DVFX7EFz-bgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSnas:%2BStochastic%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DVFX7EFz-bgJ&ei=KGJeYsuiJovMsQK69Y7ABg&json=", "num_citations": 664, "citedby_url": "/scholar?cites=13328811299154907405&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DVFX7EFz-bgJ:scholar.google.com/&scioq=Snas:+Stochastic+Neural+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09926"}, "Generative Question Answering: Learning To Answer The Whole Question": {"container_type": "Publication", "bib": {"title": "Generative question answering: Learning to answer the whole question", "author": ["M Lewis", "A Fan"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Discriminative question answering models can overfit to superficial biases in datasets, because their loss function saturates when any clue makes the answer likely. We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it. Our question answering (QA) model is implemented by learning a prior over answers, and a conditional language model to generate the question given the answer\u2014allowing scalable and interpretable many-hop"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bkx0RjA9tX&utm_campaign=Data%20Machina&utm_medium=email&utm_source=Revue%20newsletter", "author_id": ["SnQnQicAAAAJ", "TLZR9zgAAAAJ"], "url_scholarbib": "/scholar?q=info:872DEd4F4n0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BQuestion%2BAnswering:%2BLearning%2BTo%2BAnswer%2BThe%2BWhole%2BQuestion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=872DEd4F4n0J&ei=K2JeYr2MO5LeyQTms5KQBg&json=", "num_citations": 48, "citedby_url": "/scholar?cites=9070819050812325363&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:872DEd4F4n0J:scholar.google.com/&scioq=Generative+Question+Answering:+Learning+To+Answer+The+Whole+Question&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bkx0RjA9tX"}, "Dialogwae: Multimodal Response Generation With Conditional Wasserstein Auto-encoder": {"container_type": "Publication", "bib": {"title": "Dialogwae: Multimodal response generation with conditional wasserstein auto-encoder", "author": ["X Gu", "K Cho", "JW Ha", "S Kim"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.12352", "abstract": "Variational autoencoders~(VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (eg, unimodal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder~(WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.12352", "author_id": ["dFvwOmsAAAAJ", "0RAmmIAAAAAJ", "eGj3ay4AAAAJ", "JE_m2UgAAAAJ"], "url_scholarbib": "/scholar?q=info:UDptF0pR2DEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDialogwae:%2BMultimodal%2BResponse%2BGeneration%2BWith%2BConditional%2BWasserstein%2BAuto-encoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UDptF0pR2DEJ&ei=L2JeYq2RA8LZmQHc1ovQAg&json=", "num_citations": 107, "citedby_url": "/scholar?cites=3591710081490434640&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UDptF0pR2DEJ:scholar.google.com/&scioq=Dialogwae:+Multimodal+Response+Generation+With+Conditional+Wasserstein+Auto-encoder&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.12352"}, "Dimensionality Reduction For Representing The Knowledge Of Probabilistic Models": {"container_type": "Publication", "bib": {"title": "Dimensionality reduction for representing the knowledge of probabilistic models", "author": ["MT Law", "J Snell", "A Farahmand", "R Urtasun"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Most deep learning models rely on expressive high-dimensional representations to achieve good performance on tasks such as classification. However, the high dimensionality of these representations makes them difficult to interpret and prone to over-fitting. We propose a simple, intuitive and scalable dimension reduction framework that takes into account the soft probabilistic interpretation of standard deep models for classification. When applying our framework to visualization, our representations more accurately reflect inter-class distances"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SygD-hCcF7", "author_id": ["_7QgnUcAAAAJ", "MbXKAK8AAAAJ", "G5SAV7gAAAAJ", "jyxO2akAAAAJ"], "url_scholarbib": "/scholar?q=info:NGMUk2Le_YQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDimensionality%2BReduction%2BFor%2BRepresenting%2BThe%2BKnowledge%2BOf%2BProbabilistic%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NGMUk2Le_YQJ&ei=MmJeYrulNpHKsQKNt6-YAw&json=", "num_citations": 10, "citedby_url": "/scholar?cites=9583060097070031668&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NGMUk2Le_YQJ:scholar.google.com/&scioq=Dimensionality+Reduction+For+Representing+The+Knowledge+Of+Probabilistic+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SygD-hCcF7"}, "Scalable Unbalanced Optimal Transport Using Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Scalable unbalanced optimal transport using generative adversarial networks", "author": ["KD Yang", "C Uhler"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.11447", "abstract": "Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. In addition, we propose an algorithm for solving"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.11447", "author_id": ["I80m5QEAAAAJ", "dIJFcaoAAAAJ"], "url_scholarbib": "/scholar?q=info:PkGtd9Ck2sMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScalable%2BUnbalanced%2BOptimal%2BTransport%2BUsing%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PkGtd9Ck2sMJ&ei=N2JeYv6ZB8iBy9YP18Gi8As&json=", "num_citations": 29, "citedby_url": "/scholar?cites=14112773597586866494&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PkGtd9Ck2sMJ:scholar.google.com/&scioq=Scalable+Unbalanced+Optimal+Transport+Using+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.11447"}, "Improving Mmd-gan Training With Repulsive Loss Function": {"container_type": "Publication", "bib": {"title": "Improving mmd-gan training with repulsive loss function", "author": ["W Wang", "Y Sun", "S Halgamuge"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.09916", "abstract": "Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.09916", "author_id": ["-N4RDLMAAAAJ", "jlUCCNoAAAAJ", "9cafqywAAAAJ"], "url_scholarbib": "/scholar?q=info:YIEZ0a2JA1MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BMmd-gan%2BTraining%2BWith%2BRepulsive%2BLoss%2BFunction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YIEZ0a2JA1MJ&ei=OmJeYuqmIY-bmAGmiqCIBw&json=", "num_citations": 49, "citedby_url": "/scholar?cites=5981776109708607840&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YIEZ0a2JA1MJ:scholar.google.com/&scioq=Improving+Mmd-gan+Training+With+Repulsive+Loss+Function&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.09916"}, "Subgradient Descent Learns Orthogonal Dictionaries": {"container_type": "Publication", "bib": {"title": "Subgradient descent learns orthogonal dictionaries", "author": ["Y Bai", "Q Jiang", "J Sun"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.10702", "abstract": "This paper concerns dictionary learning, ie, sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can provably recover orthogonal dictionaries on a natural nonsmooth, nonconvex $\\ell_1 $ minimization formulation of the problem, under mild statistical assumptions on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.10702", "author_id": ["owqhKD8AAAAJ", "sOna0qoAAAAJ", "V6FaD-UAAAAJ"], "url_scholarbib": "/scholar?q=info:1v_N5rcQJTQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSubgradient%2BDescent%2BLearns%2BOrthogonal%2BDictionaries%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1v_N5rcQJTQJ&ei=PWJeYoDnM4yuyASD3KfABw&json=", "num_citations": 41, "citedby_url": "/scholar?cites=3757427846147866582&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1v_N5rcQJTQJ:scholar.google.com/&scioq=Subgradient+Descent+Learns+Orthogonal+Dictionaries&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.10702"}, "Probabilistic Recursive Reasoning For Multi-agent Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Probabilistic recursive reasoning for multi-agent reinforcement learning", "author": ["Y Wen", "Y Yang", "R Luo", "J Wang", "W Pan"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.09207", "abstract": "Humans are capable of attributing latent mental contents such as beliefs or intentions to others. The social skill is critical in daily life for reasoning about the potential consequences of others' behaviors so as to plan ahead. It is known that humans use such reasoning ability recursively by considering what others believe about their own beliefs. In this paper, we start from level-$1 $ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.09207", "author_id": ["_A1CxG8AAAAJ", "6yL0xw8AAAAJ", "5AnhSDQAAAAJ", "wIE1tY4AAAAJ", "GqryWPsAAAAJ"], "url_scholarbib": "/scholar?q=info:bVy_5ylTGbEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbabilistic%2BRecursive%2BReasoning%2BFor%2BMulti-agent%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bVy_5ylTGbEJ&ei=QWJeYufeDY-bmAGmiqCIBw&json=", "num_citations": 82, "citedby_url": "/scholar?cites=12761322458577853549&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bVy_5ylTGbEJ:scholar.google.com/&scioq=Probabilistic+Recursive+Reasoning+For+Multi-agent+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.09207"}, "Diversity-sensitive Conditional Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Diversity-sensitive conditional generative adversarial networks", "author": ["D Yang", "S Hong", "Y Jang", "T Zhao", "H Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.09024", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative Adversarial Network (cGAN). Although conditional distributions are multi-modal (ie, having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.09024", "author_id": ["", "hvr3ALkAAAAJ", "YvYBeysAAAAJ", "", "fmSHtE8AAAAJ"], "url_scholarbib": "/scholar?q=info:s_HrquNUJgIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiversity-sensitive%2BConditional%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s_HrquNUJgIJ&ei=SGJeYrK0C8iBy9YP18Gi8As&json=", "num_citations": 128, "citedby_url": "/scholar?cites=154904573992759731&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s_HrquNUJgIJ:scholar.google.com/&scioq=Diversity-sensitive+Conditional+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.09024"}, "Fluctuation-dissipation Relations For Stochastic Gradient Descent": {"container_type": "Publication", "bib": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "author": ["S Yaida"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00004", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00004", "author_id": ["EY0J8s0AAAAJ"], "url_scholarbib": "/scholar?q=info:hNFoo5pTzFoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFluctuation-dissipation%2BRelations%2BFor%2BStochastic%2BGradient%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hNFoo5pTzFoJ&ei=T2JeYsamDI-bmAGmiqCIBw&json=", "num_citations": 47, "citedby_url": "/scholar?cites=6542696282294112644&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hNFoo5pTzFoJ:scholar.google.com/&scioq=Fluctuation-dissipation+Relations+For+Stochastic+Gradient+Descent&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00004"}, "Stable Recurrent Models": {"container_type": "Publication", "bib": {"title": "Stable recurrent models", "author": ["J Miller", "M Hardt"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.10369", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.10369", "author_id": ["51I5vxkAAAAJ", "adnTgaAAAAAJ"], "url_scholarbib": "/scholar?q=info:Ww4kf2BfypYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStable%2BRecurrent%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ww4kf2BfypYJ&ei=U2JeYumuOo-bmAGmiqCIBw&json=", "num_citations": 90, "citedby_url": "/scholar?cites=10865601919039311451&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ww4kf2BfypYJ:scholar.google.com/&scioq=Stable+Recurrent+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.10369"}, "Learning Self-imitating Diverse Policies": {"container_type": "Publication", "bib": {"title": "Learning self-imitating diverse policies", "author": ["T Gangwani", "Q Liu", "J Peng"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.10309", "abstract": "The success of popular algorithms for deep reinforcement learning, such as policy-gradients and Q-learning, relies heavily on the availability of an informative reward signal at each"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.10309", "author_id": ["IUY5oVkAAAAJ", "XEx1fZkAAAAJ", "H2JX-RQAAAAJ"], "url_scholarbib": "/scholar?q=info:8930BKLDfH4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BSelf-imitating%2BDiverse%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8930BKLDfH4J&ei=V2JeYo9Vj5uYAaaKoIgH&json=", "num_citations": 44, "citedby_url": "/scholar?cites=9114374846526316019&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8930BKLDfH4J:scholar.google.com/&scioq=Learning+Self-imitating+Diverse+Policies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.10309"}, "How Important Is A Neuron": {"container_type": "Publication", "bib": {"title": "How important is a neuron?", "author": ["K Dhamdhere", "M Sundararajan", "Q Yan"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.12233", "abstract": "The problem of attributing a deep network's prediction to its\\emph {input/base} features is well-studied. We introduce the notion of\\emph {conductance} to extend the notion of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.12233", "author_id": ["ZP6gGZYAAAAJ", "q39nzokAAAAJ", "Wn8xr_gAAAAJ"], "url_scholarbib": "/scholar?q=info:KyVzQpIDd4IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BImportant%2BIs%2BA%2BNeuron%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KyVzQpIDd4IJ&ei=WmJeYsLWCIyuyASD3KfABw&json=", "num_citations": 68, "citedby_url": "/scholar?cites=9400986673874150699&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KyVzQpIDd4IJ:scholar.google.com/&scioq=How+Important+Is+A+Neuron&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.12233.pdf?ref=https://githubhelp.com"}, "Sample Efficient Adaptive Text-to-speech": {"container_type": "Publication", "bib": {"title": "Sample efficient adaptive text-to-speech", "author": ["Y Chen", "Y Assael", "B Shillingford", "D Budden"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, we learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim of training is not to produce a neural network with fixed weights, which is then deployed as a TTS system. Instead, the aim is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. We introduce and benchmark three strategies:(i) learning the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10460", "author_id": ["fAWKizAAAAAJ", "DwHtHE8AAAAJ", "0tPZW4kAAAAJ", "Tom5l8EAAAAJ"], "url_scholarbib": "/scholar?q=info:n3yo8kxqysQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSample%2BEfficient%2BAdaptive%2BText-to-speech%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n3yo8kxqysQJ&ei=XWJeYs2fNpGJmwGIxre4DA&json=", "num_citations": 87, "citedby_url": "/scholar?cites=14180263255450614943&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n3yo8kxqysQJ:scholar.google.com/&scioq=Sample+Efficient+Adaptive+Text-to-speech&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10460"}, "Neural Network Gradient-based Learning Of Black-box Function Interfaces": {"container_type": "Publication", "bib": {"title": "Neural network gradient-based learning of black-box function interfaces", "author": ["A Jacovi", "G Hadash", "E Kermany", "B Carmeli"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.03995", "author_id": ["cX9TtloAAAAJ", "2ZHcYA1R2JgC", "", ""], "url_scholarbib": "/scholar?q=info:GZKXD2ypbwEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BNetwork%2BGradient-based%2BLearning%2BOf%2BBlack-box%2BFunction%2BInterfaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GZKXD2ypbwEJ&ei=YWJeYpeQNI6pywSdh6agAg&json=", "num_citations": 12, "citedby_url": "/scholar?cites=103487598035964441&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GZKXD2ypbwEJ:scholar.google.com/&scioq=Neural+Network+Gradient-based+Learning+Of+Black-box+Function+Interfaces&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.03995"}, "Universal Stagewise Learning For Non-convex Problems With Convergence On Averaged Solutions": {"container_type": "Publication", "bib": {"title": "Universal stagewise learning for non-convex problems with convergence on averaged solutions", "author": ["Z Chen", "Z Yuan", "J Yi", "B Zhou", "E Chen"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Although stochastic gradient descent (SGD) method and its variants (eg, stochastic momentum methods, AdaGrad) are the choice of algorithms for solving non-convex problems (especially deep learning), there still remain big gaps between the theory and the practice with many questions unresolved. For example, there is still a lack of theories of convergence for SGD and its variants that use stagewise step size and return an averaged solution in practice. In addition, theoretical insights of why adaptive step size of AdaGrad"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.06296", "author_id": ["XuUU98cAAAAJ", "ZjJf6tYAAAAJ", "lZxRZ84AAAAJ", "h3Nsz6YAAAAJ", "Q9h02J0AAAAJ"], "url_scholarbib": "/scholar?q=info:Ze_vJ6-UYL0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BStagewise%2BLearning%2BFor%2BNon-convex%2BProblems%2BWith%2BConvergence%2BOn%2BAveraged%2BSolutions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ze_vJ6-UYL0J&ei=ZmJeYrDoApWMy9YPt8OamA0&json=", "num_citations": 44, "citedby_url": "/scholar?cites=13646070350942826341&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ze_vJ6-UYL0J:scholar.google.com/&scioq=Universal+Stagewise+Learning+For+Non-convex+Problems+With+Convergence+On+Averaged+Solutions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.06296"}, "Gradient Descent Aligns The Layers Of Deep Linear Networks": {"container_type": "Publication", "bib": {"title": "Gradient descent aligns the layers of deep linear networks", "author": ["Z Ji", "M Telgarsky"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.02032", "abstract": "This paper establishes risk convergence and asymptotic weight matrix alignment---a form of implicit regularization---of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes):(i) the risk converges to 0;(ii) the normalized i-th weight matrix asymptotically equals its rank-1 approximation $ u_iv_i^{\\top} $;(iii) these rank-1 matrices are aligned"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02032", "author_id": ["QiQ_FXIAAAAJ", "Fc-5yRIAAAAJ"], "url_scholarbib": "/scholar?q=info:a6npNbC1dF0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient%2BDescent%2BAligns%2BThe%2BLayers%2BOf%2BDeep%2BLinear%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a6npNbC1dF0J&ei=b2JeYpGiKs2Ny9YPqPyUgAs&json=", "num_citations": 131, "citedby_url": "/scholar?cites=6734207111249111403&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a6npNbC1dF0J:scholar.google.com/&scioq=Gradient+Descent+Aligns+The+Layers+Of+Deep+Linear+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02032"}, "Beyond Greedy Ranking: Slate Optimization Via List-cvae": {"container_type": "Publication", "bib": {"title": "Beyond greedy ranking: Slate optimization via list-CVAE", "author": ["R Jiang", "S Gowal", "TA Mann", "DJ Rezende"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The conventional solution to the recommendation problem greedily ranks individual document candidates by prediction scores. However, this method fails to optimize the slate as a whole, and hence, often struggles to capture biases caused by the page layout and document interdepedencies. The slate recommendation problem aims to directly find the optimally ordered subset of documents (ie slates) that best serve users' interests. Solving this problem is hard due to the combinatorial explosion in all combinations of document"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.01682", "author_id": ["BIcE5LMAAAAJ", "7wclGnQAAAAJ", "iIKGkhYAAAAJ", "UGlyhFMAAAAJ"], "url_scholarbib": "/scholar?q=info:rND1pLircnAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BGreedy%2BRanking:%2BSlate%2BOptimization%2BVia%2BList-cvae%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rND1pLircnAJ&ei=cmJeYvulKsLZmQHc1ovQAg&json=", "num_citations": 28, "citedby_url": "/scholar?cites=8102727489122848940&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rND1pLircnAJ:scholar.google.com/&scioq=Beyond+Greedy+Ranking:+Slate+Optimization+Via+List-cvae&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.01682"}, "Neural Speed Reading With Structural-jump-lstm": {"container_type": "Publication", "bib": {"title": "Neural speed reading with structural-jump-lstm", "author": ["C Hansen", "C Hansen", "S Alstrup", "JG Simonsen"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent neural networks (RNNs) can model natural language by sequentially'reading'input tokens and outputting a distributed representation of each token. Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance. Efforts to speed up this inference, known as' neural speed reading', either ignore or skim over part of the input. We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference. The"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.00761", "author_id": ["IhMPEDwAAAAJ", "lwXO0soAAAAJ", "Agh-Q0IAAAAJ", "JFhAzosAAAAJ"], "url_scholarbib": "/scholar?q=info:l48s0q8pfZQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BSpeed%2BReading%2BWith%2BStructural-jump-lstm%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=l48s0q8pfZQJ&ei=gmJeYuHFGc2Ny9YPqPyUgAs&json=", "num_citations": 21, "citedby_url": "/scholar?cites=10699754124824317847&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:l48s0q8pfZQJ:scholar.google.com/&scioq=Neural+Speed+Reading+With+Structural-jump-lstm&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.00761"}, "K For The Price Of 1: Parameter-efficient Multi-task And Transfer Learning": {"container_type": "Publication", "bib": {"title": "K for the price of 1: Parameter-efficient multi-task and transfer learning", "author": ["PK Mudrakarta", "M Sandler", "A Zhmoginov"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a novel method that enables parameter-efficient transfer and multi-task learning with deep neural networks. The basic approach is to learn a model patch-a small set of parameters-that will specialize to each task, instead of fine-tuning the last layer or the entire network. For instance, we show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (eg converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.10703", "author_id": ["G0YpGHYAAAAJ", "IcPc-OUAAAAJ", "jj6IfzEAAAAJ"], "url_scholarbib": "/scholar?q=info:aUCfyCN-iVMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DK%2BFor%2BThe%2BPrice%2BOf%2B1:%2BParameter-efficient%2BMulti-task%2BAnd%2BTransfer%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aUCfyCN-iVMJ&ei=hmJeYqSxGo-bmAGmiqCIBw&json=", "num_citations": 28, "citedby_url": "/scholar?cites=6019481069112213609&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aUCfyCN-iVMJ:scholar.google.com/&scioq=K+For+The+Price+Of+1:+Parameter-efficient+Multi-task+And+Transfer+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.10703"}, "Learning A Sat Solver From Single-bit Supervision": {"container_type": "Publication", "bib": {"title": "Learning a SAT solver from single-bit supervision", "author": ["D Selsam", "M Lamm", "B B\u00fcnz", "P Liang"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.03685", "author_id": ["yaSqFaEAAAAJ", "_BQoVCwAAAAJ", "Hj82r7MAAAAJ", "pouyVyUAAAAJ"], "url_scholarbib": "/scholar?q=info:WFjaj85Z9lYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BA%2BSat%2BSolver%2BFrom%2BSingle-bit%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WFjaj85Z9lYJ&ei=iWJeYsqqKo-bmAGmiqCIBw&json=", "num_citations": 260, "citedby_url": "/scholar?cites=6266294675244210264&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WFjaj85Z9lYJ:scholar.google.com/&scioq=Learning+A+Sat+Solver+From+Single-bit+Supervision&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.03685?ref=https://githubhelp.com"}, "Interpolation-prediction Networks For Irregularly Sampled Time Series": {"container_type": "Publication", "bib": {"title": "Interpolation-prediction networks for irregularly sampled time series", "author": ["SN Shukla", "BM Marlin"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.07782", "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.07782", "author_id": ["l1tsmesAAAAJ", "ey960FIAAAAJ"], "url_scholarbib": "/scholar?q=info:rvCA8nbLytYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInterpolation-prediction%2BNetworks%2BFor%2BIrregularly%2BSampled%2BTime%2BSeries%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rvCA8nbLytYJ&ei=jWJeYqj0Hd-Vy9YPs66ekAk&json=", "num_citations": 67, "citedby_url": "/scholar?cites=15477406781147246766&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rvCA8nbLytYJ:scholar.google.com/&scioq=Interpolation-prediction+Networks+For+Irregularly+Sampled+Time+Series&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.07782"}, "Dyrep: Learning Representations Over Dynamic Graphs": {"container_type": "Publication", "bib": {"title": "Dyrep: Learning representations over dynamic graphs", "author": ["R Trivedi", "M Farajtabar", "P Biswal", "H Zha"], "pub_year": "2019", "venue": "International conference on \u2026", "abstract": "Representation Learning over graph structured data has received significant attention recently due to its ubiquitous applicability. However, most advancements have been made in static graph settings while efforts for jointly learning dynamic of the graph and dynamic on the graph are still in an infant stage. Two fundamental questions arise in learning over dynamic graphs:(i) How to elegantly model dynamical processes over graphs?(ii) How to leverage such a model to effectively encode evolving graph information into low"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/biblio/10099025", "author_id": ["Jq1MCAYAAAAJ", "shkKxnQAAAAJ", "PsfCaccAAAAJ", "n1DQMIsAAAAJ"], "url_scholarbib": "/scholar?q=info:a7qfcwhO-2UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDyrep:%2BLearning%2BRepresentations%2BOver%2BDynamic%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a7qfcwhO-2UJ&ei=lmJeYu6dO8LZmQHc1ovQAg&json=", "num_citations": 170, "citedby_url": "/scholar?cites=7348553015191648875&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a7qfcwhO-2UJ:scholar.google.com/&scioq=Dyrep:+Learning+Representations+Over+Dynamic+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10099025"}, "Amortized Bayesian Meta-learning": {"container_type": "Publication", "bib": {"title": "Amortized bayesian meta-learning", "author": ["S Ravi", "A Beatson"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Meta-learning, or learning-to-learn, has proven to be a successful strategy in attacking problems in supervised learning and reinforcement learning that involve small amounts of data. State-of-the-art solutions involve learning an initialization and/or learning algorithm using a set of training episodes so that the meta learner can generalize to an evaluation episode quickly. These methods perform well but often lack good quantification of uncertainty, which can be vital to real-world applications when data is lacking. We propose a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkgpy3C5tX", "author_id": ["cr53lHIAAAAJ", "xCjAOsgAAAAJ"], "url_scholarbib": "/scholar?q=info:F2MFU4XY9RgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAmortized%2BBayesian%2BMeta-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F2MFU4XY9RgJ&ei=m2JeYrSGCc2Ny9YPqPyUgAs&json=", "num_citations": 85, "citedby_url": "/scholar?cites=1798581693339493143&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F2MFU4XY9RgJ:scholar.google.com/&scioq=Amortized+Bayesian+Meta-learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkgpy3C5tX"}, "Harmonic Unpaired Image-to-image Translation": {"container_type": "Publication", "bib": {"title": "Harmonic unpaired image-to-image translation", "author": ["R Zhang", "T Pfister", "J Li"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09727", "abstract": "The recent direction of unpaired image-to-image translation is on one hand very exciting as it alleviates the big burden in obtaining label-intensive pixel-to-pixel supervision, but it is on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09727", "author_id": ["dse6jAsAAAAJ", "ahSpJOAAAAAJ", "feX1fWAAAAAJ"], "url_scholarbib": "/scholar?q=info:F0QAYHMD64AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHarmonic%2BUnpaired%2BImage-to-image%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F0QAYHMD64AJ&ei=nmJeYuXGApHKsQKNt6-YAw&json=", "num_citations": 32, "citedby_url": "/scholar?cites=9289522450448532503&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F0QAYHMD64AJ:scholar.google.com/&scioq=Harmonic+Unpaired+Image-to-image+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09727"}, "Alista: Analytic Weights Are As Good As Learned Weights In Lista": {"container_type": "Publication", "bib": {"title": "ALISTA: Analytic weights are as good as learned weights in LISTA", "author": ["J Liu", "X Chen"], "pub_year": "2019", "venue": "International Conference on Learning Representations \u2026", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This significantly"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/biblio/10191388", "author_id": ["QS6Lj5sAAAAJ", "fEehMTwAAAAJ"], "url_scholarbib": "/scholar?q=info:9dr-zxH_G2YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAlista:%2BAnalytic%2BWeights%2BAre%2BAs%2BGood%2BAs%2BLearned%2BWeights%2BIn%2BLista%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9dr-zxH_G2YJ&ei=q2JeYva1OY6pywSdh6agAg&json=", "num_citations": 98, "citedby_url": "/scholar?cites=7357754868208950005&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9dr-zxH_G2YJ:scholar.google.com/&scioq=Alista:+Analytic+Weights+Are+As+Good+As+Learned+Weights+In+Lista&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10191388"}, "Learning Grid Cells As Vector Representation Of Self-position Coupled With Matrix Representation Of Self-motion": {"container_type": "Publication", "bib": {"title": "Learning grid cells as vector representation of self-position coupled with matrix representation of self-motion", "author": ["R Gao", "J Xie", "SC Zhu", "YN Wu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.05597", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models.(1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multiplication, ie, the vector of the next position is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05597", "author_id": ["VdlgOXoAAAAJ", "O3p4CIQAAAAJ", "Al8dyb4AAAAJ", "7k_1QFIAAAAJ"], "url_scholarbib": "/scholar?q=info:5ZAQI36XlhEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BGrid%2BCells%2BAs%2BVector%2BRepresentation%2BOf%2BSelf-position%2BCoupled%2BWith%2BMatrix%2BRepresentation%2BOf%2BSelf-motion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5ZAQI36XlhEJ&ei=sGJeYp_mI82Ny9YPqPyUgAs&json=", "num_citations": 21, "citedby_url": "/scholar?cites=1267366913161335013&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5ZAQI36XlhEJ:scholar.google.com/&scioq=Learning+Grid+Cells+As+Vector+Representation+Of+Self-position+Coupled+With+Matrix+Representation+Of+Self-motion&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05597"}, "Backpropamine: Training Self-modifying Neural Networks With Differentiable Neuromodulated Plasticity": {"container_type": "Publication", "bib": {"title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity", "author": ["T Miconi", "A Rawal", "J Clune", "KO Stanley"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.10585", "abstract": "The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10585", "author_id": ["EXun8woAAAAJ", "yDNOmsMAAAAJ", "5TZ7f5wAAAAJ", "6Q6oO1MAAAAJ"], "url_scholarbib": "/scholar?q=info:Rhu9i-GVWjMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBackpropamine:%2BTraining%2BSelf-modifying%2BNeural%2BNetworks%2BWith%2BDifferentiable%2BNeuromodulated%2BPlasticity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Rhu9i-GVWjMJ&ei=uGJeYpWlO8LZmQHc1ovQAg&json=", "num_citations": 51, "citedby_url": "/scholar?cites=3700434839782890310&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Rhu9i-GVWjMJ:scholar.google.com/&scioq=Backpropamine:+Training+Self-modifying+Neural+Networks+With+Differentiable+Neuromodulated+Plasticity&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10585"}, "Posterior Attention Models For Sequence To Sequence Learning": {"container_type": "Publication", "bib": {"title": "Posterior attention models for sequence to sequence learning", "author": ["S Shankar", "S Sarawagi"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence. We present an alternative architecture called Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes. First, the position where attention is marginalized is changed"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkltNhC9FX", "author_id": ["yK56jugAAAAJ", "Hg4HmTAAAAAJ"], "url_scholarbib": "/scholar?q=info:3Cg2mHt0LlsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPosterior%2BAttention%2BModels%2BFor%2BSequence%2BTo%2BSequence%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3Cg2mHt0LlsJ&ei=u2JeYrOvDs2Ny9YPqPyUgAs&json=", "num_citations": 28, "citedby_url": "/scholar?cites=6570316980563618012&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3Cg2mHt0LlsJ:scholar.google.com/&scioq=Posterior+Attention+Models+For+Sequence+To+Sequence+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkltNhC9FX"}, "Dynamic Sparse Graph For Efficient Deep Learning": {"container_type": "Publication", "bib": {"title": "Dynamic sparse graph for efficient deep learning", "author": ["L Liu", "L Deng", "X Hu", "M Zhu", "G Li", "Y Ding"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference. The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices. However, most of the previous studies optimize for inference while neglect training or even complicate it. Training is far more intractable, since (i) the neurons dominate the memory cost rather than the weights in inference;(ii) the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00859", "author_id": ["_lxwnx4AAAAJ", "vlqhAN4AAAAJ", "Hc3iRxUAAAAJ", "NRbwpx8AAAAJ", "qCfE--MAAAAJ", "ZuNK2XkAAAAJ"], "url_scholarbib": "/scholar?q=info:mpsNiARQWQ0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BSparse%2BGraph%2BFor%2BEfficient%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mpsNiARQWQ0J&ei=vmJeYtTuK4-bmAGmiqCIBw&json=", "num_citations": 28, "citedby_url": "/scholar?cites=961887975812995994&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mpsNiARQWQ0J:scholar.google.com/&scioq=Dynamic+Sparse+Graph+For+Efficient+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00859.pdf?ref=https://githubhelp.com"}, "Optimistic Mirror Descent In Saddle-point Problems: Going The Extra (gradient) Mile": {"container_type": "Publication", "bib": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "author": ["P Mertikopoulos", "B Lecouat", "H Zenati", "CS Foo"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.02629", "author_id": ["", "7ydObdwAAAAJ", "LBqNPp4AAAAJ", "AgbeqGkAAAAJ"], "url_scholarbib": "/scholar?q=info:g1ZzDfB5OD0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimistic%2BMirror%2BDescent%2BIn%2BSaddle-point%2BProblems:%2BGoing%2BThe%2BExtra%2B(gradient)%2BMile%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g1ZzDfB5OD0J&ei=w2JeYsa7CZWMy9YPt8OamA0&json=", "num_citations": 170, "citedby_url": "/scholar?cites=4411409906934175363&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:g1ZzDfB5OD0J:scholar.google.com/&scioq=Optimistic+Mirror+Descent+In+Saddle-point+Problems:+Going+The+Extra+(gradient)+Mile&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.02629"}, "H-detach: Modifying The Lstm Gradient Towards Better Optimization": {"container_type": "Publication", "bib": {"title": "h-detach: Modifying the LSTM gradient towards better optimization", "author": ["D Arpit", "B Kanuparthi", "G Kerg", "NR Ke"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (\\textit {h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03023", "author_id": ["oU0jQIAAAAAJ", "7q8cVU0AAAAJ", "GcJkks8AAAAJ", "dxwPYhQAAAAJ"], "url_scholarbib": "/scholar?q=info:IhFAb_YDlQoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DH-detach:%2BModifying%2BThe%2BLstm%2BGradient%2BTowards%2BBetter%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IhFAb_YDlQoJ&ei=zmJeYvnnM9-Vy9YPs66ekAk&json=", "num_citations": 29, "citedby_url": "/scholar?cites=762520068872474914&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IhFAb_YDlQoJ:scholar.google.com/&scioq=H-detach:+Modifying+The+Lstm+Gradient+Towards+Better+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03023"}, "Camou: Learning Physical Vehicle Camouflages To Adversarially Attack Detectors In The Wild": {"container_type": "Publication", "bib": {"title": "CAMOU: Learning physical vehicle camouflages to adversarially attack detectors in the wild", "author": ["Y Zhang", "H Foroosh", "P David", "B Gong"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles. In the second, we"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJgEl3A5tm", "author_id": ["9dFr77YAAAAJ", "vNHN42cAAAAJ", "", "lv9ZeVUAAAAJ"], "url_scholarbib": "/scholar?q=info:HaibLMM56QYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCamou:%2BLearning%2BPhysical%2BVehicle%2BCamouflages%2BTo%2BAdversarially%2BAttack%2BDetectors%2BIn%2BThe%2BWild%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HaibLMM56QYJ&ei=0mJeYojGE4-bmAGmiqCIBw&json=", "num_citations": 41, "citedby_url": "/scholar?cites=497992744230955037&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HaibLMM56QYJ:scholar.google.com/&scioq=Camou:+Learning+Physical+Vehicle+Camouflages+To+Adversarially+Attack+Detectors+In+The+Wild&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJgEl3A5tm"}, "Differentiable Perturb-and-parse: Semi-supervised Parsing With A Structured Variational Autoencoder": {"container_type": "Publication", "bib": {"title": "Differentiable perturb-and-parse: Semi-supervised parsing with a structured variational autoencoder", "author": ["C Corro", "I Titov"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1807.09875", "abstract": "Human annotation for syntactic parsing is expensive, and large resources are available only for a fraction of languages. A question we ask is whether one can leverage abundant unlabeled texts to improve syntactic parsers, beyond just using the texts to obtain more generalisable lexical features (ie beyond word embeddings). To this end, we propose a novel latent-variable generative model for semi-supervised syntactic dependency parsing. As exact inference is intractable, we introduce a differentiable relaxation to obtain"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.09875", "author_id": ["Q_DmlucAAAAJ", "FKUc3vsAAAAJ"], "url_scholarbib": "/scholar?q=info:VIf4kir0xcwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BPerturb-and-parse:%2BSemi-supervised%2BParsing%2BWith%2BA%2BStructured%2BVariational%2BAutoencoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VIf4kir0xcwJ&ei=1mJeYsDbMZWMy9YPt8OamA0&json=", "num_citations": 53, "citedby_url": "/scholar?cites=14755468217840863060&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VIf4kir0xcwJ:scholar.google.com/&scioq=Differentiable+Perturb-and-parse:+Semi-supervised+Parsing+With+A+Structured+Variational+Autoencoder&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.09875.pdf]"}, "Learning Localized Generative Models For 3d Point Clouds Via Graph Convolution": {"container_type": "Publication", "bib": {"title": "Learning localized generative models for 3d point clouds via graph convolution", "author": ["D Valsesia", "G Fracastoro", "E Magli"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "Point clouds are an important type of geometric data and have widespread use in computer graphics and vision. However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space. Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJeXSo09FQ&source=post_page---------------------------", "author_id": ["5YXpLhQAAAAJ", "RAF6dlIAAAAJ", "G8SHSCgAAAAJ"], "url_scholarbib": "/scholar?q=info:eTa4sVnxH4kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLocalized%2BGenerative%2BModels%2BFor%2B3d%2BPoint%2BClouds%2BVia%2BGraph%2BConvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eTa4sVnxH4kJ&ei=42JeYtvfD9-Vy9YPs66ekAk&json=", "num_citations": 88, "citedby_url": "/scholar?cites=9880881475010180729&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eTa4sVnxH4kJ:scholar.google.com/&scioq=Learning+Localized+Generative+Models+For+3d+Point+Clouds+Via+Graph+Convolution&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJeXSo09FQ"}, "Large-scale Study Of Curiosity-driven Learning": {"container_type": "Publication", "bib": {"title": "Large-scale study of curiosity-driven learning", "author": ["Y Burda", "H Edwards", "D Pathak", "A Storkey"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper:(a) We perform the first large-scale study of purely curiosity-driven learning, ie without any extrinsic rewards, across 54 standard benchmark"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.04355", "author_id": ["Amky96kAAAAJ", "0o470HsAAAAJ", "AEsPCAUAAAAJ", "3Rlc8EAAAAAJ"], "url_scholarbib": "/scholar?q=info:1wrTfPTTMGAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge-scale%2BStudy%2BOf%2BCuriosity-driven%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1wrTfPTTMGAJ&ei=5mJeYrj-I5HKsQKNt6-YAw&json=", "num_citations": 488, "citedby_url": "/scholar?cites=6931272873542879959&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1wrTfPTTMGAJ:scholar.google.com/&scioq=Large-scale+Study+Of+Curiosity-driven+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.04355.pdf?ref=https://githubhelp.com"}, "Spherical Cnns On Unstructured Grids": {"container_type": "Publication", "bib": {"title": "Spherical CNNs on unstructured grids", "author": ["C Jiang", "J Huang", "K Kashinath", "P Marcus"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.02039", "author_id": ["KD12DDMAAAAJ", "7eJBk1UAAAAJ", "sNMCgVwAAAAJ", "CHlcHGMAAAAJ"], "url_scholarbib": "/scholar?q=info:wbk4EJ0cvHwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpherical%2BCnns%2BOn%2BUnstructured%2BGrids%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wbk4EJ0cvHwJ&ei=6mJeYvydEJWMy9YPt8OamA0&json=", "num_citations": 96, "citedby_url": "/scholar?cites=8988090417232263617&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wbk4EJ0cvHwJ:scholar.google.com/&scioq=Spherical+Cnns+On+Unstructured+Grids&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.02039"}, "Combinatorial Attacks On Binarized Neural Networks": {"container_type": "Publication", "bib": {"title": "Combinatorial attacks on binarized neural networks", "author": ["EB Khalil", "A Gupta", "B Dilkina"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.03538", "abstract": "Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency. Concurrently, it has been shown that neural networks may be overly sensitive to\" attacks\"-tiny adversarial changes in the input-which may be detrimental to their use in safety-critical domains. Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks. The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03538", "author_id": ["juqDWQMAAAAJ", "qVNz3asAAAAJ", "1jjyaBYAAAAJ"], "url_scholarbib": "/scholar?q=info:c0bbUF9qNOcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCombinatorial%2BAttacks%2BOn%2BBinarized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c0bbUF9qNOcJ&ei=7WJeYpufJIvMsQK69Y7ABg&json=", "num_citations": 33, "citedby_url": "/scholar?cites=16660057879161292403&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c0bbUF9qNOcJ:scholar.google.com/&scioq=Combinatorial+Attacks+On+Binarized+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03538"}, "Lemonade: Learned Motif And Neuronal Assembly Detection In Calcium Imaging Videos": {"container_type": "Publication", "bib": {"title": "LemoNADe: Learned motif and neuronal assembly detection in calcium imaging videos", "author": ["E Kirschbaum", "M Hau\u00dfmann", "S Wolf", "H Sonntag"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or\" motifs\", are thought to be building blocks of neural representations and information processing. We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. Our nonparametric method extracts motifs directly from videos, bypassing the difficult"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.09963", "author_id": ["N8zEgccAAAAJ", "YXBAT4wAAAAJ", "ZLTMBaEAAAAJ", ""], "url_scholarbib": "/scholar?q=info:VR81ZJ6IEekJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLemonade:%2BLearned%2BMotif%2BAnd%2BNeuronal%2BAssembly%2BDetection%2BIn%2BCalcium%2BImaging%2BVideos%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VR81ZJ6IEekJ&ei=8GJeYs3GOd-Vy9YPs66ekAk&json=", "num_citations": 7, "citedby_url": "/scholar?cites=16794354699308703573&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VR81ZJ6IEekJ:scholar.google.com/&scioq=Lemonade:+Learned+Motif+And+Neuronal+Assembly+Detection+In+Calcium+Imaging+Videos&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.09963"}, "Recurrent Experience Replay In Distributed Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Recurrent experience replay in distributed reinforcement learning", "author": ["S Kapturowski", "G Ostrovski", "J Quan"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1lyTjAqYX&utm_campaign=RL%20Weekly&utm_medium=email&utm_source=Revue%20newsletter", "author_id": ["JTq6SNcAAAAJ", "a7OnyQgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:e0akf20VH4AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BExperience%2BReplay%2BIn%2BDistributed%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e0akf20VH4AJ&ei=82JeYpSkJIvMsQK69Y7ABg&json=", "num_citations": 251, "citedby_url": "/scholar?cites=9232121321169897083&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e0akf20VH4AJ:scholar.google.com/&scioq=Recurrent+Experience+Replay+In+Distributed+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1lyTjAqYX"}, "L2-nonexpansive Neural Networks": {"container_type": "Publication", "bib": {"title": "L2-nonexpansive neural networks", "author": ["H Qian", "MN Wegman"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.07896", "abstract": "This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.07896", "author_id": ["8_2Zk2QAAAAJ", ""], "url_scholarbib": "/scholar?q=info:p-tRop9HFloJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DL2-nonexpansive%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p-tRop9HFloJ&ei=9mJeYunNF5HKsQKNt6-YAw&json=", "num_citations": 66, "citedby_url": "/scholar?cites=6491454663849798567&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p-tRop9HFloJ:scholar.google.com/&scioq=L2-nonexpansive+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.07896.pdf?ref=https://githubhelp.com"}, "Discriminator-actor-critic: Addressing Sample Inefficiency And Reward Bias In Adversarial Imitation Learning": {"container_type": "Publication", "bib": {"title": "Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning", "author": ["I Kostrikov", "KK Agrawal", "D Dwibedi", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.02925", "author_id": ["PTS2AOgAAAAJ", "Wd8_fOcAAAAJ", "EPfOJwQAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:MixDgvmh0ZcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscriminator-actor-critic:%2BAddressing%2BSample%2BInefficiency%2BAnd%2BReward%2BBias%2BIn%2BAdversarial%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MixDgvmh0ZcJ&ei=-2JeYtDdFo-bmAGmiqCIBw&json=", "num_citations": 146, "citedby_url": "/scholar?cites=10939703062864014386&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MixDgvmh0ZcJ:scholar.google.com/&scioq=Discriminator-actor-critic:+Addressing+Sample+Inefficiency+And+Reward+Bias+In+Adversarial+Imitation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.02925"}, "Variational Smoothing In Recurrent Neural Network Language Models": {"container_type": "Publication", "bib": {"title": "Variational smoothing in recurrent neural network language models", "author": ["L Kong", "G Melis", "W Ling", "L Yu", "D Yogatama"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a new theoretical perspective of data noising in recurrent neural network language models (Xie et al., 2017). We show that each variant of data noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (ie, a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). We use this insight to propose a more principled method to apply at prediction time and propose natural extensions to data noising under the variational"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.09296", "author_id": ["f1hBi5wAAAAJ", "TbLw2lcAAAAJ", "gl0PhvEAAAAJ", "gX5JBc4AAAAJ", "IBlMTLwAAAAJ"], "url_scholarbib": "/scholar?q=info:LgPHdWTljYsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BSmoothing%2BIn%2BRecurrent%2BNeural%2BNetwork%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LgPHdWTljYsJ&ei=_2JeYvnJL5GJmwGIxre4DA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=10055945762623652654&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LgPHdWTljYsJ:scholar.google.com/&scioq=Variational+Smoothing+In+Recurrent+Neural+Network+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.09296"}, "Minimal Images In Deep Neural Networks: Fragile Object Recognition In Natural Images": {"container_type": "Publication", "bib": {"title": "Minimal images in deep neural networks: Fragile object recognition in natural images", "author": ["S Srivastava", "G Ben-Yosef", "X Boix"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.03227", "abstract": "The human ability to recognize objects is impaired when the object is not shown in full.\" Minimal images\" are the smallest regions of an image that remain recognizable for humans. Ullman et al. 2016 show that a slight modification of the location and size of the visible region of the minimal image produces a sharp drop in human recognition accuracy. In this paper, we demonstrate that such drops in accuracy due to changes of the visible region are a common phenomenon between humans and existing state-of-the-art deep neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.03227", "author_id": ["sqTh_dwAAAAJ", "R9JBrOIAAAAJ", "lbbKFVcAAAAJ"], "url_scholarbib": "/scholar?q=info:f5ypaq2-9HwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimal%2BImages%2BIn%2BDeep%2BNeural%2BNetworks:%2BFragile%2BObject%2BRecognition%2BIn%2BNatural%2BImages%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=f5ypaq2-9HwJ&ei=C2NeYqbbON-Vy9YPs66ekAk&json=", "num_citations": 18, "citedby_url": "/scholar?cites=9004031207048584319&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:f5ypaq2-9HwJ:scholar.google.com/&scioq=Minimal+Images+In+Deep+Neural+Networks:+Fragile+Object+Recognition+In+Natural+Images&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.03227"}, "An Analytic Theory Of Generalization Dynamics And Transfer Learning In Deep Linear Networks": {"container_type": "Publication", "bib": {"title": "An analytic theory of generalization dynamics and transfer learning in deep linear networks", "author": ["AK Lampinen", "S Ganguli"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.10374", "abstract": "Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10374", "author_id": ["_N44XxAAAAAJ", "rF2VvOgAAAAJ"], "url_scholarbib": "/scholar?q=info:ElF-azBC-AQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BAnalytic%2BTheory%2BOf%2BGeneralization%2BDynamics%2BAnd%2BTransfer%2BLearning%2BIn%2BDeep%2BLinear%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ElF-azBC-AQJ&ei=DmNeYorAL8iBy9YP18Gi8As&json=", "num_citations": 72, "citedby_url": "/scholar?cites=358108946105258258&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ElF-azBC-AQJ:scholar.google.com/&scioq=An+Analytic+Theory+Of+Generalization+Dynamics+And+Transfer+Learning+In+Deep+Linear+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10374"}, "Proxylessnas: Direct Neural Architecture Search On Target Task And Hardware": {"container_type": "Publication", "bib": {"title": "Proxylessnas: Direct neural architecture search on target task and hardware", "author": ["H Cai", "L Zhu", "S Han"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.00332", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (eg $10^ 4$ GPU hours) makes it difficult to\\emph {directly} search the architectures on large-scale tasks (eg ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly wrt candidate set size). As a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.00332", "author_id": ["x-AvvrYAAAAJ", "y0LVrtgAAAAJ", "E0iCaa4AAAAJ"], "url_scholarbib": "/scholar?q=info:QNtZABooQ_oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProxylessnas:%2BDirect%2BNeural%2BArchitecture%2BSearch%2BOn%2BTarget%2BTask%2BAnd%2BHardware%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QNtZABooQ_oJ&ei=GWNeYszAHpLeyQTms5KQBg&json=", "num_citations": 1143, "citedby_url": "/scholar?cites=18033301425061747520&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QNtZABooQ_oJ:scholar.google.com/&scioq=Proxylessnas:+Direct+Neural+Architecture+Search+On+Target+Task+And+Hardware&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.00332?ref=https://githubhelp.com"}, "A Variational Inequality Perspective On Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "A variational inequality perspective on generative adversarial networks", "author": ["G Gidel", "H Berard", "G Vignoud", "P Vincent"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.10551", "author_id": ["bDrXQPUAAAAJ", "P5d_140AAAAJ", "0PA_sngAAAAJ", "WBCKQMsAAAAJ"], "url_scholarbib": "/scholar?q=info:KM2zT3dfdFkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BVariational%2BInequality%2BPerspective%2BOn%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KM2zT3dfdFkJ&ei=HGNeYrLgBpyO6rQP-viEEA&json=", "num_citations": 240, "citedby_url": "/scholar?cites=6445881932716952872&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KM2zT3dfdFkJ:scholar.google.com/&scioq=A+Variational+Inequality+Perspective+On+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.10551"}, "Transferring Knowledge Across Learning Processes": {"container_type": "Publication", "bib": {"title": "Transferring knowledge across learning processes", "author": ["S Flennerhag", "PG Moreno", "ND Lawrence"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.01054", "author_id": ["SeMQQkcAAAAJ", "WnW5PmcAAAAJ", "r3SJcvoAAAAJ"], "url_scholarbib": "/scholar?q=info:TZaddWk0fbEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransferring%2BKnowledge%2BAcross%2BLearning%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TZaddWk0fbEJ&ei=HmNeYqr7I8iBy9YP18Gi8As&json=", "num_citations": 42, "citedby_url": "/scholar?cites=12789436144351549005&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TZaddWk0fbEJ:scholar.google.com/&scioq=Transferring+Knowledge+Across+Learning+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.01054"}, "Visual Semantic Navigation Using Scene Priors": {"container_type": "Publication", "bib": {"title": "Visual semantic navigation using scene priors", "author": ["W Yang", "X Wang", "A Farhadi", "A Gupta"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "How do humans navigate to target objects in novel scenes? Do we use the semantic/functional priors we have built over years to efficiently search and navigate? For example, to search for mugs, we search cabinets near the coffee machine and for fruits we try the fridge. In this work, we focus on incorporating semantic priors in the task of semantic navigation. We propose to use Graph Convolutional Networks for incorporating the prior knowledge into a deep reinforcement learning framework. The agent uses the features from"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06543", "author_id": ["6QQX88UAAAAJ", "Y8O9N_0AAAAJ", "jeOFRDsAAAAJ", "bqL73OkAAAAJ"], "url_scholarbib": "/scholar?q=info:Ox8VHphIIZAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisual%2BSemantic%2BNavigation%2BUsing%2BScene%2BPriors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ox8VHphIIZAJ&ei=IWNeYqOeEciBy9YP18Gi8As&json=", "num_citations": 160, "citedby_url": "/scholar?cites=10385662033870004027&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ox8VHphIIZAJ:scholar.google.com/&scioq=Visual+Semantic+Navigation+Using+Scene+Priors&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06543.pdf)."}, "Pay Less Attention With Lightweight And Dynamic Convolutions": {"container_type": "Publication", "bib": {"title": "Pay less attention with lightweight and dynamic convolutions", "author": ["F Wu", "A Fan", "A Baevski", "YN Dauphin", "M Auli"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.10430", "author_id": ["sNL8SSoAAAAJ", "TLZR9zgAAAAJ", "i7sxIX8AAAAJ", "XSforroAAAAJ", "KMcwQtcAAAAJ"], "url_scholarbib": "/scholar?q=info:KTgs6QLWmi4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPay%2BLess%2BAttention%2BWith%2BLightweight%2BAnd%2BDynamic%2BConvolutions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KTgs6QLWmi4J&ei=JGNeYvXiDYyuyASD3KfABw&json=", "num_citations": 378, "citedby_url": "/scholar?cites=3358231780148394025&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KTgs6QLWmi4J:scholar.google.com/&scioq=Pay+Less+Attention+With+Lightweight+And+Dynamic+Convolutions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.10430.pdf?ref=https://githubhelp.com"}, "Learning To Screen For Fast Softmax Inference On Large Vocabulary Neural Networks": {"container_type": "Publication", "bib": {"title": "Learning to screen for fast softmax inference on large vocabulary neural networks", "author": ["PH Chen", "S Si", "S Kumar", "Y Li", "CJ Hsieh"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.12406", "abstract": "Neural language models have been widely used in various NLP tasks, including machine translation, next word prediction and conversational agents. However, it is challenging to deploy these models on mobile devices due to their slow prediction speed, where the bottleneck is to compute top candidates in the softmax layer. In this paper, we introduce a novel softmax layer approximation algorithm by exploiting the clustering structure of context vectors. Our algorithm uses a light-weight screening model to predict a much smaller set of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.12406", "author_id": ["", "eAJfUeIAAAAJ", "08CNqrYAAAAJ", "ZZdB48QAAAAJ", "Wy89g4IAAAAJ"], "url_scholarbib": "/scholar?q=info:6JOcokryOvgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BScreen%2BFor%2BFast%2BSoftmax%2BInference%2BOn%2BLarge%2BVocabulary%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6JOcokryOvgJ&ei=KGNeYv6eFJWMy9YPt8OamA0&json=", "num_citations": 6, "citedby_url": "/scholar?cites=17886875272425018344&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6JOcokryOvgJ:scholar.google.com/&scioq=Learning+To+Screen+For+Fast+Softmax+Inference+On+Large+Vocabulary+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.12406"}, "Hierarchical Rl Using An Ensemble Of Proprioceptive Periodic Policies": {"container_type": "Publication", "bib": {"title": "Hierarchical RL using an ensemble of proprioceptive periodic policies", "author": ["K Marino", "A Gupta", "R Fergus", "A Szlam"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "In this paper we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks. The agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a``phase function.''The high-level policy is trained using a sparse, task"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJz1x20cFQ", "author_id": ["6bQxCusAAAAJ", "bqL73OkAAAAJ", "GgQ9GEkAAAAJ", "u3-FxUgAAAAJ"], "url_scholarbib": "/scholar?q=info:WOgp_ZpyT2UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BRl%2BUsing%2BAn%2BEnsemble%2BOf%2BProprioceptive%2BPeriodic%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WOgp_ZpyT2UJ&ei=LGNeYpn2Cd-Vy9YPs66ekAk&json=", "num_citations": 12, "citedby_url": "/scholar?cites=7300179530988775512&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WOgp_ZpyT2UJ:scholar.google.com/&scioq=Hierarchical+Rl+Using+An+Ensemble+Of+Proprioceptive+Periodic+Policies&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJz1x20cFQ"}, "Signsgd Via Zeroth-order Oracle": {"container_type": "Publication", "bib": {"title": "signSGD via zeroth-order oracle", "author": ["S Liu", "PY Chen", "X Chen", "M Hong"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "In this paper, we design and analyze a new zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which enjoys dual advantages of gradient-free operations and signSGD. The latter requires only the sign information of gradient estimates but is able to achieve a comparable or even better convergence speed than SGD-type algorithms. Our study shows that ZO signSGD requires $\\sqrt {d} $ times more iterations than signSGD, leading to a convergence rate of $ O (\\sqrt {d}/\\sqrt {T}) $ under mild conditions, where $ d"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJe-DsC5Fm", "author_id": ["C7dO_UgAAAAJ", "jxwlCUUAAAAJ", "M0ki5ZgAAAAJ", "qRnP-p0AAAAJ"], "url_scholarbib": "/scholar?q=info:Te1F_exokXAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSignsgd%2BVia%2BZeroth-order%2BOracle%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Te1F_exokXAJ&ei=MWNeYt6KJIvMsQK69Y7ABg&json=", "num_citations": 39, "citedby_url": "/scholar?cites=8111379770941762893&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Te1F_exokXAJ:scholar.google.com/&scioq=Signsgd+Via+Zeroth-order+Oracle&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJe-DsC5Fm"}, "Adversarial Reprogramming Of Neural Networks": {"container_type": "Publication", "bib": {"title": "Adversarial reprogramming of neural networks", "author": ["GF Elsayed", "I Goodfellow", "J Sohl-Dickstein"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep neural networks are susceptible to\\emph {adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\\em reprogram} the target model to perform a task chosen by the attacker---without the attacker"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.11146", "author_id": ["7PrTPzsAAAAJ", "iYN86KEAAAAJ", "-3zYIjQAAAAJ"], "url_scholarbib": "/scholar?q=info:H2IuM-OMmcsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BReprogramming%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H2IuM-OMmcsJ&ei=NWNeYuvtFo-bmAGmiqCIBw&json=", "num_citations": 62, "citedby_url": "/scholar?cites=14670912168580243999&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:H2IuM-OMmcsJ:scholar.google.com/&scioq=Adversarial+Reprogramming+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.11146.pdf?source=post_page---------------------------"}, "Global-to-local Memory Pointer Networks For Task-oriented Dialogue": {"container_type": "Publication", "bib": {"title": "Global-to-local memory pointer networks for task-oriented dialogue", "author": ["CS Wu", "R Socher", "C Xiong"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.04713", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.04713", "author_id": ["1G4GV2EAAAAJ", "FaOcyfMAAAAJ", "vaSdahkAAAAJ"], "url_scholarbib": "/scholar?q=info:1Wb8K0Eknm8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGlobal-to-local%2BMemory%2BPointer%2BNetworks%2BFor%2BTask-oriented%2BDialogue%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1Wb8K0Eknm8J&ei=OWNeYu-NLI-bmAGmiqCIBw&json=", "num_citations": 109, "citedby_url": "/scholar?cites=8042905846859720405&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1Wb8K0Eknm8J:scholar.google.com/&scioq=Global-to-local+Memory+Pointer+Networks+For+Task-oriented+Dialogue&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.04713.pdf?ref=https://githubhelp.com"}, "Competitive Experience Replay": {"container_type": "Publication", "bib": {"title": "Competitive experience replay", "author": ["H Liu", "A Trott", "R Socher", "C Xiong"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.00528", "abstract": "Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided. However, in sparse reward"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.00528", "author_id": ["wtK4Yh4AAAAJ", "rB4bvV0AAAAJ", "FaOcyfMAAAAJ", "vaSdahkAAAAJ"], "url_scholarbib": "/scholar?q=info:_c3bIRW6zXgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompetitive%2BExperience%2BReplay%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_c3bIRW6zXgJ&ei=RGNeYrC1LYryyASen4NI&json=", "num_citations": 37, "citedby_url": "/scholar?cites=8704818254702169597&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_c3bIRW6zXgJ:scholar.google.com/&scioq=Competitive+Experience+Replay&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.00528"}, "Toward Understanding The Impact Of Staleness In Distributed Machine Learning": {"container_type": "Publication", "bib": {"title": "Toward understanding the impact of staleness in distributed machine learning", "author": ["W Dai", "Y Zhou", "N Dong", "H Zhang", "EP Xing"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many distributed machine learning (ML) systems adopt the non-synchronous execution in order to alleviate the network communication bottleneck, resulting in stale parameters that do not reflect the latest updates. Despite much development in large-scale ML, the effects of staleness on learning are inconclusive as it is challenging to directly monitor or control staleness in complex distributed environments. In this work, we study the convergence behaviors of a wide array of ML models and algorithms under delayed updates. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03264", "author_id": ["M9oUY4cAAAAJ", "4fK8bYIAAAAJ", "0DX2YsQAAAAJ", "osTl-5IAAAAJ", "5pKTRxEAAAAJ"], "url_scholarbib": "/scholar?q=info:wKmxabfPN8YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DToward%2BUnderstanding%2BThe%2BImpact%2BOf%2BStaleness%2BIn%2BDistributed%2BMachine%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wKmxabfPN8YJ&ei=SGNeYtftKZWMy9YPt8OamA0&json=", "num_citations": 37, "citedby_url": "/scholar?cites=14283113129888033216&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wKmxabfPN8YJ:scholar.google.com/&scioq=Toward+Understanding+The+Impact+Of+Staleness+In+Distributed+Machine+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03264"}, "Deep Graph Infomax": {"container_type": "Publication", "bib": {"title": "Deep Graph Infomax.", "author": ["P Velickovic", "W Fedus", "WL Hamilton", "P Li\u00f2"], "pub_year": "2019", "venue": "ICLR \u2026", "abstract": "Petar Velickovic1,2, William Fedus2,3,5, William L. Hamilton2,4, Pietro Li`o1, Yoshua  Bengio2,3 and R Devon Hjelm6,2,3  1University of Cambridge 2Mila 3Universit\u00e9 de Montr\u00e9al"}, "filled": false, "gsrank": 1, "pub_url": "https://pdfs.semanticscholar.org/2ef6/299459f0b7c9273cddba65f7402083c3b452.pdf", "author_id": ["kcTK_FAAAAAJ", "-ZfwQOkAAAAJ", "T5tm9eQAAAAJ", "3YrWf7EAAAAJ"], "url_scholarbib": "/scholar?q=info:OToNOSNcpFwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BGraph%2BInfomax%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OToNOSNcpFwJ&ei=S2NeYquEDJyO6rQP-viEEA&json=", "num_citations": 644, "citedby_url": "/scholar?cites=6675561854020696633&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OToNOSNcpFwJ:scholar.google.com/&scioq=Deep+Graph+Infomax&hl=en&as_sdt=0,33", "eprint_url": "https://pdfs.semanticscholar.org/2ef6/299459f0b7c9273cddba65f7402083c3b452.pdf"}, "Towards Metamerism Via Foveated Style Transfer": {"container_type": "Publication", "bib": {"title": "Towards metamerism via foveated style transfer", "author": ["A Deza", "A Jonnalagadda", "M Eckstein"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1705.10041", "abstract": "The problem of $\\textit {visual metamerism} $ is defined as finding a family of perceptually indistinguishable, yet physically different images. In this paper, we propose our NeuroFovea metamer model, a foveated generative model that is based on a mixture of peripheral representations and style transfer forward-pass algorithms. Our gradient-descent free model is parametrized by a foveated VGG19 encoder-decoder which allows us to encode images in high dimensional space and interpolate between the content and texture information with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1705.10041", "author_id": ["KZLsTmQAAAAJ", "WN8AMecAAAAJ", "G5dQztgAAAAJ"], "url_scholarbib": "/scholar?q=info:2vO-4O7-6PgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BMetamerism%2BVia%2BFoveated%2BStyle%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2vO-4O7-6PgJ&ei=TWNeYoSvO46pywSdh6agAg&json=", "num_citations": 21, "citedby_url": "/scholar?cites=17935865817929282522&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2vO-4O7-6PgJ:scholar.google.com/&scioq=Towards+Metamerism+Via+Foveated+Style+Transfer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1705.10041"}, "Deep Learning 3d Shapes Using Alt-az Anisotropic 2-sphere Convolution": {"container_type": "Publication", "bib": {"title": "Deep learning 3d shapes using alt-az anisotropic 2-sphere convolution", "author": ["M Liu", "F Yao", "C Choi", "A Sinha"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks. One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces. In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere. A cascade set of geodesic disk filters rotate on the 2"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkeSiiA5Fm", "author_id": ["p0hHqOwAAAAJ", "rT_HHuAAAAAJ", "iSFDVj4AAAAJ", ""], "url_scholarbib": "/scholar?q=info:bg06fCnZek4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2B3d%2BShapes%2BUsing%2BAlt-az%2BAnisotropic%2B2-sphere%2BConvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bg06fCnZek4J&ei=UWNeYpMizY3L1g-o_JSACw&json=", "num_citations": 23, "citedby_url": "/scholar?cites=5655071054318144878&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bg06fCnZek4J:scholar.google.com/&scioq=Deep+Learning+3d+Shapes+Using+Alt-az+Anisotropic+2-sphere+Convolution&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkeSiiA5Fm"}, "Rnns Implicitly Implement Tensor-product Representations": {"container_type": "Publication", "bib": {"title": "RNNs implicitly implement tensor product representations", "author": ["RT McCoy", "T Linzen", "E Dunbar"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (eg, sequence positions) and vectors representing fillers (eg, particular words). To test this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.08718", "author_id": ["xSavR6cAAAAJ", "5mJDXjoAAAAJ", "yz1tOxgAAAAJ"], "url_scholarbib": "/scholar?q=info:Krr4xNeaC3cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRnns%2BImplicitly%2BImplement%2BTensor-product%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Krr4xNeaC3cJ&ei=VGNeYsjbEpHKsQKNt6-YAw&json=", "num_citations": 34, "citedby_url": "/scholar?cites=8578120166770522666&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Krr4xNeaC3cJ:scholar.google.com/&scioq=Rnns+Implicitly+Implement+Tensor-product+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.08718"}, "Multilingual Neural Machine Translation With Soft Decoupled Encoding": {"container_type": "Publication", "bib": {"title": "Multilingual neural machine translation with soft decoupled encoding", "author": ["X Wang", "H Pham", "P Arthur", "G Neubig"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.03499", "abstract": "Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in efficiently learning word representations in the face of paucity of data. In this paper, we propose Soft Decoupled Encoding (SDE), a multilingual lexicon encoding framework specifically designed to share lexical-level information intelligently without requiring heuristic preprocessing such as pre-segmenting the data. SDE represents a word"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.03499", "author_id": ["Fo52NKoAAAAJ", "GpcGdRkAAAAJ", "B0Q4PbIAAAAJ", "wlosgkoAAAAJ"], "url_scholarbib": "/scholar?q=info:usz9LICljxkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultilingual%2BNeural%2BMachine%2BTranslation%2BWith%2BSoft%2BDecoupled%2BEncoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=usz9LICljxkJ&ei=X2NeYokwlYzL1g-3w5qYDQ&json=", "num_citations": 37, "citedby_url": "/scholar?cites=1841872742547049658&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:usz9LICljxkJ:scholar.google.com/&scioq=Multilingual+Neural+Machine+Translation+With+Soft+Decoupled+Encoding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.03499"}, "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse Rl, And Gans By Constraining Information Flow": {"container_type": "Publication", "bib": {"title": "Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow", "author": ["XB Peng", "A Kanazawa", "S Toyer", "P Abbeel"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00821", "author_id": ["FwxfQosAAAAJ", "Ci-_QYIAAAAJ", "J8E8GQYAAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:rLEnKylKh4EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BDiscriminator%2BBottleneck:%2BImproving%2BImitation%2BLearning,%2BInverse%2BRl,%2BAnd%2BGans%2BBy%2BConstraining%2BInformation%2BFlow%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rLEnKylKh4EJ&ei=YWNeYu3tM8LZmQHc1ovQAg&json=", "num_citations": 138, "citedby_url": "/scholar?cites=9333510293426778540&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rLEnKylKh4EJ:scholar.google.com/&scioq=Variational+Discriminator+Bottleneck:+Improving+Imitation+Learning,+Inverse+Rl,+And+Gans+By+Constraining+Information+Flow&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00821.pdf%EF%BC%89"}, "Darts: Differentiable Architecture Search": {"container_type": "Publication", "bib": {"title": "Darts: Differentiable architecture search", "author": ["H Liu", "K Simonyan", "Y Yang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.09055", "abstract": "of architecture search by formulating the task in a differentiable  a discrete and non-differentiable  search space, our method is  architecture representation, allowing efficient search of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.09055", "author_id": ["IMkVH_8AAAAJ", "L7lMQkQAAAAJ", "MlZq4XwAAAAJ"], "url_scholarbib": "/scholar?q=info:zzEl1wgubQwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDarts:%2BDifferentiable%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zzEl1wgubQwJ&ei=I2FeYqr3AY-bmAGmiqCIBw&json=", "num_citations": 2460, "citedby_url": "/scholar?cites=895422516420751823&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zzEl1wgubQwJ:scholar.google.com/&scioq=Darts:+Differentiable+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.09055.pdf?ref=https://githubhelp.com"}, "Improving Generalization And Stability Of Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Improving generalization and stability of generative adversarial networks", "author": ["H Thanh-Tung", "T Tran", "S Venkatesh"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.03984", "abstract": "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.03984", "author_id": ["xZU08d0AAAAJ", "zvspVLwAAAAJ", "AEkRUQcAAAAJ"], "url_scholarbib": "/scholar?q=info:jx56qW0mVrsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGeneralization%2BAnd%2BStability%2BOf%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jx56qW0mVrsJ&ei=JmFeYvOKC9-Vy9YPs66ekAk&json=", "num_citations": 91, "citedby_url": "/scholar?cites=13499019185526283919&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jx56qW0mVrsJ:scholar.google.com/&scioq=Improving+Generalization+And+Stability+Of+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.03984"}, "Smoothing The Geometry Of Probabilistic Box Embeddings": {"container_type": "Publication", "bib": {"title": "Smoothing the geometry of probabilistic box embeddings", "author": ["X Li", "L Vilnis", "D Zhang", "M Boratko"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1xSNiRcF7&source=post_page-----c51f6102a87a----------------------", "author_id": ["UR7uLq8AAAAJ", "xWrOthYAAAAJ", "M_i8Rr8AAAAJ", "YKZGpnkAAAAJ"], "url_scholarbib": "/scholar?q=info:c3ZfEQTxU3EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmoothing%2BThe%2BGeometry%2BOf%2BProbabilistic%2BBox%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c3ZfEQTxU3EJ&ei=KWFeYtPWG8iBy9YP18Gi8As&json=", "num_citations": 45, "citedby_url": "/scholar?cites=8166135549126473331&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c3ZfEQTxU3EJ:scholar.google.com/&scioq=Smoothing+The+Geometry+Of+Probabilistic+Box+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1xSNiRcF7"}, "Attentive Neural Processes": {"container_type": "Publication", "bib": {"title": "Attentive neural processes", "author": ["H Kim", "A Mnih", "J Schwarz", "M Garnelo", "A Eslami"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Figure 1: Comparison of predictions given by a fully trained NP and Attentive NP (ANP) in  1D function regression (left) / 2D image regression (right). The contexts (crosses/top half pixels"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.05761", "author_id": ["vxU3Zk4AAAAJ", "mxiO4IkAAAAJ", "Efs3XxQAAAAJ", "Hr3zNQUAAAAJ", "skyUvycAAAAJ"], "url_scholarbib": "/scholar?q=info:jK0En_cZe1oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttentive%2BNeural%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jK0En_cZe1oJ&ei=LWFeYoS7DM2Ny9YPqPyUgAs&json=", "num_citations": 198, "citedby_url": "/scholar?cites=6519833436864425356&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jK0En_cZe1oJ:scholar.google.com/&scioq=Attentive+Neural+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.05761.pdf?ref=https://githubhelp.com"}, "On The Universal Approximability And Complexity Bounds Of Quantized Relu Neural Networks": {"container_type": "Publication", "bib": {"title": "On the universal approximability and complexity bounds of quantized relu neural networks", "author": ["Y Ding", "J Liu", "J Xiong", "Y Shi"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1802.03646", "abstract": "quantized neural networks. First, we prove the universal approximability of quantized ReLU  networks  Then we provide upper bounds on the number of weights and the memory size for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.03646", "author_id": ["3RIDnhkAAAAJ", "NV4qJcwAAAAJ", "tRt1xPYAAAAJ", "LrjbEkIAAAAJ"], "url_scholarbib": "/scholar?q=info:BTkQajQoK4oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BUniversal%2BApproximability%2BAnd%2BComplexity%2BBounds%2BOf%2BQuantized%2BRelu%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BTkQajQoK4oJ&ei=MGFeYpyGIPmQ6rQPzKCxuAY&json=", "num_citations": 12, "citedby_url": "/scholar?cites=9956095606815471877&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BTkQajQoK4oJ:scholar.google.com/&scioq=On+The+Universal+Approximability+And+Complexity+Bounds+Of+Quantized+Relu+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.03646?ref=https://githubhelp.com"}, "Rotdcf: Decomposition Of Convolutional Filters For Rotation-equivariant Deep Networks": {"container_type": "Publication", "bib": {"title": "Rotdcf: Decomposition of convolutional filters for rotation-equivariant deep networks", "author": ["X Cheng", "Q Qiu", "R Calderbank", "G Sapiro"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.06846", "abstract": "Explicit encoding of group actions in deep features makes it possible for convolutional neural networks (CNNs) to handle global deformations of images, which is critical to success in many vision tasks. This paper proposes to decompose the convolutional filters over joint steerable bases across the space and the group geometry simultaneously, namely a rotation-equivariant CNN with decomposed convolutional filters (RotDCF). This decomposition facilitates computing the joint convolution, which is proved to be necessary for the group"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.06846", "author_id": ["I2gwdssAAAAJ", "jdLtt_YAAAAJ", "XtUBAtwAAAAJ", "ISRNX3gAAAAJ"], "url_scholarbib": "/scholar?q=info:qA-aNpMYW14J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRotdcf:%2BDecomposition%2BOf%2BConvolutional%2BFilters%2BFor%2BRotation-equivariant%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qA-aNpMYW14J&ei=PGFeYpHNA82Ny9YPqPyUgAs&json=", "num_citations": 33, "citedby_url": "/scholar?cites=6799055083001221032&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qA-aNpMYW14J:scholar.google.com/&scioq=Rotdcf:+Decomposition+Of+Convolutional+Filters+For+Rotation-equivariant+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.06846"}, "Stochastic Optimization Of Sorting Networks Via Continuous Relaxations": {"container_type": "Publication", "bib": {"title": "Stochastic optimization of sorting networks via continuous relaxations", "author": ["A Grover", "E Wang", "A Zweig", "S Ermon"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.08850", "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.08850", "author_id": ["oOhnPUgAAAAJ", "T7qerZ4AAAAJ", "smtN1IIAAAAJ", "ogXTOZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:KrT3xgmOX5MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BOptimization%2BOf%2BSorting%2BNetworks%2BVia%2BContinuous%2BRelaxations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KrT3xgmOX5MJ&ei=PmFeYr2FJMiBy9YP18Gi8As&json=", "num_citations": 72, "citedby_url": "/scholar?cites=10619362619006891050&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KrT3xgmOX5MJ:scholar.google.com/&scioq=Stochastic+Optimization+Of+Sorting+Networks+Via+Continuous+Relaxations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.08850"}, "Discovery Of Natural Language Concepts In Individual Units Of Cnns": {"container_type": "Publication", "bib": {"title": "Discovery of natural language concepts in individual units of cnns", "author": ["S Na", "YJ Choe", "DH Lee", "G Kim"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.07249", "abstract": "Although deep convolutional networks have achieved improved performance in many natural language tasks, they have been treated as black boxes because they are difficult to interpret. Especially, little is known about how they represent language in their intermediate layers. In an attempt to understand the representations of deep convolutional networks trained on language tasks, we show that individual units are selectively responsive to specific morphemes, words, and phrases, rather than responding to arbitrary and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.07249", "author_id": ["", "71g2MrUAAAAJ", "Iw-G2qIAAAAJ", "CiSdOV0AAAAJ"], "url_scholarbib": "/scholar?q=info:Lq2acR1cCOcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscovery%2BOf%2BNatural%2BLanguage%2BConcepts%2BIn%2BIndividual%2BUnits%2BOf%2BCnns%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Lq2acR1cCOcJ&ei=QWFeYoL6Lt-Vy9YPs66ekAk&json=", "num_citations": 8, "citedby_url": "/scholar?cites=16647657304104807726&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Lq2acR1cCOcJ:scholar.google.com/&scioq=Discovery+Of+Natural+Language+Concepts+In+Individual+Units+Of+Cnns&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.07249"}, "Efficient Augmentation Via Data Subsampling": {"container_type": "Publication", "bib": {"title": "Efficient augmentation via data subsampling", "author": ["M Kuchnik", "V Smith"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.05222", "abstract": "Data augmentation is commonly used to encode invariances in learning methods. However, this process is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of the dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations to apply. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05222", "author_id": ["0vbCjDEAAAAJ", "bldHpWIAAAAJ"], "url_scholarbib": "/scholar?q=info:o226ldquvqcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BAugmentation%2BVia%2BData%2BSubsampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o226ldquvqcJ&ei=RWFeYuPcCZHKsQKNt6-YAw&json=", "num_citations": 19, "citedby_url": "/scholar?cites=12087290703747116451&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o226ldquvqcJ:scholar.google.com/&scioq=Efficient+Augmentation+Via+Data+Subsampling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05222"}, "The Role Of Over-parametrization In Generalization Of Neural Networks": {"container_type": "Publication", "bib": {"title": "Towards understanding the role of over-parametrization in generalization of neural networks", "author": ["B Neyshabur", "Z Li", "S Bhojanapalli", "Y LeCun"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "complexity of neural networks to get a bound on the generalization error  function class  considered, we need to choose the right function class that only captures the real trained networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.12076", "author_id": ["e1ucbCYAAAAJ", "5vVjpBsAAAAJ", "bpSF_9EAAAAJ", "WLN3QrAAAAAJ"], "url_scholarbib": "/scholar?q=info:ilObW2kg2h0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BRole%2BOf%2BOver-parametrization%2BIn%2BGeneralization%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ilObW2kg2h0J&ei=SWFeYuhYj5uYAaaKoIgH&json=", "num_citations": 364, "citedby_url": "/scholar?cites=2151067408903394186&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ilObW2kg2h0J:scholar.google.com/&scioq=The+Role+Of+Over-parametrization+In+Generalization+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.12076"}, "On The Turing Completeness Of Modern Neural Network Architectures": {"container_type": "Publication", "bib": {"title": "On the turing completeness of modern neural network architectures", "author": ["J P\u00e9rez", "J Marinkovi\u0107", "P Barcel\u00f3"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.03429", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.03429", "author_id": ["a6lUuiwAAAAJ", "", "9OH3PokAAAAJ"], "url_scholarbib": "/scholar?q=info:KeVCHqQUBJwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BTuring%2BCompleteness%2BOf%2BModern%2BNeural%2BNetwork%2BArchitectures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KeVCHqQUBJwJ&ei=S2FeYqzeNc2Ny9YPqPyUgAs&json=", "num_citations": 56, "citedby_url": "/scholar?cites=11242133264938493225&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KeVCHqQUBJwJ:scholar.google.com/&scioq=On+The+Turing+Completeness+Of+Modern+Neural+Network+Architectures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.03429"}, "Towards Robust, Locally Linear Deep Networks": {"container_type": "Publication", "bib": {"title": "Towards robust, locally linear deep networks", "author": ["GH Lee", "D Alvarez-Melis", "TS Jaakkola"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.03207", "abstract": "Deep networks realize complex mappings that are often understood by their locally linear behavior at or around points of interest. For example, we use the derivative of the mapping with respect to its inputs for sensitivity analysis, or to explain (obtain coordinate relevance for) a prediction. One key challenge is that such derivatives are themselves inherently unstable. In this paper, we propose a new learning problem to encourage deep networks to have stable derivatives over larger regions. While the problem is challenging in general, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.03207", "author_id": ["1mdLkSMAAAAJ", "XsxZrYYAAAAJ", "Ao4gtsYAAAAJ"], "url_scholarbib": "/scholar?q=info:EcgciGc7CqcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BRobust,%2BLocally%2BLinear%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EcgciGc7CqcJ&ei=TmFeYqq1DN-Vy9YPs66ekAk&json=", "num_citations": 30, "citedby_url": "/scholar?cites=12036498269952329745&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EcgciGc7CqcJ:scholar.google.com/&scioq=Towards+Robust,+Locally+Linear+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.03207"}, "Detecting Egregious Responses In Neural Sequence-to-sequence Models": {"container_type": "Publication", "bib": {"title": "Detecting egregious responses in neural sequence-to-sequence models", "author": ["T He", "J Glass"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.04113", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq) model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.04113", "author_id": ["egmfjjwAAAAJ", "pfGI-KcAAAAJ"], "url_scholarbib": "/scholar?q=info:jGDQMTDPOawJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDetecting%2BEgregious%2BResponses%2BIn%2BNeural%2BSequence-to-sequence%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jGDQMTDPOawJ&ei=UWFeYofzKpWMy9YPt8OamA0&json=", "num_citations": 16, "citedby_url": "/scholar?cites=12410178054097232012&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jGDQMTDPOawJ:scholar.google.com/&scioq=Detecting+Egregious+Responses+In+Neural+Sequence-to-sequence+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.04113"}, "The Singular Values Of Convolutional Layers": {"container_type": "Publication", "bib": {"title": "The singular values of convolutional layers", "author": ["H Sedghi", "V Gupta", "PM Long"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.10408", "abstract": "of the linear transformation computed by a convolutional layer are the key to its  these  singular values. Up until now, authors seeking to control the operator norm of convolutional layers"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.10408", "author_id": ["_9GX96fDWAMC", "eDccPdQAAAAJ", "PVty8PUAAAAJ"], "url_scholarbib": "/scholar?q=info:sVNgKDpMunEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BSingular%2BValues%2BOf%2BConvolutional%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sVNgKDpMunEJ&ei=VGFeYouUPIvMsQK69Y7ABg&json=", "num_citations": 130, "citedby_url": "/scholar?cites=8194946284623254449&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sVNgKDpMunEJ:scholar.google.com/&scioq=The+Singular+Values+Of+Convolutional+Layers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.10408?ref=https://githubhelp.com"}, "Exemplar Guided Unsupervised Image-to-image Translation With Semantic Consistency": {"container_type": "Publication", "bib": {"title": "Exemplar guided unsupervised image-to-image translation with semantic consistency", "author": ["L Ma", "X Jia", "S Georgoulis", "T Tuytelaars"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Image-to-image translation has recently received significant attention due to advances in deep learning. Most works focus on learning either a one-to-one mapping in an unsupervised way or a many-to-many mapping in a supervised way. However, a more practical setting is many-to-many mapping in an unsupervised way, which is harder due to the lack of supervision and the complex inner-and cross-domain variations. To alleviate these issues, we propose the Exemplar Guided & Semantically Consistent Image-to-image"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11145", "author_id": ["FTWcemsAAAAJ", "0YI9rvsAAAAJ", "a5GBXkEAAAAJ", "EuFF9kUAAAAJ"], "url_scholarbib": "/scholar?q=info:-93u09MxUVYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExemplar%2BGuided%2BUnsupervised%2BImage-to-image%2BTranslation%2BWith%2BSemantic%2BConsistency%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-93u09MxUVYJ&ei=WGFeYtilApHKsQKNt6-YAw&json=", "num_citations": 105, "citedby_url": "/scholar?cites=6219807346238873083&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-93u09MxUVYJ:scholar.google.com/&scioq=Exemplar+Guided+Unsupervised+Image-to-image+Translation+With+Semantic+Consistency&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11145"}, "Unsupervised Hyper-alignment For Multilingual Word Embeddings": {"container_type": "Publication", "bib": {"title": "Unsupervised hyperalignment for multilingual word embeddings", "author": ["J Alaux", "E Grave", "M Cuturi", "A Joulin"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.01124", "abstract": "We consider the problem of aligning continuous word representations, learned in multiple languages, to a common space. It was recently shown that, in the case of two languages, it is possible to learn such a mapping without supervision. This paper extends this line of work to the problem of aligning multiple languages to a common space. A solution is to independently map all languages to a pivot language. Unfortunately, this degrades the quality of indirect word translation. We thus propose a novel formulation that ensures"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.01124", "author_id": ["", "7UV4ET4AAAAJ", "kQEydDMAAAAJ", "kRJkDakAAAAJ"], "url_scholarbib": "/scholar?q=info:TyznM2d2f4QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BHyper-alignment%2BFor%2BMultilingual%2BWord%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TyznM2d2f4QJ&ei=WmFeYsf0OI-bmAGmiqCIBw&json=", "num_citations": 50, "citedby_url": "/scholar?cites=9547479920673238095&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TyznM2d2f4QJ:scholar.google.com/&scioq=Unsupervised+Hyper-alignment+For+Multilingual+Word+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.01124"}, "On The Minimal Supervision For Training Any Binary Classifier From Only Unlabeled Data": {"container_type": "Publication", "bib": {"title": "On the minimal supervision for training any binary classifier from only unlabeled data", "author": ["N Lu", "G Niu", "AK Menon", "M Sugiyama"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1808.10585", "abstract": "Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification. In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM. We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors. These two facts answer a fundamental question---what the minimal supervision is for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.10585", "author_id": ["KQUQlG4AAAAJ", "HOkcy00AAAAJ", "li4mEfcAAAAJ", "GkYIrlIAAAAJ"], "url_scholarbib": "/scholar?q=info:ysfdRPqlUK8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BMinimal%2BSupervision%2BFor%2BTraining%2BAny%2BBinary%2BClassifier%2BFrom%2BOnly%2BUnlabeled%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ysfdRPqlUK8J&ei=YGFeYsvUFcLZmQHc1ovQAg&json=", "num_citations": 54, "citedby_url": "/scholar?cites=12632779449090033610&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ysfdRPqlUK8J:scholar.google.com/&scioq=On+The+Minimal+Supervision+For+Training+Any+Binary+Classifier+From+Only+Unlabeled+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.10585"}, "Deepobs: A Deep Learning Optimizer Benchmark Suite": {"container_type": "Publication", "bib": {"title": "DeepOBS: A deep learning optimizer benchmark suite", "author": ["F Schneider", "L Balles", "P Hennig"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.05499", "abstract": "Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.05499", "author_id": ["znq-WkAAAAAJ", "2lq9JQIAAAAJ", "UeG5w08AAAAJ"], "url_scholarbib": "/scholar?q=info:xB7w9lyo6JMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeepobs:%2BA%2BDeep%2BLearning%2BOptimizer%2BBenchmark%2BSuite%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xB7w9lyo6JMJ&ei=aWFeYpSTIZLeyQTms5KQBg&json=", "num_citations": 27, "citedby_url": "/scholar?cites=10657953635405668036&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xB7w9lyo6JMJ:scholar.google.com/&scioq=Deepobs:+A+Deep+Learning+Optimizer+Benchmark+Suite&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.05499"}, "Big-little Net: An Efficient Multi-scale Feature Representation For Visual And Speech Recognition": {"container_type": "Publication", "bib": {"title": "Big-little net: An efficient multi-scale feature representation for visual and speech recognition", "author": ["CF Chen", "Q Fan", "N Mallinar", "T Sercu"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.03848", "author_id": ["9gqd5cYAAAAJ", "kCxHiwUAAAAJ", "6ogHsLsAAAAJ", "FMJePIUAAAAJ"], "url_scholarbib": "/scholar?q=info:gF1xCbv4tgcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBig-little%2BNet:%2BAn%2BEfficient%2BMulti-scale%2BFeature%2BRepresentation%2BFor%2BVisual%2BAnd%2BSpeech%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gF1xCbv4tgcJ&ei=gmFeYv6cF4vMsQK69Y7ABg&json=", "num_citations": 54, "citedby_url": "/scholar?cites=555905086227832192&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gF1xCbv4tgcJ:scholar.google.com/&scioq=Big-little+Net:+An+Efficient+Multi-scale+Feature+Representation+For+Visual+And+Speech+Recognition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.03848"}, "Building Dynamic Knowledge Graphs From Text Using Machine Reading Comprehension": {"container_type": "Publication", "bib": {"title": "Building dynamic knowledge graphs from text using machine reading comprehension", "author": ["R Das", "T Munkhdalai", "X Yuan", "A Trischler"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a neural machine-reading model that constructs dynamic knowledge graphs from procedural text. It builds these graphs recurrently for each step of the described procedure, and uses them to track the evolving states of participant entities. We harness and extend a recently proposed machine reading comprehension (MRC) model to query for entity states, since these states are generally communicated in spans of text and MRC models perform well in extracting entity-centric spans. The explicit, structured, and evolving"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05682", "author_id": ["FKoKAwIAAAAJ", "-fHyrYQAAAAJ", "dS3LulEAAAAJ", "EvUM6UUAAAAJ"], "url_scholarbib": "/scholar?q=info:h5ge8v6wp10J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBuilding%2BDynamic%2BKnowledge%2BGraphs%2BFrom%2BText%2BUsing%2BMachine%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=h5ge8v6wp10J&ei=hWFeYtC-H4vMsQK69Y7ABg&json=", "num_citations": 67, "citedby_url": "/scholar?cites=6748557175668250759&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:h5ge8v6wp10J:scholar.google.com/&scioq=Building+Dynamic+Knowledge+Graphs+From+Text+Using+Machine+Reading+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05682"}, "Meta-learning For Stochastic Gradient Mcmc": {"container_type": "Publication", "bib": {"title": "Meta-learning for stochastic gradient MCMC", "author": ["W Gong", "Y Li", "JM Hern\u00e1ndez-Lobato"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.04522", "abstract": "Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling. However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition. This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler. The learned sampler"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.04522", "author_id": ["qwYMW84AAAAJ", "gcfs8N8AAAAJ", "BEBccCQAAAAJ"], "url_scholarbib": "/scholar?q=info:OMuEUcm8F0kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BFor%2BStochastic%2BGradient%2BMcmc%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OMuEUcm8F0kJ&ei=iWFeYrq0FYryyASen4NI&json=", "num_citations": 32, "citedby_url": "/scholar?cites=5266885862075190072&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OMuEUcm8F0kJ:scholar.google.com/&scioq=Meta-learning+For+Stochastic+Gradient+Mcmc&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.04522"}, "Unsupervised Adversarial Image Reconstruction": {"container_type": "Publication", "bib": {"title": "Unsupervised adversarial image reconstruction", "author": ["A Pajot", "E De B\u00e9zenac", "P Gallinari"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "We address the problem of recovering an underlying signal from lossy, inaccurate observations in an unsupervised setting. Typically, we consider situations where there is little to no background knowledge on the structure of the underlying signal, no access to signal-measurement pairs, nor even unpaired signal-measurement data. The only available information is provided by the observations and the measurement process statistics. We cast the problem as finding the\\textit {maximum a posteriori} estimate of the signal given each"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJg4Z3RqF7", "author_id": ["9ff0JKYAAAAJ", "KvZw5gYAAAAJ", "rFaxB20AAAAJ"], "url_scholarbib": "/scholar?q=info:2AurVKiWNYEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BAdversarial%2BImage%2BReconstruction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2AurVKiWNYEJ&ei=jGFeYuK2DJHKsQKNt6-YAw&json=", "num_citations": 24, "citedby_url": "/scholar?cites=9310513454377536472&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2AurVKiWNYEJ:scholar.google.com/&scioq=Unsupervised+Adversarial+Image+Reconstruction&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJg4Z3RqF7"}, "Representing Formal Languages: A Comparison Between Finite Automata And Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Representing formal languages: A comparison between finite automata and recurrent neural networks", "author": ["JJ Michalenko"], "pub_year": "2019", "venue": "NA", "abstract": "Houston, Texas May, 2019 ef eg epresenting porm lv ngu gesX e gomp rison fetween pinite eutom t nd e urrent xeur l xetworks y toshu tF wi h lenko e investig te the intern l represent tions th t re urrent neur l network@ xxA uses while le rning to re ognize regul r form ll ngu geF pe i llyD we tr in xx on positive nd neg tive ex mples from regul rl ngu geD nd sk if there is simple de oding fun tion th tm ps st tes of this xx to st tes of the minim l deterministi finite utom ton@ whpeA for the l ngu geF yur experiments show th t su h de oding fun tion indeed"}, "filled": false, "gsrank": 1, "pub_url": "https://search.proquest.com/openview/10ead37e9e6c1943dfb2bcaf894b535d/1?pq-origsite=gscholar&cbl=18750&diss=y", "author_id": [""], "url_scholarbib": "/scholar?q=info:EiIMfzzLMhgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRepresenting%2BFormal%2BLanguages:%2BA%2BComparison%2BBetween%2BFinite%2BAutomata%2BAnd%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EiIMfzzLMhgJ&ei=kGFeYs3KAYvMsQK69Y7ABg&json=", "num_citations": 19, "citedby_url": "/scholar?cites=1743679466435781138&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EiIMfzzLMhgJ:scholar.google.com/&scioq=Representing+Formal+Languages:+A+Comparison+Between+Finite+Automata+And+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.10297"}, "Ad-vat: An Asymmetric Dueling Mechanism For Learning Visual Active Tracking": {"container_type": "Publication", "bib": {"title": "AD-VAT: An asymmetric dueling mechanism for learning visual active tracking", "author": ["F Zhong", "P Sun", "W Luo", "T Yan"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios. However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker's generalization on the unseen object moving patterns. To learn a robust tracker for VAT, in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkgYmhR9KX", "author_id": ["ejDz1bYAAAAJ", "", "g20Q12MAAAAJ", "xw0ai_MAAAAJ"], "url_scholarbib": "/scholar?q=info:5ksXAXgKSJYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAd-vat:%2BAn%2BAsymmetric%2BDueling%2BMechanism%2BFor%2BLearning%2BVisual%2BActive%2BTracking%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5ksXAXgKSJYJ&ei=k2FeYua6I4vMsQK69Y7ABg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=10828916814543014886&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5ksXAXgKSJYJ:scholar.google.com/&scioq=Ad-vat:+An+Asymmetric+Dueling+Mechanism+For+Learning+Visual+Active+Tracking&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkgYmhR9KX"}, "Code2seq: Generating Sequences From Structured Representations Of Code": {"container_type": "Publication", "bib": {"title": "code2seq: Generating sequences from structured representations of code", "author": ["U Alon", "S Brody", "O Levy", "E Yahav"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1808.01400", "abstract": "The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present ${\\rm {\\scriptsize CODE2SEQ}} $: an alternative approach that leverages the syntactic structure of programming languages to better encode source code"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.01400", "author_id": ["QBn7vq8AAAAJ", "02jdt98AAAAJ", "PZVd2h8AAAAJ", "grAfX0MAAAAJ"], "url_scholarbib": "/scholar?q=info:I2g5lGmvAc4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCode2seq:%2BGenerating%2BSequences%2BFrom%2BStructured%2BRepresentations%2BOf%2BCode%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=I2g5lGmvAc4J&ei=nmFeYqedFd-Vy9YPs66ekAk&json=", "num_citations": 355, "citedby_url": "/scholar?cites=14844338714783082531&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:I2g5lGmvAc4J:scholar.google.com/&scioq=Code2seq:+Generating+Sequences+From+Structured+Representations+Of+Code&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.01400?ref=https://codemonkey.link"}, "Unsupervised Control Through Non-parametric Discriminative Rewards": {"container_type": "Publication", "bib": {"title": "Unsupervised control through non-parametric discriminative rewards", "author": ["D Warde-Farley", "T Van de Wiele", "T Kulkarni"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptually-specified goals using only a stream of observations and actions. Our agent simultaneously learns a goal-conditioned policy and a goal achievement reward function that measures how similar a state is to the goal state. This dual optimization leads to a co-operative game, giving rise to a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.11359", "author_id": ["MOgfm8oAAAAJ", "lndc6AQAAAAJ", "rrPyvsgAAAAJ"], "url_scholarbib": "/scholar?q=info:p8kvtYlBvZgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BControl%2BThrough%2BNon-parametric%2BDiscriminative%2BRewards%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p8kvtYlBvZgJ&ei=oWFeYvagFpHKsQKNt6-YAw&json=", "num_citations": 102, "citedby_url": "/scholar?cites=11006025124069493159&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p8kvtYlBvZgJ:scholar.google.com/&scioq=Unsupervised+Control+Through+Non-parametric+Discriminative+Rewards&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.11359"}, "Active Learning With Partial Feedback": {"container_type": "Publication", "bib": {"title": "Active learning with partial feedback", "author": ["P Hu", "ZC Lipton", "A Anandkumar"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback). To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available. To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.07427", "author_id": ["SEAowdYAAAAJ", "MN9Kfg8AAAAJ", "bEcLezcAAAAJ"], "url_scholarbib": "/scholar?q=info:53sqGoirPycJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DActive%2BLearning%2BWith%2BPartial%2BFeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=53sqGoirPycJ&ei=pWFeYqvcFZHKsQKNt6-YAw&json=", "num_citations": 38, "citedby_url": "/scholar?cites=2828167692054854631&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:53sqGoirPycJ:scholar.google.com/&scioq=Active+Learning+With+Partial+Feedback&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.07427"}, "Deep Learning Generalizes Because The Parameter-function Map Is Biased Towards Simple Functions": {"container_type": "Publication", "bib": {"title": "Deep learning generalizes because the parameter-function map is biased towards simple functions", "author": ["G Valle-Perez", "CQ Camargo", "AA Louis"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.08522", "abstract": "Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.08522", "author_id": ["3hzE2UYAAAAJ", "FT9VxuwAAAAJ", "akIfLQ0AAAAJ"], "url_scholarbib": "/scholar?q=info:fEFhXfJvVtgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2BGeneralizes%2BBecause%2BThe%2BParameter-function%2BMap%2BIs%2BBiased%2BTowards%2BSimple%2BFunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fEFhXfJvVtgJ&ei=qWFeYpuCBcLZmQHc1ovQAg&json=", "num_citations": 86, "citedby_url": "/scholar?cites=15588770246928974204&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fEFhXfJvVtgJ:scholar.google.com/&scioq=Deep+Learning+Generalizes+Because+The+Parameter-function+Map+Is+Biased+Towards+Simple+Functions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.08522"}, "Diffusion Scattering Transforms On Graphs": {"container_type": "Publication", "bib": {"title": "Diffusion scattering transforms on graphs", "author": ["F Gama", "A Ribeiro", "J Bruna"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.08829", "abstract": "Stability is a key aspect of data analysis. In many applications, the natural notion of stability is geometric, as illustrated for example in computer vision. Scattering transforms construct deep convolutional representations which are certified stable to input deformations. This stability to deformations can be interpreted as stability with respect to changes in the metric structure of the domain. In this work, we show that scattering transforms can be generalized to non-Euclidean domains using diffusion wavelets, while preserving a notion of stability"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.08829", "author_id": ["DwYn408AAAAJ", "7mrPM4kAAAAJ", "L4bNmsMAAAAJ"], "url_scholarbib": "/scholar?q=info:UACH_i2RBm0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiffusion%2BScattering%2BTransforms%2BOn%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UACH_i2RBm0J&ei=rWFeYszMFOHDywTjooCQBQ&json=", "num_citations": 60, "citedby_url": "/scholar?cites=7856126226724225104&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UACH_i2RBm0J:scholar.google.com/&scioq=Diffusion+Scattering+Transforms+On+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.08829.pdf?ref=https://githubhelp.com"}, "Feature Intertwiner For Object Detection": {"container_type": "Publication", "bib": {"title": "Feature intertwiner for object detection", "author": ["H Li", "B Dai", "S Shi", "W Ouyang", "X Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.11851", "abstract": "A well-trained model should classify objects with a unanimous score for every category. This requires the high-level semantic features should be as much alike as possible among samples. To achive this, previous works focus on re-designing the loss or proposing new regularization constraints. In this paper, we provide a new perspective. For each category, it is assumed that there are two feature sets: one with reliable information and the other with less reliable source. We argue that the reliable set could guide the feature learning of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.11851", "author_id": ["Hfrih1EAAAAJ", "KNWTvgEAAAAJ", "DC9wzBgAAAAJ", "pw_0Z_UAAAAJ", "-B5JgjsAAAAJ"], "url_scholarbib": "/scholar?q=info:EtisXKVEexIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeature%2BIntertwiner%2BFor%2BObject%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EtisXKVEexIJ&ei=sGFeYt_vLs6E6rQPz8uiuAc&json=", "num_citations": 11, "citedby_url": "/scholar?cites=1331733591833237522&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EtisXKVEexIJ:scholar.google.com/&scioq=Feature+Intertwiner+For+Object+Detection&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.11851.pdf?ref=https://githubhelp.com"}, "Don't Let Your Discriminator Be Fooled": {"container_type": "Publication", "bib": {"title": "Don't let your Discriminator be fooled", "author": ["B Zhou", "P Kr\u00e4henb\u00fchl"], "pub_year": "2018", "venue": "International conference on learning \u2026", "abstract": "Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel. Some of the most promising adversarial models today minimize a Wasserstein objective. It is smoother and more stable to optimize. In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties. By making the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJE6X305Fm", "author_id": ["K-k47CMAAAAJ", "dzOd2hgAAAAJ"], "url_scholarbib": "/scholar?q=info:DQugFexWbW8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDon%2527t%2BLet%2BYour%2BDiscriminator%2BBe%2BFooled%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DQugFexWbW8J&ei=tGFeYtG7GZWMy9YPt8OamA0&json=", "num_citations": 16, "citedby_url": "/scholar?cites=8029169282646543117&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DQugFexWbW8J:scholar.google.com/&scioq=Don%27t+Let+Your+Discriminator+Be+Fooled&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJE6X305Fm"}, "Music Transformer: Generating Music With Long-term Structure": {"container_type": "Publication", "bib": {"title": "Music transformer", "author": ["CZA Huang", "A Vaswani", "J Uszkoreit", "N Shazeer"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "successful use of Transformers in generating music that exhibits long-term structure. Before  our  The compelling long-term structure in the samples from our model leaves us enthusiastic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.04281", "author_id": ["NRz_EVgAAAAJ", "oR9sCGYAAAAJ", "mOG0bwsAAAAJ", "wsGvgA8AAAAJ"], "url_scholarbib": "/scholar?q=info:Mi0sBQhVHq4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMusic%2BTransformer:%2BGenerating%2BMusic%2BWith%2BLong-term%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Mi0sBQhVHq4J&ei=t2FeYoitIJWMy9YPt8OamA0&json=", "num_citations": 372, "citedby_url": "/scholar?cites=12546559104835661106&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Mi0sBQhVHq4J:scholar.google.com/&scioq=Music+Transformer:+Generating+Music+With+Long-term+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.04281.pdf?ref=https://githubhelp.com"}, "Invase: Instance-wise Variable Selection Using Neural Networks": {"container_type": "Publication", "bib": {"title": "INVASE: Instance-wise variable selection using neural networks", "author": ["J Yoon", "J Jordon", "M van der Schaar"], "pub_year": "2018", "venue": "International Conference on \u2026", "abstract": "The advent of big data brings with it data with more and more dimensions and thus a growing need to be able to efficiently select which features to use for a variety of problems. While global feature selection has been a well-studied problem for quite some time, only recently has the paradigm of instance-wise feature selection been developed. In this paper, we propose a new instance-wise feature selection method, which we term INVASE. INVASE consists of 3 neural networks, a selector network, a predictor network and a baseline"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJg_roAcK7", "author_id": ["kiFd6A8AAAAJ", "jiyonF0AAAAJ", "DZ3S--MAAAAJ"], "url_scholarbib": "/scholar?q=info:SuKiUaZsDC4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInvase:%2BInstance-wise%2BVariable%2BSelection%2BUsing%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SuKiUaZsDC4J&ei=vGFeYo-uM-HDywTjooCQBQ&json=", "num_citations": 83, "citedby_url": "/scholar?cites=3318146487055213130&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SuKiUaZsDC4J:scholar.google.com/&scioq=Invase:+Instance-wise+Variable+Selection+Using+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJg_roAcK7"}, "Hyperbolic Attention Networks": {"container_type": "Publication", "bib": {"title": "Hyperbolic attention networks", "author": ["C Gulcehre", "M Denil", "M Malinowski", "A Razavi"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We   hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.09786", "author_id": ["7hwJ2ckAAAAJ", "XrKLUO0AAAAJ", "IqJ3zskAAAAJ", "LpxwpFAAAAAJ"], "url_scholarbib": "/scholar?q=info:leZugkoAIxYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHyperbolic%2BAttention%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=leZugkoAIxYJ&ei=wWFeYsuJEpHKsQKNt6-YAw&json=", "num_citations": 136, "citedby_url": "/scholar?cites=1595119013035173525&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:leZugkoAIxYJ:scholar.google.com/&scioq=Hyperbolic+Attention+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.09786?ref=https://githubhelp.com"}, "A Kernel Random Matrix-based Approach For Sparse Pca": {"container_type": "Publication", "bib": {"title": "A kernel random matrix-based approach for sparse PCA", "author": ["MEA Seddik", "M Tamaazousti"], "pub_year": "2019", "venue": "ICLR 2019 \u2026", "abstract": "In this paper, we present a random matrix approach to recover sparse principal components from n p-dimensional vectors. Specifically, considering the large dimensional setting where n, p\u2192\u221e with p/n\u2192 c\u2208(0,\u221e) and under Gaussian vector observations, we study kernel random matrices of the type f (\u0108), where f is a three-times continuously differentiable function applied entry-wise to the sample covariance matrix\u0108 of the data. Then, assuming that the principal components are sparse, we show that taking f in such a way that f (0)= f (0)= 0"}, "filled": false, "gsrank": 1, "pub_url": "https://hal.archives-ouvertes.fr/hal-02971198/", "author_id": ["85Hxd24AAAAJ", "GECYQPMAAAAJ"], "url_scholarbib": "/scholar?q=info:IPKF1sNHWMgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BKernel%2BRandom%2BMatrix-based%2BApproach%2BFor%2BSparse%2BPca%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IPKF1sNHWMgJ&ei=xGFeYqW8GpGJmwGIxre4DA&json=", "num_citations": 12, "citedby_url": "/scholar?cites=14436367511979422240&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IPKF1sNHWMgJ:scholar.google.com/&scioq=A+Kernel+Random+Matrix-based+Approach+For+Sparse+Pca&hl=en&as_sdt=0,33", "eprint_url": "https://hal.archives-ouvertes.fr/hal-02971198/file/2019Ciclr.pdf"}, "Adaptive Gradient Methods With Dynamic Bound Of Learning Rate": {"container_type": "Publication", "bib": {"title": "Adaptive gradient methods with dynamic bound of learning rate", "author": ["L Luo", "Y Xiong", "Y Liu", "X Sun"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09843", "abstract": "Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09843", "author_id": ["8ei4_E4AAAAJ", "DVKxiMkAAAAJ", "UUKLPMYAAAAJ", "tpXiQkYAAAAJ"], "url_scholarbib": "/scholar?q=info:CsrDHbimhWgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BGradient%2BMethods%2BWith%2BDynamic%2BBound%2BOf%2BLearning%2BRate%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CsrDHbimhWgJ&ei=x2FeYqXWJJGJmwGIxre4DA&json=", "num_citations": 424, "citedby_url": "/scholar?cites=7531609261550586378&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CsrDHbimhWgJ:scholar.google.com/&scioq=Adaptive+Gradient+Methods+With+Dynamic+Bound+Of+Learning+Rate&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09843"}, "Max-mig: An Information Theoretic Approach For Joint Learning From Crowds": {"container_type": "Publication", "bib": {"title": "Max-mig: an information theoretic approach for joint learning from crowds", "author": ["P Cao", "Y Xu", "Y Kong", "Y Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.13436", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved:\\emph {learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (eg randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.13436", "author_id": ["UlaXT00AAAAJ", "MU6vjxQAAAAJ", "U5k-29kAAAAJ", ""], "url_scholarbib": "/scholar?q=info:8migNE-2FNAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMax-mig:%2BAn%2BInformation%2BTheoretic%2BApproach%2BFor%2BJoint%2BLearning%2BFrom%2BCrowds%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8migNE-2FNAJ&ei=y2FeYt6ZC4-bmAGmiqCIBw&json=", "num_citations": 16, "citedby_url": "/scholar?cites=14993809510724823282&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8migNE-2FNAJ:scholar.google.com/&scioq=Max-mig:+An+Information+Theoretic+Approach+For+Joint+Learning+From+Crowds&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.13436"}, "Som-vae: Interpretable Discrete Representation Learning On Time Series": {"container_type": "Publication", "bib": {"title": "Som-vae: Interpretable discrete representation learning on time series", "author": ["V Fortuin", "M H\u00fcser", "F Locatello", "H Strathmann"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.02199", "author_id": ["XBlrYTIAAAAJ", "UDPYYlgAAAAJ", "wQanfTIAAAAJ", "QFseZ2gAAAAJ"], "url_scholarbib": "/scholar?q=info:9K-tWMKJgYgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSom-vae:%2BInterpretable%2BDiscrete%2BRepresentation%2BLearning%2BOn%2BTime%2BSeries%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9K-tWMKJgYgJ&ei=zmFeYqqOEoyuyASD3KfABw&json=", "num_citations": 85, "citedby_url": "/scholar?cites=9836294528958312436&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9K-tWMKJgYgJ:scholar.google.com/&scioq=Som-vae:+Interpretable+Discrete+Representation+Learning+On+Time+Series&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.02199"}, "Large Scale Gan Training For High Fidelity Natural Image Synthesis": {"container_type": "Publication", "bib": {"title": "Large scale GAN training for high fidelity natural image synthesis", "author": ["A Brock", "J Donahue", "K Simonyan"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.11096", "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple\" truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.11096", "author_id": ["NIxD36wAAAAJ", "UfbuDH8AAAAJ", "L7lMQkQAAAAJ"], "url_scholarbib": "/scholar?q=info:_L-o1VgS3YQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2BScale%2BGan%2BTraining%2BFor%2BHigh%2BFidelity%2BNatural%2BImage%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_L-o1VgS3YQJ&ei=1mFeYseeHcLZmQHc1ovQAg&json=", "num_citations": 2778, "citedby_url": "/scholar?cites=9573828555610570748&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_L-o1VgS3YQJ:scholar.google.com/&scioq=Large+Scale+Gan+Training+For+High+Fidelity+Natural+Image+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.11096.pdf%20http://arxiv.org/abs/1809.11096"}, "Structured Adversarial Attack: Towards General Implementation And Better Interpretability": {"container_type": "Publication", "bib": {"title": "Structured adversarial attack: Towards general implementation and better interpretability", "author": ["K Xu", "S Liu", "P Zhao", "PY Chen", "H Zhang", "Q Fan"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "When generating adversarial examples to attack deep neural networks (DNNs), Lp norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, ie, the structured attack (StrAttack), which explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.01664", "author_id": ["lYK0wlsAAAAJ", "C7dO_UgAAAAJ", "rWZLnpwAAAAJ", "jxwlCUUAAAAJ", "LTa3GzEAAAAJ", "kCxHiwUAAAAJ"], "url_scholarbib": "/scholar?q=info:7EOipt7BiiEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructured%2BAdversarial%2BAttack:%2BTowards%2BGeneral%2BImplementation%2BAnd%2BBetter%2BInterpretability%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7EOipt7BiiEJ&ei=2mFeYpCbLJWMy9YPt8OamA0&json=", "num_citations": 124, "citedby_url": "/scholar?cites=2416957312060244972&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7EOipt7BiiEJ:scholar.google.com/&scioq=Structured+Adversarial+Attack:+Towards+General+Implementation+And+Better+Interpretability&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.01664"}, "Learning Sparse Relational Transition Models": {"container_type": "Publication", "bib": {"title": "Learning sparse relational transition models", "author": ["V Xia", "Z Wang", "LP Kaelbling"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.11177", "abstract": "We present a representation for describing transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct a set of deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects' properties. This"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.11177", "author_id": ["", "U0egIsIAAAAJ", "IcasIiwAAAAJ"], "url_scholarbib": "/scholar?q=info:pZqpFI9x-24J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BSparse%2BRelational%2BTransition%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pZqpFI9x-24J&ei=3mFeYsvrJJHKsQKNt6-YAw&json=", "num_citations": 18, "citedby_url": "/scholar?cites=7997110422667369125&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pZqpFI9x-24J:scholar.google.com/&scioq=Learning+Sparse+Relational+Transition+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.11177"}, "Eidetic 3d Lstm: A Model For Video Prediction And Beyond": {"container_type": "Publication", "bib": {"title": "Eidetic 3d lstm: A model for video prediction and beyond", "author": ["Y Wang", "L Jiang", "MH Yang", "LJ Li", "M Long"], "pub_year": "2018", "venue": "International \u2026", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1lKS2AqtX", "author_id": ["C8bGfr0AAAAJ", "jIKjjSYAAAAJ", "p9-ohHsAAAAJ", "feX1fWAAAAAJ", "_MjXpXkAAAAJ"], "url_scholarbib": "/scholar?q=info:8Y16tzM1HBUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEidetic%2B3d%2BLstm:%2BA%2BModel%2BFor%2BVideo%2BPrediction%2BAnd%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8Y16tzM1HBUJ&ei=4WFeYrLAHM2Ny9YPqPyUgAs&json=", "num_citations": 163, "citedby_url": "/scholar?cites=1521149270382251505&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8Y16tzM1HBUJ:scholar.google.com/&scioq=Eidetic+3d+Lstm:+A+Model+For+Video+Prediction+And+Beyond&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1lKS2AqtX"}, "Label Super-resolution Networks": {"container_type": "Publication", "bib": {"title": "Label super-resolution networks", "author": ["K Malkin", "C Robinson", "L Hou", "R Soobitsky"], "pub_year": "2018", "venue": "International \u2026", "abstract": "our network fails to produce the label super-resolution results described in the main text.  While the bounds may satisfied, the true distributions of high-res classes given NLCD labels is"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkxwShA9Ym", "author_id": ["S_Nn-XYAAAAJ", "cjYgLT0AAAAJ", "kQ0HeQIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:sdND4eYFxwgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLabel%2BSuper-resolution%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sdND4eYFxwgJ&ei=5WFeYqiAIM6E6rQPz8uiuAc&json=", "num_citations": 19, "citedby_url": "/scholar?cites=632480761848779697&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sdND4eYFxwgJ:scholar.google.com/&scioq=Label+Super-resolution+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkxwShA9Ym"}, "Noodl: Provable Online Dictionary Learning And Sparse Coding": {"container_type": "Publication", "bib": {"title": "NOODL: Provable online dictionary learning and sparse coding", "author": ["S Rambhatla", "X Li", "J Haupt"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.11261", "abstract": "We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.11261", "author_id": ["EOSZeBMAAAAJ", "MkLPs5EAAAAJ", "BXJ3LwEAAAAJ"], "url_scholarbib": "/scholar?q=info:zJ9oXHn9hK8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNoodl:%2BProvable%2BOnline%2BDictionary%2BLearning%2BAnd%2BSparse%2BCoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zJ9oXHn9hK8J&ei=6GFeYsHKK5WMy9YPt8OamA0&json=", "num_citations": 14, "citedby_url": "/scholar?cites=12647512351246426060&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zJ9oXHn9hK8J:scholar.google.com/&scioq=Noodl:+Provable+Online+Dictionary+Learning+And+Sparse+Coding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.11261"}, "Kernel Change-point Detection With Auxiliary Deep Generative Models": {"container_type": "Publication", "bib": {"title": "Kernel change-point detection with auxiliary deep generative models", "author": ["WC Chang", "CL Li", "Y Yang", "B P\u00f3czos"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.06077", "abstract": "Detecting the emergence of abrupt property changes in time series is a challenging problem. Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches. However, selecting kernels is non-trivial in practice. Although kernel selection for two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms. In this paper, we propose KL-CPD, a novel"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.06077", "author_id": ["nxBGx-0AAAAJ", "vqHIt_sAAAAJ", "MlZq4XwAAAAJ", "sUriZlUAAAAJ"], "url_scholarbib": "/scholar?q=info:vxKEjoRKMdUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKernel%2BChange-point%2BDetection%2BWith%2BAuxiliary%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vxKEjoRKMdUJ&ei=7WFeYtLBA9-Vy9YPs66ekAk&json=", "num_citations": 31, "citedby_url": "/scholar?cites=15362141737124631231&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vxKEjoRKMdUJ:scholar.google.com/&scioq=Kernel+Change-point+Detection+With+Auxiliary+Deep+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.06077"}, "Variational Bayesian Phylogenetic Inference": {"container_type": "Publication", "bib": {"title": "Variational Bayesian phylogenetic inference", "author": ["C Zhang", "FA Matsen IV"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates. In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis. We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJVmjjR9FX", "author_id": ["-o1HbVYAAAAJ", "BuHO6ssAAAAJ"], "url_scholarbib": "/scholar?q=info:e9WxYLrh_1YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BBayesian%2BPhylogenetic%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e9WxYLrh_1YJ&ei=8GFeYqb3D4-bmAGmiqCIBw&json=", "num_citations": 12, "citedby_url": "/scholar?cites=6268977396925453691&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e9WxYLrh_1YJ:scholar.google.com/&scioq=Variational+Bayesian+Phylogenetic+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJVmjjR9FX"}, "Spreading Vectors For Similarity Search": {"container_type": "Publication", "bib": {"title": "Spreading vectors for similarity search", "author": ["A Sablayrolles", "M Douze", "C Schmid"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.03198", "author_id": ["Wy8wM-cAAAAJ", "0eFZtREAAAAJ", "IvqCXP4AAAAJ"], "url_scholarbib": "/scholar?q=info:jAYSWKPHz20J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpreading%2BVectors%2BFor%2BSimilarity%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jAYSWKPHz20J&ei=9WFeYs3VB8LZmQHc1ovQAg&json=", "num_citations": 51, "citedby_url": "/scholar?cites=7912762574684423820&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jAYSWKPHz20J:scholar.google.com/&scioq=Spreading+Vectors+For+Similarity+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.03198"}, "Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method For Image Restoration": {"container_type": "Publication", "bib": {"title": "Dynamically unfolding recurrent restorer: A moving endpoint control method for image restoration", "author": ["X Zhang", "Y Lu", "J Liu", "B Dong"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.07709", "abstract": "In this paper, we propose a new control framework called the moving endpoint control to restore images corrupted by different degradation levels in one model. The proposed control problem contains a restoration dynamics which is modeled by an RNN. The moving endpoint, which is essentially the terminal time of the associated dynamics, is determined by a policy network. We call the proposed model the dynamically unfolding recurrent restorer (DURR). Numerical experiments show that DURR is able to achieve state-of-the-art"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.07709", "author_id": ["cTGxuQQAAAAJ", "NmhvVBgAAAAJ", "-OcSne0AAAAJ", "zLXcC90AAAAJ"], "url_scholarbib": "/scholar?q=info:1v-32bDVAUsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamically%2BUnfolding%2BRecurrent%2BRestorer:%2BA%2BMoving%2BEndpoint%2BControl%2BMethod%2BFor%2BImage%2BRestoration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1v-32bDVAUsJ&ei=_WFeYoyHOoryyASen4NI&json=", "num_citations": 37, "citedby_url": "/scholar?cites=5404835983364980694&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1v-32bDVAUsJ:scholar.google.com/&scioq=Dynamically+Unfolding+Recurrent+Restorer:+A+Moving+Endpoint+Control+Method+For+Image+Restoration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.07709.pdf?ref=https://githubhelp.com"}, "Learning Particle Dynamics For Manipulating Rigid Bodies, Deformable Objects, And Fluids": {"container_type": "Publication", "bib": {"title": "Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids", "author": ["Y Li", "J Wu", "R Tedrake", "JB Tenenbaum"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Real-life control tasks involve matters of various substances---rigid or soft bodies, liquid, gas---each with distinct physical behaviors. This poses challenges to traditional rigid-body physics engines. Particle-based simulators have been developed to model the dynamics of these complex scenes; however, relying on approximation techniques, their simulation often deviates from real-world physics, especially in the long term. In this paper, we propose to learn a particle-based simulator for complex control tasks. Combining learning with particle"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01566", "author_id": ["WlA92lcAAAAJ", "2efgcS0AAAAJ", "nxNkEiYAAAAJ", "rRJ9wTJMUB8C"], "url_scholarbib": "/scholar?q=info:uM122Qb3exgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BParticle%2BDynamics%2BFor%2BManipulating%2BRigid%2BBodies,%2BDeformable%2BObjects,%2BAnd%2BFluids%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uM122Qb3exgJ&ei=AGJeYteZOI-bmAGmiqCIBw&json=", "num_citations": 158, "citedby_url": "/scholar?cites=1764275287835987384&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uM122Qb3exgJ:scholar.google.com/&scioq=Learning+Particle+Dynamics+For+Manipulating+Rigid+Bodies,+Deformable+Objects,+And+Fluids&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01566"}, "Unsupervised Learning Via Meta-learning": {"container_type": "Publication", "bib": {"title": "Unsupervised learning via meta-learning", "author": ["K Hsu", "S Levine", "C Finn"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.02334", "abstract": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02334", "author_id": ["KCdL5B0AAAAJ", "8R35rCwAAAAJ", "vfPE6hgAAAAJ"], "url_scholarbib": "/scholar?q=info:XQuuoUZquwAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2BVia%2BMeta-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XQuuoUZquwAJ&ei=A2JeYvrjKJWMy9YPt8OamA0&json=", "num_citations": 155, "citedby_url": "/scholar?cites=52752672237685597&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XQuuoUZquwAJ:scholar.google.com/&scioq=Unsupervised+Learning+Via+Meta-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02334"}, "Bayesian Prediction Of Future Street Scenes Using Synthetic Likelihoods": {"container_type": "Publication", "bib": {"title": "Bayesian prediction of future street scenes using synthetic likelihoods", "author": ["A Bhattacharyya", "M Fritz", "B Schiele"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00746", "abstract": "For autonomous agents to successfully operate in the real world, the ability to anticipate future scene states is a key competence. In real-world scenarios, future states become increasingly uncertain and multi-modal, particularly on long time horizons. Dropout based Bayesian inference provides a computationally tractable, theoretically well grounded approach to learn likely hypotheses/models to deal with uncertain futures and make predictions that correspond well to observations--are well calibrated. However, it turns out"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00746", "author_id": ["SKb4VyUAAAAJ", "4V1nNm4AAAAJ", "z76PBfYAAAAJ"], "url_scholarbib": "/scholar?q=info:eTdIuWukqnoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2BPrediction%2BOf%2BFuture%2BStreet%2BScenes%2BUsing%2BSynthetic%2BLikelihoods%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eTdIuWukqnoJ&ei=CGJeYsz3C5GJmwGIxre4DA&json=", "num_citations": 37, "citedby_url": "/scholar?cites=8839058001244993401&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eTdIuWukqnoJ:scholar.google.com/&scioq=Bayesian+Prediction+Of+Future+Street+Scenes+Using+Synthetic+Likelihoods&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00746"}, "Predict Then Propagate: Graph Neural Networks Meet Personalized Pagerank": {"container_type": "Publication", "bib": {"title": "Predict then propagate: Graph neural networks meet personalized pagerank", "author": ["J Klicpera", "A Bojchevski", "S G\u00fcnnemann"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.05997", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.05997", "author_id": ["QqdUw8MAAAAJ", "F1APiN4AAAAJ", "npqoAWwAAAAJ"], "url_scholarbib": "/scholar?q=info:ztLWR6fDYXwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPredict%2BThen%2BPropagate:%2BGraph%2BNeural%2BNetworks%2BMeet%2BPersonalized%2BPagerank%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ztLWR6fDYXwJ&ei=C2JeYpnxN4vMsQK69Y7ABg&json=", "num_citations": 495, "citedby_url": "/scholar?cites=8962659856676213454&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ztLWR6fDYXwJ:scholar.google.com/&scioq=Predict+Then+Propagate:+Graph+Neural+Networks+Meet+Personalized+Pagerank&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.05997?ref=https://githubhelp.com"}, "Cost-sensitive Robustness Against Adversarial Examples": {"container_type": "Publication", "bib": {"title": "Cost-sensitive robustness against adversarial examples", "author": ["X Zhang", "D Evans"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.09225", "abstract": "Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications. We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others. We encode the potential harm of each adversarial transformation in a cost"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.09225", "author_id": ["L-lz7CUAAAAJ", "DsR4PucAAAAJ"], "url_scholarbib": "/scholar?q=info:ahSaOybjZuAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCost-sensitive%2BRobustness%2BAgainst%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ahSaOybjZuAJ&ei=D2JeYsq7LsLZmQHc1ovQAg&json=", "num_citations": 17, "citedby_url": "/scholar?cites=16169861265468560490&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ahSaOybjZuAJ:scholar.google.com/&scioq=Cost-sensitive+Robustness+Against+Adversarial+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.09225"}, "Variance Networks: When Expectation Does Not Meet Your Expectations": {"container_type": "Publication", "bib": {"title": "Variance networks: When expectation does not meet your expectations", "author": ["K Neklyudov", "D Molchanov", "A Ashukha"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging. In this paper, we introduce variance layers, a different kind of stochastic layers. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. We show that such layers can learn surprisingly well, can serve as an efficient exploration"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1803.03764", "author_id": ["eOttYWgAAAAJ", "tJ6JXRYAAAAJ", "IU-kuP8AAAAJ"], "url_scholarbib": "/scholar?q=info:v8UBIaWtqTYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance%2BNetworks:%2BWhen%2BExpectation%2BDoes%2BNot%2BMeet%2BYour%2BExpectations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v8UBIaWtqTYJ&ei=E2JeYu-nPN-Vy9YPs66ekAk&json=", "num_citations": 18, "citedby_url": "/scholar?cites=3938870273847182783&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v8UBIaWtqTYJ:scholar.google.com/&scioq=Variance+Networks:+When+Expectation+Does+Not+Meet+Your+Expectations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1803.03764"}, "Antisymmetricrnn: A Dynamical System View On Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "AntisymmetricRNN: A dynamical system view on recurrent neural networks", "author": ["B Chang", "M Chen", "E Haber", "EH Chi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09689", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09689", "author_id": ["s9F-vo0AAAAJ", "kR7DersAAAAJ", "NZmEIS8AAAAJ", "VuWl-KUAAAAJ"], "url_scholarbib": "/scholar?q=info:n3XSiE3_NUsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAntisymmetricrnn:%2BA%2BDynamical%2BSystem%2BView%2BOn%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n3XSiE3_NUsJ&ei=F2JeYui_KciBy9YP18Gi8As&json=", "num_citations": 116, "citedby_url": "/scholar?cites=5419518435083318687&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n3XSiE3_NUsJ:scholar.google.com/&scioq=Antisymmetricrnn:+A+Dynamical+System+View+On+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09689"}, "How Powerful Are Graph Neural Networks?": {"container_type": "Publication", "bib": {"title": "How powerful are graph neural networks?", "author": ["K Xu", "W Hu", "J Leskovec", "S Jegelka"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.00826", "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00826", "author_id": ["eV2tuR8AAAAJ", "wAFMjfkAAAAJ", "Q_kKkIUAAAAJ", "gTWUZlsAAAAJ"], "url_scholarbib": "/scholar?q=info:N_ml42J6KooJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BPowerful%2BAre%2BGraph%2BNeural%2BNetworks%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N_ml42J6KooJ&ei=G2JeYs-hFYryyASen4NI&json=", "num_citations": 2588, "citedby_url": "/scholar?cites=9955904491400591671&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:N_ml42J6KooJ:scholar.google.com/&scioq=How+Powerful+Are+Graph+Neural+Networks%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00826"}, "How To Train Your Maml": {"container_type": "Publication", "bib": {"title": "How to train your MAML", "author": ["A Antoniou", "H Edwards", "A Storkey"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.09502", "abstract": "MAML is simple,  to MAML that not only stabilize the system, but also substantially improve  the generalization performance, convergence speed and computational overhead of MAML,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.09502", "author_id": ["LNO4XZQAAAAJ", "0o470HsAAAAJ", "3Rlc8EAAAAAJ"], "url_scholarbib": "/scholar?q=info:CdazevsUZrIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BTo%2BTrain%2BYour%2BMaml%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CdazevsUZrIJ&ei=H2JeYqvDMZHKsQKNt6-YAw&json=", "num_citations": 378, "citedby_url": "/scholar?cites=12854985256703612425&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CdazevsUZrIJ:scholar.google.com/&scioq=How+To+Train+Your+Maml&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.09502.pdf?ref=https://githubhelp.com"}, "Infobot: Transfer And Exploration Via The Information Bottleneck": {"container_type": "Publication", "bib": {"title": "Infobot: Transfer and exploration via the information bottleneck", "author": ["A Goyal", "R Islam", "D Strouse", "Z Ahmed"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.10902", "author_id": ["krrh6OUAAAAJ", "2_4Rs44AAAAJ", "K8E0T7MAAAAJ", "KZHIDrIAAAAJ"], "url_scholarbib": "/scholar?q=info:OJE9CoSRUzkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInfobot:%2BTransfer%2BAnd%2BExploration%2BVia%2BThe%2BInformation%2BBottleneck%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OJE9CoSRUzkJ&ei=ImJeYvOlL9-Vy9YPs66ekAk&json=", "num_citations": 95, "citedby_url": "/scholar?cites=4130805279522394424&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OJE9CoSRUzkJ:scholar.google.com/&scioq=Infobot:+Transfer+And+Exploration+Via+The+Information+Bottleneck&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.10902"}, "Gradient Descent Provably Optimizes Over-parameterized Neural Networks": {"container_type": "Publication", "bib": {"title": "Gradient descent provably optimizes over-parameterized neural networks", "author": ["SS Du", "X Zhai", "B Poczos", "A Singh"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.02054", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $ m $ hidden node shallow neural network with ReLU activation and $ n $ training data, we show as long as $ m $ is large enough and no two inputs are parallel, randomly initialized gradient descent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02054", "author_id": ["OttawxUAAAAJ", "", "sUriZlUAAAAJ", "vGBcNVAAAAAJ"], "url_scholarbib": "/scholar?q=info:w2VQe0wrz3AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient%2BDescent%2BProvably%2BOptimizes%2BOver-parameterized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w2VQe0wrz3AJ&ei=JWJeYq8nnI7qtA_6-IQQ&json=", "num_citations": 428, "citedby_url": "/scholar?cites=8128763459913409987&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w2VQe0wrz3AJ:scholar.google.com/&scioq=Gradient+Descent+Provably+Optimizes+Over-parameterized+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02054"}, "Equi-normalization Of Neural Networks": {"container_type": "Publication", "bib": {"title": "Equi-normalization of neural networks", "author": ["P Stock", "B Graham", "R Gribonval", "H J\u00e9gou"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and output weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.10416", "author_id": ["3e2-59cAAAAJ", "", "", "1lcY2z4AAAAJ"], "url_scholarbib": "/scholar?q=info:9cLPXfiN6GgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEqui-normalization%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9cLPXfiN6GgJ&ei=KmJeYtgjlYzL1g-3w5qYDQ&json=", "num_citations": 7, "citedby_url": "/scholar?cites=7559448072406680309&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9cLPXfiN6GgJ:scholar.google.com/&scioq=Equi-normalization+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.10416"}, "Quasi-hyperbolic Momentum And Adam For Deep Learning": {"container_type": "Publication", "bib": {"title": "Quasi-hyperbolic momentum and adam for deep learning", "author": ["J Ma", "D Yarats"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.06801", "abstract": "Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06801", "author_id": ["qukcWBAAAAAJ", "7kaXqgMAAAAJ"], "url_scholarbib": "/scholar?q=info:e1ryHwFmxDcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuasi-hyperbolic%2BMomentum%2BAnd%2BAdam%2BFor%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e1ryHwFmxDcJ&ei=N2JeYv3KBsLZmQHc1ovQAg&json=", "num_citations": 77, "citedby_url": "/scholar?cites=4018448922538302075&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e1ryHwFmxDcJ:scholar.google.com/&scioq=Quasi-hyperbolic+Momentum+And+Adam+For+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06801"}, "Minimal Random Code Learning: Getting Bits Back From Compressed Model Parameters": {"container_type": "Publication", "bib": {"title": "Minimal random code learning: Getting bits back from compressed model parameters", "author": ["M Havasi", "R Peharz", "JM Hern\u00e1ndez-Lobato"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.00440", "author_id": ["EaYZfmoAAAAJ", "ywkqnqMAAAAJ", "BEBccCQAAAAJ"], "url_scholarbib": "/scholar?q=info:CIS1QNdfSPkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimal%2BRandom%2BCode%2BLearning:%2BGetting%2BBits%2BBack%2BFrom%2BCompressed%2BModel%2BParameters%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CIS1QNdfSPkJ&ei=Q2JeYoOjHsiBy9YP18Gi8As&json=", "num_citations": 33, "citedby_url": "/scholar?cites=17962712491875468296&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CIS1QNdfSPkJ:scholar.google.com/&scioq=Minimal+Random+Code+Learning:+Getting+Bits+Back+From+Compressed+Model+Parameters&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.00440?ref=https://githubhelp.com"}, "Reward Constrained Policy Optimization": {"container_type": "Publication", "bib": {"title": "Reward constrained policy optimization", "author": ["C Tessler", "DJ Mankowitz", "S Mannor"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.11074", "abstract": "solution for general constraints. In this work, we present a novel multi-timescale approach  for constrained policy optimization, called \u2018Reward Constrained Policy Optimization\u2019 (RCPO),"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11074", "author_id": ["7eLKa3IAAAAJ", "v84tWxsAAAAJ", "q1HlbIUAAAAJ"], "url_scholarbib": "/scholar?q=info:ow3xeGhOWnYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReward%2BConstrained%2BPolicy%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ow3xeGhOWnYJ&ei=RmJeYoTgKZGJmwGIxre4DA&json=", "num_citations": 202, "citedby_url": "/scholar?cites=8528215054992084387&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ow3xeGhOWnYJ:scholar.google.com/&scioq=Reward+Constrained+Policy+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11074"}, "Self-tuning Networks: Bilevel Optimization Of Hyperparameters Using Structured Best-response Functions": {"container_type": "Publication", "bib": {"title": "Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions", "author": ["M MacKay", "P Vicol", "J Lorraine", "D Duvenaud"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Hyperparameter optimization can be formulated as a bilevel optimization problem, where the optimal parameters on the training set depend on the hyperparameters. We aim to adapt regularization hyperparameters for neural networks by fitting compact approximations to the best-response function, which maps hyperparameters to optimal weights and biases. We show how to construct scalable best-response approximations for neural networks by modeling the best-response as a single network whose hidden units are gated conditionally"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.03088", "author_id": ["", "jywEQ-AAAAAJ", "Hzf8bu0AAAAJ", "ZLpO3XQAAAAJ"], "url_scholarbib": "/scholar?q=info:T2WmxRMDx74J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-tuning%2BNetworks:%2BBilevel%2BOptimization%2BOf%2BHyperparameters%2BUsing%2BStructured%2BBest-response%2BFunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=T2WmxRMDx74J&ei=TWJeYsrMB4vMsQK69Y7ABg&json=", "num_citations": 77, "citedby_url": "/scholar?cites=13746959771027006799&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:T2WmxRMDx74J:scholar.google.com/&scioq=Self-tuning+Networks:+Bilevel+Optimization+Of+Hyperparameters+Using+Structured+Best-response+Functions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.03088"}, "Beyond Pixel Norm-balls: Parametric Adversaries Using An Analytically Differentiable Renderer": {"container_type": "Publication", "bib": {"title": "Beyond pixel norm-balls: Parametric adversaries using an analytically differentiable renderer", "author": ["HTD Liu", "M Tao", "CL Li", "D Nowrouzezahrai"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many machine learning image classifiers are vulnerable to adversarial attacks, inputs with perturbations designed to intentionally trigger misclassification. Current adversarial methods directly alter pixel colors and evaluate against pixel norm-balls: pixel perturbations smaller than a specified magnitude, according to a measurement norm. This evaluation, however, has limited practical utility since perturbations in the pixel space do not correspond to underlying real-world phenomena of image formation that lead to them and has no security"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.02651", "author_id": ["-T7Au0kAAAAJ", "Q1yFGOUAAAAJ", "vqHIt_sAAAAJ", "nCZ2PMcAAAAJ"], "url_scholarbib": "/scholar?q=info:yNOTIaNCv6wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BPixel%2BNorm-balls:%2BParametric%2BAdversaries%2BUsing%2BAn%2BAnalytically%2BDifferentiable%2BRenderer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yNOTIaNCv6wJ&ei=WWJeYsriNIvMsQK69Y7ABg&json=", "num_citations": 60, "citedby_url": "/scholar?cites=12447741163485778888&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yNOTIaNCv6wJ:scholar.google.com/&scioq=Beyond+Pixel+Norm-balls:+Parametric+Adversaries+Using+An+Analytically+Differentiable+Renderer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.02651"}, "Learning To Represent Edits": {"container_type": "Publication", "bib": {"title": "Learning to represent edits", "author": ["P Yin", "G Neubig", "M Allamanis", "M Brockschmidt"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce the problem of learning distributed representations of edits. By combining a  \u201c learn to represent the salient information of an edit and can be used to apply edits to new"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.13337", "author_id": ["t5lVb6sAAAAJ", "wlosgkoAAAAJ", "rYsjwZgAAAAJ", "pF27eLMAAAAJ"], "url_scholarbib": "/scholar?q=info:MCrrjldnGdkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BRepresent%2BEdits%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MCrrjldnGdkJ&ei=XWJeYrPuEZHKsQKNt6-YAw&json=", "num_citations": 66, "citedby_url": "/scholar?cites=15643648406405720624&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MCrrjldnGdkJ:scholar.google.com/&scioq=Learning+To+Represent+Edits&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.13337"}, "On Self Modulation For Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "On self modulation for generative adversarial networks", "author": ["T Chen", "M Lucic", "N Houlsby", "S Gelly"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.01365", "abstract": "Training Generative Adversarial Networks (GANs) is notoriously challenging. We propose and study an architectural modification, self-modulation, which improves GAN performance across different data sets, architectures, losses, regularizers, and hyperparameter settings. Intuitively, self-modulation allows the intermediate feature maps of a generator to change as a function of the input noise vector. While reminiscent of other conditioning techniques, it requires no labeled data. In a large-scale empirical study we observe a relative decrease of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01365", "author_id": ["KoXUMbsAAAAJ", "SzZRlcMAAAAJ", "sm1-TZMAAAAJ", "m7LvuTkAAAAJ"], "url_scholarbib": "/scholar?q=info:9djUSOUV98gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BSelf%2BModulation%2BFor%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9djUSOUV98gJ&ei=YWJeYqWoD4yuyASD3KfABw&json=", "num_citations": 74, "citedby_url": "/scholar?cites=14481067201346722037&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9djUSOUV98gJ:scholar.google.com/&scioq=On+Self+Modulation+For+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01365"}, "Sgd Converges To Global Minimum In Deep Learning Via Star-convex Path": {"container_type": "Publication", "bib": {"title": "Sgd converges to global minimum in deep learning via star-convex path", "author": ["Y Zhou", "J Yang", "H Zhang", "Y Liang", "V Tarokh"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.00451", "author_id": ["4fK8bYIAAAAJ", "nnPKUVUAAAAJ", "w1srHyIAAAAJ", "lGgLAiIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:lasQDENi9agJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSgd%2BConverges%2BTo%2BGlobal%2BMinimum%2BIn%2BDeep%2BLearning%2BVia%2BStar-convex%2BPath%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lasQDENi9agJ&ei=ZWJeYs7qF5yO6rQP-viEEA&json=", "num_citations": 44, "citedby_url": "/scholar?cites=12174745207770753941&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lasQDENi9agJ:scholar.google.com/&scioq=Sgd+Converges+To+Global+Minimum+In+Deep+Learning+Via+Star-convex+Path&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.00451"}, "Improving Differentiable Neural Computers Through Memory Masking, De-allocation, And Link Distribution Sharpness Control": {"container_type": "Publication", "bib": {"title": "Improving differentiable neural computers through memory masking, de-allocation, and link distribution sharpness control", "author": ["R Csordas", "J Schmidhuber"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.10278", "abstract": "The Differentiable Neural Computer (DNC) can learn algorithmic and question answering tasks. An analysis of its internal activation patterns reveals three problems: Most importantly, the lack of key-value separation makes the address distribution resulting from content-based look-up noisy and flat, since the value influences the score calculation, although only the key should. Second, DNC's de-allocation of memory results in aliasing, which is a problem for content-based look-up. Thirdly, chaining memory reads with the temporal linkage matrix"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.10278", "author_id": ["av1lplwAAAAJ", "gLnCTgIAAAAJ"], "url_scholarbib": "/scholar?q=info:OG33pE10XYMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BDifferentiable%2BNeural%2BComputers%2BThrough%2BMemory%2BMasking,%2BDe-allocation,%2BAnd%2BLink%2BDistribution%2BSharpness%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OG33pE10XYMJ&ei=bWJeYqmAKMiBy9YP18Gi8As&json=", "num_citations": 21, "citedby_url": "/scholar?cites=9465849868631633208&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OG33pE10XYMJ:scholar.google.com/&scioq=Improving+Differentiable+Neural+Computers+Through+Memory+Masking,+De-allocation,+And+Link+Distribution+Sharpness+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.10278"}, "On The Sensitivity Of Adversarial Robustness To Input Data Distributions": {"container_type": "Publication", "bib": {"title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions.", "author": ["GW Ding", "KYC Lui", "X Jin", "L Wang", "R Huang"], "pub_year": "2019", "venue": "ICLR (Poster)", "abstract": "Neural networks are vulnerable to small adversarial perturbations. While existing literature largely focused on the vulnerability of learned models, we demonstrate an intriguing phenomenon that adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarially trained model that is both trained and evaluated on the new distribution. We show this by constructing"}, "filled": false, "gsrank": 1, "pub_url": "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.pdf", "author_id": ["f7AS33oAAAAJ", "", "Jd_tsuEAAAAJ", "GlOpjUoAAAAJ", "-tNbYyQAAAAJ"], "url_scholarbib": "/scholar?q=info:wyEOvIKx1gwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BSensitivity%2BOf%2BAdversarial%2BRobustness%2BTo%2BInput%2BData%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wyEOvIKx1gwJ&ei=c2JeYp7pKpGJmwGIxre4DA&json=", "num_citations": 34, "citedby_url": "/scholar?cites=925121948530123203&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wyEOvIKx1gwJ:scholar.google.com/&scioq=On+The+Sensitivity+Of+Adversarial+Robustness+To+Input+Data+Distributions&hl=en&as_sdt=0,33", "eprint_url": "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.pdf"}, "Evaluating Robustness Of Neural Networks With Mixed Integer Programming": {"container_type": "Publication", "bib": {"title": "Evaluating robustness of neural networks with mixed integer programming", "author": ["V Tjeng", "K Xiao", "R Tedrake"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1711.07356", "abstract": "Neural networks have demonstrated considerable success on a wide variety of real-world problems. However, networks trained only to optimize for training accuracy can often be fooled by adversarial examples-slightly perturbed inputs that are misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.07356", "author_id": ["GZt32DEAAAAJ", "xblGvQgAAAAJ", "nxNkEiYAAAAJ"], "url_scholarbib": "/scholar?q=info:ZbIVLL6n8fsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvaluating%2BRobustness%2BOf%2BNeural%2BNetworks%2BWith%2BMixed%2BInteger%2BProgramming%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZbIVLL6n8fsJ&ei=eGJeYq2REZHKsQKNt6-YAw&json=", "num_citations": 494, "citedby_url": "/scholar?cites=18154476008132424293&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZbIVLL6n8fsJ:scholar.google.com/&scioq=Evaluating+Robustness+Of+Neural+Networks+With+Mixed+Integer+Programming&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.07356"}, "Are Adversarial Examples Inevitable?": {"container_type": "Publication", "bib": {"title": "Are adversarial examples inevitable?", "author": ["A Shafahi", "WR Huang", "C Studer", "S Feizi"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "question: Are adversarial attacks inevitable? This paper analyzes adversarial examples  from a  of a classifier to adversarial attacks. We show that, for certain classes of problems,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.02104", "author_id": ["5Jnk00MAAAAJ", "UxdgwBUAAAAJ", "Jco5C7sAAAAJ", "lptAmrMAAAAJ"], "url_scholarbib": "/scholar?q=info:vDY1YDC8LxoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAre%2BAdversarial%2BExamples%2BInevitable%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vDY1YDC8LxoJ&ei=hGJeYpDeOeHDywTjooCQBQ&json=", "num_citations": 203, "citedby_url": "/scholar?cites=1886933684850079420&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vDY1YDC8LxoJ:scholar.google.com/&scioq=Are+Adversarial+Examples+Inevitable%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.02104?ref=https://githubhelp.com"}, "Large Scale Graph Learning From Smooth Signals": {"container_type": "Publication", "bib": {"title": "Large scale graph learning from smooth signals", "author": ["V Kalofolias", "N Perraudin"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1710.05654", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. They have been used successfully in unsupervised and semi-supervised learning. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is $\\mathcal {O}(n^ 2) $ for $ n $ samples. In this paper, we show how to scale it, obtaining"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1710.05654", "author_id": ["Bz1RQ8MAAAAJ", "Rlz7I8gAAAAJ"], "url_scholarbib": "/scholar?q=info:KD_P7bbSLAQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2BScale%2BGraph%2BLearning%2BFrom%2BSmooth%2BSignals%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KD_P7bbSLAQJ&ei=kmJeYqSDDovMsQK69Y7ABg&json=", "num_citations": 54, "citedby_url": "/scholar?cites=300846958242643752&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KD_P7bbSLAQJ:scholar.google.com/&scioq=Large+Scale+Graph+Learning+From+Smooth+Signals&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1710.05654"}, "From Hard To Soft: Understanding Deep Network Nonlinearities Via Vector Quantization And Statistical Inference": {"container_type": "Publication", "bib": {"title": "From hard to soft: Understanding deep network nonlinearities via vector quantization and statistical inference", "author": ["R Balestriero", "RG Baraniuk"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.09274", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN). To date there has been little progress understanding the menagerie of available nonlinearities, but recently progress has been made on understanding the r\\^ ole played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling. In particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators}(MASOs) that have an elegant link to vector quantization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.09274", "author_id": ["S1x_xqcAAAAJ", "N-BBA20AAAAJ"], "url_scholarbib": "/scholar?q=info:ustNHQX_iF8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrom%2BHard%2BTo%2BSoft:%2BUnderstanding%2BDeep%2BNetwork%2BNonlinearities%2BVia%2BVector%2BQuantization%2BAnd%2BStatistical%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ustNHQX_iF8J&ei=mmJeYsriL82Ny9YPqPyUgAs&json=", "num_citations": 9, "citedby_url": "/scholar?cites=6884032427867360186&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ustNHQX_iF8J:scholar.google.com/&scioq=From+Hard+To+Soft:+Understanding+Deep+Network+Nonlinearities+Via+Vector+Quantization+And+Statistical+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.09274"}, "Contingency-aware Exploration In Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Contingency-aware exploration in reinforcement learning", "author": ["J Choi", "Y Guo", "M Moczulski", "J Oh", "N Wu"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.01483", "author_id": ["UX-H08cAAAAJ", "ONuIPv0AAAAJ", "zeX8KGAAAAAJ", "LNUeOu4AAAAJ", ""], "url_scholarbib": "/scholar?q=info:4uq_aXtQt1oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContingency-aware%2BExploration%2BIn%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4uq_aXtQt1oJ&ei=nmJeYq7rIpHKsQKNt6-YAw&json=", "num_citations": 57, "citedby_url": "/scholar?cites=6536781875136948962&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4uq_aXtQt1oJ:scholar.google.com/&scioq=Contingency-aware+Exploration+In+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.01483"}, "Promp: Proximal Meta-policy Search": {"container_type": "Publication", "bib": {"title": "Promp: Proximal meta-policy search", "author": ["J Rothfuss", "D Lee", "I Clavera", "T Asfour"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "difficulty of proper meta-policy gradient estimates.  Proximal MetaPolicy Search (ProMP),  an efficient and stable meta-learning algorithm for RL. In our experiments, we show that ProMP"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06784", "author_id": ["EfLpX8QAAAAJ", "vOLXDDAAAAAJ", "yABlzrsAAAAJ", "65bIT4oAAAAJ"], "url_scholarbib": "/scholar?q=info:wvS3KD_DKUkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPromp:%2BProximal%2BMeta-policy%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wvS3KD_DKUkJ&ei=qGJeYq_IBc2Ny9YPqPyUgAs&json=", "num_citations": 134, "citedby_url": "/scholar?cites=5271959514847376578&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wvS3KD_DKUkJ:scholar.google.com/&scioq=Promp:+Proximal+Meta-policy+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06784.pdf?ref=https://githubhelp.com"}, "Generating Liquid Simulations With Deformation-aware Neural Networks": {"container_type": "Publication", "bib": {"title": "Generating liquid simulations with deformation-aware neural networks", "author": ["L Prantl", "B Bonev", "N Thuerey"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1704.07854", "abstract": "We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields. Our method specifically targets the space-time representation of physical surfaces from liquid simulations. Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions. Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1704.07854", "author_id": ["oKLBsnsAAAAJ", "sYo-KS4AAAAJ", "GEehwv8AAAAJ"], "url_scholarbib": "/scholar?q=info:-O-lGLuUpjAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BLiquid%2BSimulations%2BWith%2BDeformation-aware%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-O-lGLuUpjAJ&ei=rWJeYvbDAs2Ny9YPqPyUgAs&json=", "num_citations": 9, "citedby_url": "/scholar?cites=3505652891247833080&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-O-lGLuUpjAJ:scholar.google.com/&scioq=Generating+Liquid+Simulations+With+Deformation-aware+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1704.07854"}, "Glue: A Multi-task Benchmark And Analysis Platform For Natural Language Understanding": {"container_type": "Publication", "bib": {"title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "author": ["A Wang", "A Singh", "J Michael", "F Hill", "O Levy"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.07461", "author_id": ["7lSuRloAAAAJ", "aWiMKLsAAAAJ", "9DDOHR8AAAAJ", "4HLUnhIAAAAJ", "PZVd2h8AAAAJ"], "url_scholarbib": "/scholar?q=info:qFfMBK1zE_IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGlue:%2BA%2BMulti-task%2BBenchmark%2BAnd%2BAnalysis%2BPlatform%2BFor%2BNatural%2BLanguage%2BUnderstanding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qFfMBK1zE_IJ&ei=sGJeYsCFKZLeyQTms5KQBg&json=", "num_citations": 2721, "citedby_url": "/scholar?cites=17443412968683100072&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qFfMBK1zE_IJ:scholar.google.com/&scioq=Glue:+A+Multi-task+Benchmark+And+Analysis+Platform+For+Natural+Language+Understanding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.07461.pdf?ref=https://githubhelp.com"}, "The Comparative Power Of Relu Networks And Polynomial Kernels In The Presence Of Sparse Latent Structure": {"container_type": "Publication", "bib": {"title": "The comparative power of relu networks and polynomial kernels in the presence of sparse latent structure", "author": ["F Koehler", "A Risteski"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: eg characterizing the best L1 or L_ {infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJgTTjA9tX", "author_id": ["Atc5w-4AAAAJ", "NWPDSEsAAAAJ"], "url_scholarbib": "/scholar?q=info:6BXmr1HZFkUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BComparative%2BPower%2BOf%2BRelu%2BNetworks%2BAnd%2BPolynomial%2BKernels%2BIn%2BThe%2BPresence%2BOf%2BSparse%2BLatent%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6BXmr1HZFkUJ&ei=s2JeYpiYNpHKsQKNt6-YAw&json=", "num_citations": 5, "citedby_url": "/scholar?cites=4978405382971332072&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6BXmr1HZFkUJ:scholar.google.com/&scioq=The+Comparative+Power+Of+Relu+Networks+And+Polynomial+Kernels+In+The+Presence+Of+Sparse+Latent+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJgTTjA9tX"}, "Learning To Design Rna": {"container_type": "Publication", "bib": {"title": "Learning to design RNA", "author": ["F Runge", "D Stoll", "S Falkner", "F Hutter"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.11951", "abstract": "Design problem is to find an RNA sequence which  RNA Design problem, dubbed LEARNA.  LEARNA uses deep reinforcement learning to train a policy network to sequentially design"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.11951", "author_id": ["", "", "r7FWJEkAAAAJ", "YUrxwrkAAAAJ"], "url_scholarbib": "/scholar?q=info:-7OzFmuiQu8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BDesign%2BRna%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-7OzFmuiQu8J&ei=t2JeYrP1IovMsQK69Y7ABg&json=", "num_citations": 43, "citedby_url": "/scholar?cites=17240520904353756155&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-7OzFmuiQu8J:scholar.google.com/&scioq=Learning+To+Design+Rna&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.11951"}, "Environment Probing Interaction Policies": {"container_type": "Publication", "bib": {"title": "Environment probing interaction policies", "author": ["W Zhou", "L Pinto", "A Gupta"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.11740", "abstract": "the dynamics of a given environment. We argue that instead of learning a policy that is  invariant to environment dynamics we need a policy that adapts to its environment. A popular"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.11740", "author_id": ["picvdvEAAAAJ", "pmVPj94AAAAJ", "bqL73OkAAAAJ"], "url_scholarbib": "/scholar?q=info:CunBlpVVTCgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnvironment%2BProbing%2BInteraction%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CunBlpVVTCgJ&ei=u2JeYv3LHMLZmQHc1ovQAg&json=", "num_citations": 30, "citedby_url": "/scholar?cites=2903789960714905866&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CunBlpVVTCgJ:scholar.google.com/&scioq=Environment+Probing+Interaction+Policies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.11740"}, "Episodic Curiosity Through Reachability": {"container_type": "Publication", "bib": {"title": "Episodic curiosity through reachability", "author": ["N Savinov", "A Raichuk", "R Marinier", "D Vincent"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a new curiosity method which uses episodic memory to form the novelty bonus.   the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns lo"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.02274", "author_id": ["qUIOyQYAAAAJ", "fquIpvgAAAAJ", "w951T-EAAAAJ", ""], "url_scholarbib": "/scholar?q=info:Ibtgv0occiwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEpisodic%2BCuriosity%2BThrough%2BReachability%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ibtgv0occiwJ&ei=vmJeYsekLZWMy9YPt8OamA0&json=", "num_citations": 173, "citedby_url": "/scholar?cites=3202653392377789217&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ibtgv0occiwJ:scholar.google.com/&scioq=Episodic+Curiosity+Through+Reachability&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.02274"}, "Learning To Learn Without Forgetting By Maximizing Transfer And Minimizing Interference": {"container_type": "Publication", "bib": {"title": "Learning to learn without forgetting by maximizing transfer and minimizing interference", "author": ["M Riemer", "I Cases", "R Ajemian", "M Liu", "I Rish"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.11910", "author_id": ["PK7UzAwAAAAJ", "9-TdgYMAAAAJ", "", "7QHvAEYAAAAJ", "Avse5gIAAAAJ"], "url_scholarbib": "/scholar?q=info:0miHdy-x4xUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BLearn%2BWithout%2BForgetting%2BBy%2BMaximizing%2BTransfer%2BAnd%2BMinimizing%2BInterference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0miHdy-x4xUJ&ei=w2JeYqDjG82Ny9YPqPyUgAs&json=", "num_citations": 303, "citedby_url": "/scholar?cites=1577299111936747730&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0miHdy-x4xUJ:scholar.google.com/&scioq=Learning+To+Learn+Without+Forgetting+By+Maximizing+Transfer+And+Minimizing+Interference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.11910"}, "Meta-learning Update Rules For Unsupervised Representation Learning": {"container_type": "Publication", "bib": {"title": "Meta-learning update rules for unsupervised representation learning", "author": ["L Metz", "N Maheswaranathan", "B Cheung"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1804.00222", "author_id": ["jCOmCb4AAAAJ", "bEOT7ScAAAAJ", "7N-ethYAAAAJ"], "url_scholarbib": "/scholar?q=info:3eOYLXq6H1MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BUpdate%2BRules%2BFor%2BUnsupervised%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3eOYLXq6H1MJ&ei=xmJeYtihNc2Ny9YPqPyUgAs&json=", "num_citations": 74, "citedby_url": "/scholar?cites=5989711063339819997&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3eOYLXq6H1MJ:scholar.google.com/&scioq=Meta-learning+Update+Rules+For+Unsupervised+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1804.00222"}, "Adversarial Attacks On Graph Neural Networks Via Meta Learning": {"container_type": "Publication", "bib": {"title": "Adversarial attacks on graph neural networks via meta learning", "author": ["D Z\u00fcgner", "S G\u00fcnnemann"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.08412", "abstract": "time attacks on graph neural networks for node classification that perturb the discrete graph   by our algorithm can misguide the graph neural networks such that they perform worse than"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.08412", "author_id": ["zLYI3MwAAAAJ", "npqoAWwAAAAJ"], "url_scholarbib": "/scholar?q=info:J_TDHN0Y-SIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BAttacks%2BOn%2BGraph%2BNeural%2BNetworks%2BVia%2BMeta%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J_TDHN0Y-SIJ&ei=ymJeYvoe35XL1g-zrp6QCQ&json=", "num_citations": 243, "citedby_url": "/scholar?cites=2520072804439946279&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:J_TDHN0Y-SIJ:scholar.google.com/&scioq=Adversarial+Attacks+On+Graph+Neural+Networks+Via+Meta+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.08412"}, "Visceral Machines: Risk-aversion In Reinforcement Learning With Intrinsic Physiological Rewards": {"container_type": "Publication", "bib": {"title": "Visceral machines: Risk-aversion in reinforcement learning with intrinsic physiological rewards", "author": ["D McDuff", "A Kapoor"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.09975", "abstract": "As people learn to navigate the world, autonomic nervous system (eg,\" fight or flight\") responses provide intrinsic feedback about the potential consequence of action choices (eg, becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger. We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.09975", "author_id": ["m7Jr-b4AAAAJ", "4D1n8scAAAAJ"], "url_scholarbib": "/scholar?q=info:D_3iwPNwqL0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisceral%2BMachines:%2BRisk-aversion%2BIn%2BReinforcement%2BLearning%2BWith%2BIntrinsic%2BPhysiological%2BRewards%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D_3iwPNwqL0J&ei=zGJeYqvDO4yuyASD3KfABw&json=", "num_citations": 11, "citedby_url": "/scholar?cites=13666297261471235343&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D_3iwPNwqL0J:scholar.google.com/&scioq=Visceral+Machines:+Risk-aversion+In+Reinforcement+Learning+With+Intrinsic+Physiological+Rewards&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.09975"}, "Explaining Image Classifiers By Counterfactual Generation": {"container_type": "Publication", "bib": {"title": "Explaining image classifiers by counterfactual generation", "author": ["CH Chang", "E Creager", "A Goldenberg"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren't. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier's decision after in-fill. Our approach"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.08024", "author_id": ["p7FEdokAAAAJ", "boebIUcAAAAJ", "cEepZOEAAAAJ"], "url_scholarbib": "/scholar?q=info:Uh1hXNngnVcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExplaining%2BImage%2BClassifiers%2BBy%2BCounterfactual%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Uh1hXNngnVcJ&ei=12JeYqK_Ns2Ny9YPqPyUgAs&json=", "num_citations": 132, "citedby_url": "/scholar?cites=6313449476805696850&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Uh1hXNngnVcJ:scholar.google.com/&scioq=Explaining+Image+Classifiers+By+Counterfactual+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.08024"}, "Bias-reduced Uncertainty Estimation For Deep Neural Classifiers": {"container_type": "Publication", "bib": {"title": "Bias-reduced uncertainty estimation for deep neural classifiers", "author": ["Y Geifman", "G Uziel", "R El-Yaniv"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.08206", "abstract": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.08206", "author_id": ["77CIClsAAAAJ", "xl39otIAAAAJ", "D9eVSd8AAAAJ"], "url_scholarbib": "/scholar?q=info:v7uDUTg8xasJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBias-reduced%2BUncertainty%2BEstimation%2BFor%2BDeep%2BNeural%2BClassifiers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v7uDUTg8xasJ&ei=22JeYsCNAYyuyASD3KfABw&json=", "num_citations": 57, "citedby_url": "/scholar?cites=12377365363481099199&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v7uDUTg8xasJ:scholar.google.com/&scioq=Bias-reduced+Uncertainty+Estimation+For+Deep+Neural+Classifiers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.08206"}, "Learning Finite State Representations Of Recurrent Policy Networks": {"container_type": "Publication", "bib": {"title": "Learning finite state representations of recurrent policy networks", "author": ["A Koul", "S Greydanus", "A Fern"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.12530", "abstract": "Recurrent neural networks (RNNs) are an effective representation of control policies for a wide range of reinforcement and imitation learning problems. RNN policies, however, are particularly difficult to explain, understand, and analyze due to their use of continuous-valued memory vectors and observation features. In this paper, we introduce a new technique, Quantized Bottleneck Insertion, to learn finite representations of these vectors and features. The result is a quantized representation of the RNN that can be analyzed to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.12530", "author_id": ["K-Q0Xq4AAAAJ", "SECnlpMAAAAJ", "GaKxFrcAAAAJ"], "url_scholarbib": "/scholar?q=info:WM_iUJgSBoAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFinite%2BState%2BRepresentations%2BOf%2BRecurrent%2BPolicy%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WM_iUJgSBoAJ&ei=3mJeYpWJDYvMsQK69Y7ABg&json=", "num_citations": 54, "citedby_url": "/scholar?cites=9225081332116410200&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WM_iUJgSBoAJ:scholar.google.com/&scioq=Learning+Finite+State+Representations+Of+Recurrent+Policy+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.12530"}, "Hierarchical Visuomotor Control Of Humanoids": {"container_type": "Publication", "bib": {"title": "Hierarchical visuomotor control of humanoids", "author": ["J Merel", "A Ahuja", "V Pham", "S Tunyasuvunakool"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.09656", "author_id": ["K4OcFXUAAAAJ", "HFV9GmMAAAAJ", "", "9dAjSlYAAAAJ"], "url_scholarbib": "/scholar?q=info:IfIF2lCEP94J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BVisuomotor%2BControl%2BOf%2BHumanoids%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IfIF2lCEP94J&ei=4WJeYqryFI-bmAGmiqCIBw&json=", "num_citations": 78, "citedby_url": "/scholar?cites=16014664282742845985&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IfIF2lCEP94J:scholar.google.com/&scioq=Hierarchical+Visuomotor+Control+Of+Humanoids&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.09656"}, "Adv-bnn: Improved Adversarial Defense Through Robust Bayesian Neural Network": {"container_type": "Publication", "bib": {"title": "Adv-bnn: Improved adversarial defense through robust bayesian neural network", "author": ["X Liu", "Y Li", "C Wu", "CJ Hsieh"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.01279", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. Our algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. Instead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01279", "author_id": ["47EBD1QAAAAJ", "bQ6YhCwAAAAJ", "rhVberEAAAAJ", "Wy89g4IAAAAJ"], "url_scholarbib": "/scholar?q=info:AfmHzLUul98J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdv-bnn:%2BImproved%2BAdversarial%2BDefense%2BThrough%2BRobust%2BBayesian%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AfmHzLUul98J&ei=5WJeYvS9IMiBy9YP18Gi8As&json=", "num_citations": 100, "citedby_url": "/scholar?cites=16111397550296660225&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AfmHzLUul98J:scholar.google.com/&scioq=Adv-bnn:+Improved+Adversarial+Defense+Through+Robust+Bayesian+Neural+Network&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01279"}, "Meta-learning Probabilistic Inference For Prediction": {"container_type": "Publication", "bib": {"title": "Meta-learning probabilistic inference for prediction", "author": ["J Gordon", "J Bronskill", "M Bauer", "S Nowozin"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.09921", "author_id": ["IZGi3pEAAAAJ", "aH2jZsoAAAAJ", "lWDq-ygAAAAJ", "7-B7aQkAAAAJ"], "url_scholarbib": "/scholar?q=info:4r5RBcwh2P0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BProbabilistic%2BInference%2BFor%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4r5RBcwh2P0J&ei=6GJeYuK7Oo-bmAGmiqCIBw&json=", "num_citations": 155, "citedby_url": "/scholar?cites=18291407046711557858&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4r5RBcwh2P0J:scholar.google.com/&scioq=Meta-learning+Probabilistic+Inference+For+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.09921"}, "Quaternion Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Quaternion recurrent neural networks", "author": ["T Parcollet", "M Ravanelli", "M Morchid", "G Linar\u00e8s"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "to the capability of recurrent neural networks in a natural and  quaternion neural networks,  we hypothesize that quaternion  space of the quaternion, leading to better generalization."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.04418", "author_id": ["oZ3hd1YAAAAJ", "-6Pj3IYAAAAJ", "hIsqY04AAAAJ", "mOe-Gh0AAAAJ"], "url_scholarbib": "/scholar?q=info:ImMrIugv07wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuaternion%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ImMrIugv07wJ&ei=7WJeYrbEGpWMy9YPt8OamA0&json=", "num_citations": 83, "citedby_url": "/scholar?cites=13606271573268587298&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ImMrIugv07wJ:scholar.google.com/&scioq=Quaternion+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.04418"}, "Convolutional Neural Networks On Non-uniform Geometrical Signals Using Euclidean Spectral Transformation": {"container_type": "Publication", "bib": {"title": "Convolutional neural networks on non-uniform geometrical signals using euclidean spectral transformation", "author": ["C Jiang", "D Wang", "J Huang", "P Marcus"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Convolutional Neural Networks (CNN) have been successful in processing data signals that are uniformly sampled in the spatial domain (eg, images). However, most data signals do not natively exist on a grid, and in the process of being sampled onto a uniform physical grid suffer significant aliasing error and information loss. Moreover, signals can exist in different topological structures as, for example, points, lines, surfaces and volumes. It has been challenging to analyze signals with mixed topologies (for example, point cloud with surface"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.02070", "author_id": ["KD12DDMAAAAJ", "OZ7PjVoAAAAJ", "7eJBk1UAAAAJ", "CHlcHGMAAAAJ"], "url_scholarbib": "/scholar?q=info:9KJnAqbtoNgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConvolutional%2BNeural%2BNetworks%2BOn%2BNon-uniform%2BGeometrical%2BSignals%2BUsing%2BEuclidean%2BSpectral%2BTransformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9KJnAqbtoNgJ&ei=-WJeYp_oFY-bmAGmiqCIBw&json=", "num_citations": 7, "citedby_url": "/scholar?cites=15609737605726839540&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9KJnAqbtoNgJ:scholar.google.com/&scioq=Convolutional+Neural+Networks+On+Non-uniform+Geometrical+Signals+Using+Euclidean+Spectral+Transformation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.02070"}, "Optimal Control Via Neural Networks: A Convex Approach": {"container_type": "Publication", "bib": {"title": "Optimal control via neural networks: A convex approach", "author": ["Y Chen", "Y Shi", "B Zhang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1805.11835", "abstract": "Control of complex systems involves both system identification and controller design. Deep neural networks have proven to be successful in many identification tasks, however, from model-based control perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex. Therefore many systems are still identified and controlled based on simple linear models despite their poor representation capability. In this paper we bridge the gap between model accuracy and control tractability faced by neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.11835", "author_id": ["G1NiRmwAAAAJ", "kQyQ_vwAAAAJ", "3svZOGAAAAAJ"], "url_scholarbib": "/scholar?q=info:VP7fQFxG7Q4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BControl%2BVia%2BNeural%2BNetworks:%2BA%2BConvex%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VP7fQFxG7Q4J&ei=_WJeYvbIKcLZmQHc1ovQAg&json=", "num_citations": 96, "citedby_url": "/scholar?cites=1075593248050773588&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VP7fQFxG7Q4J:scholar.google.com/&scioq=Optimal+Control+Via+Neural+Networks:+A+Convex+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.11835"}, "The Limitations Of Adversarial Training And The Blind-spot Attack": {"container_type": "Publication", "bib": {"title": "The limitations of adversarial training and the blind-spot attack", "author": ["H Zhang", "H Chen", "Z Song", "D Boning", "IS Dhillon"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The adversarial training procedure proposed by Madry et al.(2018) is one of the most effective methods to defend against adversarial examples in deep neural networks (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.04684", "author_id": ["LTa3GzEAAAAJ", "KFtsQvIAAAAJ", "yDZct7UAAAAJ", "oIdI_PcAAAAJ", "xBv5ZfkAAAAJ"], "url_scholarbib": "/scholar?q=info:Qm3ryyCdV1YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLimitations%2BOf%2BAdversarial%2BTraining%2BAnd%2BThe%2BBlind-spot%2BAttack%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Qm3ryyCdV1YJ&ei=AWNeYujHOt-Vy9YPs66ekAk&json=", "num_citations": 81, "citedby_url": "/scholar?cites=6221614174421347650&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Qm3ryyCdV1YJ:scholar.google.com/&scioq=The+Limitations+Of+Adversarial+Training+And+The+Blind-spot+Attack&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.04684"}, "Ba-net: Dense Bundle Adjustment Networks": {"container_type": "Publication", "bib": {"title": "Ba-net: Dense bundle adjustment network", "author": ["C Tang", "P Tan"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1806.04807", "abstract": "a network architecture to solve the structure-from-motion (SfM) problem via feature-metric  bundle adjustment ( The whole pipeline is differentiable, so that the network can learn suitable"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.04807", "author_id": ["E04MaJcAAAAJ", "XhyKVFMAAAAJ"], "url_scholarbib": "/scholar?q=info:H3g-EVm4rC4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBa-net:%2BDense%2BBundle%2BAdjustment%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H3g-EVm4rC4J&ei=BWNeYseXEMiBy9YP18Gi8As&json=", "num_citations": 150, "citedby_url": "/scholar?cites=3363265714419824671&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:H3g-EVm4rC4J:scholar.google.com/&scioq=Ba-net:+Dense+Bundle+Adjustment+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.04807"}, "Training For Faster Adversarial Robustness Verification Via Inducing Relu Stability": {"container_type": "Publication", "bib": {"title": "Training for faster adversarial robustness verification via inducing relu stability", "author": ["KY Xiao", "V Tjeng", "NM Shafiullah", "A Madry"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "in an amount of time that is small2 compared to previous verification techniques inducing  ReLU stability, we are able to train models that can be verified an additional 4\u201313x times as fast"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.03008", "author_id": ["xblGvQgAAAAJ", "GZt32DEAAAAJ", "vAOw6aQAAAAJ", "SupjsEUAAAAJ"], "url_scholarbib": "/scholar?q=info:4vLW4fKSUKIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BFor%2BFaster%2BAdversarial%2BRobustness%2BVerification%2BVia%2BInducing%2BRelu%2BStability%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4vLW4fKSUKIJ&ei=CGNeYrvJLM2Ny9YPqPyUgAs&json=", "num_citations": 127, "citedby_url": "/scholar?cites=11696009804149879522&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4vLW4fKSUKIJ:scholar.google.com/&scioq=Training+For+Faster+Adversarial+Robustness+Verification+Via+Inducing+Relu+Stability&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.03008.pdf?ref=https://githubhelp.com"}, "Preventing Posterior Collapse With Delta-vaes": {"container_type": "Publication", "bib": {"title": "Preventing posterior collapse with delta-vaes", "author": ["A Razavi", "A Oord", "B Poole", "O Vinyals"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.03416", "abstract": "In this case, the approximate posterior q(z|x) equals the prior p(z), thus the latent  posterior  collapse problem with structural constraints so that the KL divergence between the posterior"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.03416", "author_id": ["LpxwpFAAAAAJ", "TqUN-LwAAAAJ", "i5FMLA4AAAAJ", "NkzyCvUAAAAJ"], "url_scholarbib": "/scholar?q=info:o3FnpcFfNpkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPreventing%2BPosterior%2BCollapse%2BWith%2BDelta-vaes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o3FnpcFfNpkJ&ei=DGNeYo-qJcLZmQHc1ovQAg&json=", "num_citations": 105, "citedby_url": "/scholar?cites=11040116821853696419&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o3FnpcFfNpkJ:scholar.google.com/&scioq=Preventing+Posterior+Collapse+With+Delta-vaes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.03416"}, "Near-optimal Representation Learning For Hierarchical Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Near-optimal representation learning for hierarchical reinforcement learning", "author": ["O Nachum", "S Gu", "H Lee", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.01257", "abstract": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation--the mapping of observation space to goal space--is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.01257", "author_id": ["C-ZlBWMAAAAJ", "B8wslVsAAAAJ", "fmSHtE8AAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:ncQgTSK_ZfUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNear-optimal%2BRepresentation%2BLearning%2BFor%2BHierarchical%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ncQgTSK_ZfUJ&ei=D2NeYquuNeHDywTjooCQBQ&json=", "num_citations": 118, "citedby_url": "/scholar?cites=17682749665983906973&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ncQgTSK_ZfUJ:scholar.google.com/&scioq=Near-optimal+Representation+Learning+For+Hierarchical+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.01257"}, "Towards Gan Benchmarks Which Require Generalization": {"container_type": "Publication", "bib": {"title": "Towards GAN benchmarks which require generalization", "author": ["I Gulrajani", "C Raffel", "L Metz"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.03653", "abstract": "For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic. We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model. In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.03653", "author_id": ["E2SLBwIAAAAJ", "I66ZBYwAAAAJ", "jCOmCb4AAAAJ"], "url_scholarbib": "/scholar?q=info:H2w5aXPV83wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BGan%2BBenchmarks%2BWhich%2BRequire%2BGeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H2w5aXPV83wJ&ei=E2NeYvyIJN-Vy9YPs66ekAk&json=", "num_citations": 41, "citedby_url": "/scholar?cites=9003774771707079711&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:H2w5aXPV83wJ:scholar.google.com/&scioq=Towards+Gan+Benchmarks+Which+Require+Generalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.03653"}, "Learning Protein Sequence Embeddings Using Information From Structure": {"container_type": "Publication", "bib": {"title": "Learning protein sequence embeddings using information from structure", "author": ["T Bepler", "B Berger"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.08661", "abstract": "Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.08661", "author_id": ["Roxjki8AAAAJ", "bYjKaowAAAAJ"], "url_scholarbib": "/scholar?q=info:WzTA171tc9IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BProtein%2BSequence%2BEmbeddings%2BUsing%2BInformation%2BFrom%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WzTA171tc9IJ&ei=FmNeYpqaEZGJmwGIxre4DA&json=", "num_citations": 136, "citedby_url": "/scholar?cites=15164585032422536283&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WzTA171tc9IJ:scholar.google.com/&scioq=Learning+Protein+Sequence+Embeddings+Using+Information+From+Structure&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.08661"}, "Meta-learning With Differentiable Closed-form Solvers": {"container_type": "Publication", "bib": {"title": "Meta-learning with differentiable closed-form solvers", "author": ["L Bertinetto", "JF Henriques", "PHS Torr"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This requires back-propagating errors through the solver steps. While normally the cost of the   We propose both closed-form and iterative solvers, based on ridge regression and logistic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1805.08136", "author_id": ["zEy5CTkAAAAJ", "aCQjyp0AAAAJ", "kPxa2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:A6Vsow9I9rMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BWith%2BDifferentiable%2BClosed-form%2BSolvers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=A6Vsow9I9rMJ&ei=G2NeYv7QGpWMy9YPt8OamA0&json=", "num_citations": 480, "citedby_url": "/scholar?cites=12967631409063437571&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:A6Vsow9I9rMJ:scholar.google.com/&scioq=Meta-learning+With+Differentiable+Closed-form+Solvers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1805.08136.pdf?ref=https://githubhelp.com"}, "Mae: Mutual Posterior-divergence Regularization For Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "MAE: Mutual posterior-divergence regularization for variational autoencoders", "author": ["X Ma", "C Zhou", "E Hovy"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.01498", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.01498", "author_id": ["6_MQLIcAAAAJ", "mR5W7EgAAAAJ", "PUFxrroAAAAJ"], "url_scholarbib": "/scholar?q=info:sjdXQXT3WRUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMae:%2BMutual%2BPosterior-divergence%2BRegularization%2BFor%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sjdXQXT3WRUJ&ei=HmNeYp7hMciBy9YP18Gi8As&json=", "num_citations": 25, "citedby_url": "/scholar?cites=1538532826408236978&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sjdXQXT3WRUJ:scholar.google.com/&scioq=Mae:+Mutual+Posterior-divergence+Regularization+For+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.01498"}, "G-sgd: Optimizing Relu Neural Networks In Its Positively Scale-invariant Space": {"container_type": "Publication", "bib": {"title": "-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space", "author": ["Q Meng", "S Zheng", "H Zhang", "W Chen", "ZM Ma"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is:\\emph {can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1802.03713", "author_id": ["t-z3K34AAAAJ", "rPhGUw0AAAAJ", "w1srHyIAAAAJ", "y0lN_XsAAAAJ", ""], "url_scholarbib": "/scholar?q=info:i709IFynnvcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DG-sgd:%2BOptimizing%2BRelu%2BNeural%2BNetworks%2BIn%2BIts%2BPositively%2BScale-invariant%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i709IFynnvcJ&ei=IWNeYtDxLJGJmwGIxre4DA&json=", "num_citations": 17, "citedby_url": "/scholar?cites=17842882787808230795&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i709IFynnvcJ:scholar.google.com/&scioq=G-sgd:+Optimizing+Relu+Neural+Networks+In+Its+Positively+Scale-invariant+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1802.03713"}, "Hindsight Policy Gradients": {"container_type": "Publication", "bib": {"title": "Hindsight policy gradients", "author": ["P Rauber", "A Ummadisingu", "F Mutz"], "pub_year": "2017", "venue": "arXiv preprint arXiv \u2026", "abstract": "hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient   Our experiments on a diverse selection of sparse-reward environments show that hindsight"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1711.06006", "author_id": ["hHN2lf8AAAAJ", "ktaK03YAAAAJ", "9YIVWNgAAAAJ"], "url_scholarbib": "/scholar?q=info:QFtfpocANtAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHindsight%2BPolicy%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QFtfpocANtAJ&ei=JGNeYo_7Kd-Vy9YPs66ekAk&json=", "num_citations": 59, "citedby_url": "/scholar?cites=15003179791243238208&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QFtfpocANtAJ:scholar.google.com/&scioq=Hindsight+Policy+Gradients&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1711.06006"}, "Unsupervised Domain Adaptation For Distance Metric Learning": {"container_type": "Publication", "bib": {"title": "Unsupervised domain adaptation for distance metric learning", "author": ["K Sohn", "W Shang", "X Yu"], "pub_year": "2018", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BklhAj09K7", "author_id": ["VxpypngAAAAJ", "", "QJbtEKMAAAAJ"], "url_scholarbib": "/scholar?q=info:ExfGYmSb4XQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BDomain%2BAdaptation%2BFor%2BDistance%2BMetric%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ExfGYmSb4XQJ&ei=J2NeYvWkPI6pywSdh6agAg&json=", "num_citations": 43, "citedby_url": "/scholar?cites=8422183633615722259&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ExfGYmSb4XQJ:scholar.google.com/&scioq=Unsupervised+Domain+Adaptation+For+Distance+Metric+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BklhAj09K7"}, "Deep Decoder: Concise Image Representations From Untrained Non-convolutional Networks": {"container_type": "Publication", "bib": {"title": "Deep decoder: Concise image representations from untrained non-convolutional networks", "author": ["R Heckel", "P Hand"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.03982", "abstract": "Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03982", "author_id": ["ZWV0I7cAAAAJ", "zzG75zEAAAAJ"], "url_scholarbib": "/scholar?q=info:fxtWuZq11EUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BDecoder:%2BConcise%2BImage%2BRepresentations%2BFrom%2BUntrained%2BNon-convolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fxtWuZq11EUJ&ei=LGNeYorgLJHKsQKNt6-YAw&json=", "num_citations": 150, "citedby_url": "/scholar?cites=5031846359818705791&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fxtWuZq11EUJ:scholar.google.com/&scioq=Deep+Decoder:+Concise+Image+Representations+From+Untrained+Non-convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03982"}, "Gamepad: A Learning Environment For Theorem Proving": {"container_type": "Publication", "bib": {"title": "Gamepad: A learning environment for theorem proving", "author": ["D Huang", "P Dhariwal", "D Song", "I Sutskever"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.00608", "author_id": ["VTe4SGUAAAAJ", "0pOgVVAAAAAJ", "84WzBlYAAAAJ", "x04W_mMAAAAJ"], "url_scholarbib": "/scholar?q=info:HTWFvg6FK5EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGamepad:%2BA%2BLearning%2BEnvironment%2BFor%2BTheorem%2BProving%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HTWFvg6FK5EJ&ei=L2NeYqiLPI-bmAGmiqCIBw&json=", "num_citations": 64, "citedby_url": "/scholar?cites=10460600857870546205&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HTWFvg6FK5EJ:scholar.google.com/&scioq=Gamepad:+A+Learning+Environment+For+Theorem+Proving&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.00608"}, "A Direct Approach To Robust Deep Learning Using Adversarial Networks": {"container_type": "Publication", "bib": {"title": "A direct approach to robust deep learning using adversarial networks", "author": ["H Wang", "CN Yu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.09591", "abstract": "Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks. However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans. Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs. Including adversarial examples during training is a popular defense mechanism against adversarial"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.09591", "author_id": ["fna2KGYAAAAJ", "urpJRigAAAAJ"], "url_scholarbib": "/scholar?q=info:xO00poT4XSAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDirect%2BApproach%2BTo%2BRobust%2BDeep%2BLearning%2BUsing%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xO00poT4XSAJ&ei=NGNeYofcG5HKsQKNt6-YAw&json=", "num_citations": 45, "citedby_url": "/scholar?cites=2332293430655643076&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xO00poT4XSAJ:scholar.google.com/&scioq=A+Direct+Approach+To+Robust+Deep+Learning+Using+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.09591"}, "Misgan: Learning From Incomplete Data With Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Misgan: Learning from incomplete data with generative adversarial networks", "author": ["SCX Li", "B Jiang", "B Marlin"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09599", "abstract": "how MisGAN behaves under different missing patterns and  study to justify the construction  of MisGAN. Finally, we compare MisGAN with various baseline methods on the missing data"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09599", "author_id": ["LvzbCPoAAAAJ", "WxAIZtMAAAAJ", "ey960FIAAAAJ"], "url_scholarbib": "/scholar?q=info:MnGBhVWQRz0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMisgan:%2BLearning%2BFrom%2BIncomplete%2BData%2BWith%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MnGBhVWQRz0J&ei=PWNeYqCODZWMy9YPt8OamA0&json=", "num_citations": 111, "citedby_url": "/scholar?cites=4415656656646533426&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MnGBhVWQRz0J:scholar.google.com/&scioq=Misgan:+Learning+From+Incomplete+Data+With+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09599"}, "Learning A Meta-solver For Syntax-guided Program Synthesis": {"container_type": "Publication", "bib": {"title": "Learning a meta-solver for syntax-guided program synthesis", "author": ["X Si", "Y Yang", "H Dai", "M Naik", "L Song"], "pub_year": "2018", "venue": "International conference on \u2026", "abstract": "We study a general formulation of program synthesis called syntax-guided synthesis (SyGuS) that concerns synthesizing a program that follows a given grammar and satisfies a given logical specification. Both the logical specification and the grammar have complex structures and can vary from task to task, posing significant challenges for learning across different tasks. Furthermore, training data is often unavailable for domain specific synthesis tasks. To address these challenges, we propose a meta-learning framework that learns a"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Syl8Sn0cK7&noteId;=BJlUkwHxeV", "author_id": ["Ru-jrx4AAAAJ", "Lt4tmL8AAAAJ", "obpl7GQAAAAJ", "fmsV6nEAAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:_CaIEB8DHcQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BA%2BMeta-solver%2BFor%2BSyntax-guided%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_CaIEB8DHcQJ&ei=Q2NeYuuNEM2Ny9YPqPyUgAs&json=", "num_citations": 31, "citedby_url": "/scholar?cites=14131454637714712316&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_CaIEB8DHcQJ:scholar.google.com/&scioq=Learning+A+Meta-solver+For+Syntax-guided+Program+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Syl8Sn0cK7"}, "Babyai: A Platform To Study The Sample Efficiency Of Grounded Language Learning": {"container_type": "Publication", "bib": {"title": "Babyai: A platform to study the sample efficiency of grounded language learning", "author": ["M Chevalier-Boisvert", "D Bahdanau", "S Lahlou"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.08272", "author_id": ["lSg6I8gAAAAJ", "Nq0dVMcAAAAJ", "xLSkCrIAAAAJ"], "url_scholarbib": "/scholar?q=info:rcgPrUNPl-YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBabyai:%2BA%2BPlatform%2BTo%2BStudy%2BThe%2BSample%2BEfficiency%2BOf%2BGrounded%2BLanguage%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rcgPrUNPl-YJ&ei=RmNeYtGHFI-bmAGmiqCIBw&json=", "num_citations": 74, "citedby_url": "/scholar?cites=16615836502291630253&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rcgPrUNPl-YJ:scholar.google.com/&scioq=Babyai:+A+Platform+To+Study+The+Sample+Efficiency+Of+Grounded+Language+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.08272"}, "Understanding And Improving Interpolation In Autoencoders Via An Adversarial Regularizer": {"container_type": "Publication", "bib": {"title": "Understanding and improving interpolation in autoencoders via an adversarial regularizer", "author": ["D Berthelot", "C Raffel", "A Roy", "I Goodfellow"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can\" interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1807.07543", "author_id": ["46--eogAAAAJ", "I66ZBYwAAAAJ", "mCmda68AAAAJ", "iYN86KEAAAAJ"], "url_scholarbib": "/scholar?q=info:W6BnF3tSe0IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BAnd%2BImproving%2BInterpolation%2BIn%2BAutoencoders%2BVia%2BAn%2BAdversarial%2BRegularizer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W6BnF3tSe0IJ&ei=SWNeYofyFt-Vy9YPs66ekAk&json=", "num_citations": 173, "citedby_url": "/scholar?cites=4790513317265776731&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W6BnF3tSe0IJ:scholar.google.com/&scioq=Understanding+And+Improving+Interpolation+In+Autoencoders+Via+An+Adversarial+Regularizer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1807.07543.pdf,"}, "Learning Multi-level Hierarchies With Hindsight": {"container_type": "Publication", "bib": {"title": "Learning multi-level hierarchies with hindsight", "author": ["A Levy", "G Konidaris", "R Platt", "K Saenko"], "pub_year": "2017", "venue": "arXiv preprint arXiv:1712.00948", "abstract": "Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1712.00948", "author_id": ["t2eNzb8AAAAJ", "9UERvVEAAAAJ", "Z4Y5S2oAAAAJ", "9xDADY4AAAAJ"], "url_scholarbib": "/scholar?q=info:XuoYvSr0ZqAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BMulti-level%2BHierarchies%2BWith%2BHindsight%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XuoYvSr0ZqAJ&ei=S2NeYqmFPIyuyASD3KfABw&json=", "num_citations": 121, "citedby_url": "/scholar?cites=11558193958091287134&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XuoYvSr0ZqAJ:scholar.google.com/&scioq=Learning+Multi-level+Hierarchies+With+Hindsight&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1712.00948"}, "Generalized Tensor Models For Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Generalized tensor models for recurrent neural networks", "author": ["V Khrulkov", "O Hrinchuk", "I Oseledets"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.10801", "abstract": "Recurrent Neural Networks (RNNs) are very successful at solving challenging problems with sequential data. However, this observed efficiency is not yet entirely explained by theory. It is known that a certain class of multiplicative RNNs enjoys the property of depth efficiency---a shallow network of exponentially large width is necessary to realize the same score function as computed by such an RNN. Such networks, however, are not very often applied to real life tasks. In this work, we attempt to reduce the gap between theory and practice by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.10801", "author_id": ["GS5HTlkAAAAJ", "Z8GCLksAAAAJ", "5kMqBQEAAAAJ"], "url_scholarbib": "/scholar?q=info:v_oeqmem2-kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralized%2BTensor%2BModels%2BFor%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v_oeqmem2-kJ&ei=TmNeYvTkMMiBy9YP18Gi8As&json=", "num_citations": 20, "citedby_url": "/scholar?cites=16851245394902842047&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v_oeqmem2-kJ:scholar.google.com/&scioq=Generalized+Tensor+Models+For+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.10801"}, "On The Loss Landscape Of A Class Of Deep Neural Networks With No Bad Local Valleys": {"container_type": "Publication", "bib": {"title": "On the loss landscape of a class of deep neural networks with no bad local valleys", "author": ["Q Nguyen", "MC Mukkamala", "M Hein"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1809.10749", "abstract": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.10749", "author_id": ["", "aLrBW2QAAAAJ", "0ZAb3tsAAAAJ"], "url_scholarbib": "/scholar?q=info:9ckfQ5p5S5wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BLoss%2BLandscape%2BOf%2BA%2BClass%2BOf%2BDeep%2BNeural%2BNetworks%2BWith%2BNo%2BBad%2BLocal%2BValleys%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9ckfQ5p5S5wJ&ei=VGNeYpewNIySyATlkbrQCA&json=", "num_citations": 63, "citedby_url": "/scholar?cites=11262228996628138485&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9ckfQ5p5S5wJ:scholar.google.com/&scioq=On+The+Loss+Landscape+Of+A+Class+Of+Deep+Neural+Networks+With+No+Bad+Local+Valleys&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.10749"}, "Lanczosnet: Multi-scale Deep Graph Convolutional Networks": {"container_type": "Publication", "bib": {"title": "Lanczosnet: Multi-scale deep graph convolutional networks", "author": ["R Liao", "Z Zhao", "R Urtasun", "RS Zemel"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.01484", "abstract": "We propose the Lanczos network (LanczosNet), which uses the Lanczos algorithm to construct low rank approximations of the graph Laplacian for graph convolution. Relying on the tridiagonal decomposition of the Lanczos algorithm, we not only efficiently exploit multi-scale information via fast approximated computation of matrix power but also design learnable spectral filters. Being fully differentiable, LanczosNet facilitates both graph kernel learning as well as learning node embeddings. We show the connection between our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.01484", "author_id": ["2wrS35MAAAAJ", "1j4S5pUAAAAJ", "jyxO2akAAAAJ", "iBeDoRAAAAAJ"], "url_scholarbib": "/scholar?q=info:HREv1d5vyUAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLanczosnet:%2BMulti-scale%2BDeep%2BGraph%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HREv1d5vyUAJ&ei=V2NeYq7YM82Ny9YPqPyUgAs&json=", "num_citations": 162, "citedby_url": "/scholar?cites=4668385491596284189&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HREv1d5vyUAJ:scholar.google.com/&scioq=Lanczosnet:+Multi-scale+Deep+Graph+Convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.01484.pdf?utm_term="}, "Multiple-attribute Text Rewriting": {"container_type": "Publication", "bib": {"title": "Multiple-attribute text rewriting", "author": ["G Lample", "S Subramanian", "E Smith"], "pub_year": "2018", "venue": "International \u2026", "abstract": "with the original attribute, followed by its rewrite when given a different attribute value.  ;  and (b) we extend this model to support multiple attribute control. (iii) Finally, in Sec. 4.1 we point"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1g2NhC5KQ", "author_id": ["H7sVDmIAAAAJ", "7i101rcAAAAJ", ""], "url_scholarbib": "/scholar?q=info:aqjnCKOKboMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultiple-attribute%2BText%2BRewriting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aqjnCKOKboMJ&ei=W2NeYqmjBMLZmQHc1ovQAg&json=", "num_citations": 142, "citedby_url": "/scholar?cites=9470659499240433770&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aqjnCKOKboMJ:scholar.google.com/&scioq=Multiple-attribute+Text+Rewriting&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1g2NhC5KQ"}, "Learning Representations Of Sets Through Optimized Permutations": {"container_type": "Publication", "bib": {"title": "Learning representations of sets through optimized permutations", "author": ["Y Zhang", "J Hare", "A Pr\u00fcgel-Bennett"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.03928", "abstract": "Representations of sets are challenging to learn because operations on sets should be permutation-invariant. To this end, we propose a Permutation-Optimisation module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a permutation-invariant representation of that set, avoiding a bottleneck in traditional set models. We demonstrate our model's ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which we achieve state-of-the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.03928", "author_id": ["XtCqbfEAAAAJ", "UFeON5oAAAAJ", "oQgxYjkAAAAJ"], "url_scholarbib": "/scholar?q=info:xBjs8BSFFf8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRepresentations%2BOf%2BSets%2BThrough%2BOptimized%2BPermutations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xBjs8BSFFf8J&ei=XmNeYr67N9-Vy9YPs66ekAk&json=", "num_citations": 15, "citedby_url": "/scholar?cites=18380743779170392260&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xBjs8BSFFf8J:scholar.google.com/&scioq=Learning+Representations+Of+Sets+Through+Optimized+Permutations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.03928"}, "Diversity And Depth In Per-example Routing Models": {"container_type": "Publication", "bib": {"title": "Diversity and depth in per-example routing models", "author": ["P Ramachandran", "QV Le"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "Routing models, a form of conditional computation where examples are routed through a subset of components in a larger network, have shown promising results in recent works. Surprisingly, routing models to date have lacked important properties, such as architectural diversity and large numbers of routing decisions. Both architectural diversity and routing depth can increase the representational power of a routing network. In this work, we address both of these deficiencies. We discuss the significance of architectural diversity in routing"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkxWJnC9tX", "author_id": ["ktKXDuMAAAAJ", "vfT6-XIAAAAJ"], "url_scholarbib": "/scholar?q=info:Bs6xQYRvOdUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiversity%2BAnd%2BDepth%2BIn%2BPer-example%2BRouting%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Bs6xQYRvOdUJ&ei=YWNeYvL7O5HKsQKNt6-YAw&json=", "num_citations": 23, "citedby_url": "/scholar?cites=15364434217579695622&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Bs6xQYRvOdUJ:scholar.google.com/&scioq=Diversity+And+Depth+In+Per-example+Routing+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkxWJnC9tX"}, "Sparse Dictionary Learning By Dynamical Neural Networks": {"container_type": "Publication", "bib": {"title": "Sparse dictionary learning by dynamical neural networks", "author": ["TH Lin", "PTP Tang"], "pub_year": "2018", "venue": "International Conference on Learning \u2026", "abstract": "A dynamical neural network consists of a set of interconnected neurons that interact over time continuously. It can exhibit computational properties in the sense that the dynamical system's evolution and/or limit points in the associated state space can correspond to numerical solutions to certain mathematical optimization or learning problems. Such a computational system is particularly attractive in that it can be mapped to a massively parallel computer architecture for power and throughput efficiency, especially if each neuron"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1gstsCqt7", "author_id": ["XcX1w8YAAAAJ", ""], "url_scholarbib": "/scholar?q=info:9EcBOeqhMOsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSparse%2BDictionary%2BLearning%2BBy%2BDynamical%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9EcBOeqhMOsJ&ei=amNeYtqKK5yO6rQP-viEEA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=16947223425145980916&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9EcBOeqhMOsJ:scholar.google.com/&scioq=Sparse+Dictionary+Learning+By+Dynamical+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1gstsCqt7"}, "Don't Settle For Average, Go For The Max: Fuzzy Sets And Max-pooled Word Vectors": {"container_type": "Publication", "bib": {"title": "Don't settle for average, go for the max: fuzzy sets and max-pooled word vectors", "author": ["V Zhelezniak", "A Savkov", "A Shen", "F Moramarco"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these insights, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-words (FBoW) representation for text that contains all the words in the vocabulary simultaneously"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.13264", "author_id": ["HnnnfW4AAAAJ", "29R3NtYAAAAJ", "", "TBcAynYAAAAJ"], "url_scholarbib": "/scholar?q=info:G81Yllyor-4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDon%2527t%2BSettle%2BFor%2BAverage,%2BGo%2BFor%2BThe%2BMax:%2BFuzzy%2BSets%2BAnd%2BMax-pooled%2BWord%2BVectors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G81Yllyor-4J&ei=cmNeYu8mj5uYAaaKoIgH&json=", "num_citations": 29, "citedby_url": "/scholar?cites=17199150617564073243&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:G81Yllyor-4J:scholar.google.com/&scioq=Don%27t+Settle+For+Average,+Go+For+The+Max:+Fuzzy+Sets+And+Max-pooled+Word+Vectors&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.13264"}, "Complement Objective Training": {"container_type": "Publication", "bib": {"title": "Complement objective training", "author": ["HY Chen", "PH Wang", "CH Liu", "SC Chang", "JY Pan"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "For all the experiments conducted in this paper, we use this normalized complement  entropy as the complement objective to improve the baselines without further tuning of learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.01182", "author_id": ["zbhpHsYAAAAJ", "", "", "LXaFMMAAAAAJ", "bEn7ySYAAAAJ"], "url_scholarbib": "/scholar?q=info:abt0WBoy4wAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComplement%2BObjective%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=abt0WBoy4wAJ&ei=dWNeYqXCNIyuyASD3KfABw&json=", "num_citations": 24, "citedby_url": "/scholar?cites=63949908447902569&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:abt0WBoy4wAJ:scholar.google.com/&scioq=Complement+Objective+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.01182"}, "Cbow Is Not All You Need: Combining Cbow With The Compositional Matrix Space Model": {"container_type": "Publication", "bib": {"title": "CBOW is not all you need: Combining CBOW with the compositional matrix space model", "author": ["F Mai", "L Galke", "A Scherp"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.06423", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, ie, embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a learning algorithm for the Continuous Matrix Space Model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.06423", "author_id": ["MfETM20AAAAJ", "AHGGdYQAAAAJ", "LFTRuMEAAAAJ"], "url_scholarbib": "/scholar?q=info:DsKBt7MRzVMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCbow%2BIs%2BNot%2BAll%2BYou%2BNeed:%2BCombining%2BCbow%2BWith%2BThe%2BCompositional%2BMatrix%2BSpace%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DsKBt7MRzVMJ&ei=emNeYuzYIpGJmwGIxre4DA&json=", "num_citations": 10, "citedby_url": "/scholar?cites=6038502138949255694&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DsKBt7MRzVMJ:scholar.google.com/&scioq=Cbow+Is+Not+All+You+Need:+Combining+Cbow+With+The+Compositional+Matrix+Space+Model&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.06423"}, "Deep Frank-wolfe For Neural Network Optimization": {"container_type": "Publication", "bib": {"title": "Deep Frank-Wolfe for neural network optimization", "author": ["L Berrada", "A Zisserman", "MP Kumar"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.07591", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.07591", "author_id": ["zoae83AAAAAJ", "UZ5wscMAAAAJ", "BfmcfEAAAAAJ"], "url_scholarbib": "/scholar?q=info:mGY0FBk6CvQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BFrank-wolfe%2BFor%2BNeural%2BNetwork%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mGY0FBk6CvQJ&ei=fWNeYuijNOHDywTjooCQBQ&json=", "num_citations": 29, "citedby_url": "/scholar?cites=17584931574409094808&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mGY0FBk6CvQJ:scholar.google.com/&scioq=Deep+Frank-wolfe+For+Neural+Network+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.07591"}, "Function Space Particle Optimization For Bayesian Neural Networks": {"container_type": "Publication", "bib": {"title": "Function space particle optimization for bayesian neural networks", "author": ["Z Wang", "T Ren", "J Zhu", "B Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09754", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09754", "author_id": ["zMAlv2kAAAAJ", "VgNDYeYAAAAJ", "axsP38wAAAAJ", ""], "url_scholarbib": "/scholar?q=info:LZAv8K7RTy0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFunction%2BSpace%2BParticle%2BOptimization%2BFor%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LZAv8K7RTy0J&ei=gWNeYqfRI82Ny9YPqPyUgAs&json=", "num_citations": 33, "citedby_url": "/scholar?cites=3265058804151062573&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LZAv8K7RTy0J:scholar.google.com/&scioq=Function+Space+Particle+Optimization+For+Bayesian+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09754"}}