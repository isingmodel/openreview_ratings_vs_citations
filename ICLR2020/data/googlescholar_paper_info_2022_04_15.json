{"Learning From Explanations With Neural Module Execution Tree": {"container_type": "Publication", "bib": {"title": "Learning from explanations with neural execution tree", "author": ["Z Wang", "Y Qin", "W Zhou", "J Yan", "Q Ye", "L Neves"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural Execution Tree (NExT) framework for deep neural networks to learn from NL  explanations Given a raw corpus and a set of NL explanations, we first parse the NL explanations"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.01352", "author_id": ["xYRZiZkAAAAJ", "njm-G8wAAAAJ", "HHvcwrIAAAAJ", "rhNj2RcAAAAJ", "g230ERwAAAAJ", "dR3W0oQAAAAJ"], "url_scholarbib": "/scholar?q=info:sdmV05vyVW0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFrom%2BExplanations%2BWith%2BNeural%2BModule%2BExecution%2BTree%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sdmV05vyVW0J&ei=KthXYqziLo2EmgH6u5u4BA&json=", "num_citations": 25, "citedby_url": "/scholar?cites=7878469874238216625&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sdmV05vyVW0J:scholar.google.com/&scioq=Learning+From+Explanations+With+Neural+Module+Execution+Tree&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.01352"}, "Co-attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-occurring In Data": {"container_type": "Publication", "bib": {"title": "Co-attentive equivariant neural networks: Focusing equivariance on transformations co-occurring in data", "author": ["DW Romero", "M Hoogendoorn"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.07849", "abstract": "Equivariance is a nice property to have as it produces much more parameter efficient neural architectures and preserves the structure of the input through the feature mapping. Even though some combinations of transformations might never appear (eg an upright face with a horizontal nose), current equivariant architectures consider the set of all possible transformations in a transformation group when learning feature representations. Contrarily, the human visual system is able to attend to the set of relevant transformations occurring in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.07849", "author_id": ["7tdzmVoAAAAJ", "3s4lqHkAAAAJ"], "url_scholarbib": "/scholar?q=info:Ui-_Q5cO0OYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCo-attentive%2BEquivariant%2BNeural%2BNetworks:%2BFocusing%2BEquivariance%2BOn%2BTransformations%2BCo-occurring%2BIn%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ui-_Q5cO0OYJ&ei=LdhXYqD-OJGJmwGIxre4DA&json=", "num_citations": 18, "citedby_url": "/scholar?cites=16631809466718695250&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ui-_Q5cO0OYJ:scholar.google.com/&scioq=Co-attentive+Equivariant+Neural+Networks:+Focusing+Equivariance+On+Transformations+Co-occurring+In+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.07849"}, "Learning Self-correctable Policies And Value Functions From Demonstrations With Negative Sampling": {"container_type": "Publication", "bib": {"title": "Learning self-correctable policies and value functions from demonstrations with negative sampling", "author": ["Y Luo", "H Xu", "T Ma"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.05634", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results in cascading errors of the learned policy. We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.05634", "author_id": ["r1Fn_YsAAAAJ", "t9HPFawAAAAJ", "i38QlUwAAAAJ"], "url_scholarbib": "/scholar?q=info:bgdQygh8h1sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BSelf-correctable%2BPolicies%2BAnd%2BValue%2BFunctions%2BFrom%2BDemonstrations%2BWith%2BNegative%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bgdQygh8h1sJ&ei=MdhXYp3HJc2Ny9YPqPyUgAs&json=", "num_citations": 9, "citedby_url": "/scholar?cites=6595376556503205742&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bgdQygh8h1sJ:scholar.google.com/&scioq=Learning+Self-correctable+Policies+And+Value+Functions+From+Demonstrations+With+Negative+Sampling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.05634"}, "Bertscore: Evaluating Text Generation With Bert": {"container_type": "Publication", "bib": {"title": "Bertscore: Evaluating text generation with bert", "author": ["T Zhang", "V Kishore", "F Wu", "KQ Weinberger"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.09675", "author_id": ["OI0HSa0AAAAJ", "B8UeYcEAAAAJ", "sNL8SSoAAAAJ", "8RVWMycAAAAJ"], "url_scholarbib": "/scholar?q=info:KwnqbvBWnkkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBertscore:%2BEvaluating%2BText%2BGeneration%2BWith%2BBert%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KwnqbvBWnkkJ&ei=NdhXYtqNAYvMsQK69Y7ABg&json=", "num_citations": 837, "citedby_url": "/scholar?cites=5304773001741994283&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KwnqbvBWnkkJ:scholar.google.com/&scioq=Bertscore:+Evaluating+Text+Generation+With+Bert&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.09675"}, "Multiplicative Interactions And Where To Find Them": {"container_type": "Publication", "bib": {"title": "Multiplicative interactions and where to find them", "author": ["SM Jayakumar", "WM Czarnecki", "J Menick", "J Schwarz"], "pub_year": "2020", "venue": "NA", "abstract": "We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others. Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=PkNPqapnjaE", "author_id": ["rJUAY8QAAAAJ", "aOvr9eMAAAAJ", "te_gpaEAAAAJ", "Efs3XxQAAAAJ"], "url_scholarbib": "/scholar?q=info:CoxXUcx6QEMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultiplicative%2BInteractions%2BAnd%2BWhere%2BTo%2BFind%2BThem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CoxXUcx6QEMJ&ei=OdhXYqLFGovMsQK69Y7ABg&json=", "num_citations": 51, "citedby_url": "/scholar?cites=4846008217007262730&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CoxXUcx6QEMJ:scholar.google.com/&scioq=Multiplicative+Interactions+And+Where+To+Find+Them&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=PkNPqapnjaE"}, "Cophy: Counterfactual Learning Of Physical Dynamics": {"container_type": "Publication", "bib": {"title": "Cophy: Counterfactual learning of physical dynamics", "author": ["F Baradel", "N Neverova", "J Mille", "G Mori"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12000", "author_id": ["egECWaEAAAAJ", "cLPaHcIAAAAJ", "eyuTangAAAAJ", "Bl9FSL0AAAAJ"], "url_scholarbib": "/scholar?q=info:bSTxeBo4VOcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCophy:%2BCounterfactual%2BLearning%2BOf%2BPhysical%2BDynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bSTxeBo4VOcJ&ei=PdhXYoH-C4vMsQK69Y7ABg&json=", "num_citations": 47, "citedby_url": "/scholar?cites=16669009807154422893&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bSTxeBo4VOcJ:scholar.google.com/&scioq=Cophy:+Counterfactual+Learning+Of+Physical+Dynamics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12000"}, "Contrastive Representation Distillation": {"container_type": "Publication", "bib": {"title": "Contrastive representation distillation", "author": ["Y Tian", "D Krishnan", "P Isola"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.10699", "abstract": "the teacher's representation of the data. We formulate this objective as contrastive learning.  Experiments demonstrate that our resulting new objective outperforms knowledge distillation"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.10699", "author_id": ["OsP7JHAAAAAJ", "_MEuWIMAAAAJ", "ROILf3EAAAAJ"], "url_scholarbib": "/scholar?q=info:78Gu24p596AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContrastive%2BRepresentation%2BDistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=78Gu24p596AJ&ei=QdhXYpDxDI6pywSdh6agAg&json=", "num_citations": 334, "citedby_url": "/scholar?cites=11598873002614112751&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:78Gu24p596AJ:scholar.google.com/&scioq=Contrastive+Representation+Distillation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.10699.pdf?ref=https://githubhelp.com"}, "Target-embedding Autoencoders For Supervised Representation Learning": {"container_type": "Publication", "bib": {"title": "Target-embedding autoencoders for supervised representation learning", "author": ["D Jarrett", "M van der Schaar"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.08345", "abstract": "Autoencoder-based learning has emerged as a staple for disciplining representations in unsupervised and semi-supervised settings. This paper analyzes a framework for improving generalization in a purely supervised setting, where the target space is high-dimensional. We motivate and formalize the general framework of target-embedding autoencoders (TEA) for supervised prediction, learning intermediate latent representations jointly optimized to be both predictable from features as well as predictive of targets---encoding the prior that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.08345", "author_id": ["Pczk-PQAAAAJ", "DZ3S--MAAAAJ"], "url_scholarbib": "/scholar?q=info:DnfH8U2gY0UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTarget-embedding%2BAutoencoders%2BFor%2BSupervised%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DnfH8U2gY0UJ&ei=Q9hXYqupOM2Ny9YPqPyUgAs&json=", "num_citations": 4, "citedby_url": "/scholar?cites=5000016267940689678&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DnfH8U2gY0UJ:scholar.google.com/&scioq=Target-embedding+Autoencoders+For+Supervised+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.08345"}, "You Can Teach An Old Dog New Tricks! On Training Knowledge Graph Embeddings": {"container_type": "Publication", "bib": {"title": "You can teach an old dog new tricks! on training knowledge graph embeddings", "author": ["D Ruffinelli", "S Broscheit", "R Gemulla"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Knowledge graph embedding (KGE) models learn algebraic representations of the entities and relations in a knowledge graph. A vast number of KGE techniques for multi-relational link prediction have been proposed in the recent literature, often with state-of-the-art performance. These approaches differ along a number of dimensions, including different model architectures, different training strategies, and different approaches to hyperparameter optimization. In this paper, we take a step back and aim to summarize and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkxSmlBFvr", "author_id": ["", "Lz0pTxcAAAAJ", "s_GmFv0AAAAJ"], "url_scholarbib": "/scholar?q=info:EJLS-I49WAQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DYou%2BCan%2BTeach%2BAn%2BOld%2BDog%2BNew%2BTricks!%2BOn%2BTraining%2BKnowledge%2BGraph%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EJLS-I49WAQJ&ei=RthXYsKiJ4vMsQK69Y7ABg&json=", "num_citations": 93, "citedby_url": "/scholar?cites=313067858371449360&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EJLS-I49WAQJ:scholar.google.com/&scioq=You+Can+Teach+An+Old+Dog+New+Tricks!+On+Training+Knowledge+Graph+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkxSmlBFvr"}, "Differentiable Learning Of Numerical Rules In Knowledge Graphs": {"container_type": "Publication", "bib": {"title": "Differentiable learning of numerical rules in knowledge graphs", "author": ["PW Wang", "D Stepanova", "C Domokos"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Rules over a knowledge graph (KG) capture interpretable patterns in data and can be used for KG cleaning and completion. Inspired by the TensorLog differentiable logic framework, which compiles rule inference into a sequence of differentiable operations, recently a method called Neural LP has been proposed for learning the parameters as well as the structure of rules. However, it is limited with respect to the treatment of numerical features like age, weight or scientific measurements. We address this limitation by extending Neural"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJleKgrKwS", "author_id": ["vWP0uTIAAAAJ", "bKey58cAAAAJ", "ouFnqFgAAAAJ"], "url_scholarbib": "/scholar?q=info:aGl09XJKnB0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BLearning%2BOf%2BNumerical%2BRules%2BIn%2BKnowledge%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aGl09XJKnB0J&ei=SdhXYtKTHeHDywTjooCQBQ&json=", "num_citations": 17, "citedby_url": "/scholar?cites=2133662181071546728&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aGl09XJKnB0J:scholar.google.com/&scioq=Differentiable+Learning+Of+Numerical+Rules+In+Knowledge+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJleKgrKwS"}, "Deepsphere: A Graph-based Spherical Cnn": {"container_type": "Publication", "bib": {"title": "DeepSphere: a graph-based spherical CNN", "author": ["M Defferrard", "M Milani", "F Gusset"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the sampled sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2012.15000", "author_id": ["Ztj2-gUAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:8gc0khzfj_kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeepsphere:%2BA%2BGraph-based%2BSpherical%2BCnn%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8gc0khzfj_kJ&ei=TNhXYpDDI4ySyATlkbrQCA&json=", "num_citations": 38, "citedby_url": "/scholar?cites=17982837150918641650&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8gc0khzfj_kJ:scholar.google.com/&scioq=Deepsphere:+A+Graph-based+Spherical+Cnn&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2012.15000"}, "Learning To Control Pdes With Differentiable Physics": {"container_type": "Publication", "bib": {"title": "Learning to control pdes with differentiable physics", "author": ["P Holl", "V Koltun", "N Thuerey"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.07457", "abstract": "Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. We present a novel hierarchical predictor-corrector scheme which enables neural networks to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.07457", "author_id": ["LilimmEAAAAJ", "kg4bCpgAAAAJ", "GEehwv8AAAAJ"], "url_scholarbib": "/scholar?q=info:469QALoHr2oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BControl%2BPdes%2BWith%2BDifferentiable%2BPhysics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=469QALoHr2oJ&ei=T9hXYrDwKs6E6rQPz8uiuAc&json=", "num_citations": 81, "citedby_url": "/scholar?cites=7687371584395325411&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:469QALoHr2oJ:scholar.google.com/&scioq=Learning+To+Control+Pdes+With+Differentiable+Physics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.07457"}, "Infinite-horizon Off-policy Policy Evaluation With Multiple Behavior Policies": {"container_type": "Publication", "bib": {"title": "Infinite-horizon off-policy policy evaluation with multiple behavior policies", "author": ["X Chen", "L Wang", "Y Hang", "H Ge", "H Zha"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.04849", "abstract": "We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.04849", "author_id": ["qn7VHWIAAAAJ", "hqlU92YAAAAJ", "", "", "n1DQMIsAAAAJ"], "url_scholarbib": "/scholar?q=info:1Vi9HK6BJqcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInfinite-horizon%2BOff-policy%2BPolicy%2BEvaluation%2BWith%2BMultiple%2BBehavior%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1Vi9HK6BJqcJ&ei=UthXYt7vJYvMsQK69Y7ABg&json=", "num_citations": 3, "citedby_url": "/scholar?cites=12044456838255433941&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1Vi9HK6BJqcJ:scholar.google.com/&scioq=Infinite-horizon+Off-policy+Policy+Evaluation+With+Multiple+Behavior+Policies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.04849"}, "The Gambler's Problem And Beyond": {"container_type": "Publication", "bib": {"title": "The Gambler's Problem and Beyond", "author": ["B Wang", "S Li", "J Li", "SO Chan"], "pub_year": "2019", "venue": "arXiv preprint arXiv:2001.00102", "abstract": "With these characterizations being observed in simple problems like Mountain Car and the  Gambler\u2019s problem, our results are expected to be generalized to a variety of reinforcement"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00102", "author_id": ["cQe4OeYAAAAJ", "kMZgQxcAAAAJ", "IwwqUhYAAAAJ", ""], "url_scholarbib": "/scholar?q=info:yX7rlaoCmkQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BGambler%2527s%2BProblem%2BAnd%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yX7rlaoCmkQJ&ei=VthXYtLEG8iBy9YP18Gi8As&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:yX7rlaoCmkQJ:scholar.google.com/&scioq=The+Gambler%27s+Problem+And+Beyond&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00102"}, "Emergence Of Functional And Structural Properties Of The Head Direction System By Optimization Of Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks", "author": ["CJ Cueva", "PY Wang", "M Chin", "XX Wei"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.10189", "abstract": "Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain. While response properties of neurons in artificial neural networks bear similarities to those in the brain, the network architectures are often constrained to be different. Here we ask if a neural network can recover both neural representations and, if the architecture is unconstrained and optimized, the anatomical properties of neural circuits. We demonstrate this in a system where the connectivity and the functional organization have"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10189", "author_id": ["BxLgolsAAAAJ", "GjCIDGQAAAAJ", "", "7Pd1QzwAAAAJ"], "url_scholarbib": "/scholar?q=info:MMjMbAjmF3IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergence%2BOf%2BFunctional%2BAnd%2BStructural%2BProperties%2BOf%2BThe%2BHead%2BDirection%2BSystem%2BBy%2BOptimization%2BOf%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MMjMbAjmF3IJ&ei=WdhXYs_OIOHDywTjooCQBQ&json=", "num_citations": 14, "citedby_url": "/scholar?cites=8221292568647616560&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MMjMbAjmF3IJ:scholar.google.com/&scioq=Emergence+Of+Functional+And+Structural+Properties+Of+The+Head+Direction+System+By+Optimization+Of+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10189"}, "Model Based Reinforcement Learning For Atari": {"container_type": "Publication", "bib": {"title": "Model-based reinforcement learning for atari", "author": ["L Kaiser", "M Babaeizadeh", "P Milos", "B Osinski"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work, we aim to learn Atari games with a budget of just 100K agent steps (400K frames),  corresponding to about two hours of play time. Prior methods are generally not evaluated"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.00374", "author_id": ["JWmiQR0AAAAJ", "3Y4egcYAAAAJ", "Se68XecAAAAJ", "WuWWdKcAAAAJ"], "url_scholarbib": "/scholar?q=info:hTN05sa1LkMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel%2BBased%2BReinforcement%2BLearning%2BFor%2BAtari%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hTN05sa1LkMJ&ei=XNhXYv2xNs6E6rQPz8uiuAc&json=", "num_citations": 436, "citedby_url": "/scholar?cites=4841006515344388997&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hTN05sa1LkMJ:scholar.google.com/&scioq=Model+Based+Reinforcement+Learning+For+Atari&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.00374.pdf?ref=https://githubhelp.com"}, "Learning To Explore Using Active Neural Mapping": {"container_type": "Publication", "bib": {"title": "Learning to explore using active neural slam", "author": ["DS Chaplot", "D Gandhi", "S Gupta", "A Gupta"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "use Adam optimizer with a learning rate of 0.0001. We use binary cross-entropy loss for  obstacle map and explored  The obstacle map and explored area loss coefficients are 1 and the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.05155", "author_id": ["1MSpdmQAAAAJ", "YzxIuqwAAAAJ", "1HO5UacAAAAJ", "bqL73OkAAAAJ"], "url_scholarbib": "/scholar?q=info:Td2ibb17UqIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BExplore%2BUsing%2BActive%2BNeural%2BMapping%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Td2ibb17UqIJ&ei=YdhXYtfhK-HDywTjooCQBQ&json=", "num_citations": 176, "citedby_url": "/scholar?cites=11696547235753024845&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Td2ibb17UqIJ:scholar.google.com/&scioq=Learning+To+Explore+Using+Active+Neural+Mapping&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.05155"}, "Meta-q-learning": {"container_type": "Publication", "bib": {"title": "Meta-q-learning", "author": ["R Fakoor", "P Chaudhari", "S Soatto", "AJ Smola"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement  Learning (meta-RL). MQL builds upon three simple ideas. First, we show that Q-"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.00125", "author_id": ["nVsOPtQAAAAJ", "c_z5hWEAAAAJ", "lH1PdF8AAAAJ", "Tb0ZrYwAAAAJ"], "url_scholarbib": "/scholar?q=info:vqMtlhLowycJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-q-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vqMtlhLowycJ&ei=ZdhXYomtMsiBy9YP18Gi8As&json=", "num_citations": 53, "citedby_url": "/scholar?cites=2865388954464396222&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vqMtlhLowycJ:scholar.google.com/&scioq=Meta-q-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.00125"}, "Learning Transport Cost From Subset Correspondence": {"container_type": "Publication", "bib": {"title": "Learning transport cost from subset correspondence", "author": ["R Liu", "A Balsubramani", "J Zou"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.13203", "abstract": "Learning to align multiple datasets is an important problem with many applications, and it is especially useful when we need to integrate multiple experiments or correct for confounding. Optimal transport (OT) is a principled approach to align datasets, but a key challenge in applying OT is that we need to specify a transport cost function that accurately captures how the two datasets are related. Reliable cost functions are typically not available and practitioners often resort to using hand-crafted or Euclidean cost even if it may not be"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.13203", "author_id": ["7UkqY6gAAAAJ", "Hd80zWMAAAAJ", "23ZXZvEAAAAJ"], "url_scholarbib": "/scholar?q=info:4d8HVPbTrQ8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTransport%2BCost%2BFrom%2BSubset%2BCorrespondence%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4d8HVPbTrQ8J&ei=bthXYtWMDeHDywTjooCQBQ&json=", "num_citations": 7, "citedby_url": "/scholar?cites=1129792136465080289&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4d8HVPbTrQ8J:scholar.google.com/&scioq=Learning+Transport+Cost+From+Subset+Correspondence&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.13203"}, "How The Choice Of Activation Affects Training Of Overparametrized Neural Nets": {"container_type": "Publication", "bib": {"title": "Effect of activation functions on the training of overparametrized neural nets", "author": ["A Panigrahi", "A Shetty", "N Goyal"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.05660", "abstract": "effect of activation function on the training of highly overparametrized 2-layer neural  networks A crucial property that governs the performance of an activation is whether or not it is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.05660", "author_id": ["oMhp8p8AAAAJ", "M-y2aLUAAAAJ", "w8WJCnkAAAAJ"], "url_scholarbib": "/scholar?q=info:m6V7i1Q9v4gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BThe%2BChoice%2BOf%2BActivation%2BAffects%2BTraining%2BOf%2BOverparametrized%2BNeural%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m6V7i1Q9v4gJ&ei=cdhXYuuYLovEmgH7846QCg&json=", "num_citations": 21, "citedby_url": "/scholar?cites=9853661943036618139&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m6V7i1Q9v4gJ:scholar.google.com/&scioq=How+The+Choice+Of+Activation+Affects+Training+Of+Overparametrized+Neural+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.05660"}, "Dynamically Pruned Message Passing Networks For Large-scale Knowledge Graph Reasoning": {"container_type": "Publication", "bib": {"title": "Dynamically pruned message passing networks for large-scale knowledge graph reasoning", "author": ["X Xu", "W Feng", "Y Jiang", "X Xie", "Z Sun"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose Dynamically Pruned Message Passing Networks (DPMPN) for large-scale knowledge graph reasoning. In contrast to existing models, embedding-based or path-based, we learn an input-dependent subgraph to explicitly model reasoning process. Subgraphs are dynamically constructed and expanded by applying graphical attention mechanism conditioned on input queries. In this way, we not only construct graph-structured explanations but also enable message passing designed in Graph Neural Networks (GNNs)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11334", "author_id": ["zUpczmkAAAAJ", "cYrpt1kAAAAJ", "", "", "xkH30GgAAAAJ"], "url_scholarbib": "/scholar?q=info:qGhQBRuSoVcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamically%2BPruned%2BMessage%2BPassing%2BNetworks%2BFor%2BLarge-scale%2BKnowledge%2BGraph%2BReasoning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qGhQBRuSoVcJ&ei=dNhXYrO7M8iBy9YP18Gi8As&json=", "num_citations": 32, "citedby_url": "/scholar?cites=6314488797301074088&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qGhQBRuSoVcJ:scholar.google.com/&scioq=Dynamically+Pruned+Message+Passing+Networks+For+Large-scale+Knowledge+Graph+Reasoning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11334"}, "Image-guided Neural Object Rendering": {"container_type": "Publication", "bib": {"title": "Ignor: Image-guided neural object rendering", "author": ["J Thies", "M Zollh\u00f6fer", "C Theobalt", "M Stamminger"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "image-guided rendering technique that combines the benefits of image-based rendering   The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.10720", "author_id": ["4vpQvuwAAAAJ", "eQ8ZIG4AAAAJ", "eIWg8NMAAAAJ", "cx4AaqoAAAAJ"], "url_scholarbib": "/scholar?q=info:i-jwE2ZaaHYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImage-guided%2BNeural%2BObject%2BRendering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i-jwE2ZaaHYJ&ei=d9hXYsCwMMiBy9YP18Gi8As&json=", "num_citations": 40, "citedby_url": "/scholar?cites=8532168888521123979&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i-jwE2ZaaHYJ:scholar.google.com/&scioq=Image-guided+Neural+Object+Rendering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.10720"}, "Structured Object-aware Physics Prediction For Video Modeling And Planning": {"container_type": "Publication", "bib": {"title": "Structured object-aware physics prediction for video modeling and planning", "author": ["J Kossen", "K Stelzner", "M Hussing", "C Voelcker"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "When humans observe a physical system, they can easily locate objects, understand their interactions, and anticipate future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by combining an image model and a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02425", "author_id": ["i1FIOV0AAAAJ", "6eyxiGIAAAAJ", "LEDpvJEAAAAJ", "UZq8qZ8AAAAJ"], "url_scholarbib": "/scholar?q=info:npSpuNh3PoYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructured%2BObject-aware%2BPhysics%2BPrediction%2BFor%2BVideo%2BModeling%2BAnd%2BPlanning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=npSpuNh3PoYJ&ei=ethXYqrdJpaM6rQPlISayA8&json=", "num_citations": 35, "citedby_url": "/scholar?cites=9673300822333166750&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:npSpuNh3PoYJ:scholar.google.com/&scioq=Structured+Object-aware+Physics+Prediction+For+Video+Modeling+And+Planning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02425"}, "Going Beyond Token-level Pre-training For Embedding-based Large-scale Retrieval": {"container_type": "Publication", "bib": {"title": "Pre-training tasks for embedding-based large-scale retrieval", "author": ["WC Chang", "FX Yu", "YW Chang", "Y Yang"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider the large-scale query-document retrieval problem: given a query (eg, a question), return the set of relevant documents (eg, paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.03932", "author_id": ["nxBGx-0AAAAJ", "lYvF6cUAAAAJ", "ohIxH_QAAAAJ", "MlZq4XwAAAAJ"], "url_scholarbib": "/scholar?q=info:qAa_EHsesRYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGoing%2BBeyond%2BToken-level%2BPre-training%2BFor%2BEmbedding-based%2BLarge-scale%2BRetrieval%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qAa_EHsesRYJ&ei=fthXYqisA82Ny9YPqPyUgAs&json=", "num_citations": 117, "citedby_url": "/scholar?cites=1635121653622965928&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qAa_EHsesRYJ:scholar.google.com/&scioq=Going+Beyond+Token-level+Pre-training+For+Embedding-based+Large-scale+Retrieval&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.03932"}, "Measuring The Reliability Of Reinforcement Learning Algorithms": {"container_type": "Publication", "bib": {"title": "Measuring the reliability of reinforcement learning algorithms", "author": ["SCY Chan", "S Fishman", "J Canny", "A Korattikara"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.05663", "author_id": ["bXOt49QAAAAJ", "", "LAv0HTEAAAAJ", "20_pofsAAAAJ"], "url_scholarbib": "/scholar?q=info:oArdWDAEygwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2BThe%2BReliability%2BOf%2BReinforcement%2BLearning%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oArdWDAEygwJ&ei=gdhXYquvG5WMy9YPt8OamA0&json=", "num_citations": 27, "citedby_url": "/scholar?cites=921553679446510240&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oArdWDAEygwJ:scholar.google.com/&scioq=Measuring+The+Reliability+Of+Reinforcement+Learning+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.05663"}, "Fantastic Generalization Measures And Where To Find Them": {"container_type": "Publication", "bib": {"title": "Fantastic generalization measures and where to find them", "author": ["Y Jiang", "B Neyshabur", "H Mobahi", "D Krishnan"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02178", "author_id": ["x9qzWg8AAAAJ", "e1ucbCYAAAAJ", "GSHmKZkAAAAJ", "_MEuWIMAAAAJ"], "url_scholarbib": "/scholar?q=info:nyfGaAdwrYsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFantastic%2BGeneralization%2BMeasures%2BAnd%2BWhere%2BTo%2BFind%2BThem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nyfGaAdwrYsJ&ei=hNhXYqrLHIvMsQK69Y7ABg&json=", "num_citations": 199, "citedby_url": "/scholar?cites=10064823919367825311&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nyfGaAdwrYsJ:scholar.google.com/&scioq=Fantastic+Generalization+Measures+And+Where+To+Find+Them&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02178"}, "Generative Models For Effective Ml On Private, Decentralized Datasets": {"container_type": "Publication", "bib": {"title": "Generative models for effective ML on private, decentralized datasets", "author": ["S Augenstein", "HB McMahan", "D Ramage"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data-of representative samples, of outliers, of misclassifications-is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning or refining human-provided labels. However, manual data inspection is problematic for privacy sensitive datasets, such as those representing the behavior of real"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.06679", "author_id": ["QeKlROMAAAAJ", "iKPWydkAAAAJ", "D0NeJxMAAAAJ"], "url_scholarbib": "/scholar?q=info:RQ-de15CDIMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BModels%2BFor%2BEffective%2BMl%2BOn%2BPrivate,%2BDecentralized%2BDatasets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RQ-de15CDIMJ&ei=h9hXYqakMpWMy9YPt8OamA0&json=", "num_citations": 81, "citedby_url": "/scholar?cites=9442995492257337157&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RQ-de15CDIMJ:scholar.google.com/&scioq=Generative+Models+For+Effective+Ml+On+Private,+Decentralized+Datasets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.06679.pdf?ref=https://githubhelp.com"}, "Empirical Studies On The Properties Of Linear Regions In Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Empirical studies on the properties of linear regions in deep neural networks", "author": ["X Zhang", "D Wu"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.01072", "abstract": "A deep neural network (DNN) with piecewise linear activations can partition the input space into numerous small linear regions, where different linear functions are fitted. It is believed that the number of these regions represents the expressivity of the DNN. This paper provides a novel and meticulous perspective to look into DNNs: Instead of just counting the number of the linear regions, we study their local properties, such as the inspheres, the directions of the corresponding hyperplanes, the decision boundaries, and the relevance of the surrounding"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.01072", "author_id": ["jgzwvDIAAAAJ", "UYGzCPEAAAAJ"], "url_scholarbib": "/scholar?q=info:85Rp8vHRoWAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmpirical%2BStudies%2BOn%2BThe%2BProperties%2BOf%2BLinear%2BRegions%2BIn%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=85Rp8vHRoWAJ&ei=i9hXYteQBZaM6rQPlISayA8&json=", "num_citations": 22, "citedby_url": "/scholar?cites=6963077335975826675&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:85Rp8vHRoWAJ:scholar.google.com/&scioq=Empirical+Studies+On+The+Properties+Of+Linear+Regions+In+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.01072"}, "Mixout: Effective Regularization To Finetune Large-scale Pretrained Language Models": {"container_type": "Publication", "bib": {"title": "Mixout: Effective regularization to finetune large-scale pretrained language models", "author": ["C Lee", "K Cho", "W Kang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11299", "abstract": "In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as\" mixout\", motivated by"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11299", "author_id": ["KY9dTr8AAAAJ", "0RAmmIAAAAAJ", ""], "url_scholarbib": "/scholar?q=info:p3QBWlmwnAYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMixout:%2BEffective%2BRegularization%2BTo%2BFinetune%2BLarge-scale%2BPretrained%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p3QBWlmwnAYJ&ei=jthXYtf6OYySyATlkbrQCA&json=", "num_citations": 61, "citedby_url": "/scholar?cites=476449558403052711&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p3QBWlmwnAYJ:scholar.google.com/&scioq=Mixout:+Effective+Regularization+To+Finetune+Large-scale+Pretrained+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11299"}, "Combining Q-learning And Search With Amortized Value Estimates": {"container_type": "Publication", "bib": {"title": "Combining q-learning and search with amortized value estimates", "author": ["JB Hamrick", "V Bapst", "A Sanchez-Gonzalez"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce\" Search with Amortized Value Estimates\"(SAVE), an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In SAVE, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTS, resulting in a cooperative relationship between model-free learning and model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02807", "author_id": ["2ylcZSsAAAAJ", "95mnc80AAAAJ", "d1oQ8NcAAAAJ"], "url_scholarbib": "/scholar?q=info:lAOfXlAkfqsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCombining%2BQ-learning%2BAnd%2BSearch%2BWith%2BAmortized%2BValue%2BEstimates%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lAOfXlAkfqsJ&ei=kdhXYvn3JZaM6rQPlISayA8&json=", "num_citations": 34, "citedby_url": "/scholar?cites=12357354355154682772&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lAOfXlAkfqsJ:scholar.google.com/&scioq=Combining+Q-learning+And+Search+With+Amortized+Value+Estimates&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02807"}, "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints": {"container_type": "Publication", "bib": {"title": "Budgeted training: Rethinking deep neural network training under resource constraints", "author": ["M Li", "E Yumer", "D Ramanan"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.04753", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, ie, budgeted training. We analyze the following problem:\""}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.04753", "author_id": ["U4w1hqAAAAAJ", "s4Q8hbUAAAAJ", "9B8PoXUAAAAJ"], "url_scholarbib": "/scholar?q=info:shCaHDvdv3MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBudgeted%2BTraining:%2BRethinking%2BDeep%2BNeural%2BNetwork%2BTraining%2BUnder%2BResource%2BConstraints%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=shCaHDvdv3MJ&ei=lNhXYtrNJ4vMsQK69Y7ABg&json=", "num_citations": 20, "citedby_url": "/scholar?cites=8340628280866115762&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:shCaHDvdv3MJ:scholar.google.com/&scioq=Budgeted+Training:+Rethinking+Deep+Neural+Network+Training+Under+Resource+Constraints&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.04753"}, "On The Geometry And Learning Low-dimensional Embeddings For Directed Graphs": {"container_type": "Publication", "bib": {"title": "Low-dimensional hyperbolic knowledge graph embeddings", "author": ["I Chami", "A Wolf", "DC Juan", "F Sala", "S Ravi"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "hyperbolic geometry offers an exciting approach to learn low-dimensional embeddings while   Krackhardt hierarchy score For the directed graph Gr spanned by the relation r, we let R be"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.00545", "author_id": ["9ceELFsAAAAJ", "aQJylHMAAAAJ", "-bgrJfsAAAAJ", "9KhIkNkAAAAJ", "sM-dHicAAAAJ"], "url_scholarbib": "/scholar?q=info:sQlQ-IUODKEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BGeometry%2BAnd%2BLearning%2BLow-dimensional%2BEmbeddings%2BFor%2BDirected%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sQlQ-IUODKEJ&ei=mNhXYu6vCcS4ywTtzb_QDA&json=", "num_citations": 124, "citedby_url": "/scholar?cites=11604666308386359729&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sQlQ-IUODKEJ:scholar.google.com/&scioq=On+The+Geometry+And+Learning+Low-dimensional+Embeddings+For+Directed+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.00545"}, "Physics-aware Difference Graph Networks For Sparsely-observed Dynamics": {"container_type": "Publication", "bib": {"title": "Physics-aware difference graph networks for sparsely-observed dynamics", "author": ["S Seo", "C Meng", "Y Liu"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "Sparsely available data points cause numerical error on finite differences which hinders us from modeling the dynamics of physical systems. The discretization error becomes even larger when the sparse data are irregularly distributed or defined on an unstructured grid, making it hard to build deep learning models to handle physics-governing observations on the unstructured grid. In this paper, we propose a novel architecture, Physics-aware Difference Graph Networks (PA-DGN), which exploits neighboring information to learn finite"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1gelyrtwH", "author_id": ["spYH0tEAAAAJ", "nzkOdekAAAAJ", "UUKLPMYAAAAJ"], "url_scholarbib": "/scholar?q=info:1bD0JnAAmSgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPhysics-aware%2BDifference%2BGraph%2BNetworks%2BFor%2BSparsely-observed%2BDynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1bD0JnAAmSgJ&ei=m9hXYrqSGJaM6rQPlISayA8&json=", "num_citations": 23, "citedby_url": "/scholar?cites=2925369914643755221&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1bD0JnAAmSgJ:scholar.google.com/&scioq=Physics-aware+Difference+Graph+Networks+For+Sparsely-observed+Dynamics&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1gelyrtwH"}, "Meta-dataset: A Dataset Of Datasets For Learning To Learn From Few Examples": {"container_type": "Publication", "bib": {"title": "Meta-dataset: A dataset of datasets for learning to learn from few examples", "author": ["E Triantafillou", "T Zhu", "V Dumoulin", "P Lamblin"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.03096", "author_id": ["Y5x2ZgQAAAAJ", "_II5lqMAAAAJ", "mZfgLA4AAAAJ", "bn4xHHIAAAAJ"], "url_scholarbib": "/scholar?q=info:EV12claC_cUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-dataset:%2BA%2BDataset%2BOf%2BDatasets%2BFor%2BLearning%2BTo%2BLearn%2BFrom%2BFew%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EV12claC_cUJ&ei=nthXYo-yLMmUywTMkLbABQ&json=", "num_citations": 308, "citedby_url": "/scholar?cites=14266702502378757393&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EV12claC_cUJ:scholar.google.com/&scioq=Meta-dataset:+A+Dataset+Of+Datasets+For+Learning+To+Learn+From+Few+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.03096.pdf?ref=https://githubhelp.com"}, "Robust And Interpretable Blind Image Denoising Via Bias-free Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Robust and interpretable blind image denoising via bias-free convolutional neural networks", "author": ["S Mohan", "Z Kadkhodaie", "EP Simoncelli"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep convolutional networks often append additive constant (\" bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of\" batch normalization\"). Recent state-of-the-art blind denoising methods (eg, DnCNN) seem to require these terms for their success. Here, however, we show that these networks systematically overfit the noise levels for which they are trained: when deployed at"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.05478", "author_id": ["jaobZDsAAAAJ", "_b5JdjYAAAAJ", "MplR7_cAAAAJ"], "url_scholarbib": "/scholar?q=info:wyv9_siQeaIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BAnd%2BInterpretable%2BBlind%2BImage%2BDenoising%2BVia%2BBias-free%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wyv9_siQeaIJ&ei=odhXYpmYOo6pywSdh6agAg&json=", "num_citations": 37, "citedby_url": "/scholar?cites=11707547899272178627&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wyv9_siQeaIJ:scholar.google.com/&scioq=Robust+And+Interpretable+Blind+Image+Denoising+Via+Bias-free+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.05478"}, "Vid2game: Controllable Characters Extracted From Real-world Videos": {"container_type": "Publication", "bib": {"title": "Vid2game: Controllable characters extracted from real-world videos", "author": ["O Gafni", "L Wolf", "Y Taigman"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.08379", "abstract": "We are given a video of a person performing a certain activity, from which we extract a controllable model. The model generates novel image sequences of that person, according to arbitrary user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person. The method is based on two networks. The first network maps a current pose, and a single-instance control signal to the next pose. The"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.08379", "author_id": ["LAVRNjwAAAAJ", "UbFrXTsAAAAJ", "mbB3MRIAAAAJ"], "url_scholarbib": "/scholar?q=info:Aub2qp_1B6EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVid2game:%2BControllable%2BCharacters%2BExtracted%2BFrom%2BReal-world%2BVideos%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Aub2qp_1B6EJ&ei=pNhXYtG_MsLZmQHc1ovQAg&json=", "num_citations": 28, "citedby_url": "/scholar?cites=11603513031060284930&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Aub2qp_1B6EJ:scholar.google.com/&scioq=Vid2game:+Controllable+Characters+Extracted+From+Real-world+Videos&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.08379.pdf&xid=17259,15700021,15700186,15700191,15700253,15700256,15700259"}, "On The \"steerability\" Of Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "On the\" steerability\" of generative adversarial networks", "author": ["A Jahanian", "L Chai", "P Isola"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.07171", "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise-these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.07171", "author_id": ["nMpyjcwAAAAJ", "bunnQWQAAAAJ", "ROILf3EAAAAJ"], "url_scholarbib": "/scholar?q=info:GlFeVD18W_gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2B%2522steerability%2522%2BOf%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GlFeVD18W_gJ&ei=p9hXYpvIMo2EmgH6u5u4BA&json=", "num_citations": 192, "citedby_url": "/scholar?cites=17896034147137114394&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GlFeVD18W_gJ:scholar.google.com/&scioq=On+The+%22steerability%22+Of+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.07171"}, "Learning Hierarchical Discrete Linguistic Units From Visually-grounded Speech": {"container_type": "Publication", "bib": {"title": "Learning hierarchical discrete linguistic units from visually-grounded speech", "author": ["D Harwath", "WN Hsu", "J Glass"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.09602", "abstract": "In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.09602", "author_id": ["C0kDOzcAAAAJ", "N5HDmqoAAAAJ", "pfGI-KcAAAAJ"], "url_scholarbib": "/scholar?q=info:Cw-6rRlPv5kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BHierarchical%2BDiscrete%2BLinguistic%2BUnits%2BFrom%2BVisually-grounded%2BSpeech%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Cw-6rRlPv5kJ&ei=vthXYumfC4vEmgH7846QCg&json=", "num_citations": 58, "citedby_url": "/scholar?cites=11078660580062138123&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Cw-6rRlPv5kJ:scholar.google.com/&scioq=Learning+Hierarchical+Discrete+Linguistic+Units+From+Visually-grounded+Speech&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.09602"}, "Infinite-horizon Differentiable Model Predictive Control": {"container_type": "Publication", "bib": {"title": "Infinite-horizon differentiable model predictive control", "author": ["S East", "M Gallieri", "J Masci", "J Koutn\u00edk"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper proposes a differentiable linear quadratic Model Predictive Control (MPC) framework for safe imitation learning. The infinite-horizon cost is enforced using a terminal cost function obtained from the discrete-time algebraic Riccati equation (DARE), so that the learned controller can be proven to be stabilizing in closed-loop. A central contribution is the derivation of the analytical derivative of the solution of the DARE, thereby allowing the use of differentiation-based learning methods. A further contribution is the structure of the MPC"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02244", "author_id": ["6akT-SIAAAAJ", "moNjsXoAAAAJ", "HwDTzQEAAAAJ", ""], "url_scholarbib": "/scholar?q=info:-pzeMlnHFGwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInfinite-horizon%2BDifferentiable%2BModel%2BPredictive%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-pzeMlnHFGwJ&ei=yNhXYtL8LOHDywTjooCQBQ&json=", "num_citations": 15, "citedby_url": "/scholar?cites=7788068841549896954&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-pzeMlnHFGwJ:scholar.google.com/&scioq=Infinite-horizon+Differentiable+Model+Predictive+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02244"}, "Advectivenet: An Eulerian-lagrangian Fluidic Reservoir For Point Cloud Processing": {"container_type": "Publication", "bib": {"title": "Advectivenet: An Eulerian-Lagrangian fluidic reservoir for point cloud processing", "author": ["X He", "HL Cao", "B Zhu"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.00118", "abstract": "This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. Our learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, we are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.00118", "author_id": ["25tDZpwAAAAJ", "", "atNjbs0AAAAJ"], "url_scholarbib": "/scholar?q=info:HWgDD2JctesJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdvectivenet:%2BAn%2BEulerian-lagrangian%2BFluidic%2BReservoir%2BFor%2BPoint%2BCloud%2BProcessing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HWgDD2JctesJ&ei=y9hXYsqjIo6pywSdh6agAg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=16984583145926125597&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HWgDD2JctesJ:scholar.google.com/&scioq=Advectivenet:+An+Eulerian-lagrangian+Fluidic+Reservoir+For+Point+Cloud+Processing&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.00118.pdf?ref=https://githubhelp.com"}, "The Break-even Point On The Optimization Trajectories Of Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "The break-even point on optimization trajectories of deep neural networks", "author": ["S Jastrzebski", "M Szymczak", "S Fort", "D Arpit"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the\" break-even\" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.09572", "author_id": ["wbJxGQ8AAAAJ", "", "eu2Kzn0AAAAJ", "oU0jQIAAAAAJ"], "url_scholarbib": "/scholar?q=info:XbAD77No4xUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BBreak-even%2BPoint%2BOn%2BThe%2BOptimization%2BTrajectories%2BOf%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XbAD77No4xUJ&ei=zthXYsbTOOHDywTjooCQBQ&json=", "num_citations": 51, "citedby_url": "/scholar?cites=1577219416528236637&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XbAD77No4xUJ:scholar.google.com/&scioq=The+Break-even+Point+On+The+Optimization+Trajectories+Of+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.09572"}, "Understanding And Improving Information Transfer In Multi-task Learning": {"container_type": "Publication", "bib": {"title": "Understanding and improving information transfer in multi-task learning", "author": ["S Wu", "HR Zhang", "C R\u00e9"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2005.00944", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.00944", "author_id": ["PzoN2hgAAAAJ", "Sx-673sAAAAJ", "DnnCWN0AAAAJ"], "url_scholarbib": "/scholar?q=info:2wnw7LT63WAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BAnd%2BImproving%2BInformation%2BTransfer%2BIn%2BMulti-task%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2wnw7LT63WAJ&ei=2thXYvS6BZLeyQTms5KQBg&json=", "num_citations": 63, "citedby_url": "/scholar?cites=6980010652470348251&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2wnw7LT63WAJ:scholar.google.com/&scioq=Understanding+And+Improving+Information+Transfer+In+Multi-task+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.00944"}, "Abductive Commonsense Reasoning": {"container_type": "Publication", "bib": {"title": "Abductive commonsense reasoning", "author": ["C Bhagavatula", "RL Bras", "C Malaviya"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": ", but there are two critical distinctions that make abductive reasoning uniquely challenging.  First, abduction requires reasoning about commonsense implications of observations (eg, if"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.05739", "author_id": ["AsgHp14AAAAJ", "8dXLDSsAAAAJ", "s3MzzwwAAAAJ"], "url_scholarbib": "/scholar?q=info:1uavYGDOmOUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAbductive%2BCommonsense%2BReasoning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1uavYGDOmOUJ&ei=3thXYvPAEY6pywSdh6agAg&json=", "num_citations": 156, "citedby_url": "/scholar?cites=16544200144479839958&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1uavYGDOmOUJ:scholar.google.com/&scioq=Abductive+Commonsense+Reasoning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.05739?ref=https://githubhelp.com"}, "Tree-structured Attention With Hierarchical Accumulation": {"container_type": "Publication", "bib": {"title": "Tree-structured attention with hierarchical accumulation", "author": ["XP Nguyen", "S Joty", "SCH Hoi", "R Socher"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.08046", "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with\" Hierarchical Accumulation\" to encode parse tree"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.08046", "author_id": ["HN8VxX4AAAAJ", "hR249csAAAAJ", "JoLjflYAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:MyPkrf4lLPwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTree-structured%2BAttention%2BWith%2BHierarchical%2BAccumulation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MyPkrf4lLPwJ&ei=4thXYvTQD5aM6rQPlISayA8&json=", "num_citations": 33, "citedby_url": "/scholar?cites=18170940372302439219&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MyPkrf4lLPwJ:scholar.google.com/&scioq=Tree-structured+Attention+With+Hierarchical+Accumulation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.08046"}, "Guiding Program Synthesis By Learning To Generate Examples": {"container_type": "Publication", "bib": {"title": "Guiding program synthesis by learning to generate examples", "author": ["L Laich", "P Bielik", "M Vechev"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "A key challenge of existing program synthesizers is ensuring that the synthesized program generalizes well. This can be difficult to achieve as the specification provided by the end user is often limited, containing as few as one or two input-output examples. In this paper we address this challenge via an iterative approach that finds ambiguities in the provided specification and learns to resolve these by generating additional input-output examples. The main insight is to reduce the problem of selecting which program generalizes well to the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJl07ySKvS", "author_id": ["", "JNTwnawAAAAJ", "aZ1Rh50AAAAJ"], "url_scholarbib": "/scholar?q=info:-o_va6KtRNQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGuiding%2BProgram%2BSynthesis%2BBy%2BLearning%2BTo%2BGenerate%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-o_va6KtRNQJ&ei=5dhXYvSdI46pywSdh6agAg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=15295541147564216314&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-o_va6KtRNQJ:scholar.google.com/&scioq=Guiding+Program+Synthesis+By+Learning+To+Generate+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJl07ySKvS"}, "Once For All: Train One Network And Specialize It For Efficient Deployment": {"container_type": "Publication", "bib": {"title": "Once-for-all: Train one network and specialize it for efficient deployment", "author": ["H Cai", "C Gan", "T Wang", "Z Zhang", "S Han"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.09791", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $ CO_2 $ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.09791", "author_id": ["x-AvvrYAAAAJ", "PTeSCbIAAAAJ", "KfLHpPQAAAAJ", "", "E0iCaa4AAAAJ"], "url_scholarbib": "/scholar?q=info:nZ6Kgvf4cUUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOnce%2BFor%2BAll:%2BTrain%2BOne%2BNetwork%2BAnd%2BSpecialize%2BIt%2BFor%2BEfficient%2BDeployment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nZ6Kgvf4cUUJ&ei=6NhXYr3uFM2Ny9YPqPyUgAs&json=", "num_citations": 479, "citedby_url": "/scholar?cites=5004054402916064925&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nZ6Kgvf4cUUJ:scholar.google.com/&scioq=Once+For+All:+Train+One+Network+And+Specialize+It+For+Efficient+Deployment&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.09791"}, "Symplectic Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Symplectic recurrent neural networks", "author": ["Z Chen", "J Zhang", "M Arjovsky", "L Bottou"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.13334", "abstract": "We propose the Symplectic Recurrent Neural Network, which learns the dynamics of  Hamiltonian systems from data. Thanks to symplectic integration, multi-step training and initial state"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.13334", "author_id": ["sNCfPikAAAAJ", "srn7ay8AAAAJ", "A6qfFPkAAAAJ", "kbN88gsAAAAJ"], "url_scholarbib": "/scholar?q=info:kYvG13snVeMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSymplectic%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kYvG13snVeMJ&ei=7NhXYuaNK5LeyQTms5KQBg&json=", "num_citations": 99, "citedby_url": "/scholar?cites=16381042632484621201&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kYvG13snVeMJ:scholar.google.com/&scioq=Symplectic+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.13334"}, "Pay Attention To Features, Transfer Learn Faster Cnns": {"container_type": "Publication", "bib": {"title": "Pay attention to features, transfer learn faster CNNs", "author": ["K Wang", "X Gao", "Y Zhao", "X Li", "D Dou"], "pub_year": "2019", "venue": "\u2026 conference on learning \u2026", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryxyCeHtPB", "author_id": ["", "-YIUCL8AAAAJ", "lOOmgEgAAAAJ", "f9V0NZkAAAAJ", "qBHsQ04AAAAJ"], "url_scholarbib": "/scholar?q=info:rS9uyuzAt4IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPay%2BAttention%2BTo%2BFeatures,%2BTransfer%2BLearn%2BFaster%2BCnns%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rS9uyuzAt4IJ&ei=8NhXYqi1EuHDywTjooCQBQ&json=", "num_citations": 40, "citedby_url": "/scholar?cites=9419209268909715373&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rS9uyuzAt4IJ:scholar.google.com/&scioq=Pay+Attention+To+Features,+Transfer+Learn+Faster+Cnns&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryxyCeHtPB"}, "Compositional Continual Language Learning": {"container_type": "Publication", "bib": {"title": "Compositional continual language learning", "author": ["Y Li", "L Zhao", "K Church", "M Elhoseiny"], "pub_year": "2020", "venue": "NA", "abstract": "scenario of continual learning which handles sequence-to-sequence tasks common in language  learning. We  By leveraging the compositional learning approach, we propose the con-"}, "filled": false, "gsrank": 1, "pub_url": "https://repository.kaust.edu.sa/handle/10754/662643", "author_id": ["", "9xMR_iQAAAAJ", "E6aqGvYAAAAJ", "iRBUTOAAAAAJ"], "url_scholarbib": "/scholar?q=info:BBmk0KhpjiMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompositional%2BContinual%2BLanguage%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BBmk0KhpjiMJ&ei=89hXYtu4Ds2Ny9YPqPyUgAs&json=", "num_citations": 6, "citedby_url": "/scholar?cites=2562101411796228356&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BBmk0KhpjiMJ:scholar.google.com/&scioq=Compositional+Continual+Language+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://repository.kaust.edu.sa/bitstream/handle/10754/662643/compositional_language_continual_learning.pdf?sequence=1&isAllowed=y"}, "Gradientless Descent: High-dimensional Zeroth-order Optimization": {"container_type": "Publication", "bib": {"title": "Gradientless descent: High-dimensional zeroth-order optimization", "author": ["D Golovin", "J Karro", "G Kochanski", "C Lee", "X Song"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Zeroth-order optimization is the process of minimizing an objective $ f (x) $, given oracle access to evaluations at adaptively chosen inputs $ x $. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and present a novel analysis that shows convergence within an $\\epsilon $-ball of the optimum in $ O (kQ\\log (n)\\log (R/\\epsilon)) $ evaluations, for any"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.06317", "author_id": ["k8Px6S8AAAAJ", "", "NGEB-0wAAAAJ", "uQjGXWkAAAAJ", "GnpHmO8AAAAJ"], "url_scholarbib": "/scholar?q=info:oNTF9Di1llIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradientless%2BDescent:%2BHigh-dimensional%2BZeroth-order%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oNTF9Di1llIJ&ei=-NhXYvesEMWemAHB5baIBQ&json=", "num_citations": 36, "citedby_url": "/scholar?cites=5951143213846090912&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oNTF9Di1llIJ:scholar.google.com/&scioq=Gradientless+Descent:+High-dimensional+Zeroth-order+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.06317"}, "Graph Inference Learning For Semi-supervised Classification": {"container_type": "Publication", "bib": {"title": "Graph inference learning for semi-supervised classification", "author": ["C Xu", "Z Cui", "X Hong", "T Zhang", "J Yang", "W Liu"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work, we address semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner, but the performance could degrade significantly when labeled data is scarce. To this end, we propose a Graph Inference Learning (GIL) framework to boost the performance of semi-supervised node classification by learning the inference of node labels"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.06137", "author_id": ["VM_IRfMAAAAJ", "", "86XYNlgAAAAJ", "p6s1JlsAAAAJ", "6CIDtZQAAAAJ", "AjxoEpIAAAAJ"], "url_scholarbib": "/scholar?q=info:68fqWgqQmzMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BInference%2BLearning%2BFor%2BSemi-supervised%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=68fqWgqQmzMJ&ei=-9hXYs_jBcS4ywTtzb_QDA&json=", "num_citations": 20, "citedby_url": "/scholar?cites=3718724291473885163&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:68fqWgqQmzMJ:scholar.google.com/&scioq=Graph+Inference+Learning+For+Semi-supervised+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.06137"}, "Theory And Evaluation Metrics For Learning Disentangled Representations": {"container_type": "Publication", "bib": {"title": "Theory and evaluation metrics for learning disentangled representations", "author": ["K Do", "T Tran"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.09961", "abstract": "We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept\" disentangled representations\" used in supervised and unsupervised methods along three dimensions-informativeness, separability and interpretability-which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.09961", "author_id": ["aD6y8joAAAAJ", "zvspVLwAAAAJ"], "url_scholarbib": "/scholar?q=info:QTNccoV8e2cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTheory%2BAnd%2BEvaluation%2BMetrics%2BFor%2BLearning%2BDisentangled%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QTNccoV8e2cJ&ei=_thXYse1HeHDywTjooCQBQ&json=", "num_citations": 38, "citedby_url": "/scholar?cites=7456690520633127745&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QTNccoV8e2cJ:scholar.google.com/&scioq=Theory+And+Evaluation+Metrics+For+Learning+Disentangled+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.09961"}, "Data-independent Neural Pruning Via Coresets": {"container_type": "Publication", "bib": {"title": "Data-independent neural pruning via coresets", "author": ["B Mussay", "M Osadchy", "V Braverman", "S Zhou"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "first efficient, data-independent neural pruning algorithm with  Our method is based on the  coreset framework, which finds a  of a layer of neurons by a coreset of neurons in the previous"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.04018", "author_id": ["2n4_QNAAAAAJ", "nZEtlZoAAAAJ", "DTthB48AAAAJ", "NpjsgocAAAAJ"], "url_scholarbib": "/scholar?q=info:2-7_Rl4cI2oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DData-independent%2BNeural%2BPruning%2BVia%2BCoresets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2-7_Rl4cI2oJ&ei=BtlXYoGsDIySyATlkbrQCA&json=", "num_citations": 23, "citedby_url": "/scholar?cites=7647987783448915675&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2-7_Rl4cI2oJ:scholar.google.com/&scioq=Data-independent+Neural+Pruning+Via+Coresets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.04018"}, "End To End Trainable Active Contours Via Differentiable Rendering": {"container_type": "Publication", "bib": {"title": "End to end trainable active contours via differentiable rendering", "author": ["S Gur", "T Shaharabany", "L Wolf"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.00367", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.00367", "author_id": ["uuuU23UAAAAJ", "SyA5pnoAAAAJ", "UbFrXTsAAAAJ"], "url_scholarbib": "/scholar?q=info:nmcLebI1MUAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnd%2BTo%2BEnd%2BTrainable%2BActive%2BContours%2BVia%2BDifferentiable%2BRendering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nmcLebI1MUAJ&ei=DNlXYqyIG8WemAHB5baIBQ&json=", "num_citations": 18, "citedby_url": "/scholar?cites=4625537332937451422&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nmcLebI1MUAJ:scholar.google.com/&scioq=End+To+End+Trainable+Active+Contours+Via+Differentiable+Rendering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.00367"}, "Knowledge Consistency Between Neural Networks And Beyond": {"container_type": "Publication", "bib": {"title": "Knowledge consistency between neural networks and beyond", "author": ["R Liang", "T Li", "L Li", "J Wang", "Q Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.01581", "abstract": "This paper aims to analyze knowledge consistency between pre-trained deep neural networks. We propose a generic definition for knowledge consistency between neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge consistency as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.01581", "author_id": ["wTwaTMYAAAAJ", "XB6CydwAAAAJ", "", "", "iFFhHK0AAAAJ"], "url_scholarbib": "/scholar?q=info:_dNEigOXDdYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKnowledge%2BConsistency%2BBetween%2BNeural%2BNetworks%2BAnd%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_dNEigOXDdYJ&ei=DtlXYpjWNI2EmgH6u5u4BA&json=", "num_citations": 16, "citedby_url": "/scholar?cites=15424150340274279421&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_dNEigOXDdYJ:scholar.google.com/&scioq=Knowledge+Consistency+Between+Neural+Networks+And+Beyond&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.01581.pdf?ref=https://githubhelp.com"}, "Single Episode Transfer For Differing Environmental Dynamics In Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Single episode policy transfer in reinforcement learning", "author": ["J Yang", "B Petersen", "H Zha", "D Faissol"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.07719", "abstract": "single episode transfer in reinforcement learning (RL), we propose a general algorithm for  near-optimal test-time performance in a family of environments where differences in dynamics"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.07719", "author_id": ["7spgYVAAAAAJ", "N2WFmdgAAAAJ", "n1DQMIsAAAAJ", ""], "url_scholarbib": "/scholar?q=info:zuQQFSGDSx8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSingle%2BEpisode%2BTransfer%2BFor%2BDiffering%2BEnvironmental%2BDynamics%2BIn%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zuQQFSGDSx8J&ei=EdlXYrOBHJaM6rQPlISayA8&json=", "num_citations": 21, "citedby_url": "/scholar?cites=2255040216539653326&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zuQQFSGDSx8J:scholar.google.com/&scioq=Single+Episode+Transfer+For+Differing+Environmental+Dynamics+In+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.07719"}, "What Can Neural Networks Reason About?": {"container_type": "Publication", "bib": {"title": "What can neural networks reason about?", "author": ["K Xu", "J Li", "M Zhang", "SS Du", "K Kawarabayashi"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, eg, Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.13211", "author_id": ["eV2tuR8AAAAJ", "9TbXgQ0AAAAJ", "2_gFWe4AAAAJ", "OttawxUAAAAJ", "DWERCmsAAAAJ"], "url_scholarbib": "/scholar?q=info:wMTQ84H7m4gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhat%2BCan%2BNeural%2BNetworks%2BReason%2BAbout%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wMTQ84H7m4gJ&ei=FNlXYr2kOYvEmgH7846QCg&json=", "num_citations": 124, "citedby_url": "/scholar?cites=9843737946108249280&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wMTQ84H7m4gJ:scholar.google.com/&scioq=What+Can+Neural+Networks+Reason+About%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.13211.pdf?ref=https://githubhelp.com"}, "Learning From Rules Generalizing Labeled Exemplars": {"container_type": "Publication", "bib": {"title": "Learning from rules generalizing labeled exemplars", "author": ["A Awasthi", "S Ghosh", "R Goyal", "S Sarawagi"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.06025", "author_id": ["dqHSxE8AAAAJ", "GXxmD4QAAAAJ", "", "Hg4HmTAAAAAJ"], "url_scholarbib": "/scholar?q=info:qA9Uug2m1vwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFrom%2BRules%2BGeneralizing%2BLabeled%2BExemplars%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qA9Uug2m1vwJ&ei=GtlXYo_hDsmUywTMkLbABQ&json=", "num_citations": 35, "citedby_url": "/scholar?cites=18218931920464777128&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qA9Uug2m1vwJ:scholar.google.com/&scioq=Learning+From+Rules+Generalizing+Labeled+Exemplars&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.06025"}, "Learning Heuristics For Quantified Boolean Formulas Through Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning heuristics for quantified boolean formulas through reinforcement learning", "author": ["G Lederman", "M Rabe", "S Seshia"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms for  quantified Boolean formulas through deep reinforcement learning. We focus on a backtracking"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJluxREKDB", "author_id": ["", "gCp7X74AAAAJ", "SlZavnIAAAAJ"], "url_scholarbib": "/scholar?q=info:RdHhrAlCOVIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BHeuristics%2BFor%2BQuantified%2BBoolean%2BFormulas%2BThrough%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RdHhrAlCOVIJ&ei=HtlXYtPeN5WMy9YPt8OamA0&json=", "num_citations": 29, "citedby_url": "/scholar?cites=5924839394105217349&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RdHhrAlCOVIJ:scholar.google.com/&scioq=Learning+Heuristics+For+Quantified+Boolean+Formulas+Through+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJluxREKDB"}, "Discriminative Particle Filter Reinforcement Learning For Complex Partial Observations": {"container_type": "Publication", "bib": {"title": "Discriminative particle filter reinforcement learning for complex partial observations", "author": ["X Ma", "P Karkus", "D Hsu", "WS Lee", "N Ye"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.09884", "abstract": "Deep reinforcement learning is successful in decision making for sophisticated games, such as Atari, Go, etc. However, real-world decision making often requires reasoning with partial information extracted from complex visual observations. This paper presents Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for complex partial observations. DPFRL encodes a differentiable particle filter in the neural network policy for explicit reasoning with partial observations over time. The particle filter"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.09884", "author_id": ["hR4G6hoAAAAJ", "cjUid0YAAAAJ", "S9LHLKEAAAAJ", "8PCrLgwAAAAJ", "5XCxGRIAAAAJ"], "url_scholarbib": "/scholar?q=info:OLHOinwdaxYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscriminative%2BParticle%2BFilter%2BReinforcement%2BLearning%2BFor%2BComplex%2BPartial%2BObservations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OLHOinwdaxYJ&ei=IdlXYtjRKMWemAHB5baIBQ&json=", "num_citations": 16, "citedby_url": "/scholar?cites=1615417312084406584&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OLHOinwdaxYJ:scholar.google.com/&scioq=Discriminative+Particle+Filter+Reinforcement+Learning+For+Complex+Partial+Observations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.09884"}, "Variational Recurrent Models For Solving Partially Observable Control Tasks": {"container_type": "Publication", "bib": {"title": "Variational recurrent models for solving partially observable control tasks", "author": ["D Han", "K Doya", "J Tani"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.10703", "abstract": "In partially observable (PO) environments, deep reinforcement learning (RL) agents often suffer from unsatisfactory performance, since two problems need to be tackled together: how to extract information from the raw observations to solve the task, and how to improve the policy. In this study, we propose an RL algorithm for solving PO tasks. Our method comprises two parts: a variational recurrent model (VRM) for modeling the environment, and an RL controller that has access to both the environment and the VRM. The proposed"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10703", "author_id": ["3V_9fRUAAAAJ", "SHufeXQAAAAJ", "9tkWMqgAAAAJ"], "url_scholarbib": "/scholar?q=info:SppRRZiLYJMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BRecurrent%2BModels%2BFor%2BSolving%2BPartially%2BObservable%2BControl%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SppRRZiLYJMJ&ei=JdlXYumeJsWemAHB5baIBQ&json=", "num_citations": 28, "citedby_url": "/scholar?cites=10619641407453895242&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SppRRZiLYJMJ:scholar.google.com/&scioq=Variational+Recurrent+Models+For+Solving+Partially+Observable+Control+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10703"}, "Thieves On Sesame Street! Model Extraction Of Bert-based Apis": {"container_type": "Publication", "bib": {"title": "Thieves on sesame street! model extraction of bert-based apis", "author": ["K Krishna", "GS Tomar", "AP Parikh", "N Papernot"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.12366", "author_id": ["9g2BsMUAAAAJ", "p1SDN0oAAAAJ", "bRpjhycAAAAJ", "cGxq0cMAAAAJ"], "url_scholarbib": "/scholar?q=info:cSwrF9KlMN8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThieves%2BOn%2BSesame%2BStreet!%2BModel%2BExtraction%2BOf%2BBert-based%2BApis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cSwrF9KlMN8J&ei=KNlXYviUO8S4ywTtzb_QDA&json=", "num_citations": 65, "citedby_url": "/scholar?cites=16082536591090461809&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cSwrF9KlMN8J:scholar.google.com/&scioq=Thieves+On+Sesame+Street!+Model+Extraction+Of+Bert-based+Apis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.12366"}, "Rgbd-gan: Unsupervised 3d Representation Learning From Natural Image Datasets Via Rgbd Image Synthesis": {"container_type": "Publication", "bib": {"title": "Rgbd-gan: Unsupervised 3d representation learning from natural image datasets via rgbd image synthesis", "author": ["A Noguchi", "T Harada"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12573", "abstract": "Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter-conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12573", "author_id": ["FYp-TCMAAAAJ", "k8rlJ8AAAAAJ"], "url_scholarbib": "/scholar?q=info:4UbnMBTZkSYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRgbd-gan:%2BUnsupervised%2B3d%2BRepresentation%2BLearning%2BFrom%2BNatural%2BImage%2BDatasets%2BVia%2BRgbd%2BImage%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4UbnMBTZkSYJ&ei=K9lXYpikOYySyATlkbrQCA&json=", "num_citations": 9, "citedby_url": "/scholar?cites=2779241125807343329&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4UbnMBTZkSYJ:scholar.google.com/&scioq=Rgbd-gan:+Unsupervised+3d+Representation+Learning+From+Natural+Image+Datasets+Via+Rgbd+Image+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12573"}, "The Implicit Bias Of Depth: How Incremental Learning Drives Generalization": {"container_type": "Publication", "bib": {"title": "The implicit bias of depth: How incremental learning drives generalization", "author": ["D Gissin", "S Shalev-Shwartz", "A Daniely"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12051", "abstract": "A leading hypothesis for the surprising generalization of neural networks is that the dynamics of gradient descent bias the model towards simple solutions, by searching through the solution space in an incremental order of complexity. We formally define the notion of incremental learning dynamics and derive the conditions on depth and initialization for which this phenomenon arises in deep linear models. Our main theoretical contribution is a dynamical depth separation result, proving that while shallow models can exhibit"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12051", "author_id": ["2dXObkoAAAAJ", "uYVc9koAAAAJ", "jUtYwE0AAAAJ"], "url_scholarbib": "/scholar?q=info:ek6KnFPM0L0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BImplicit%2BBias%2BOf%2BDepth:%2BHow%2BIncremental%2BLearning%2BDrives%2BGeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ek6KnFPM0L0J&ei=L9lXYuKLAcWemAHB5baIBQ&json=", "num_citations": 19, "citedby_url": "/scholar?cites=13677656727804857978&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ek6KnFPM0L0J:scholar.google.com/&scioq=The+Implicit+Bias+Of+Depth:+How+Incremental+Learning+Drives+Generalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12051"}, "High Fidelity Speech Synthesis With Adversarial Networks": {"container_type": "Publication", "bib": {"title": "High fidelity speech synthesis with adversarial networks", "author": ["M Bi\u0144kowski", "J Donahue", "S Dieleman", "A Clark"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11646", "author_id": ["wVZXAk0AAAAJ", "UfbuDH8AAAAJ", "yNNIKJsAAAAJ", "FON6hKEAAAAJ"], "url_scholarbib": "/scholar?q=info:qXI2T5_NiKMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHigh%2BFidelity%2BSpeech%2BSynthesis%2BWith%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qXI2T5_NiKMJ&ei=M9lXYs34FMLZmQHc1ovQAg&json=", "num_citations": 145, "citedby_url": "/scholar?cites=11783894509127365289&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qXI2T5_NiKMJ:scholar.google.com/&scioq=High+Fidelity+Speech+Synthesis+With+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11646"}, "Towards Hierarchical Importance Attribution: Explaining Compositional Semantics For Neural Sequence Models": {"container_type": "Publication", "bib": {"title": "Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models", "author": ["X Jin", "Z Wei", "J Du", "X Xue", "X Ren"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.06194", "abstract": "The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, eg contextual"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.06194", "author_id": ["5g3xIVIAAAAJ", "AjLDxxgAAAAJ", "Z2d0glgAAAAJ", "DTbhX6oAAAAJ", "_moJlrIAAAAJ"], "url_scholarbib": "/scholar?q=info:c4AWkVRsVZoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BHierarchical%2BImportance%2BAttribution:%2BExplaining%2BCompositional%2BSemantics%2BFor%2BNeural%2BSequence%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c4AWkVRsVZoJ&ei=NtlXYpWrEsiBy9YP18Gi8As&json=", "num_citations": 56, "citedby_url": "/scholar?cites=11120913965328531571&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c4AWkVRsVZoJ:scholar.google.com/&scioq=Towards+Hierarchical+Importance+Attribution:+Explaining+Compositional+Semantics+For+Neural+Sequence+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.06194"}, "Disagreement-regularized Imitation Learning": {"container_type": "Publication", "bib": {"title": "Disagreement-regularized imitation learning", "author": ["K Brantley", "W Sun", "M Henaff"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkgbYyHtwB", "author_id": ["a-OUzk4AAAAJ", "iOLC30YAAAAJ", "bX__wkYAAAAJ"], "url_scholarbib": "/scholar?q=info:jTu9JO-0l-QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDisagreement-regularized%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jTu9JO-0l-QJ&ei=OdlXYvewKJGJmwGIxre4DA&json=", "num_citations": 50, "citedby_url": "/scholar?cites=16471833101337443213&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jTu9JO-0l-QJ:scholar.google.com/&scioq=Disagreement-regularized+Imitation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkgbYyHtwB"}, "A Stochastic Derivative Free Optimization Method With Momentum": {"container_type": "Publication", "bib": {"title": "A stochastic derivative free optimization method with momentum", "author": ["E Gorbunov", "A Bibi", "O Sener", "EH Bergou"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider the problem of unconstrained minimization of a smooth objective function in $\\mathbb {R}^ d $ in setting where only function evaluations are possible. We propose and analyze stochastic zeroth-order method with heavy ball momentum. In particular, we propose, SMTP, a momentum version of the stochastic three-point method (STP)\\cite {Bergou_2018}. We show new complexity results for non-convex, convex and strongly convex functions. We test our method on a collection of learning to continuous control tasks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.13278", "author_id": ["85j2RqQAAAAJ", "Q4j2laYAAAAJ", "BI8xFr4AAAAJ", "qzxprWoAAAAJ"], "url_scholarbib": "/scholar?q=info:yhmAjYMCE10J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BStochastic%2BDerivative%2BFree%2BOptimization%2BMethod%2BWith%2BMomentum%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yhmAjYMCE10J&ei=P9lXYqKNA5GJmwGIxre4DA&json=", "num_citations": 12, "citedby_url": "/scholar?cites=6706707034122754506&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yhmAjYMCE10J:scholar.google.com/&scioq=A+Stochastic+Derivative+Free+Optimization+Method+With+Momentum&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.13278"}, "Triple Wins: Boosting Accuracy, Robustness And Efficiency Together By Enabling Input-adaptive Inference": {"container_type": "Publication", "bib": {"title": "Triple wins: Boosting accuracy, robustness and efficiency together by enabling input-adaptive inference", "author": ["TK Hu", "T Chen", "H Wang", "Z Wang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.10025", "abstract": "Deep networks were recently suggested to face the odds between accuracy (on clean natural images) and robustness (on adversarially perturbed images)(Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win between accuracy and robustness, yet at the expense of model size"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10025", "author_id": ["rwdBklIAAAAJ", "LE3ctn0AAAAJ", "aMIJhlEAAAAJ", "pxFyKAIAAAAJ"], "url_scholarbib": "/scholar?q=info:OTleDgYZcusJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTriple%2BWins:%2BBoosting%2BAccuracy,%2BRobustness%2BAnd%2BEfficiency%2BTogether%2BBy%2BEnabling%2BInput-adaptive%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OTleDgYZcusJ&ei=QtlXYq2XCpaM6rQPlISayA8&json=", "num_citations": 40, "citedby_url": "/scholar?cites=16965650260059633977&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OTleDgYZcusJ:scholar.google.com/&scioq=Triple+Wins:+Boosting+Accuracy,+Robustness+And+Efficiency+Together+By+Enabling+Input-adaptive+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10025"}, "Tranquil Clouds: Neural Networks For Learning Temporally Coherent Features In Point Clouds": {"container_type": "Publication", "bib": {"title": "Tranquil clouds: Neural networks for learning temporally coherent features in point clouds", "author": ["L Prantl", "N Chentanez", "S Jeschke"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Point clouds, as a form of Lagrangian representation, allow for powerful and flexible applications in a large number of computational disciplines. We propose a novel deep-learning method to learn stable and temporally coherent feature spaces for points clouds that change over time. We identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.05279", "author_id": ["oKLBsnsAAAAJ", "sM41KiAAAAAJ", "e_7oynAAAAAJ"], "url_scholarbib": "/scholar?q=info:ib6oB4ecjGYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTranquil%2BClouds:%2BNeural%2BNetworks%2BFor%2BLearning%2BTemporally%2BCoherent%2BFeatures%2BIn%2BPoint%2BClouds%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ib6oB4ecjGYJ&ei=RNlXYoOnK5WMy9YPt8OamA0&json=", "num_citations": 11, "citedby_url": "/scholar?cites=7389453192371158665&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ib6oB4ecjGYJ:scholar.google.com/&scioq=Tranquil+Clouds:+Neural+Networks+For+Learning+Temporally+Coherent+Features+In+Point+Clouds&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.05279.pdf?ref=https://githubhelp.com"}, "Stochastic Auc Maximization With Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Stochastic auc maximization with deep neural networks", "author": ["M Liu", "Z Yuan", "Y Ying", "T Yang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.10831", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.10831", "author_id": ["KFoEnFQAAAAJ", "ZjJf6tYAAAAJ", "xnA_lMMAAAAJ", "BCxFU0EAAAAJ"], "url_scholarbib": "/scholar?q=info:gV9SQHAUzzsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BAuc%2BMaximization%2BWith%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gV9SQHAUzzsJ&ei=SNlXYuqIDJGJmwGIxre4DA&json=", "num_citations": 30, "citedby_url": "/scholar?cites=4309685840764886913&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gV9SQHAUzzsJ:scholar.google.com/&scioq=Stochastic+Auc+Maximization+With+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.10831"}, "Augmenting Genetic Algorithms With Deep Neural Networks For Exploring The Chemical Space": {"container_type": "Publication", "bib": {"title": "Augmenting genetic algorithms with deep neural networks for exploring the chemical space", "author": ["AK Nigam", "P Friederich", "M Krenn"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Challenges in natural sciences can often be phrased as optimization problems. Machine learning techniques have recently been applied to solve such problems. One example in chemistry is the design of tailor-made organic materials and molecules, which requires efficient methods to explore the chemical space. We present a genetic algorithm (GA) that is enhanced with a neural network (DNN) based discriminator model to improve the diversity of generated molecules and at the same time steer the GA. We show that our algorithm"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11655", "author_id": ["uw-qjMwAAAAJ", "3B5h6u0AAAAJ", "jzG7GC8AAAAJ"], "url_scholarbib": "/scholar?q=info:zvuyoSIBGUEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAugmenting%2BGenetic%2BAlgorithms%2BWith%2BDeep%2BNeural%2BNetworks%2BFor%2BExploring%2BThe%2BChemical%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zvuyoSIBGUEJ&ei=TtlXYvOxHoySyATlkbrQCA&json=", "num_citations": 64, "citedby_url": "/scholar?cites=4690781735136459726&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zvuyoSIBGUEJ:scholar.google.com/&scioq=Augmenting+Genetic+Algorithms+With+Deep+Neural+Networks+For+Exploring+The+Chemical+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11655"}, "A Signal Propagation Perspective For Pruning Neural Networks At Initialization": {"container_type": "Publication", "bib": {"title": "A signal propagation perspective for pruning neural networks at initialization", "author": ["N Lee", "T Ajanthan", "S Gould", "PHS Torr"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.06307", "abstract": "Network pruning is a promising avenue for compressing deep neural networks. A typical approach to pruning starts by training a model and then removing redundant parameters while minimizing the impact on what is learned. Alternatively, a recent approach shows that pruning can be done at initialization prior to training, based on a saliency criterion called connection sensitivity. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. In this work, by noting connection sensitivity"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.06307", "author_id": ["wi9q5T8AAAAJ", "Rza8c10AAAAJ", "YvdzeM8AAAAJ", "kPxa2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:w9sHQ4eDjvgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BSignal%2BPropagation%2BPerspective%2BFor%2BPruning%2BNeural%2BNetworks%2BAt%2BInitialization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w9sHQ4eDjvgJ&ei=UdlXYoTTKYvMsQK69Y7ABg&json=", "num_citations": 75, "citedby_url": "/scholar?cites=17910397385067453379&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w9sHQ4eDjvgJ:scholar.google.com/&scioq=A+Signal+Propagation+Perspective+For+Pruning+Neural+Networks+At+Initialization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.06307"}, "Neural Tangents: Fast And Easy Infinite Neural Networks In Python": {"container_type": "Publication", "bib": {"title": "Neural tangents: Fast and easy infinite neural networks in python", "author": ["R Novak", "L Xiao", "J Hron", "J Lee", "AA Alemi"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural Tangents is a library designed to enable research into infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02803", "author_id": ["syG6krEAAAAJ", "fvwzUnIAAAAJ", "Jp7hKlAAAAAJ", "d3YhiooAAAAJ", "68hTs9wAAAAJ"], "url_scholarbib": "/scholar?q=info:klSY32yt7zcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BTangents:%2BFast%2BAnd%2BEasy%2BInfinite%2BNeural%2BNetworks%2BIn%2BPython%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=klSY32yt7zcJ&ei=WdlXYoe0EovEmgH7846QCg&json=", "num_citations": 110, "citedby_url": "/scholar?cites=4030630874639258770&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:klSY32yt7zcJ:scholar.google.com/&scioq=Neural+Tangents:+Fast+And+Easy+Infinite+Neural+Networks+In+Python&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02803"}, "Stable Rank Normalization For Improved Generalization In Neural Networks And Gans": {"container_type": "Publication", "bib": {"title": "Stable rank normalization for improved generalization in neural networks and gans", "author": ["A Sanyal", "PHS Torr", "PK Dokania"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.04659", "abstract": "Exciting new work on the generalization bounds for neural networks (NN) given by Neyshabur et al., Bartlett et al. closely depend on two parameter-depenedent quantities: the Lipschitz constant upper-bound and the stable rank (a softer version of the rank operator). This leads to an interesting question of whether controlling these quantities might improve the generalization behaviour of NNs. To this end, we propose stable rank normalization (SRN), a novel, optimal, and computationally efficient weight-normalization scheme which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.04659", "author_id": ["cRLqsyYAAAAJ", "kPxa2w0AAAAJ", "WsM7ybkAAAAJ"], "url_scholarbib": "/scholar?q=info:7sMEXEeGkxkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStable%2BRank%2BNormalization%2BFor%2BImproved%2BGeneralization%2BIn%2BNeural%2BNetworks%2BAnd%2BGans%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7sMEXEeGkxkJ&ei=XNlXYovXCpGJmwGIxre4DA&json=", "num_citations": 25, "citedby_url": "/scholar?cites=1842964313569281006&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7sMEXEeGkxkJ:scholar.google.com/&scioq=Stable+Rank+Normalization+For+Improved+Generalization+In+Neural+Networks+And+Gans&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.04659"}, "Structbert: Incorporating Language Structures Into Pre-training For Deep Language Understanding": {"container_type": "Publication", "bib": {"title": "Structbert: Incorporating language structures into pre-training for deep language understanding", "author": ["W Wang", "B Bi", "M Yan", "C Wu", "Z Bao", "J Xia"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.04577", "author_id": ["QtvY1cYAAAAJ", "", "", "ZvGfyVUAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:JTdgM_jqADQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructbert:%2BIncorporating%2BLanguage%2BStructures%2BInto%2BPre-training%2BFor%2BDeep%2BLanguage%2BUnderstanding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JTdgM_jqADQJ&ei=X9lXYqeqA5LeyQTms5KQBg&json=", "num_citations": 60, "citedby_url": "/scholar?cites=3747253241706985253&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JTdgM_jqADQJ:scholar.google.com/&scioq=Structbert:+Incorporating+Language+Structures+Into+Pre-training+For+Deep+Language+Understanding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.04577"}, "The Logical Expressiveness Of Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "The logical expressiveness of graph neural networks", "author": ["P Barcel\u00f3", "E Kostylev", "M Monet"], "pub_year": "2020", "venue": "8th International \u2026", "abstract": "The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (ie, functions classifying nodes in graphs as true or false) can be expressed by GNNs. We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC 2, a well-studied fragment of first order logic. FOC 2 is tightly related to the"}, "filled": false, "gsrank": 1, "pub_url": "https://hal.archives-ouvertes.fr/hal-03356968/", "author_id": ["9OH3PokAAAAJ", "Y2gdVmIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:yzpbtiOX05QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLogical%2BExpressiveness%2BOf%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yzpbtiOX05QJ&ei=YtlXYvGGEI2EmgH6u5u4BA&json=", "num_citations": 76, "citedby_url": "/scholar?cites=10724081317338364619&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yzpbtiOX05QJ:scholar.google.com/&scioq=The+Logical+Expressiveness+Of+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://hal.archives-ouvertes.fr/hal-03356968/document"}, "Spike-based Causal Inference For Weight Alignment": {"container_type": "Publication", "bib": {"title": "Spike-based causal inference for weight alignment", "author": ["J Guerguiev", "KP Kording", "BA Richards"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.01689", "abstract": "In artificial neural networks trained with gradient descent, the weights used for processing stimuli are also used during backward passes to calculate gradients. For the real brain to approximate gradients, gradient information would have to be propagated separately, such that one set of synaptic weights is used for processing and another set is used for backward passes. This produces the so-called\" weight transport problem\" for biological models of learning, where the backward weights used to calculate gradients need to mirror the forward"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01689", "author_id": ["hbmVVF0AAAAJ", "MiFqJGcAAAAJ", "1CPY1LsAAAAJ"], "url_scholarbib": "/scholar?q=info:Dy21z19c_EwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpike-based%2BCausal%2BInference%2BFor%2BWeight%2BAlignment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Dy21z19c_EwJ&ei=ZdlXYvnWGpaM6rQPlISayA8&json=", "num_citations": 17, "citedby_url": "/scholar?cites=5547410407590014223&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Dy21z19c_EwJ:scholar.google.com/&scioq=Spike-based+Causal+Inference+For+Weight+Alignment&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01689"}, "Sample Efficient Policy Gradient Methods With Recursive Variance Reduction": {"container_type": "Publication", "bib": {"title": "Sample efficient policy gradient methods with recursive variance reduction", "author": ["P Xu", "F Gao", "Q Gu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.08610", "abstract": "Improving the sample efficiency in reinforcement learning has been a long-standing research problem. In this work, we aim to reduce the sample complexity of existing policy gradient methods. We propose a novel policy gradient algorithm called SRVR-PG, which only requires $ O (1/\\epsilon^{3/2}) $ episodes to find an $\\epsilon $-approximate stationary point of the nonconcave performance function $ J (\\boldsymbol {\\theta}) $(ie, $\\boldsymbol {\\theta} $ such that $\\|\\nabla J (\\boldsymbol {\\theta})\\| _2^ 2\\leq\\epsilon $). This sample"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.08610", "author_id": ["UkYBx6YAAAAJ", "", "GU9HgNAAAAAJ"], "url_scholarbib": "/scholar?q=info:s54KeqkFu8wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSample%2BEfficient%2BPolicy%2BGradient%2BMethods%2BWith%2BRecursive%2BVariance%2BReduction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s54KeqkFu8wJ&ei=ctlXYrKfMsWemAHB5baIBQ&json=", "num_citations": 47, "citedby_url": "/scholar?cites=14752391229837319859&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s54KeqkFu8wJ:scholar.google.com/&scioq=Sample+Efficient+Policy+Gradient+Methods+With+Recursive+Variance+Reduction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.08610"}, "Nas-bench-1shot1: Benchmarking And Dissecting One-shot Neural Architecture Search": {"container_type": "Publication", "bib": {"title": "Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search", "author": ["A Zela", "J Siems", "F Hutter"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.10422", "abstract": "One-shot neural architecture search (NAS) has played a crucial role in making NAS methods computationally feasible in practice. Nevertheless, there is still a lack of understanding on how these weight-sharing algorithms exactly work due to the many factors controlling the dynamics of the process. In order to allow a scientific study of these components, we introduce a general framework for one-shot NAS that can be instantiated to many recently-introduced variants and introduce a general benchmarking framework that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.10422", "author_id": ["hD_6YioAAAAJ", "", "YUrxwrkAAAAJ"], "url_scholarbib": "/scholar?q=info:673atQOaRcYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNas-bench-1shot1:%2BBenchmarking%2BAnd%2BDissecting%2BOne-shot%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=673atQOaRcYJ&ei=ddlXYsT4NZaM6rQPlISayA8&json=", "num_citations": 106, "citedby_url": "/scholar?cites=14286994733629357547&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:673atQOaRcYJ:scholar.google.com/&scioq=Nas-bench-1shot1:+Benchmarking+And+Dissecting+One-shot+Neural+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.10422"}, "Doubly Robust Bias Reduction In Infinite Horizon Off-policy Estimation": {"container_type": "Publication", "bib": {"title": "Doubly robust bias reduction in infinite horizon off-policy estimation", "author": ["Z Tang", "Y Feng", "L Li", "D Zhou", "Q Liu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.07186", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al.(2018a) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high biases due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.07186", "author_id": ["u8y0FJ4AAAAJ", "uqnNle0AAAAJ", "Rqy5KDEAAAAJ", "UwLsYw8AAAAJ", "XEx1fZkAAAAJ"], "url_scholarbib": "/scholar?q=info:Vu3vEmzF3jgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDoubly%2BRobust%2BBias%2BReduction%2BIn%2BInfinite%2BHorizon%2BOff-policy%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Vu3vEmzF3jgJ&ei=edlXYqSSOcWemAHB5baIBQ&json=", "num_citations": 34, "citedby_url": "/scholar?cites=4097929778918583638&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Vu3vEmzF3jgJ:scholar.google.com/&scioq=Doubly+Robust+Bias+Reduction+In+Infinite+Horizon+Off-policy+Estimation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.07186"}, "Exploring Model-based Planning With Policy Networks": {"container_type": "Publication", "bib": {"title": "Exploring model-based planning with policy networks", "author": ["T Wang", "J Ba"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.08649", "abstract": "Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in terms of both sample efficiency and asymptotic performance. Despite their initial successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.08649", "author_id": ["HvJj-pEAAAAJ", "ymzxRhAAAAAJ"], "url_scholarbib": "/scholar?q=info:e7nJGleeVFAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploring%2BModel-based%2BPlanning%2BWith%2BPolicy%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e7nJGleeVFAJ&ei=fdlXYrPeIJaM6rQPlISayA8&json=", "num_citations": 88, "citedby_url": "/scholar?cites=5788425518026701179&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e7nJGleeVFAJ:scholar.google.com/&scioq=Exploring+Model-based+Planning+With+Policy+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.08649"}, "Metapix: Few-shot Video Retargeting": {"container_type": "Publication", "bib": {"title": "Metapix: Few-shot video retargeting", "author": ["J Lee", "D Ramanan", "R Girdhar"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.04742", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.04742", "author_id": ["rimGNwUAAAAJ", "9B8PoXUAAAAJ", "7cuwdr8AAAAJ"], "url_scholarbib": "/scholar?q=info:2uq38xK8TZYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMetapix:%2BFew-shot%2BVideo%2BRetargeting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2uq38xK8TZYJ&ei=gNlXYprCIIvEmgH7846QCg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=10830519468480260826&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2uq38xK8TZYJ:scholar.google.com/&scioq=Metapix:+Few-shot+Video+Retargeting&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.04742"}, "Adversarially Robust Representations With Smooth Encoders": {"container_type": "Publication", "bib": {"title": "Adversarially robust representations with smooth encoders", "author": ["T Cemgil", "S Ghaisas", "KD Dvijotham"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1gfFaEYDS", "author_id": ["X3ZFZ7AAAAAJ", "", "1tOFY1IAAAAJ"], "url_scholarbib": "/scholar?q=info:UnuH4abWWdIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarially%2BRobust%2BRepresentations%2BWith%2BSmooth%2BEncoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UnuH4abWWdIJ&ei=g9lXYsOEA42EmgH6u5u4BA&json=", "num_citations": 17, "citedby_url": "/scholar?cites=15157382033128782674&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UnuH4abWWdIJ:scholar.google.com/&scioq=Adversarially+Robust+Representations+With+Smooth+Encoders&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1gfFaEYDS"}, "Videoflow: A Conditional Flow-based Model For Stochastic Video Generation": {"container_type": "Publication", "bib": {"title": "Videoflow: A conditional flow-based model for stochastic video generation", "author": ["M Kumar", "M Babaeizadeh", "D Erhan", "C Finn"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.01434", "author_id": ["XQJN7dsAAAAJ", "3Y4egcYAAAAJ", "wfGiqXEAAAAJ", "vfPE6hgAAAAJ"], "url_scholarbib": "/scholar?q=info:dwnm9ptae7QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVideoflow:%2BA%2BConditional%2BFlow-based%2BModel%2BFor%2BStochastic%2BVideo%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dwnm9ptae7QJ&ei=htlXYobKDI2EmgH6u5u4BA&json=", "num_citations": 54, "citedby_url": "/scholar?cites=13005087974871140727&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dwnm9ptae7QJ:scholar.google.com/&scioq=Videoflow:+A+Conditional+Flow-based+Model+For+Stochastic+Video+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.01434"}, "Variational Autoencoders For Highly Multivariate Spatial Point Processes Intensities": {"container_type": "Publication", "bib": {"title": "Variational autoencoders for highly multivariate spatial point processes intensities", "author": ["B Yuan", "X Wang", "J Ma", "C Zhou"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Multivariate spatial point process models can describe heterotopic data over space. However, highly multivariate intensities are computationally challenging due to the curse of dimensionality. To bridge this gap, we introduce a declustering based hidden variable model that leads to an efficient inference procedure via a variational autoencoder (VAE). We also prove that this model is a generalization of the VAE-based model for collaborative filtering. This leads to an interesting application of spatial point process models to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1lj20NFDS", "author_id": ["SwSL9NIAAAAJ", "JBzi1-UAAAAJ", "WdDFFlIAAAAJ", "QeSoG3sAAAAJ"], "url_scholarbib": "/scholar?q=info:XthrkzjBo0gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BAutoencoders%2BFor%2BHighly%2BMultivariate%2BSpatial%2BPoint%2BProcesses%2BIntensities%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XthrkzjBo0gJ&ei=jdlXYrWBLsmUywTMkLbABQ&json=", "num_citations": 7, "citedby_url": "/scholar?cites=5234239640670296158&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XthrkzjBo0gJ:scholar.google.com/&scioq=Variational+Autoencoders+For+Highly+Multivariate+Spatial+Point+Processes+Intensities&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1lj20NFDS"}, "A Critical Analysis Of Self-supervision, Or What We Can Learn From A Single Image": {"container_type": "Publication", "bib": {"title": "A critical analysis of self-supervision, or what we can learn from a single image", "author": ["YM Asano", "C Rupprecht", "A Vedaldi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.13132", "abstract": "We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.13132", "author_id": ["CdpLhlgAAAAJ", "IrYlproAAAAJ", "bRT7t28AAAAJ"], "url_scholarbib": "/scholar?q=info:RYLXiR_dmxAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCritical%2BAnalysis%2BOf%2BSelf-supervision,%2BOr%2BWhat%2BWe%2BCan%2BLearn%2BFrom%2BA%2BSingle%2BImage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RYLXiR_dmxAJ&ei=mdlXYsDgE8LZmQHc1ovQAg&json=", "num_citations": 85, "citedby_url": "/scholar?cites=1196793253523325509&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RYLXiR_dmxAJ:scholar.google.com/&scioq=A+Critical+Analysis+Of+Self-supervision,+Or+What+We+Can+Learn+From+A+Single+Image&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.13132"}, "Generalization Through Memorization: Nearest Neighbor Language Models": {"container_type": "Publication", "bib": {"title": "Generalization through memorization: Nearest neighbor language models", "author": ["U Khandelwal", "O Levy", "D Jurafsky"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce $ k $ NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $ k $-nearest neighbors ($ k $ NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $ k $ NN-LM achieves a new state-of-the-art perplexity of 15.79-a 2.9 point"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.00172", "author_id": ["2ITGSdgAAAAJ", "PZVd2h8AAAAJ", "uZg9l58AAAAJ"], "url_scholarbib": "/scholar?q=info:0shB49IV8fEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralization%2BThrough%2BMemorization:%2BNearest%2BNeighbor%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0shB49IV8fEJ&ei=nNlXYqHxIY6pywSdh6agAg&json=", "num_citations": 153, "citedby_url": "/scholar?cites=17433739628027955410&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0shB49IV8fEJ:scholar.google.com/&scioq=Generalization+Through+Memorization:+Nearest+Neighbor+Language+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.00172"}, "Iterative Energy-based Projection On A Normal Data Manifold For Anomaly Localization": {"container_type": "Publication", "bib": {"title": "Iterative energy-based projection on a normal data manifold for anomaly localization", "author": ["D Dehaene", "O Frigo", "S Combrexelle", "P Eline"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.03734", "author_id": ["", "AMC-F2oAAAAJ", "", "azq17QYAAAAJ"], "url_scholarbib": "/scholar?q=info:s52-q3CffNQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIterative%2BEnergy-based%2BProjection%2BOn%2BA%2BNormal%2BData%2BManifold%2BFor%2BAnomaly%2BLocalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s52-q3CffNQJ&ei=qNlXYrjRJY2EmgH6u5u4BA&json=", "num_citations": 54, "citedby_url": "/scholar?cites=15311288139419393459&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s52-q3CffNQJ:scholar.google.com/&scioq=Iterative+Energy-based+Projection+On+A+Normal+Data+Manifold+For+Anomaly+Localization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.03734"}, "Kernelized Wasserstein Natural Gradient": {"container_type": "Publication", "bib": {"title": "Kernelized wasserstein natural gradient", "author": ["M Arbel", "A Gretton", "W Li", "G Mont\u00fafar"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.09652", "abstract": "In Section 2, after a brief description of natural gradients, we discuss Legendre duality   Wasserstein natural gradient. In Section 3, we present our kernel estimator of the natural gradient."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.09652", "author_id": ["NsOqVtkAAAAJ", "OUv7J6QAAAAJ", "rlAIMRMAAAAJ", "pDIuuVwAAAAJ"], "url_scholarbib": "/scholar?q=info:7N9MwXU_4UIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKernelized%2BWasserstein%2BNatural%2BGradient%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7N9MwXU_4UIJ&ei=q9lXYradGsS4ywTtzb_QDA&json=", "num_citations": 11, "citedby_url": "/scholar?cites=4819202851249905644&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7N9MwXU_4UIJ:scholar.google.com/&scioq=Kernelized+Wasserstein+Natural+Gradient&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.09652"}, "Can Gradient Clipping Mitigate Label Noise?": {"container_type": "Publication", "bib": {"title": "Can gradient clipping mitigate label noise?", "author": ["AK Menon", "AS Rawat", "SJ Reddi"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "that gradient clipping alone does not endow label noise  gradient descent with linear models,  gradient clipping is related  (Lemma 3), they are not robust to label noise (Proposition 4). (b)"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rklB76EKPr", "author_id": ["li4mEfcAAAAJ", "U0_ab4cAAAAJ", "70lgwYwAAAAJ"], "url_scholarbib": "/scholar?q=info:v3Znmt1GGQcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCan%2BGradient%2BClipping%2BMitigate%2BLabel%2BNoise%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v3Znmt1GGQcJ&ei=rtlXYr66LoySyATlkbrQCA&json=", "num_citations": 63, "citedby_url": "/scholar?cites=511517950275450559&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v3Znmt1GGQcJ:scholar.google.com/&scioq=Can+Gradient+Clipping+Mitigate+Label+Noise%3F&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rklB76EKPr"}, "Scalable Object-oriented Sequential Generative Models": {"container_type": "Publication", "bib": {"title": "Scalable object-oriented sequential generative models", "author": ["J Jiang", "S Janghorbani", "G de Melo", "S Ahn"], "pub_year": "2019", "venue": "NA", "abstract": "The main limitation of previous approaches to unsupervised sequential object-oriented representation learning is in scalability. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a generative model for SCALable sequential Object-oriented Representation. With the proposed spatially-parallel attention and proposal-rejection mechanism, SCALOR can deal with orders of magnitude more number of objects compared to the current state-of-the-art models. Besides"}, "filled": false, "gsrank": 1, "pub_url": "https://onikle.com/articles/197595", "author_id": ["6oo8xOQAAAAJ", "1c_ttzoAAAAJ", "WCQXaGkAAAAJ", "nfHyDeUAAAAJ"], "url_scholarbib": "/scholar?q=info:LI3V1NcFi0oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScalable%2BObject-oriented%2BSequential%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LI3V1NcFi0oJ&ei=t9lXYuv9PJWMy9YPt8OamA0&json=", "num_citations": 7, "citedby_url": "/scholar?cites=5371393405116321068&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LI3V1NcFi0oJ:scholar.google.com/&scioq=Scalable+Object-oriented+Sequential+Generative+Models&hl=en&as_sdt=0,33"}, "Decentralized Deep Learning With Arbitrary Communication Compression": {"container_type": "Publication", "bib": {"title": "Decentralized deep learning with arbitrary communication compression", "author": ["A Koloskova", "T Lin", "SU Stich", "M Jaggi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.09356", "abstract": "Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches suffer from limited bandwidth of the network, we propose the use of communication compression in the decentralized training context. We show that Choco-SGD $-$ recently introduced and analyzed for strongly-convex objectives only $-$ converges under arbitrary high compression ratio on general non-convex functions at the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.09356", "author_id": ["ldJpvE8AAAAJ", "QE9pa_cAAAAJ", "8l-mDfQAAAAJ", "r1TJBr8AAAAJ"], "url_scholarbib": "/scholar?q=info:3G6f7q-TcKIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecentralized%2BDeep%2BLearning%2BWith%2BArbitrary%2BCommunication%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3G6f7q-TcKIJ&ei=vdlXYqnUDJGJmwGIxre4DA&json=", "num_citations": 113, "citedby_url": "/scholar?cites=11705017815367904988&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3G6f7q-TcKIJ:scholar.google.com/&scioq=Decentralized+Deep+Learning+With+Arbitrary+Communication+Compression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.09356"}, "Escaping Saddle Points Faster With Stochastic Momentum": {"container_type": "Publication", "bib": {"title": "Escaping saddle points faster with stochastic momentum", "author": ["JK Wang", "CH Lin", "J Abernethy"], "pub_year": "2021", "venue": "arXiv preprint arXiv:2106.02985", "abstract": "Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a``momentum''term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2106.02985", "author_id": ["FUjQpmgAAAAJ", "", "FDu4ciwAAAAJ"], "url_scholarbib": "/scholar?q=info:IYryfez4L5MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEscaping%2BSaddle%2BPoints%2BFaster%2BWith%2BStochastic%2BMomentum%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IYryfez4L5MJ&ei=xtlXYuLvGIvEmgH7846QCg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=10605969342089824801&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IYryfez4L5MJ:scholar.google.com/&scioq=Escaping+Saddle+Points+Faster+With+Stochastic+Momentum&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2106.02985"}, "Rethinking The Security Of Skip Connections In Resnet-like Neural Networks": {"container_type": "Publication", "bib": {"title": "Skip connections matter: On the transferability of adversarial examples generated with resnets", "author": ["D Wu", "Y Wang", "ST Xia", "J Bailey", "X Ma"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05990", "abstract": "\u2022 We identify one surprising property of skip connections in ResNet-like neural networks,  ie, they allow an easy generation of highly transferable adversarial examples."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05990", "author_id": ["ZQzqQqwAAAAJ", "uMWPDboAAAAJ", "koAXTXgAAAAJ", "ujsYC98AAAAJ", "XQViiyYAAAAJ"], "url_scholarbib": "/scholar?q=info:ZQgnooO7MlYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2BThe%2BSecurity%2BOf%2BSkip%2BConnections%2BIn%2BResnet-like%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZQgnooO7MlYJ&ei=ytlXYuDCFZmM6rQPjaOSEA&json=", "num_citations": 110, "citedby_url": "/scholar?cites=6211233010132912229&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZQgnooO7MlYJ:scholar.google.com/&scioq=Rethinking+The+Security+Of+Skip+Connections+In+Resnet-like+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05990"}, "Reinforcement Learning Based Graph-to-sequence Model For Natural Question Generation": {"container_type": "Publication", "bib": {"title": "Reinforcement learning based graph-to-sequence model for natural question generation", "author": ["Y Chen", "L Wu", "MJ Zaki"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.04942", "abstract": "Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text,(ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.04942", "author_id": ["m6Sj1yoAAAAJ", "GjcORkUAAAAJ", "UmwJklEAAAAJ"], "url_scholarbib": "/scholar?q=info:VYGLT_I6mUwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforcement%2BLearning%2BBased%2BGraph-to-sequence%2BModel%2BFor%2BNatural%2BQuestion%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VYGLT_I6mUwJ&ei=zdlXYvX2JMWemAHB5baIBQ&json=", "num_citations": 88, "citedby_url": "/scholar?cites=5519507630710292821&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VYGLT_I6mUwJ:scholar.google.com/&scioq=Reinforcement+Learning+Based+Graph-to-sequence+Model+For+Natural+Question+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.04942"}, "Deep Neuroethology Of A Virtual Rodent": {"container_type": "Publication", "bib": {"title": "Deep neuroethology of a virtual rodent", "author": ["J Merel", "D Aldarondo", "J Marshall", "Y Tassa"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this work, we develop a virtual rodent as a platform for the grounded study  virtual model  of a rodent to facilitate grounded investigation of embodied motor systems. The virtual rodent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.09451", "author_id": ["K4OcFXUAAAAJ", "eQawuBgAAAAJ", "PWquzC4AAAAJ", "CjOTm_4AAAAJ"], "url_scholarbib": "/scholar?q=info:OZ6lsKVoRkwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BNeuroethology%2BOf%2BA%2BVirtual%2BRodent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OZ6lsKVoRkwJ&ei=0NlXYsuDI42EmgH6u5u4BA&json=", "num_citations": 38, "citedby_url": "/scholar?cites=5496195456094805561&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OZ6lsKVoRkwJ:scholar.google.com/&scioq=Deep+Neuroethology+Of+A+Virtual+Rodent&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.09451"}, "Size-free Generalization Bounds For Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Size-free generalization bounds for convolutional neural networks", "author": ["PM Long", "H Sedghi"], "pub_year": "2019", "venue": "NA", "abstract": "We prove bounds on the generalization error of convolutional networks. The bounds are in terms of the training loss, the number of parameters, the Lipschitz constant of the loss and the distance from the weights to the initial weights. They are independent of the number of pixels in the input, and the height and width of hidden feature maps. We present experiments using CIFAR-10 with varying hyperparameters of a deep convolutional network, comparing our bounds with practical generalization gaps."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=a3dt0tSsHZ0", "author_id": ["PVty8PUAAAAJ", "_9GX96fDWAMC"], "url_scholarbib": "/scholar?q=info:dVd19wC-VacJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSize-free%2BGeneralization%2BBounds%2BFor%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dVd19wC-VacJ&ei=1NlXYuPHDIvMsQK69Y7ABg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=12057752488715310965&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dVd19wC-VacJ:scholar.google.com/&scioq=Size-free+Generalization+Bounds+For+Convolutional+Neural+Networks&hl=en&as_sdt=0,33"}, "Pretrained Encyclopedia: Weakly Supervised Knowledge-pretrained Language Model": {"container_type": "Publication", "bib": {"title": "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model", "author": ["W Xiong", "J Du", "WY Wang", "V Stoyanov"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.09637", "abstract": "Recent breakthroughs of pretrained language models have shown the effectiveness of self-supervised learning for a wide range of natural language processing (NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained models achieve strong improvements on tasks that involve real-world knowledge, suggesting that large-scale language modeling could be an implicit method to capture knowledge. In this work, we further investigate the extent to which pretrained models such as BERT capture knowledge"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.09637", "author_id": ["o9lKgksAAAAJ", "", "gf8Ms_8AAAAJ", "xdfWqboAAAAJ"], "url_scholarbib": "/scholar?q=info:Gu_1mee6aoQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPretrained%2BEncyclopedia:%2BWeakly%2BSupervised%2BKnowledge-pretrained%2BLanguage%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Gu_1mee6aoQJ&ei=19lXYseNHJmM6rQPjaOSEA&json=", "num_citations": 95, "citedby_url": "/scholar?cites=9541644264421060378&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Gu_1mee6aoQJ:scholar.google.com/&scioq=Pretrained+Encyclopedia:+Weakly+Supervised+Knowledge-pretrained+Language+Model&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.09637"}, "Fasterseg: Searching For Faster Real-time Semantic Segmentation": {"container_type": "Publication", "bib": {"title": "Fasterseg: Searching for faster real-time semantic segmentation", "author": ["W Chen", "X Gong", "X Liu", "Q Zhang", "Y Li"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10917", "author_id": ["RHOcEtsAAAAJ", "A8e8UNAAAAAJ", "697UEEIAAAAJ", "pCY-bikAAAAJ", "nh5a4LQAAAAJ"], "url_scholarbib": "/scholar?q=info:JLeZqUWizaAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFasterseg:%2BSearching%2BFor%2BFaster%2BReal-time%2BSemantic%2BSegmentation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JLeZqUWizaAJ&ei=2tlXYr-1No2EmgH6u5u4BA&json=", "num_citations": 86, "citedby_url": "/scholar?cites=11587095836376020772&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JLeZqUWizaAJ:scholar.google.com/&scioq=Fasterseg:+Searching+For+Faster+Real-time+Semantic+Segmentation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10917.pdf?ref=https://githubhelp.com"}, "Linear Symmetric Quantization Of Neural Networks For Low-precision Integer Hardware": {"container_type": "Publication", "bib": {"title": "Linear symmetric quantization of neural networks for low-precision integer hardware", "author": ["X Zhao", "Y Wang", "X Cai", "C Liu", "L Zhang"], "pub_year": "2020", "venue": "NA", "abstract": "With the proliferation of specialized neural network processors that operate on low-precision integers, the performance of Deep Neural Network inference becomes increasingly dependent on the result of quantization. Despite plenty of prior work on the quantization of weights or activations for neural networks, there is still a wide gap between the software quantizers and the low-precision accelerator implementation, which degrades either the efficiency of networks or that of the hardware for the lack of software and hardware"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=yOCGcQmCIR", "author_id": ["R64ZGdoAAAAJ", "5LoFPSQAAAAJ", "AH4J6pEAAAAJ", "LVWU_VQAAAAJ", "g7owGakAAAAJ"], "url_scholarbib": "/scholar?q=info:U98CGQS2QmYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLinear%2BSymmetric%2BQuantization%2BOf%2BNeural%2BNetworks%2BFor%2BLow-precision%2BInteger%2BHardware%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=U98CGQS2QmYJ&ei=39lXYo23B4vMsQK69Y7ABg&json=", "num_citations": 28, "citedby_url": "/scholar?cites=7368652069047295827&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:U98CGQS2QmYJ:scholar.google.com/&scioq=Linear+Symmetric+Quantization+Of+Neural+Networks+For+Low-precision+Integer+Hardware&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=yOCGcQmCIR"}, "Mixup Inference: Better Exploiting Mixup To Defend Adversarial Attacks": {"container_type": "Publication", "bib": {"title": "Mixup inference: Better exploiting mixup to defend adversarial attacks", "author": ["T Pang", "K Xu", "J Zhu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11515", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11515", "author_id": ["wYDbtFsAAAAJ", "gfl-HVYAAAAJ", "axsP38wAAAAJ"], "url_scholarbib": "/scholar?q=info:sXnbRT6ot_IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMixup%2BInference:%2BBetter%2BExploiting%2BMixup%2BTo%2BDefend%2BAdversarial%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sXnbRT6ot_IJ&ei=4tlXYsThKZWMy9YPt8OamA0&json=", "num_citations": 65, "citedby_url": "/scholar?cites=17489632663330060721&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sXnbRT6ot_IJ:scholar.google.com/&scioq=Mixup+Inference:+Better+Exploiting+Mixup+To+Defend+Adversarial+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11515"}, "Towards A Deep Network Architecture For Structured Smoothness": {"container_type": "Publication", "bib": {"title": "Towards a deep network architecture for structured smoothness", "author": ["H Habeeb", "O Koyejo"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "We propose the Fixed Grouping Layer (FGL); a novel feedforward layer designed to incorporate the inductive bias of structured smoothness into a deep learning model. FGL achieves this goal by connecting nodes across layers based on spatial similarity. The use of structured smoothness, as implemented by FGL, is motivated by applications to structured spatial data, which is, in turn, motivated by domain knowledge. The proposed model architecture outperforms conventional neural network architectures across a variety of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hklr204Fvr", "author_id": ["UBe16ewAAAAJ", ""], "url_scholarbib": "/scholar?q=info:npxnguEgyCcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BA%2BDeep%2BNetwork%2BArchitecture%2BFor%2BStructured%2BSmoothness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=npxnguEgyCcJ&ei=5tlXYsPTL-HDywTjooCQBQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=2866577315748879518&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:npxnguEgyCcJ:scholar.google.com/&scioq=Towards+A+Deep+Network+Architecture+For+Structured+Smoothness&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hklr204Fvr"}, "Understanding L4-based Dictionary Learning: Interpretation, Stability, And Robustness": {"container_type": "Publication", "bib": {"title": "Understanding l4-based dictionary learning: Interpretation, stability, and robustness", "author": ["Y Zhai", "H Mehta", "Z Zhou", "Y Ma"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Recently, the $\\ell^ 4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by\\cite {zhai2019a} has proved surprisingly efficient and effective. This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal},{\\em"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJeY-1BKDS", "author_id": ["78WTKm4AAAAJ", "GcGVcyoAAAAJ", "hiGI9v0AAAAJ", "XqLiBQMAAAAJ"], "url_scholarbib": "/scholar?q=info:zLo2HMWL0JAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BL4-based%2BDictionary%2BLearning:%2BInterpretation,%2BStability,%2BAnd%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zLo2HMWL0JAJ&ei=6tlXYtaKDo2EmgH6u5u4BA&json=", "num_citations": 4, "citedby_url": "/scholar?cites=10434994015315606220&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zLo2HMWL0JAJ:scholar.google.com/&scioq=Understanding+L4-based+Dictionary+Learning:+Interpretation,+Stability,+And+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJeY-1BKDS"}, "Robust Subspace Recovery Layer For Unsupervised Anomaly Detection": {"container_type": "Publication", "bib": {"title": "Robust subspace recovery layer for unsupervised anomaly detection", "author": ["CH Lai", "D Zou", "G Lerman"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.00152", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a\" manifold\" close to the original inliers. Inliers and outliers are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.00152", "author_id": ["cIEmRUkAAAAJ", "zxKo6tYAAAAJ", "zCzuViIAAAAJ"], "url_scholarbib": "/scholar?q=info:ynKengwjx58J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BSubspace%2BRecovery%2BLayer%2BFor%2BUnsupervised%2BAnomaly%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ynKengwjx58J&ei=7dlXYrewK5LeyQTms5KQBg&json=", "num_citations": 32, "citedby_url": "/scholar?cites=11513209509503726282&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ynKengwjx58J:scholar.google.com/&scioq=Robust+Subspace+Recovery+Layer+For+Unsupervised+Anomaly+Detection&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.00152"}, "Memory-based Graph Networks": {"container_type": "Publication", "bib": {"title": "Memory-based graph networks", "author": ["AHK Ahmadi"], "pub_year": "2020", "venue": "NA", "abstract": "two new networks based on our memory layer: Memory-Based Graph Neural  two networks  based on the proposed layer: Memory-based Graph Neural Network (MemGNN) and Graph"}, "filled": false, "gsrank": 1, "pub_url": "https://search.proquest.com/openview/cfef8d5ab74d4eaa3dc8a825c53ecd81/1?pq-origsite=gscholar&cbl=51922&diss=y", "author_id": [""], "url_scholarbib": "/scholar?q=info:xfH3j_hTJHYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemory-based%2BGraph%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xfH3j_hTJHYJ&ei=8NlXYsr4OZaM6rQPlISayA8&json=", "num_citations": 39, "citedby_url": "/scholar?cites=8513021522669466053&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xfH3j_hTJHYJ:scholar.google.com/&scioq=Memory-based+Graph+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.09518"}, "Conservative Uncertainty Estimation By Fitting Prior Networks": {"container_type": "Publication", "bib": {"title": "Conservative uncertainty estimation by fitting prior networks", "author": ["K Ciosek", "V Fortuin", "R Tomioka"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Obtaining high-quality uncertainty estimates is essential for many applications of deep neural networks. In this paper, we theoretically justify a scheme for estimating uncertainties, based on sampling from a prior distribution. Crucially, the uncertainty estimates are shown to be conservative in the sense that they never underestimate a posterior uncertainty obtained by a hypothetical Bayesian algorithm. We also show concentration, implying that the uncertainty estimates converge to zero as we get more data. Uncertainty estimates obtained"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJlahxHYDS", "author_id": ["", "XBlrYTIAAAAJ", "TxdeO-UAAAAJ"], "url_scholarbib": "/scholar?q=info:USSt5v0WmnAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConservative%2BUncertainty%2BEstimation%2BBy%2BFitting%2BPrior%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=USSt5v0WmnAJ&ei=9dlXYommFcLZmQHc1ovQAg&json=", "num_citations": 27, "citedby_url": "/scholar?cites=8113822958414013521&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:USSt5v0WmnAJ:scholar.google.com/&scioq=Conservative+Uncertainty+Estimation+By+Fitting+Prior+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJlahxHYDS"}, "Bridging Mode Connectivity In Loss Landscapes And Adversarial Robustness": {"container_type": "Publication", "bib": {"title": "Bridging mode connectivity in loss landscapes and adversarial robustness", "author": ["P Zhao", "PY Chen", "P Das", "KN Ramamurthy"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.00060", "author_id": ["rWZLnpwAAAAJ", "jxwlCUUAAAAJ", "", "mG8HuhEAAAAJ"], "url_scholarbib": "/scholar?q=info:fXOLu7usAtAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBridging%2BMode%2BConnectivity%2BIn%2BLoss%2BLandscapes%2BAnd%2BAdversarial%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fXOLu7usAtAJ&ei=-NlXYt_RO4vEmgH7846QCg&json=", "num_citations": 43, "citedby_url": "/scholar?cites=14988732432147772285&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fXOLu7usAtAJ:scholar.google.com/&scioq=Bridging+Mode+Connectivity+In+Loss+Landscapes+And+Adversarial+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.00060.pdf?ref=https://githubhelp.com"}, "Rapp: Novelty Detection With Reconstruction Along Projection Pathway": {"container_type": "Publication", "bib": {"title": "Rapp: Novelty detection with reconstruction along projection pathway", "author": ["KH Kim", "S Shim", "Y Lim", "J Jeon", "J Choi"], "pub_year": "2019", "venue": "International \u2026", "abstract": "We propose RaPP, a new methodology for novelty detection by utilizing hidden space activation values obtained from a deep autoencoder. Precisely, RaPP compares input and its autoencoder reconstruction not only in the input space but also in the hidden spaces. We show that if we feed a reconstructed input to the same autoencoder again, its activated values in a hidden space are equivalent to the corresponding reconstruction in that hidden space given the original input. In order to aggregate the hidden space activation values, we"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkgeGeBYDB", "author_id": ["m5eYgl4AAAAJ", "6aEEAyoAAAAJ", "mhebX5oAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:nWVU2RZ2oicJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRapp:%2BNovelty%2BDetection%2BWith%2BReconstruction%2BAlong%2BProjection%2BPathway%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nWVU2RZ2oicJ&ei=_NlXYoKSBpaM6rQPlISayA8&json=", "num_citations": 30, "citedby_url": "/scholar?cites=2855974954213860765&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nWVU2RZ2oicJ:scholar.google.com/&scioq=Rapp:+Novelty+Detection+With+Reconstruction+Along+Projection+Pathway&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkgeGeBYDB"}, "Novelty Detection Via Blurring": {"container_type": "Publication", "bib": {"title": "Novelty detection via blurring", "author": ["S Choi", "SY Chung"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.11943", "abstract": "blurred data as adversarial examples. When we test novelty detection schemes on the blurred  data  (SVD), we found that the novelty detection schemes assign higher confidence to the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.11943", "author_id": ["H0QB0PwAAAAJ", "k-o3JBIAAAAJ"], "url_scholarbib": "/scholar?q=info:S8RwftlmLjwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNovelty%2BDetection%2BVia%2BBlurring%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S8RwftlmLjwJ&ei=_9lXYtSnBeHDywTjooCQBQ&json=", "num_citations": 18, "citedby_url": "/scholar?cites=4336516575519622219&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:S8RwftlmLjwJ:scholar.google.com/&scioq=Novelty+Detection+Via+Blurring&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.11943"}, "Detecting Extrapolation With Local Ensembles": {"container_type": "Publication", "bib": {"title": "Detecting extrapolation with local ensembles", "author": ["D Madras", "J Atwood", "A D'Amour"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.09573", "abstract": "We present local ensembles, a method for detecting extrapolation at test time in a pre-trained model. We focus on underdetermination as a key component of extrapolation: we aim to detect when many possible predictions are consistent with the training data and model class. Our method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. We compute this approximation by estimating the norm of the component of a test point's gradient that aligns"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.09573", "author_id": ["MgnNDpkAAAAJ", "cYk1Xg4AAAAJ", "okP0uukAAAAJ"], "url_scholarbib": "/scholar?q=info:cE63D9QUSl4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDetecting%2BExtrapolation%2BWith%2BLocal%2BEnsembles%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cE63D9QUSl4J&ei=AtpXYvACjqnLBJ2HpqAC&json=", "num_citations": 9, "citedby_url": "/scholar?cites=6794265888871108208&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cE63D9QUSl4J:scholar.google.com/&scioq=Detecting+Extrapolation+With+Local+Ensembles&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.09573"}, "N-beats: Neural Basis Expansion Analysis For Interpretable Time Series Forecasting": {"container_type": "Publication", "bib": {"title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "author": ["BN Oreshkin", "D Carpov", "N Chapados"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10437", "author_id": ["48MBCeIAAAAJ", "afiLRE0AAAAJ", "QdnjDj8AAAAJ"], "url_scholarbib": "/scholar?q=info:mHUWS8ZTu2UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DN-beats:%2BNeural%2BBasis%2BExpansion%2BAnalysis%2BFor%2BInterpretable%2BTime%2BSeries%2BForecasting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mHUWS8ZTu2UJ&ei=BdpXYoaYNciBy9YP18Gi8As&json=", "num_citations": 196, "citedby_url": "/scholar?cites=7330544929604007320&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mHUWS8ZTu2UJ:scholar.google.com/&scioq=N-beats:+Neural+Basis+Expansion+Analysis+For+Interpretable+Time+Series+Forecasting&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10437"}, "Coherent Gradients: An Approach To Understanding Generalization In Gradient Descent-based Optimization": {"container_type": "Publication", "bib": {"title": "Coherent gradients: An approach to understanding generalization in gradient descent-based optimization", "author": ["S Chatterjee"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.10657", "abstract": "An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10657", "author_id": ["Nh_5ogYAAAAJ"], "url_scholarbib": "/scholar?q=info:pg7N94RFrboJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCoherent%2BGradients:%2BAn%2BApproach%2BTo%2BUnderstanding%2BGeneralization%2BIn%2BGradient%2BDescent-based%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pg7N94RFrboJ&ei=C9pXYtTEK5LeyQTms5KQBg&json=", "num_citations": 21, "citedby_url": "/scholar?cites=13451484099420950182&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pg7N94RFrboJ:scholar.google.com/&scioq=Coherent+Gradients:+An+Approach+To+Understanding+Generalization+In+Gradient+Descent-based+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10657"}, "Learning-augmented Data Stream Algorithms": {"container_type": "Publication", "bib": {"title": "Learning-augmented data stream algorithms", "author": ["T Jiang", "Y Li", "H Lin", "Y Ruan"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "The data stream model is a fundamental model for processing massive data sets with limited memory and fast processing time. Recently Hsu et al.(2019) incorporated machine learning techniques into the data stream model in order to learn relevant patterns in the input data. Such techniques were encapsulated by training an oracle to predict item frequencies in the streaming model. In this paper we explore the full power of such an oracle, showing that it can be applied to a wide array of problems in data streams, sometimes resulting in the first"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyxJ1xBYDH", "author_id": ["UQBgOP4AAAAJ", "lyltposAAAAJ", "1RQ0geoAAAAJ", ""], "url_scholarbib": "/scholar?q=info:fuuWbgcRd00J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning-augmented%2BData%2BStream%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fuuWbgcRd00J&ei=EtpXYsbiFZWMy9YPt8OamA0&json=", "num_citations": 18, "citedby_url": "/scholar?cites=5581948986766846846&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fuuWbgcRd00J:scholar.google.com/&scioq=Learning-augmented+Data+Stream+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyxJ1xBYDH"}, "Nesterov Accelerated Gradient And Scale Invariance For Adversarial Attacks": {"container_type": "Publication", "bib": {"title": "Nesterov accelerated gradient and scale invariance for adversarial attacks", "author": ["J Lin", "C Song", "K He", "L Wang", "JE Hopcroft"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep learning models are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on benign inputs. However, under the black-box setting, most existing adversaries often have a poor transferability to attack other defense models. In this work, from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.06281", "author_id": ["UqIG9yAAAAAJ", "el17bJoAAAAJ", "YTQnGJsAAAAJ", "VZHxoh8AAAAJ", "4Z6vo5QAAAAJ"], "url_scholarbib": "/scholar?q=info:UjjvykI1sJMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNesterov%2BAccelerated%2BGradient%2BAnd%2BScale%2BInvariance%2BFor%2BAdversarial%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UjjvykI1sJMJ&ei=FdpXYrnxMo6pywSdh6agAg&json=", "num_citations": 89, "citedby_url": "/scholar?cites=10642064480465270866&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UjjvykI1sJMJ:scholar.google.com/&scioq=Nesterov+Accelerated+Gradient+And+Scale+Invariance+For+Adversarial+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.06281"}, "Learning To Solve The Credit Assignment Problem": {"container_type": "Publication", "bib": {"title": "Learning to solve the credit assignment problem", "author": ["BJ Lansdell", "PR Prakash", "KP Kording"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.00889", "abstract": "Backpropagation is driving today's artificial neural networks (ANNs). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.00889", "author_id": ["vM_KsrQAAAAJ", "REpr1JsAAAAJ", "MiFqJGcAAAAJ"], "url_scholarbib": "/scholar?q=info:E-SaImhWIRsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BSolve%2BThe%2BCredit%2BAssignment%2BProblem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=E-SaImhWIRsJ&ei=GNpXYrb3Mc6E6rQPz8uiuAc&json=", "num_citations": 32, "citedby_url": "/scholar?cites=1954938718512669715&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:E-SaImhWIRsJ:scholar.google.com/&scioq=Learning+To+Solve+The+Credit+Assignment+Problem&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.00889"}, "Identity Crisis: Memorization And Generalization Under Extreme Overparameterization": {"container_type": "Publication", "bib": {"title": "Identity crisis: Memorization and generalization under extreme overparameterization", "author": ["C Zhang", "S Bengio", "M Hardt", "MC Mozer"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the interplay between memorization and generalization of overparameterized networks in the extreme case of a single training example and an identity-mapping task. We examine fully-connected and convolutional networks (FCN and CNN), both linear and nonlinear, initialized randomly and then trained to minimize the reconstruction error. The trained networks stereotypically take one of two forms: the constant function (memorization) and the identity function (generalization). We formally characterize generalization in single"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.04698", "author_id": ["l_G2vr0AAAAJ", "Vs-MdPcAAAAJ", "adnTgaAAAAAJ", "lmjR_qMAAAAJ"], "url_scholarbib": "/scholar?q=info:XOtlta3C1ooJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIdentity%2BCrisis:%2BMemorization%2BAnd%2BGeneralization%2BUnder%2BExtreme%2BOverparameterization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XOtlta3C1ooJ&ei=HNpXYpLbNM2Ny9YPqPyUgAs&json=", "num_citations": 49, "citedby_url": "/scholar?cites=10004397673578621788&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XOtlta3C1ooJ:scholar.google.com/&scioq=Identity+Crisis:+Memorization+And+Generalization+Under+Extreme+Overparameterization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.04698"}, "Scalable And Order-robust Continual Learning With Additive Parameter Decomposition": {"container_type": "Publication", "bib": {"title": "Scalable and order-robust continual learning with additive parameter decomposition", "author": ["J Yoon", "S Kim", "E Yang", "SJ Hwang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.09432", "abstract": "While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, some issues remain to be tackled to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.09432", "author_id": ["-5comoUAAAAJ", "_ZfueMIAAAAJ", "UWO1mloAAAAJ", "RP4Qx3QAAAAJ"], "url_scholarbib": "/scholar?q=info:QVJRltnIURkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScalable%2BAnd%2BOrder-robust%2BContinual%2BLearning%2BWith%2BAdditive%2BParameter%2BDecomposition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QVJRltnIURkJ&ei=H9pXYr-eKcWemAHB5baIBQ&json=", "num_citations": 39, "citedby_url": "/scholar?cites=1824460160917131841&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QVJRltnIURkJ:scholar.google.com/&scioq=Scalable+And+Order-robust+Continual+Learning+With+Additive+Parameter+Decomposition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.09432"}, "Seed Rl: Scalable And Efficient Deep-rl With Accelerated Central Inference": {"container_type": "Publication", "bib": {"title": "Seed rl: Scalable and efficient deep-rl with accelerated central inference", "author": ["L Espeholt", "R Marinier", "P Stanczyk", "K Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state of the art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.06591", "author_id": ["TxLjpCYAAAAJ", "w951T-EAAAAJ", "fKVK0dYAAAAJ", ""], "url_scholarbib": "/scholar?q=info:WFTeD3mWxEsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSeed%2BRl:%2BScalable%2BAnd%2BEfficient%2BDeep-rl%2BWith%2BAccelerated%2BCentral%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WFTeD3mWxEsJ&ei=I9pXYuSEBJWMy9YPt8OamA0&json=", "num_citations": 69, "citedby_url": "/scholar?cites=5459654094981321816&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WFTeD3mWxEsJ:scholar.google.com/&scioq=Seed+Rl:+Scalable+And+Efficient+Deep-rl+With+Accelerated+Central+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.06591.pdf?ref=https://githubhelp.com"}, "The Variational Bandwidth Bottleneck: Stochastic Evaluation On An Information Budget": {"container_type": "Publication", "bib": {"title": "The variational bandwidth bottleneck: Stochastic evaluation on an information budget", "author": ["A Goyal", "Y Bengio", "M Botvinick", "S Levine"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.11935", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant. The information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.11935", "author_id": ["krrh6OUAAAAJ", "kukA0LcAAAAJ", "eM916YMAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:tz8n8ogu7EcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BVariational%2BBandwidth%2BBottleneck:%2BStochastic%2BEvaluation%2BOn%2BAn%2BInformation%2BBudget%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tz8n8ogu7EcJ&ei=JdpXYoPqNcS4ywTtzb_QDA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=5182568436909686711&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tz8n8ogu7EcJ:scholar.google.com/&scioq=The+Variational+Bandwidth+Bottleneck:+Stochastic+Evaluation+On+An+Information+Budget&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.11935"}, "Uncertainty-guided Continual Learning With Bayesian Neural Networks": {"container_type": "Publication", "bib": {"title": "Uncertainty-guided continual learning with bayesian neural networks", "author": ["S Ebrahimi", "M Elhoseiny", "T Darrell"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters'\\textit {importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.02425", "author_id": ["wRyjJfMAAAAJ", "iRBUTOAAAAAJ", "bh-uRFMAAAAJ"], "url_scholarbib": "/scholar?q=info:neiay_wj7IsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUncertainty-guided%2BContinual%2BLearning%2BWith%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=neiay_wj7IsJ&ei=KdpXYq70C8WemAHB5baIBQ&json=", "num_citations": 102, "citedby_url": "/scholar?cites=10082473234430355613&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:neiay_wj7IsJ:scholar.google.com/&scioq=Uncertainty-guided+Continual+Learning+With+Bayesian+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.02425"}, "Toward Amortized Ranking-critical Training For Collaborative Filtering": {"container_type": "Publication", "bib": {"title": "Towards Amortized Ranking-Critical Training for Collaborative Filtering", "author": ["S Lobel", "C Li", "J Gao", "L Carin"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.04281", "abstract": "Collaborative filtering is widely used in modern recommender systems. Recent research shows that variational autoencoders (VAEs) yield state-of-the-art performance by integrating flexible representations from deep neural networks into latent variable models, mitigating limitations of traditional linear factor models. VAEs are typically trained by maximizing the likelihood (MLE) of users interacting with ground-truth items. While simple and often effective, MLE-based training does not directly maximize the recommendation-quality"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.04281", "author_id": ["H8gdqsUAAAAJ", "Zd7WmXUAAAAJ", "CQ1cqKkAAAAJ", "ZhGL6WcAAAAJ"], "url_scholarbib": "/scholar?q=info:_zbhBJW_snYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DToward%2BAmortized%2BRanking-critical%2BTraining%2BFor%2BCollaborative%2BFiltering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_zbhBJW_snYJ&ei=LtpXYoC8HYvEmgH7846QCg&json=", "num_citations": 6, "citedby_url": "/scholar?cites=8553109289082894079&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_zbhBJW_snYJ:scholar.google.com/&scioq=Toward+Amortized+Ranking-critical+Training+For+Collaborative+Filtering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.04281"}, "Pac Confidence Sets For Deep Neural Networks Via Calibrated Prediction": {"container_type": "Publication", "bib": {"title": "PAC confidence sets for deep neural networks via calibrated prediction", "author": ["S Park", "O Bastani", "N Matni", "I Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:2001.00106", "abstract": "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees---ie, the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00106", "author_id": ["Vi2E2F4AAAAJ", "cxYepGkAAAAJ", "ZDPCh_EAAAAJ", "qPlUgrgAAAAJ"], "url_scholarbib": "/scholar?q=info:q8m3oIiY3LoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPac%2BConfidence%2BSets%2BFor%2BDeep%2BNeural%2BNetworks%2BVia%2BCalibrated%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=q8m3oIiY3LoJ&ei=MtpXYujMF4ySyATlkbrQCA&json=", "num_citations": 16, "citedby_url": "/scholar?cites=13464804698510313899&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:q8m3oIiY3LoJ:scholar.google.com/&scioq=Pac+Confidence+Sets+For+Deep+Neural+Networks+Via+Calibrated+Prediction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00106"}, "Four Things Everyone Should Know To Improve Batch Normalization": {"container_type": "Publication", "bib": {"title": "Four things everyone should know to improve batch normalization", "author": ["C Summers", "MJ Dinneen"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.03548", "abstract": "A key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically improve upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, we identify four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.03548", "author_id": ["9h2gGfoAAAAJ", "NspX1ZIAAAAJ"], "url_scholarbib": "/scholar?q=info:Ehe8g5rxkHoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFour%2BThings%2BEveryone%2BShould%2BKnow%2BTo%2BImprove%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ehe8g5rxkHoJ&ei=NtpXYqXLHM2Ny9YPqPyUgAs&json=", "num_citations": 25, "citedby_url": "/scholar?cites=8831824515210942226&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ehe8g5rxkHoJ:scholar.google.com/&scioq=Four+Things+Everyone+Should+Know+To+Improve+Batch+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.03548"}, "Revisiting Self-training For Neural Sequence Generation": {"container_type": "Publication", "bib": {"title": "Revisiting self-training for neural sequence generation", "author": ["J He", "J Gu", "J Shen", "MA Ranzato"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.13788", "abstract": "Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (ie the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (eg machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.13788", "author_id": ["BIFGeoUAAAAJ", "cB1mFBsAAAAJ", "qckHL1AAAAAJ", "NbXF7T8AAAAJ"], "url_scholarbib": "/scholar?q=info:PlAqC7i0NWEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRevisiting%2BSelf-training%2BFor%2BNeural%2BSequence%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PlAqC7i0NWEJ&ei=OtpXYvWjD42EmgH6u5u4BA&json=", "num_citations": 104, "citedby_url": "/scholar?cites=7004703497998979134&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PlAqC7i0NWEJ:scholar.google.com/&scioq=Revisiting+Self-training+For+Neural+Sequence+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.13788"}, "Neural Arithmetic Units": {"container_type": "Publication", "bib": {"title": "Neural arithmetic units", "author": ["A Madsen", "AR Johansen"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.05016", "abstract": "new neural network components: the Neural Addition Unit (NAU), which can learn exact  addition and subtraction; and the Neural Multiplication Unit ( , the first arithmetic neural network"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.05016", "author_id": ["X0zwAXYAAAAJ", "YWLFeUkAAAAJ"], "url_scholarbib": "/scholar?q=info:Ru3NERqEBpUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BArithmetic%2BUnits%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ru3NERqEBpUJ&ei=RdpXYszcLoySyATlkbrQCA&json=", "num_citations": 31, "citedby_url": "/scholar?cites=10738415609014250822&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ru3NERqEBpUJ:scholar.google.com/&scioq=Neural+Arithmetic+Units&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.05016"}, "Dynamical Distance Learning For Semi-supervised And Unsupervised Skill Discovery": {"container_type": "Publication", "bib": {"title": "Dynamical distance learning for semi-supervised and unsupervised skill discovery", "author": ["K Hartikainen", "X Geng", "T Haarnoja", "S Levine"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.08225", "author_id": ["eVYhlDQAAAAJ", "vYougn0AAAAJ", "VT7peyEAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:sqzkrZ4FbEAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamical%2BDistance%2BLearning%2BFor%2BSemi-supervised%2BAnd%2BUnsupervised%2BSkill%2BDiscovery%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sqzkrZ4FbEAJ&ei=SNpXYpPlHMLZmQHc1ovQAg&json=", "num_citations": 24, "citedby_url": "/scholar?cites=4642091494992555186&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sqzkrZ4FbEAJ:scholar.google.com/&scioq=Dynamical+Distance+Learning+For+Semi-supervised+And+Unsupervised+Skill+Discovery&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.08225"}, "The Local Elasticity Of Neural Networks": {"container_type": "Publication", "bib": {"title": "The local elasticity of neural networks", "author": ["H He", "WJ Su"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.06943", "abstract": "This paper presents a phenomenon in neural networks that we refer to as\\textit {local elasticity}. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector $\\bx'$ is\\textit {not} significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector $\\bx $ that is\\textit {dissimilar} to $\\bx'$ in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.06943", "author_id": ["BbpI6QoAAAAJ", "Uhf4nBkAAAAJ"], "url_scholarbib": "/scholar?q=info:udDqQTd4qSIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLocal%2BElasticity%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=udDqQTd4qSIJ&ei=StpXYuDTMMiBy9YP18Gi8As&json=", "num_citations": 19, "citedby_url": "/scholar?cites=2497659647078092985&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:udDqQTd4qSIJ:scholar.google.com/&scioq=The+Local+Elasticity+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.06943"}, "Infograph: Unsupervised And Semi-supervised Graph-level Representation Learning Via Mutual Information Maximization": {"container_type": "Publication", "bib": {"title": "Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization", "author": ["FY Sun", "J Hoffmann", "V Verma", "J Tang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.01000", "abstract": "This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (eg"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.01000", "author_id": ["TOw2RMMAAAAJ", "MOGvppgAAAAJ", "wo_M4uQAAAAJ", "1ir6WUEAAAAJ"], "url_scholarbib": "/scholar?q=info:aTt9TNj5WucJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInfograph:%2BUnsupervised%2BAnd%2BSemi-supervised%2BGraph-level%2BRepresentation%2BLearning%2BVia%2BMutual%2BInformation%2BMaximization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aTt9TNj5WucJ&ei=TdpXYpXnOMiBy9YP18Gi8As&json=", "num_citations": 246, "citedby_url": "/scholar?cites=16670911678056840041&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aTt9TNj5WucJ:scholar.google.com/&scioq=Infograph:+Unsupervised+And+Semi-supervised+Graph-level+Representation+Learning+Via+Mutual+Information+Maximization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.01000.pdf?ref=https://githubhelp.com"}, "Are Transformers Universal Approximators Of Sequence-to-sequence Functions?": {"container_type": "Publication", "bib": {"title": "Are transformers universal approximators of sequence-to-sequence functions?", "author": ["C Yun", "S Bhojanapalli", "AS Rawat", "SJ Reddi"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10077", "author_id": ["Ukl64ggAAAAJ", "bpSF_9EAAAAJ", "U0_ab4cAAAAJ", "70lgwYwAAAAJ"], "url_scholarbib": "/scholar?q=info:xuARrPm-7EsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAre%2BTransformers%2BUniversal%2BApproximators%2BOf%2BSequence-to-sequence%2BFunctions%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xuARrPm-7EsJ&ei=UdpXYpDZL46pywSdh6agAg&json=", "num_citations": 68, "citedby_url": "/scholar?cites=5470957626891296966&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xuARrPm-7EsJ:scholar.google.com/&scioq=Are+Transformers+Universal+Approximators+Of+Sequence-to-sequence+Functions%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10077"}, "Es-maml: Simple Hessian-free Meta Learning": {"container_type": "Publication", "bib": {"title": "Es-maml: Simple hessian-free meta learning", "author": ["X Song", "W Gao", "Y Yang", "K Choromanski"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01215", "author_id": ["GnpHmO8AAAAJ", "02uME-gAAAAJ", "2NQKmzIAAAAJ", "J8OgouwAAAAJ"], "url_scholarbib": "/scholar?q=info:RXkahA92LpgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEs-maml:%2BSimple%2BHessian-free%2BMeta%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RXkahA92LpgJ&ei=VNpXYq_6N5aM6rQPlISayA8&json=", "num_citations": 54, "citedby_url": "/scholar?cites=10965831951706650949&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RXkahA92LpgJ:scholar.google.com/&scioq=Es-maml:+Simple+Hessian-free+Meta+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01215"}, "Impact: Importance Weighted Asynchronous Architectures With Clipped Target Networks": {"container_type": "Publication", "bib": {"title": "Impact: Importance weighted asynchronous architectures with clipped target networks", "author": ["M Luo", "J Yao", "R Liaw", "E Liang", "I Stoica"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.00167", "abstract": "The practical usage of reinforcement learning agents is often bottlenecked by the duration of training time. To accelerate training, practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate the training process. However, modern methods for scalable reinforcement learning (RL) often tradeoff between the throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample (sample efficiency). In these scalable RL architectures, as one increases"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.00167", "author_id": ["XpO6-kEAAAAJ", "", "IcaU830AAAAJ", "wmZTE5gAAAAJ", "vN-is70AAAAJ"], "url_scholarbib": "/scholar?q=info:UVrx7xPU9iwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImpact:%2BImportance%2BWeighted%2BAsynchronous%2BArchitectures%2BWith%2BClipped%2BTarget%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UVrx7xPU9iwJ&ei=WNpXYr-cLovMsQK69Y7ABg&json=", "num_citations": 4, "citedby_url": "/scholar?cites=3240010164034689617&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UVrx7xPU9iwJ:scholar.google.com/&scioq=Impact:+Importance+Weighted+Asynchronous+Architectures+With+Clipped+Target+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.00167"}, "Weakly Supervised Disentanglement With Guarantees": {"container_type": "Publication", "bib": {"title": "Weakly supervised disentanglement with guarantees", "author": ["R Shu", "Y Chen", "A Kumar", "S Ermon", "B Poole"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.09772", "author_id": ["UB7UZEYAAAAJ", "4a6iPeUAAAAJ", "6vghMS0AAAAJ", "ogXTOZ4AAAAJ", "i5FMLA4AAAAJ"], "url_scholarbib": "/scholar?q=info:987Mp3Y-wz0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWeakly%2BSupervised%2BDisentanglement%2BWith%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=987Mp3Y-wz0J&ei=XtpXYq2CLJLeyQTms5KQBg&json=", "num_citations": 77, "citedby_url": "/scholar?cites=4450469536114462455&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:987Mp3Y-wz0J:scholar.google.com/&scioq=Weakly+Supervised+Disentanglement+With+Guarantees&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.09772"}, "A Neural Dirichlet Process Mixture Model For Task-free Continual Learning": {"container_type": "Publication", "bib": {"title": "A neural dirichlet process mixture model for task-free continual learning", "author": ["S Lee", "J Ha", "D Zhang", "G Kim"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.00689", "abstract": "Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00689", "author_id": ["8O3MKJkAAAAJ", "LfC0tYAAAAAJ", "ydEYx7QAAAAJ", "CiSdOV0AAAAJ"], "url_scholarbib": "/scholar?q=info:7uzzksnWJ8YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BNeural%2BDirichlet%2BProcess%2BMixture%2BModel%2BFor%2BTask-free%2BContinual%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7uzzksnWJ8YJ&ei=YdpXYp7ZHM6E6rQPz8uiuAc&json=", "num_citations": 73, "citedby_url": "/scholar?cites=14278617304843676910&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7uzzksnWJ8YJ:scholar.google.com/&scioq=A+Neural+Dirichlet+Process+Mixture+Model+For+Task-free+Continual+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00689"}, "Meta-learning With Warped Gradient Descent": {"container_type": "Publication", "bib": {"title": "Meta-learning with warped gradient descent", "author": ["S Flennerhag", "AA Rusu", "R Pascanu", "F Visin"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.00025", "author_id": ["SeMQQkcAAAAJ", "qcZB864AAAAJ", "eSPY8LwAAAAJ", "kaAnZw0AAAAJ"], "url_scholarbib": "/scholar?q=info:reC2Da3bGZsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BWith%2BWarped%2BGradient%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=reC2Da3bGZsJ&ei=ZNpXYs3mLM2Ny9YPqPyUgAs&json=", "num_citations": 122, "citedby_url": "/scholar?cites=11176205486602510509&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:reC2Da3bGZsJ:scholar.google.com/&scioq=Meta-learning+With+Warped+Gradient+Descent&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.00025"}, "Ride: Rewarding Impact-driven Exploration For Procedurally-generated Environments": {"container_type": "Publication", "bib": {"title": "Ride: Rewarding impact-driven exploration for procedurally-generated environments", "author": ["R Raileanu", "T Rockt\u00e4schel"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.12292", "abstract": "Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.12292", "author_id": ["9hVXpJ0AAAAJ", "mWBY8aIAAAAJ"], "url_scholarbib": "/scholar?q=info:6UazS5AEEAMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRide:%2BRewarding%2BImpact-driven%2BExploration%2BFor%2BProcedurally-generated%2BEnvironments%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6UazS5AEEAMJ&ei=bNpXYuffHYvMsQK69Y7ABg&json=", "num_citations": 61, "citedby_url": "/scholar?cites=220681399532996329&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6UazS5AEEAMJ:scholar.google.com/&scioq=Ride:+Rewarding+Impact-driven+Exploration+For+Procedurally-generated+Environments&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.12292"}, "B-spline Cnns On Lie Groups": {"container_type": "Publication", "bib": {"title": "B-spline cnns on lie groups", "author": ["EJ Bekkers"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12057", "abstract": "This paper presents a flexible framework for building G-CNNs for arbitrary Lie groups. The  proposed B-spline basis functions, which are used to represent convolution kernels, have"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12057", "author_id": ["yeWrfR4AAAAJ"], "url_scholarbib": "/scholar?q=info:PCeM7WiBKswJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DB-spline%2BCnns%2BOn%2BLie%2BGroups%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PCeM7WiBKswJ&ei=b9pXYvSPNc2Ny9YPqPyUgAs&json=", "num_citations": 62, "citedby_url": "/scholar?cites=14711713420421113660&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PCeM7WiBKswJ:scholar.google.com/&scioq=B-spline+Cnns+On+Lie+Groups&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12057"}, "Differentiable Reasoning Over A Virtual Knowledge Base": {"container_type": "Publication", "bib": {"title": "Differentiable reasoning over a virtual knowledge base", "author": ["B Dhingra", "M Zaheer", "V Balachandran"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so the full system can be trained end-to-end"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10640", "author_id": ["SLISZFAAAAAJ", "A33FhJMAAAAJ", "LgitgaIAAAAJ"], "url_scholarbib": "/scholar?q=info:Y98ZH2IZE3YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BReasoning%2BOver%2BA%2BVirtual%2BKnowledge%2BBase%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Y98ZH2IZE3YJ&ei=c9pXYoqmIs2Ny9YPqPyUgAs&json=", "num_citations": 57, "citedby_url": "/scholar?cites=8508172030252277603&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Y98ZH2IZE3YJ:scholar.google.com/&scioq=Differentiable+Reasoning+Over+A+Virtual+Knowledge+Base&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10640"}, "Adversarially Robust Transfer Learning": {"container_type": "Publication", "bib": {"title": "Adversarially robust transfer learning", "author": ["A Shafahi", "P Saadatpanah", "C Zhu", "A Ghiasi"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "When the goal is to produce a model that is not only accurate but also adversarially robust,   robust transfer learning, in which we transfer not only performance but also robustness from"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.08232", "author_id": ["5Jnk00MAAAAJ", "", "m-om5O8AAAAJ", "tNQWOxUAAAAJ"], "url_scholarbib": "/scholar?q=info:-OZ6jPK-cAMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarially%2BRobust%2BTransfer%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-OZ6jPK-cAMJ&ei=dtpXYt32KIySyATlkbrQCA&json=", "num_citations": 59, "citedby_url": "/scholar?cites=247907928453605112&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-OZ6jPK-cAMJ:scholar.google.com/&scioq=Adversarially+Robust+Transfer+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.08232"}, "Neurquri: Neural Question Requirement Inspector For Answerability Prediction In Machine Reading Comprehension": {"container_type": "Publication", "bib": {"title": "NeurQuRI: Neural question requirement inspector for answerability prediction in machine reading comprehension", "author": ["S Back", "SC Chinthakindi", "A Kedia", "H Lee"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question\" What was the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryxgsCVYPr", "author_id": ["A1mZQ5cAAAAJ", "7kRuceAAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:Kvo_L9_-r2QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeurquri:%2BNeural%2BQuestion%2BRequirement%2BInspector%2BFor%2BAnswerability%2BPrediction%2BIn%2BMachine%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Kvo_L9_-r2QJ&ei=etpXYuhxwtmZAdzWi9AC&json=", "num_citations": 16, "citedby_url": "/scholar?cites=7255297759241042474&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Kvo_L9_-r2QJ:scholar.google.com/&scioq=Neurquri:+Neural+Question+Requirement+Inspector+For+Answerability+Prediction+In+Machine+Reading+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryxgsCVYPr"}, "Stochastic Conditional Generative Networks With Basis Decomposition": {"container_type": "Publication", "bib": {"title": "Stochastic conditional generative networks with basis decomposition", "author": ["Z Wang", "X Cheng", "G Sapiro", "Q Qiu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11286", "abstract": "While generative adversarial networks (GANs) have revolutionized machine learning, a number of open questions remain to fully understand them and exploit their power. One of these questions is how to efficiently achieve proper diversity and sampling of the multi-mode data space. To address this, we introduce BasisGAN, a stochastic conditional multi-mode image generator. By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we learn a plug-and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11286", "author_id": ["80Jw_w8AAAAJ", "I2gwdssAAAAJ", "ISRNX3gAAAAJ", "jdLtt_YAAAAJ"], "url_scholarbib": "/scholar?q=info:YiYbBIsDlqAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BConditional%2BGenerative%2BNetworks%2BWith%2BBasis%2BDecomposition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YiYbBIsDlqAJ&ei=fdpXYp_wDsLZmQHc1ovQAg&json=", "num_citations": 14, "citedby_url": "/scholar?cites=11571440188179293794&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YiYbBIsDlqAJ:scholar.google.com/&scioq=Stochastic+Conditional+Generative+Networks+With+Basis+Decomposition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11286"}, "Actor-critic Provably Finds Nash Equilibria Of Linear-quadratic Mean-field Games": {"container_type": "Publication", "bib": {"title": "Actor-critic provably finds Nash equilibria of linear-quadratic mean-field games", "author": ["Z Fu", "Z Yang", "Y Chen", "Z Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.07498", "abstract": "We study discrete-time mean-field Markov games with infinite numbers of agents where each agent aims to minimize its ergodic cost. We consider the setting where the agents have identical linear state transitions and quadratic cost functions, while the aggregated effect of the agents is captured by the population mean of their states, namely, the mean-field state. For such a game, based on the Nash certainty equivalence principle, we provide sufficient conditions for the existence and uniqueness of its Nash equilibrium. Moreover, to find the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.07498", "author_id": ["EQlYpjAAAAAJ", "k7NgVSUAAAAJ", "X8BYiV4AAAAJ", "HSx0BgQAAAAJ"], "url_scholarbib": "/scholar?q=info:73zBi0-HAFUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DActor-critic%2BProvably%2BFinds%2BNash%2BEquilibria%2BOf%2BLinear-quadratic%2BMean-field%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=73zBi0-HAFUJ&ei=f9pXYp_MMZaM6rQPlISayA8&json=", "num_citations": 35, "citedby_url": "/scholar?cites=6125044268940754159&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:73zBi0-HAFUJ:scholar.google.com/&scioq=Actor-critic+Provably+Finds+Nash+Equilibria+Of+Linear-quadratic+Mean-field+Games&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.07498"}, "Geometric Insights Into The Convergence Of Nonlinear Td Learning": {"container_type": "Publication", "bib": {"title": "Geometric insights into the convergence of nonlinear TD learning", "author": ["D Brandfonbrener", "J Bruna"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.12185", "abstract": "While there are convergence guarantees for temporal difference (TD) learning when using linear function approximators, the situation for nonlinear models is far less understood, and divergent examples are known. Here we take a first step towards extending theoretical convergence guarantees to TD learning with nonlinear function approximation. More precisely, we consider the expected learning dynamics of the TD (0) algorithm for value estimation. As the step-size converges to zero, these dynamics are defined by a nonlinear"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.12185", "author_id": ["AmubonoAAAAJ", "L4bNmsMAAAAJ"], "url_scholarbib": "/scholar?q=info:h6BXYYUHjIQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeometric%2BInsights%2BInto%2BThe%2BConvergence%2BOf%2BNonlinear%2BTd%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=h6BXYYUHjIQJ&ei=g9pXYtqbCcS4ywTtzb_QDA&json=", "num_citations": 9, "citedby_url": "/scholar?cites=9551017179191156871&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:h6BXYYUHjIQJ:scholar.google.com/&scioq=Geometric+Insights+Into+The+Convergence+Of+Nonlinear+Td+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.12185"}, "Transferable Perturbations Of Deep Feature Distributions": {"container_type": "Publication", "bib": {"title": "Transferable perturbations of deep feature distributions", "author": ["N Inkawhich", "KJ Liang", "L Carin", "Y Chen"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.12519", "abstract": "Almost all current adversarial attacks of CNN classifiers rely on information derived from the output layer of the network. This work presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions. We achieve state-of-the-art targeted blackbox transfer-based attack results for undefended ImageNet models. Further, we place a priority on explainability and interpretability of the attacking process. Our methodology affords an analysis of how adversarial attacks change"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.12519", "author_id": ["NZh50oIAAAAJ", "DBqwS2YAAAAJ", "ZhGL6WcAAAAJ", "3G-nnjMAAAAJ"], "url_scholarbib": "/scholar?q=info:3U2jVV9AgOYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransferable%2BPerturbations%2BOf%2BDeep%2BFeature%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3U2jVV9AgOYJ&ei=htpXYtDUK5GJmwGIxre4DA&json=", "num_citations": 29, "citedby_url": "/scholar?cites=16609346203945225693&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3U2jVV9AgOYJ:scholar.google.com/&scioq=Transferable+Perturbations+Of+Deep+Feature+Distributions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.12519"}, "Differentiation Of Blackbox Combinatorial Solvers": {"container_type": "Publication", "bib": {"title": "Differentiation of blackbox combinatorial solvers", "author": ["M Vlastelica", "A Paulus", "V Musil", "G Martius"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02175", "author_id": ["bmOnRUcAAAAJ", "njZL5CQAAAAJ", "hA1rlU4AAAAJ", "b-JF-UIAAAAJ"], "url_scholarbib": "/scholar?q=info:kmMKW1tM8H4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiation%2BOf%2BBlackbox%2BCombinatorial%2BSolvers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kmMKW1tM8H4J&ei=itpXYuGtDIvEmgH7846QCg&json=", "num_citations": 45, "citedby_url": "/scholar?cites=9146894798442619794&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kmMKW1tM8H4J:scholar.google.com/&scioq=Differentiation+Of+Blackbox+Combinatorial+Solvers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02175"}, "Measuring And Improving The Use Of Graph Information In Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Measuring and improving the use of graph information in graph neural networks", "author": ["Y Hou", "J Zhang", "J Cheng", "K Ma", "RTB Ma"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Graph neural networks (GNNs) have been widely used for representation learning on graph data. However, there is limited understanding on how much performance GNNs actually gain from graph data. This paper introduces a context-surrounding GNN framework and proposes two smoothness metrics to measure the quantity and quality of information obtained from graph data. A new, improved GNN model, called CS-GNN, is then devised to improve the use of graph information based on the smoothness values of a graph. CS-GNN"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkeIIkHKvS", "author_id": ["Bm23WyIAAAAJ", "", "", "", "gdyYJz4AAAAJ"], "url_scholarbib": "/scholar?q=info:MPrzpsUF6nEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2BAnd%2BImproving%2BThe%2BUse%2BOf%2BGraph%2BInformation%2BIn%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MPrzpsUF6nEJ&ei=n9pXYs7QDo6pywSdh6agAg&json=", "num_citations": 59, "citedby_url": "/scholar?cites=8208379617303853616&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MPrzpsUF6nEJ:scholar.google.com/&scioq=Measuring+And+Improving+The+Use+Of+Graph+Information+In+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkeIIkHKvS"}, "Define: Deep Factorized Input Word Embeddings For Neural Sequence Modeling": {"container_type": "Publication", "bib": {"title": "DeFINE: DEep Factorized INput Word Embeddings for Neural Sequence Modeling.", "author": ["S Mehta", "R Koncel-Kedziorski", "M Rastegari"], "venue": "NA", "pub_year": "NA", "abstract": ""}, "filled": false, "gsrank": 1, "author_id": ["", "", "N4-2Z_cAAAAJ"], "url_scholarbib": "/scholar?q=info:zEJ58z9pHjwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDefine:%2BDeep%2BFactorized%2BInput%2BWord%2BEmbeddings%2BFor%2BNeural%2BSequence%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zEJ58z9pHjwJ&ei=o9pXYsSyJMWemAHB5baIBQ&json=", "num_citations": 2, "citedby_url": "/scholar?cites=4332015614965662412&as_sdt=5,33&sciodt=0,33&hl=en"}, "Lipschitz Constant Estimation For Neural Networks Via Sparse Polynomial Optimization": {"container_type": "Publication", "bib": {"title": "Lipschitz constant estimation of neural networks via sparse polynomial optimization", "author": ["F Latorre", "P Rolland", "V Cevher"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.08688", "abstract": "We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.08688", "author_id": ["B46S5NwAAAAJ", "68qmwmUAAAAJ", "hlWhzU8AAAAJ"], "url_scholarbib": "/scholar?q=info:icLVL5WOzUAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLipschitz%2BConstant%2BEstimation%2BFor%2BNeural%2BNetworks%2BVia%2BSparse%2BPolynomial%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=icLVL5WOzUAJ&ei=p9pXYuqnNpWMy9YPt8OamA0&json=", "num_citations": 49, "citedby_url": "/scholar?cites=4669545160056881801&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:icLVL5WOzUAJ:scholar.google.com/&scioq=Lipschitz+Constant+Estimation+For+Neural+Networks+Via+Sparse+Polynomial+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.08688"}, "A Latent Morphology Model For Open-vocabulary Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "A latent morphology model for open-vocabulary neural machine translation", "author": ["D Ataman", "W Aziz", "A Birch"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.13890", "abstract": "Translation into morphologically-rich languages challenges neural machine translation (NMT) models with extremely sparse vocabularies where atomic treatment of surface forms is unrealistic. This problem is typically addressed by either pre-processing words into subword units or performing translation directly at the level of characters. The former is based on word segmentation algorithms optimized using corpus-level statistics with no regard to the translation task. The latter learns directly from translation data but requires"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.13890", "author_id": ["nFEUTOIAAAAJ", "phgBJXYAAAAJ", "gZOV9kMAAAAJ"], "url_scholarbib": "/scholar?q=info:vOEgvfMi94gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BLatent%2BMorphology%2BModel%2BFor%2BOpen-vocabulary%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vOEgvfMi94gJ&ei=q9pXYuaYK46pywSdh6agAg&json=", "num_citations": 17, "citedby_url": "/scholar?cites=9869395538651177404&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vOEgvfMi94gJ:scholar.google.com/&scioq=A+Latent+Morphology+Model+For+Open-vocabulary+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.13890"}, "Neural Epitome Search For Architecture-agnostic Network Compression": {"container_type": "Publication", "bib": {"title": "Neural epitome search for architecture-agnostic network compression", "author": ["D Zhou", "X Jin", "Q Hou", "K Wang", "J Yang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The recent WSNet [1] is a new model compression method through sampling filterweights from a compact set and has demonstrated to be effective for 1D convolutionneural networks (CNNs). However, the weights sampling strategy of WSNet ishandcrafted and fixed which may severely limit the expression ability of the resultedCNNs and weaken its compression ability. In this work, we present a novel auto-sampling method that is applicable to both 1D and 2D CNNs with significantperformance improvement over WSNet. Specifically, our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.05642", "author_id": ["DdCAbWwAAAAJ", "OEZ816YAAAAJ", "fF8OFV8AAAAJ", "hHbFHqsAAAAJ", "HWFvq_wAAAAJ"], "url_scholarbib": "/scholar?q=info:e5jwaCTpMtwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BEpitome%2BSearch%2BFor%2BArchitecture-agnostic%2BNetwork%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e5jwaCTpMtwJ&ei=rtpXYoP9IMS4ywTtzb_QDA&json=", "num_citations": 9, "citedby_url": "/scholar?cites=15867000779768371323&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e5jwaCTpMtwJ:scholar.google.com/&scioq=Neural+Epitome+Search+For+Architecture-agnostic+Network+Compression&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.05642"}, "Are Pre-trained Language Models Aware Of Phrases? Simple But Strong Baselines For Grammar Induction": {"container_type": "Publication", "bib": {"title": "Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction", "author": ["T Kim", "J Choi", "D Edmiston", "S Lee"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.00737", "abstract": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.00737", "author_id": ["eH5uq7wAAAAJ", "ni9rA8sAAAAJ", "ckKv01EAAAAJ", ""], "url_scholarbib": "/scholar?q=info:ZZPSVeNAPLQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAre%2BPre-trained%2BLanguage%2BModels%2BAware%2BOf%2BPhrases%253F%2BSimple%2BBut%2BStrong%2BBaselines%2BFor%2BGrammar%2BInduction%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZZPSVeNAPLQJ&ei=stpXYuG4CovMsQK69Y7ABg&json=", "num_citations": 44, "citedby_url": "/scholar?cites=12987326770571285349&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZZPSVeNAPLQJ:scholar.google.com/&scioq=Are+Pre-trained+Language+Models+Aware+Of+Phrases%3F+Simple+But+Strong+Baselines+For+Grammar+Induction&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.00737"}, "On The Equivalence Between Node Embeddings And Structural Graph Representations": {"container_type": "Publication", "bib": {"title": "On the equivalence between node embeddings and structural graph representations", "author": ["B Srinivasan", "B Ribeiro"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.00452", "abstract": "This work provides the first unifying theoretical framework for node embeddings and structural graph representations, bridging methods like matrix factorization and graph neural networks. Using invariant theory, we show that the relationship between structural representations and node embeddings is analogous to that of a distribution and its samples. We prove that all tasks that can be performed by node embeddings can also be performed by structural representations and vice-versa. We also show that the concept of transductive"}, "filled": false, "gsrank": 1, "pub_url": "https://128.84.4.13/abs/1910.00452v2", "author_id": ["uM4EhgEAAAAJ", "KIEleCsAAAAJ"], "url_scholarbib": "/scholar?q=info:DprIHZOF-PcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BEquivalence%2BBetween%2BNode%2BEmbeddings%2BAnd%2BStructural%2BGraph%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DprIHZOF-PcJ&ei=tNpXYo6MMY6pywSdh6agAg&json=", "num_citations": 7, "citedby_url": "/scholar?cites=17868178388498815502&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DprIHZOF-PcJ:scholar.google.com/&scioq=On+The+Equivalence+Between+Node+Embeddings+And+Structural+Graph+Representations&hl=en&as_sdt=0,33"}, "Attributes Obfuscation With Complex-valued Features": {"container_type": "Publication", "bib": {"title": "Interpretable complex-valued neural networks for privacy protection", "author": ["L Xiang", "H Ma", "H Zhang", "Y Zhang", "J Ren"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "of inferring input attributes from features, while maintaining  transforms real-valued features  into complex-valued ones, in  feature a contains sufficient information to cause obfuscation"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.09546", "author_id": ["lnE2EFIAAAAJ", "A1vG9BAAAAAJ", "3g6LlgwAAAAJ", "", "Os9wmpkAAAAJ"], "url_scholarbib": "/scholar?q=info:kBHcdYnBAGoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttributes%2BObfuscation%2BWith%2BComplex-valued%2BFeatures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kBHcdYnBAGoJ&ei=t9pXYuH6C5WMy9YPt8OamA0&json=", "num_citations": 14, "citedby_url": "/scholar?cites=7638317764152398224&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kBHcdYnBAGoJ:scholar.google.com/&scioq=Attributes+Obfuscation+With+Complex-valued+Features&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.09546"}, "Automated Relational Meta-learning": {"container_type": "Publication", "bib": {"title": "Automated relational meta-learning", "author": ["H Yao", "X Wu", "Z Tao", "Y Li", "B Ding", "R Li", "Z Li"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The proposed meta-learning framework is named as Automated Relational Meta-Learning  ( task, we construct a prototype-based relational graph for each class, where each vertex"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00745", "author_id": ["A20BZnQAAAAJ", "DFBHS5QAAAAJ", "sEKglOkAAAAJ", "CCPBcdYAAAAJ", "AjYkTi8AAAAJ", "gYCtd6cAAAAJ", ""], "url_scholarbib": "/scholar?q=info:x5ayEXHfRLAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomated%2BRelational%2BMeta-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x5ayEXHfRLAJ&ei=vdpXYra0O4ySyATlkbrQCA&json=", "num_citations": 55, "citedby_url": "/scholar?cites=12701522525812856519&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:x5ayEXHfRLAJ:scholar.google.com/&scioq=Automated+Relational+Meta-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00745"}, "Pad\u00e9 Activation Units: End-to-end Learning Of Flexible Activation Functions In Deep Networks": {"container_type": "Publication", "bib": {"title": "Pad\\'e Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "author": ["A Molina", "P Schramowski", "K Kersting"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.06732", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial, and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\\'e Activation Units (PAUs) can both"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.06732", "author_id": ["VIHj44oAAAAJ", "GD481RkAAAAJ", "QY-earAAAAAJ"], "url_scholarbib": "/scholar?q=info:_tnaXCrYnYsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPad%25C3%25A9%2BActivation%2BUnits:%2BEnd-to-end%2BLearning%2BOf%2BFlexible%2BActivation%2BFunctions%2BIn%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_tnaXCrYnYsJ&ei=wtpXYtqdE4ySyATlkbrQCA&json=", "num_citations": 28, "citedby_url": "/scholar?cites=10060434819073628670&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_tnaXCrYnYsJ:scholar.google.com/&scioq=Pad%C3%A9+Activation+Units:+End-to-end+Learning+Of+Flexible+Activation+Functions+In+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.06732"}, "Improved Memory In Recurrent Neural Networks With Sequential Non-normal Dynamics": {"container_type": "Publication", "bib": {"title": "Improved memory in recurrent neural networks with sequential non-normal dynamics", "author": ["AE Orhan", "X Pitkow"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.13715", "abstract": "increased non-normality. Given the benefits of chain-like feedforward non-normal structures  in RNNs for improved memory structures were responsible for their increased non-normality."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.13715", "author_id": ["YkT8jLoAAAAJ", "ony4DjAAAAAJ"], "url_scholarbib": "/scholar?q=info:XKPygsR4TyIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproved%2BMemory%2BIn%2BRecurrent%2BNeural%2BNetworks%2BWith%2BSequential%2BNon-normal%2BDynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XKPygsR4TyIJ&ei=xdpXYtGlKIvEmgH7846QCg&json=", "num_citations": 8, "citedby_url": "/scholar?cites=2472327505855554396&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XKPygsR4TyIJ:scholar.google.com/&scioq=Improved+Memory+In+Recurrent+Neural+Networks+With+Sequential+Non-normal+Dynamics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.13715"}, "Harnessing The Power Of Infinitely Wide Deep Nets On Small-data Tasks": {"container_type": "Publication", "bib": {"title": "Harnessing the power of infinitely wide deep nets on small-data tasks", "author": ["S Arora", "SS Du", "Z Li", "R Salakhutdinov", "R Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recent research shows that the following two models are equivalent:(a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs)(Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al.(2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01663", "author_id": ["RUP4S68AAAAJ", "OttawxUAAAAJ", "5vVjpBsAAAAJ", "ITZ1e7MAAAAJ", "n8ZpnWMAAAAJ"], "url_scholarbib": "/scholar?q=info:YxE9CJOZFlIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHarnessing%2BThe%2BPower%2BOf%2BInfinitely%2BWide%2BDeep%2BNets%2BOn%2BSmall-data%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YxE9CJOZFlIJ&ei=yNpXYuOeII6pywSdh6agAg&json=", "num_citations": 93, "citedby_url": "/scholar?cites=5915084017375187299&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YxE9CJOZFlIJ:scholar.google.com/&scioq=Harnessing+The+Power+Of+Infinitely+Wide+Deep+Nets+On+Small-data+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01663"}, "Towards Stable And Efficient Training Of Verifiably Robust Neural Networks": {"container_type": "Publication", "bib": {"title": "Towards stable and efficient training of verifiably robust neural networks", "author": ["H Zhang", "H Chen", "C Xiao", "S Gowal", "R Stanforth"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.06316", "author_id": ["LTa3GzEAAAAJ", "KFtsQvIAAAAJ", "Juoqtj8AAAAJ", "7wclGnQAAAAJ", ""], "url_scholarbib": "/scholar?q=info:Nja16icJUmYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BStable%2BAnd%2BEfficient%2BTraining%2BOf%2BVerifiably%2BRobust%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Nja16icJUmYJ&ei=zNpXYvuMHMLZmQHc1ovQAg&json=", "num_citations": 140, "citedby_url": "/scholar?cites=7372965607005042230&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Nja16icJUmYJ:scholar.google.com/&scioq=Towards+Stable+And+Efficient+Training+Of+Verifiably+Robust+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.06316.pdf?ref=https://githubhelp.com"}, "The Ingredients Of Real World Robotic Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "The ingredients of real-world robotic reinforcement learning", "author": ["H Zhu", "J Yu", "A Gupta", "D Shah", "K Hartikainen"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The success of reinforcement learning for real world robotics has been, in many cases limited to instrumented laboratory scenarios, often requiring arduous human effort and oversight to enable continuous learning. In this work, we discuss the elements that are needed for a robotic learning system that can continually and autonomously improve with data collected in the real world. We propose a particular instantiation of such a system, using dexterous manipulation as our case study. Subsequently, we investigate a number of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.12570", "author_id": ["52T2LYoAAAAJ", "", "1wLVDP4AAAAJ", "d5y4iKAAAAAJ", "eVYhlDQAAAAJ"], "url_scholarbib": "/scholar?q=info:n2Dq5pZM01EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BIngredients%2BOf%2BReal%2BWorld%2BRobotic%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n2Dq5pZM01EJ&ei=z9pXYoGpII2EmgH6u5u4BA&json=", "num_citations": 79, "citedby_url": "/scholar?cites=5896140548161036447&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n2Dq5pZM01EJ:scholar.google.com/&scioq=The+Ingredients+Of+Real+World+Robotic+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.12570"}, "Composition-based Multi-relational Graph Convolutional Networks": {"container_type": "Publication", "bib": {"title": "Composition-based multi-relational graph convolutional networks", "author": ["S Vashishth", "S Sanyal", "V Nitin", "P Talukdar"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.03082", "author_id": ["gXW8J2wAAAAJ", "KvaizyQAAAAJ", "FLiz6csAAAAJ", "CIZwXAcAAAAJ"], "url_scholarbib": "/scholar?q=info:y0abf5TtYUQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComposition-based%2BMulti-relational%2BGraph%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y0abf5TtYUQJ&ei=0tpXYpm2CsiBy9YP18Gi8As&json=", "num_citations": 196, "citedby_url": "/scholar?cites=4927480689371858635&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y0abf5TtYUQJ:scholar.google.com/&scioq=Composition-based+Multi-relational+Graph+Convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.03082?ref=https://githubhelp.com"}, "Efficient Riemannian Optimization On The Stiefel Manifold Via The Cayley Transform": {"container_type": "Publication", "bib": {"title": "Efficient riemannian optimization on the stiefel manifold via the cayley transform", "author": ["J Li", "L Fuxin", "S Todorovic"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.01113", "abstract": "Strictly enforcing orthonormality constraints on parameter matrices has been shown advantageous in deep learning. This amounts to Riemannian optimization on the Stiefel manifold, which, however, is computationally expensive. To address this challenge, we present two main contributions:(1) A new efficient retraction map based on an iterative Cayley transform for optimization updates, and (2) An implicit vector transport mechanism based on the combination of a projection of the momentum and the Cayley transform on the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.01113", "author_id": ["fyQZYz8AAAAJ", "snDpfA0AAAAJ", "uoVID-0AAAAJ"], "url_scholarbib": "/scholar?q=info:9ZLZySwoofIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BRiemannian%2BOptimization%2BOn%2BThe%2BStiefel%2BManifold%2BVia%2BThe%2BCayley%2BTransform%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9ZLZySwoofIJ&ei=1dpXYsSOKeHDywTjooCQBQ&json=", "num_citations": 41, "citedby_url": "/scholar?cites=17483299401259127541&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9ZLZySwoofIJ:scholar.google.com/&scioq=Efficient+Riemannian+Optimization+On+The+Stiefel+Manifold+Via+The+Cayley+Transform&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.01113"}, "Transferring Optimality Across Data Distributions Via Homotopy Methods": {"container_type": "Publication", "bib": {"title": "Transferring Optimality Across Data Distributions via Homotopy Methods", "author": ["M Gargiani", "A Zanelli", "QT Dinh", "M Diehl"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Homotopy methods, also known as continuation methods, are a powerful mathematical tool to efficiently solve various problems in numerical analysis, including complex non-convex optimization problems where no or only little prior knowledge regarding the localization of the solutions is available. In this work, we propose a novel homotopy-based numerical method that can be used to transfer knowledge regarding the localization of an optimum across different task distributions in deep learning applications. We validate the proposed"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1gEIerYwH", "author_id": ["gejXFzwAAAAJ", "KVtkWqwAAAAJ", "Wyxqvt8AAAAJ", "JEwMX6wAAAAJ"], "url_scholarbib": "/scholar?q=info:8O4c0AjK3EAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransferring%2BOptimality%2BAcross%2BData%2BDistributions%2BVia%2BHomotopy%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8O4c0AjK3EAJ&ei=2NpXYpCuI8iBy9YP18Gi8As&json=", "num_citations": 1, "citedby_url": "/scholar?cites=4673832652503838448&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8O4c0AjK3EAJ:scholar.google.com/&scioq=Transferring+Optimality+Across+Data+Distributions+Via+Homotopy+Methods&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1gEIerYwH"}, "Probability Calibration For Knowledge Graph Embedding Models": {"container_type": "Publication", "bib": {"title": "Probability calibration for knowledge graph embedding models", "author": ["P Tabacof", "L Costabello"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.10000", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10000", "author_id": ["mbEp-w0AAAAJ", "O1Blx4AAAAAJ"], "url_scholarbib": "/scholar?q=info:4X_KtLDHnNAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbability%2BCalibration%2BFor%2BKnowledge%2BGraph%2BEmbedding%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4X_KtLDHnNAJ&ei=3NpXYoHgEIvMsQK69Y7ABg&json=", "num_citations": 18, "citedby_url": "/scholar?cites=15032109218017214433&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4X_KtLDHnNAJ:scholar.google.com/&scioq=Probability+Calibration+For+Knowledge+Graph+Embedding+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10000"}, "Neural Tangent Kernels, Transportation Mappings, And Universal Approximation": {"container_type": "Publication", "bib": {"title": "Neural tangent kernels, transportation mappings, and universal approximation", "author": ["Z Ji", "M Telgarsky", "R Xian"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.06956", "abstract": "This paper establishes rates of universal approximation for the shallow neural tangent kernel (NTK): network weights are only allowed microscopic changes from random initialization, which entails that activations are mostly unchanged, and the network is nearly equivalent to its linearization. Concretely, the paper has two main contributions: a generic scheme to approximate functions with the NTK by sampling from transport mappings between the initial weights and their desired values, and the construction of transport"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.06956", "author_id": ["QiQ_FXIAAAAJ", "Fc-5yRIAAAAJ", "Nmk26z4AAAAJ"], "url_scholarbib": "/scholar?q=info:BNlDEjE5KUkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BTangent%2BKernels,%2BTransportation%2BMappings,%2BAnd%2BUniversal%2BApproximation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BNlDEjE5KUkJ&ei=39pXYvyWIY2EmgH6u5u4BA&json=", "num_citations": 26, "citedby_url": "/scholar?cites=5271807721736493316&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BNlDEjE5KUkJ:scholar.google.com/&scioq=Neural+Tangent+Kernels,+Transportation+Mappings,+And+Universal+Approximation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.06956"}, "Watch The Unobserved: A Simple Approach To Parallelizing Monte Carlo Tree Search": {"container_type": "Publication", "bib": {"title": "Watch the unobserved: A simple approach to parallelizing monte carlo tree search", "author": ["A Liu", "J Chen", "M Yu", "Y Zhai", "X Zhou", "J Liu"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many challenging benchmarks (eg, Computer Go). However, they generally require a large number of rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (eg, node visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation tradeoff. In spite of these difficulties, we develop"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.11755", "author_id": ["k_4zYecAAAAJ", "jQeFWdoAAAAJ", "", "", "", "RRzVwKkAAAAJ"], "url_scholarbib": "/scholar?q=info:FEmSNHx2YKQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWatch%2BThe%2BUnobserved:%2BA%2BSimple%2BApproach%2BTo%2BParallelizing%2BMonte%2BCarlo%2BTree%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FEmSNHx2YKQJ&ei=4tpXYvCCLcS4ywTtzb_QDA&json=", "num_citations": 10, "citedby_url": "/scholar?cites=11844597295814428948&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FEmSNHx2YKQJ:scholar.google.com/&scioq=Watch+The+Unobserved:+A+Simple+Approach+To+Parallelizing+Monte+Carlo+Tree+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.11755"}, "Meta-learning Acquisition Functions For Transfer Learning In Bayesian Optimization": {"container_type": "Publication", "bib": {"title": "Meta-learning acquisition functions for transfer learning in bayesian optimization", "author": ["M Volpp", "LP Fr\u00f6hlich", "K Fischer", "A Doerr"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.02642", "author_id": ["K4fbAT8AAAAJ", "6sU3uXUAAAAJ", "", "k9Hczv4AAAAJ"], "url_scholarbib": "/scholar?q=info:uzikTouZ5s0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BAcquisition%2BFunctions%2BFor%2BTransfer%2BLearning%2BIn%2BBayesian%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uzikTouZ5s0J&ei=5dpXYpfRJ86E6rQPz8uiuAc&json=", "num_citations": 22, "citedby_url": "/scholar?cites=14836714846017566907&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uzikTouZ5s0J:scholar.google.com/&scioq=Meta-learning+Acquisition+Functions+For+Transfer+Learning+In+Bayesian+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.02642"}, "Structpool: Structured Graph Pooling Via Conditional Random Fields": {"container_type": "Publication", "bib": {"title": "Structpool: Structured graph pooling via conditional random fields", "author": ["H Yuan", "S Ji"], "pub_year": "2020", "venue": "Proceedings of the 8th International Conference on \u2026", "abstract": "Learning high-level representations for graphs is of great importance for graph analysis tasks. In addition to graph convolution, graph pooling is an important but less explored research area. In particular, most of existing graph pooling techniques do not consider the graph structural information explicitly. We argue that such information is important and develop a novel graph pooling technique, know as the STRUCTPOOL, in this work. We consider the graph pooling as a node clustering problem, which requires the learning of a"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/servlets/purl/10159731", "author_id": ["dooagDcAAAAJ", "BZGj6sAAAAAJ"], "url_scholarbib": "/scholar?q=info:c_W71yJGvb0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructpool:%2BStructured%2BGraph%2BPooling%2BVia%2BConditional%2BRandom%2BFields%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c_W71yJGvb0J&ei=6NpXYq6BNJLeyQTms5KQBg&json=", "num_citations": 68, "citedby_url": "/scholar?cites=13672161159228945779&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:c_W71yJGvb0J:scholar.google.com/&scioq=Structpool:+Structured+Graph+Pooling+Via+Conditional+Random+Fields&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10159731"}, "Federated Adversarial Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Federated adversarial domain adaptation", "author": ["X Peng", "Z Huang", "Y Zhu", "K Saenko"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.02054", "abstract": "a way that reduces domain shift. First, we analyze the federated domain adaptation problem  from a  an efficient adaptation algorithm based on adversarial adaptation and representation"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.02054", "author_id": ["66lkylsAAAAJ", "", "hPXUR0cAAAAJ", "9xDADY4AAAAJ"], "url_scholarbib": "/scholar?q=info:N66WIsdhAkIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFederated%2BAdversarial%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N66WIsdhAkIJ&ei=69pXYoewHpaM6rQPlISayA8&json=", "num_citations": 88, "citedby_url": "/scholar?cites=4756471664363351607&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:N66WIsdhAkIJ:scholar.google.com/&scioq=Federated+Adversarial+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.02054.pdf?ref=https://githubhelp.com"}, "Bounds On Over-parameterization For Guaranteed Existence Of Descent Paths In Shallow Relu Networks": {"container_type": "Publication", "bib": {"title": "Bounds on over-parameterization for guaranteed existence of descent paths in shallow ReLU networks", "author": ["A Sharifnassab", "S Salehkaleybar"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let $ m $ and $ d $ be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size $ n\\geq m+ 2d-2$. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkgXHTNtvS", "author_id": ["", "-f0AwZcAAAAJ"], "url_scholarbib": "/scholar?q=info:W8IBu_K9kZ4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBounds%2BOn%2BOver-parameterization%2BFor%2BGuaranteed%2BExistence%2BOf%2BDescent%2BPaths%2BIn%2BShallow%2BRelu%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=W8IBu_K9kZ4J&ei=7tpXYqrRKovMsQK69Y7ABg&json=", "num_citations": 7, "citedby_url": "/scholar?cites=11426122579832848987&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:W8IBu_K9kZ4J:scholar.google.com/&scioq=Bounds+On+Over-parameterization+For+Guaranteed+Existence+Of+Descent+Paths+In+Shallow+Relu+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkgXHTNtvS"}, "On Generalization Error Bounds Of Noisy Gradient Methods For Non-convex Learning": {"container_type": "Publication", "bib": {"title": "On generalization error bounds of noisy gradient methods for non-convex learning", "author": ["J Li", "X Luo", "M Qiao"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.00621", "abstract": "Generalization error (also known as the out-of-sample error) measures how well the hypothesis learned from training data generalizes to previously unseen data. Proving tight generalization error bounds is a central question in statistical learning theory. In this paper, we obtain generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. We develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalization error bounds. The new"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.00621", "author_id": ["zX7i1EkAAAAJ", "IahKIPMAAAAJ", "mV9LQUoAAAAJ"], "url_scholarbib": "/scholar?q=info:P2a2erA1jXMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BGeneralization%2BError%2BBounds%2BOf%2BNoisy%2BGradient%2BMethods%2BFor%2BNon-convex%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=P2a2erA1jXMJ&ei=89pXYpDzBcmUywTMkLbABQ&json=", "num_citations": 36, "citedby_url": "/scholar?cites=8326370318167205439&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:P2a2erA1jXMJ:scholar.google.com/&scioq=On+Generalization+Error+Bounds+Of+Noisy+Gradient+Methods+For+Non-convex+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.00621"}, "Online And Stochastic Optimization Beyond Lipschitz Continuity: A Riemannian Approach": {"container_type": "Publication", "bib": {"title": "Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach", "author": ["K Antonakopoulos", "EV Belmega"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Motivated by applications to machine learning and imaging science, we study a class of online and stochastic optimization problems with loss functions that are not Lipschitz continuous; in particular, the loss functions encountered by the optimizer could exhibit gradient singularities or be singular themselves. Drawing on tools and techniques from Riemannian geometry, we examine a Riemann\u2013Lipschitz (RL) continuity condition which is tailored to the singularity landscape of the problem's loss functions. In this way, we are able"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkxZyaNtwB", "author_id": ["8VQSMx4AAAAJ", "ODy3eccAAAAJ"], "url_scholarbib": "/scholar?q=info:_bHJWB_B01wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOnline%2BAnd%2BStochastic%2BOptimization%2BBeyond%2BLipschitz%2BContinuity:%2BA%2BRiemannian%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_bHJWB_B01wJ&ei=HdhXYtb1Hc2Ny9YPqPyUgAs&json=", "num_citations": 9, "citedby_url": "/scholar?cites=6688902211953078781&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_bHJWB_B01wJ:scholar.google.com/&scioq=Online+And+Stochastic+Optimization+Beyond+Lipschitz+Continuity:+A+Riemannian+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkxZyaNtwB"}, "Gap-aware Mitigation Of Gradient Staleness": {"container_type": "Publication", "bib": {"title": "Gap Aware Mitigation of Gradient Staleness", "author": ["S Barkai", "I Hakimi", "A Schuster"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.10802", "abstract": "Cloud computing is becoming increasingly popular as a platform for distributed training of deep neural networks. Synchronous stochastic gradient descent (SSGD) suffers from substantial slowdowns due to stragglers if the environment is non-dedicated, as is common in cloud computing. Asynchronous SGD (ASGD) methods are immune to these slowdowns but are scarcely used due to gradient staleness, which encumbers the convergence process. Recent techniques have had limited success mitigating the gradient staleness"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.10802", "author_id": ["", "N0EnDYsAAAAJ", "KfwgjswAAAAJ"], "url_scholarbib": "/scholar?q=info:-jWb3RY5HasJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGap-aware%2BMitigation%2BOf%2BGradient%2BStaleness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-jWb3RY5HasJ&ei=IdhXYpvaBovMsQK69Y7ABg&json=", "num_citations": 4, "citedby_url": "/scholar?cites=12330074125180286458&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-jWb3RY5HasJ:scholar.google.com/&scioq=Gap-aware+Mitigation+Of+Gradient+Staleness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.10802"}, "Deep Orientation Uncertainty Learning Based On A Bingham Loss": {"container_type": "Publication", "bib": {"title": "Deep orientation uncertainty learning based on a bingham loss", "author": ["I Gilitschenski", "R Sahoo", "W Schwarting"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Reasoning about uncertain orientations is one of the core problems in many perception tasks such as object pose estimation or motion estimation. In these scenarios, poor illumination conditions, sensor limitations, or appearance invariance may result in highly uncertain estimates. In this work, we propose a novel learning-based representation for orientation uncertainty. By characterizing uncertainty over unit quaternions with the Bingham distribution, we formulate a loss that naturally captures the antipodal symmetry of the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryloogSKDS", "author_id": ["Nuw1Y4oAAAAJ", "hGO6cWYAAAAJ", "YI1EqBoAAAAJ"], "url_scholarbib": "/scholar?q=info:25DhzndNe2kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BOrientation%2BUncertainty%2BLearning%2BBased%2BOn%2BA%2BBingham%2BLoss%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=25DhzndNe2kJ&ei=JthXYrn9H5WMy9YPt8OamA0&json=", "num_citations": 23, "citedby_url": "/scholar?cites=7600753973085180123&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:25DhzndNe2kJ:scholar.google.com/&scioq=Deep+Orientation+Uncertainty+Learning+Based+On+A+Bingham+Loss&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryloogSKDS"}, "From Variational To Deterministic Autoencoders": {"container_type": "Publication", "bib": {"title": "From variational to deterministic autoencoders", "author": ["P Ghosh", "MSM Sajjadi", "A Vergari", "M Black"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.12436", "author_id": ["U9FXf-AAAAAJ", "rHF25YEAAAAJ", "YK0NLaUAAAAJ", "6NjbexEAAAAJ"], "url_scholarbib": "/scholar?q=info:vzDHWOr_4JIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrom%2BVariational%2BTo%2BDeterministic%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vzDHWOr_4JIJ&ei=KdhXYqWcN5WMy9YPt8OamA0&json=", "num_citations": 149, "citedby_url": "/scholar?cites=10583740506297544895&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vzDHWOr_4JIJ:scholar.google.com/&scioq=From+Variational+To+Deterministic+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.12436.pdf%20http://arxiv.org/abs/1903.12436"}, "Encoding Word Order In Complex Embeddings": {"container_type": "Publication", "bib": {"title": "Encoding word order in complex embeddings", "author": ["B Wang", "D Zhao", "C Lioma", "Q Li", "P Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Sequential word order is important when processing text. Currently, neural networks (NNs) address this by modeling word position using position embeddings. The problem is that position embeddings capture the position of individual words, but not the ordered relationship (eg, adjacency or precedence) between individual word positions. We present a novel and principled solution for modeling both the global absolute positions of words and their order relationships. Our solution generalizes word embeddings, previously defined as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.12333", "author_id": ["Jk4vJU8AAAAJ", "", "dXkFCYoAAAAJ", "vZ3-V8oAAAAJ", "tvDb5_cAAAAJ"], "url_scholarbib": "/scholar?q=info:CkJ9OPSsWDwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEncoding%2BWord%2BOrder%2BIn%2BComplex%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CkJ9OPSsWDwJ&ei=NthXYqirBZLeyQTms5KQBg&json=", "num_citations": 42, "citedby_url": "/scholar?cites=4348415605145944586&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CkJ9OPSsWDwJ:scholar.google.com/&scioq=Encoding+Word+Order+In+Complex+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.12333"}, "Phase Transitions For The Information Bottleneck In Representation Learning": {"container_type": "Publication", "bib": {"title": "Phase transitions for the information bottleneck in representation learning", "author": ["T Wu", "I Fischer"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.01878", "abstract": "In the Information Bottleneck (IB), when tuning the relative strength between compression and prediction terms, how do the two terms behave, and what's their relationship with the dataset and the learned representation? In this paper, we set out to answer these questions by studying multiple phase transitions in the IB objective: $\\text {IB} _\\beta [p (z| x)]= I (X; Z)-\\beta I (Y; Z) $ defined on the encoding distribution p (z| x) for input $ X $, target $ Y $ and representation $ Z $, where sudden jumps of $ dI (Y; Z)/d\\beta $ and prediction accuracy are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.01878", "author_id": ["axv_UT0AAAAJ", "Z63Zf_0AAAAJ"], "url_scholarbib": "/scholar?q=info:F8CBNeZRrVMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPhase%2BTransitions%2BFor%2BThe%2BInformation%2BBottleneck%2BIn%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F8CBNeZRrVMJ&ei=OdhXYpT4AovMsQK69Y7ABg&json=", "num_citations": 14, "citedby_url": "/scholar?cites=6029565525300985879&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F8CBNeZRrVMJ:scholar.google.com/&scioq=Phase+Transitions+For+The+Information+Bottleneck+In+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.01878"}, "Lazy-cfr: Fast And Near-optimal Regret Minimization For Extensive Games With Imperfect Information": {"container_type": "Publication", "bib": {"title": "Lazy-cfr: fast and near optimal regret minimization for extensive games with imperfect information", "author": ["Y Zhou", "T Ren", "J Li", "D Yan", "J Zhu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.04433", "abstract": "Counterfactual regret minimization (CFR) is the most popular algorithm on solving two-player zero-sum extensive games with imperfect information and achieves state-of-the-art performance in practice. However, the performance of CFR is not fully understood, since empirical results on the regret are much better than the upper bound proved in\\cite {zinkevich2008regret}. Another issue is that CFR has to traverse the whole game tree in each round, which is time-consuming in large scale games. In this paper, we present a novel"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.04433", "author_id": ["", "VgNDYeYAAAAJ", "", "lvztRUkAAAAJ", "axsP38wAAAAJ"], "url_scholarbib": "/scholar?q=info:azy0bCv0cZIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLazy-cfr:%2BFast%2BAnd%2BNear-optimal%2BRegret%2BMinimization%2BFor%2BExtensive%2BGames%2BWith%2BImperfect%2BInformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=azy0bCv0cZIJ&ei=PNhXYoDeIZaM6rQPlISayA8&json=", "num_citations": 3, "citedby_url": "/scholar?cites=10552483869250305131&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:azy0bCv0cZIJ:scholar.google.com/&scioq=Lazy-cfr:+Fast+And+Near-optimal+Regret+Minimization+For+Extensive+Games+With+Imperfect+Information&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.04433"}, "A Probabilistic Formulation Of Unsupervised Text Style Transfer": {"container_type": "Publication", "bib": {"title": "A probabilistic formulation of unsupervised text style transfer", "author": ["J He", "X Wang", "G Neubig", "T Berg-Kirkpatrick"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (eg the HMM), our model makes few"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.03912", "author_id": ["BIFGeoUAAAAJ", "Fo52NKoAAAAJ", "wlosgkoAAAAJ", "mN6_BKAAAAAJ"], "url_scholarbib": "/scholar?q=info:zPCNtHjUdKsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BProbabilistic%2BFormulation%2BOf%2BUnsupervised%2BText%2BStyle%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zPCNtHjUdKsJ&ei=R9hXYqYelYzL1g-3w5qYDQ&json=", "num_citations": 68, "citedby_url": "/scholar?cites=12354733292674478284&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zPCNtHjUdKsJ:scholar.google.com/&scioq=A+Probabilistic+Formulation+Of+Unsupervised+Text+Style+Transfer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.03912"}, "Black-box Adversarial Attack With Transferable Model-based Embedding": {"container_type": "Publication", "bib": {"title": "Black-box adversarial attack with transferable model-based embedding", "author": ["Z Huang", "T Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.07140", "abstract": "We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.07140", "author_id": ["MILCJzAAAAAJ", "LurWtuYAAAAJ"], "url_scholarbib": "/scholar?q=info:N_Vdt7MrGScJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBlack-box%2BAdversarial%2BAttack%2BWith%2BTransferable%2BModel-based%2BEmbedding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N_Vdt7MrGScJ&ei=SdhXYoLCMYvEmgH7846QCg&json=", "num_citations": 50, "citedby_url": "/scholar?cites=2817331092772484407&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:N_Vdt7MrGScJ:scholar.google.com/&scioq=Black-box+Adversarial+Attack+With+Transferable+Model-based+Embedding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.07140"}, "And The Bit Goes Down: Revisiting The Quantization Of Neural Networks": {"container_type": "Publication", "bib": {"title": "And the bit goes down: Revisiting the quantization of neural networks", "author": ["P Stock", "A Joulin", "R Gribonval", "B Graham"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we address the problem of reducing the memory footprint of convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The principle of our approach is that it minimizes the loss reconstruction error for in-domain inputs. Our method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.05686", "author_id": ["3e2-59cAAAAJ", "kRJkDakAAAAJ", "EcqbX1QAAAAJ", "jQkkhlkAAAAJ"], "url_scholarbib": "/scholar?q=info:LtW0kw-k9H8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnd%2BThe%2BBit%2BGoes%2BDown:%2BRevisiting%2BThe%2BQuantization%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LtW0kw-k9H8J&ei=TNhXYq6lLMS4ywTtzb_QDA&json=", "num_citations": 86, "citedby_url": "/scholar?cites=9220174723943814446&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LtW0kw-k9H8J:scholar.google.com/&scioq=And+The+Bit+Goes+Down:+Revisiting+The+Quantization+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.05686.pdf?utm_campaign=ExternalTechRecSeptember52019&utm_content=100275348&utm_medium=social&utm_source=twitter&hss_channel=tw-992153930095251456"}, "Efficient Probabilistic Logic Reasoning With Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Efficient probabilistic logic reasoning with graph neural networks", "author": ["Y Zhang", "X Chen", "Y Yang", "A Ramamurthy", "B Li"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Markov Logic Networks (MLNs), which elegantly combine logic rules and probabilistic graphical models, can be used to address many knowledge graph problems. However, inference in MLN is computationally intensive, making the industrial-scale application of MLN very difficult. In recent years, graph neural networks (GNNs) have emerged as efficient and effective tools for large-scale graph problems. Nevertheless, GNNs do not explicitly incorporate prior logic rules into the models, and may require many labeled examples for a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.11850", "author_id": ["TIC2ujUAAAAJ", "irg46EUAAAAJ", "Lt4tmL8AAAAJ", "13uSmOkAAAAJ", "K8vJkTcAAAAJ"], "url_scholarbib": "/scholar?q=info:2ZHFt0pTJ64J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BProbabilistic%2BLogic%2BReasoning%2BWith%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2ZHFt0pTJ64J&ei=T9hXYuuIDMmUywTMkLbABQ&json=", "num_citations": 50, "citedby_url": "/scholar?cites=12549090467067040217&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2ZHFt0pTJ64J:scholar.google.com/&scioq=Efficient+Probabilistic+Logic+Reasoning+With+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.11850"}, "Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree On The Truth": {"container_type": "Publication", "bib": {"title": "Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth", "author": ["I Lovchinsky", "A Daks", "I Malkin"], "pub_year": "2019", "venue": "International \u2026", "abstract": "In most machine learning tasks unambiguous ground truth labels can easily be acquired. However, this luxury is often not afforded to many high-stakes, real-world scenarios such as medical image interpretation, where even expert human annotators typically exhibit very high levels of disagreement with one another. While prior works have focused on overcoming noisy labels during training, the question of how to evaluate models when annotators disagree about ground truth has remained largely unexplored. To address this"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byg-wJSYDS", "author_id": ["NdeXo-EAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:rZ14qBb_irEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscrepancy%2BRatio:%2BEvaluating%2BModel%2BPerformance%2BWhen%2BEven%2BExperts%2BDisagree%2BOn%2BThe%2BTruth%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rZ14qBb_irEJ&ei=UthXYoz7GM2Ny9YPqPyUgAs&json=", "num_citations": 1, "citedby_url": "/scholar?cites=12793318164280155565&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rZ14qBb_irEJ:scholar.google.com/&scioq=Discrepancy+Ratio:+Evaluating+Model+Performance+When+Even+Experts+Disagree+On+The+Truth&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byg-wJSYDS"}, "On The Weaknesses Of Reinforcement Learning For Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "On the weaknesses of reinforcement learning for neural machine translation", "author": ["L Choshen", "L Fox", "Z Aizenbud", "O Abend"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.01752", "abstract": "Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.01752", "author_id": ["8b8IhUYAAAAJ", "6mCoDiUAAAAJ", "", "BD_hRzYAAAAJ"], "url_scholarbib": "/scholar?q=info:oAqXfuPBUNAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BWeaknesses%2BOf%2BReinforcement%2BLearning%2BFor%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oAqXfuPBUNAJ&ei=VNhXYp6LOsLZmQHc1ovQAg&json=", "num_citations": 23, "citedby_url": "/scholar?cites=15010710740851428000&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oAqXfuPBUNAJ:scholar.google.com/&scioq=On+The+Weaknesses+Of+Reinforcement+Learning+For+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.01752"}, "Empir: Ensembles Of Mixed Precision Deep Networks For Increased Robustness Against Adversarial Attacks": {"container_type": "Publication", "bib": {"title": "Empir: Ensembles of mixed precision deep networks for increased robustness against adversarial attacks", "author": ["S Sen", "B Ravindran", "A Raghunathan"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.10162", "abstract": "Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.10162", "author_id": ["InjzEk8AAAAJ", "nGUcGrYAAAAJ", "OP7F8jEAAAAJ"], "url_scholarbib": "/scholar?q=info:jUMG42MBAOYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmpir:%2BEnsembles%2BOf%2BMixed%2BPrecision%2BDeep%2BNetworks%2BFor%2BIncreased%2BRobustness%2BAgainst%2BAdversarial%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jUMG42MBAOYJ&ei=X9hXYv_mFZmM6rQPjaOSEA&json=", "num_citations": 31, "citedby_url": "/scholar?cites=16573248157245653901&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jUMG42MBAOYJ:scholar.google.com/&scioq=Empir:+Ensembles+Of+Mixed+Precision+Deep+Networks+For+Increased+Robustness+Against+Adversarial+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.10162"}, "Neural Text Generation With Unlikelihood Training": {"container_type": "Publication", "bib": {"title": "Neural text generation with unlikelihood training", "author": ["S Welleck", "I Kulikov", "S Roller", "E Dinan", "K Cho"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-$ k $ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.04319", "author_id": ["Ac6n5pQAAAAJ", "fN7fYXIAAAAJ", "22TE5qkAAAAJ", "pfqzHqUAAAAJ", "0RAmmIAAAAAJ"], "url_scholarbib": "/scholar?q=info:30EjD6zz5-YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BText%2BGeneration%2BWith%2BUnlikelihood%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=30EjD6zz5-YJ&ei=athXYqHcKMWemAHB5baIBQ&json=", "num_citations": 170, "citedby_url": "/scholar?cites=16638535268657480159&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:30EjD6zz5-YJ:scholar.google.com/&scioq=Neural+Text+Generation+With+Unlikelihood+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.04319"}, "Pitfalls Of In-domain Uncertainty Estimation And Ensembling In Deep Learning": {"container_type": "Publication", "bib": {"title": "Pitfalls of in-domain uncertainty estimation and ensembling in deep learning", "author": ["A Ashukha", "A Lyzhov", "D Molchanov"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty estimation is one of the main benchmarks for assessment of ensembling performance. At the same time, deep learning ensembles have provided state-of-the-art results in uncertainty estimation. In this work, we focus on in-domain uncertainty for image classification. We explore the standards for its quantification and point out pitfalls of existing metrics. Avoiding these pitfalls, we perform a broad study of different ensembling techniques. To provide more"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06470", "author_id": ["IU-kuP8AAAAJ", "5LXTi40AAAAJ", "tJ6JXRYAAAAJ"], "url_scholarbib": "/scholar?q=info:s18srVGhYmAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPitfalls%2BOf%2BIn-domain%2BUncertainty%2BEstimation%2BAnd%2BEnsembling%2BIn%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s18srVGhYmAJ&ei=bdhXYv2VLcLZmQHc1ovQAg&json=", "num_citations": 130, "citedby_url": "/scholar?cites=6945290947528515507&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s18srVGhYmAJ:scholar.google.com/&scioq=Pitfalls+Of+In-domain+Uncertainty+Estimation+And+Ensembling+In+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06470"}, "Learning The Arrow Of Time For Problems In Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Learning the arrow of time for problems in reinforcement learning", "author": ["N Rahaman", "S Wolf", "A Goyal", "R Remme", "Y Bengio"], "pub_year": "2020", "venue": "NA", "abstract": "We humans have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we approach the problem of learning an arrow of time in a Markov (Decision) Process. We illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, we propose a simple yet effective algorithm to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=nkiGCTgz3by", "author_id": ["iH9DuY0AAAAJ", "ZLTMBaEAAAAJ", "krrh6OUAAAAJ", "", "kukA0LcAAAAJ"], "url_scholarbib": "/scholar?q=info:-cRq-vKILCkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BThe%2BArrow%2BOf%2BTime%2BFor%2BProblems%2BIn%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-cRq-vKILCkJ&ei=cNhXYqfYK5aM6rQPlISayA8&json=", "num_citations": 4, "citedby_url": "/scholar?cites=2966896831695078649&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-cRq-vKILCkJ:scholar.google.com/&scioq=Learning+The+Arrow+Of+Time+For+Problems+In+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=nkiGCTgz3by"}, "Model-based Reinforcement Learning For Biological Sequence Design": {"container_type": "Publication", "bib": {"title": "Model-based reinforcement learning for biological sequence design", "author": ["C Angermueller", "D Dohan", "D Belanger"], "pub_year": "2019", "venue": "International \u2026", "abstract": "The ability to design biological structures such as DNA or proteins would have considerable medical and industrial impact. Doing so presents a challenging black-box optimization problem characterized by the large-batch, low round setting due to the need for labor-intensive wet lab evaluations. In response, we propose using reinforcement learning (RL) based on proximal-policy optimization (PPO) for biological sequence design. RL provides a flexible framework for optimization generative sequence models to achieve specific criteria"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HklxbgBKvr&fileGuid=3xgr169o12oUrbxS", "author_id": ["OXZC0mQAAAAJ", "iZ5cY0AAAAAJ", "AGnp8NAAAAAJ"], "url_scholarbib": "/scholar?q=info:YzNp1tAUKdcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel-based%2BReinforcement%2BLearning%2BFor%2BBiological%2BSequence%2BDesign%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YzNp1tAUKdcJ&ei=dNhXYvrhAsmUywTMkLbABQ&json=", "num_citations": 41, "citedby_url": "/scholar?cites=15503946079382614883&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YzNp1tAUKdcJ:scholar.google.com/&scioq=Model-based+Reinforcement+Learning+For+Biological+Sequence+Design&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HklxbgBKvr"}, "Polylogarithmic Width Suffices For Gradient Descent To Achieve Arbitrarily Small Test Error With Shallow Relu Networks": {"container_type": "Publication", "bib": {"title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks", "author": ["Z Ji", "M Telgarsky"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12292", "abstract": "Recent theoretical work has guaranteed that overparameterized networks trained by gradient descent achieve arbitrarily low training error, and sometimes even low test error. The required width, however, is always polynomial in at least one of the sample size $ n $, the (inverse) target error $1/\\epsilon $, and the (inverse) failure probability $1/\\delta $. This work shows that $\\widetilde {\\Theta}(1/\\epsilon) $ iterations of gradient descent with $\\widetilde {\\Omega}(1/\\epsilon^ 2) $ training examples on two-layer ReLU networks of any"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12292", "author_id": ["QiQ_FXIAAAAJ", "Fc-5yRIAAAAJ"], "url_scholarbib": "/scholar?q=info:Btp0g05AVlYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPolylogarithmic%2BWidth%2BSuffices%2BFor%2BGradient%2BDescent%2BTo%2BAchieve%2BArbitrarily%2BSmall%2BTest%2BError%2BWith%2BShallow%2BRelu%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Btp0g05AVlYJ&ei=d9hXYszmBpLeyQTms5KQBg&json=", "num_citations": 95, "citedby_url": "/scholar?cites=6221230641216018950&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Btp0g05AVlYJ:scholar.google.com/&scioq=Polylogarithmic+Width+Suffices+For+Gradient+Descent+To+Achieve+Arbitrarily+Small+Test+Error+With+Shallow+Relu+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12292"}, "Transformer-xh: Multi-hop Question Answering With Extra Hop Attention": {"container_type": "Publication", "bib": {"title": "Transformer-XH: Multi-hop question answering with extra hop attention", "author": ["C Zhao", "C Xiong", "C Rosset", "X Song", "P Bennett"], "pub_year": "2020", "venue": "NA", "abstract": ""}, "filled": false, "gsrank": 1, "author_id": ["", "", "", "", ""], "url_scholarbib": "/scholar?q=info:70ghG81-jvAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransformer-xh:%2BMulti-hop%2BQuestion%2BAnswering%2BWith%2BExtra%2BHop%2BAttention%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=70ghG81-jvAJ&ei=ethXYpjoA4vMsQK69Y7ABg&json=", "num_citations": 2, "citedby_url": "/scholar?cites=17333931435184179439&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:70ghG81-jvAJ:scholar.google.com/&scioq=Transformer-xh:+Multi-hop+Question+Answering+With+Extra+Hop+Attention&hl=en&as_sdt=0,33"}, "Learning Compositional Koopman Operators For Model-based Control": {"container_type": "Publication", "bib": {"title": "Learning compositional koopman operators for model-based control", "author": ["Y Li", "H He", "J Wu", "D Katabi", "A Torralba"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.08264", "abstract": "Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.08264", "author_id": ["WlA92lcAAAAJ", "v1sUoqwAAAAJ", "2efgcS0AAAAJ", "nst5fHgAAAAJ", "8cxDHS4AAAAJ"], "url_scholarbib": "/scholar?q=info:_WSG9NtzuboJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BCompositional%2BKoopman%2BOperators%2BFor%2BModel-based%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_WSG9NtzuboJ&ei=fthXYtDCB86E6rQPz8uiuAc&json=", "num_citations": 47, "citedby_url": "/scholar?cites=13454912750283547901&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_WSG9NtzuboJ:scholar.google.com/&scioq=Learning+Compositional+Koopman+Operators+For+Model-based+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.08264"}, "Deep Network Classification By Scattering And Homotopy Dictionary Learning": {"container_type": "Publication", "bib": {"title": "Deep network classification by scattering and homotopy dictionary learning", "author": ["J Zarka", "L Thiry", "T Angles", "S Mallat"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.03561", "abstract": "We introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations. A sparse $\\ell^ 1$ dictionary coding reduces intra-class variability"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.03561", "author_id": ["", "CrfXOCMAAAAJ", "RIH68D0AAAAJ", "g_YTmSgAAAAJ"], "url_scholarbib": "/scholar?q=info:M3jkaPxVQXwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BNetwork%2BClassification%2BBy%2BScattering%2BAnd%2BHomotopy%2BDictionary%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M3jkaPxVQXwJ&ei=gthXYo3sBciBy9YP18Gi8As&json=", "num_citations": 24, "citedby_url": "/scholar?cites=8953532076769179699&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:M3jkaPxVQXwJ:scholar.google.com/&scioq=Deep+Network+Classification+By+Scattering+And+Homotopy+Dictionary+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.03561"}, "Meta-learning Without Memorization": {"container_type": "Publication", "bib": {"title": "Meta-learning without memorization", "author": ["M Yin", "G Tucker", "M Zhou", "S Levine", "C Finn"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "to identify and formalize the memorization problem in meta-learning, and 2)  without placing  restrictions on the task distribution. We formally differentiate the meta-learning memorization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.03820", "author_id": ["oAEsILEAAAAJ", "-gJkPHIAAAAJ", "LXwCIisAAAAJ", "8R35rCwAAAAJ", "vfPE6hgAAAAJ"], "url_scholarbib": "/scholar?q=info:B4BoGeMEN2UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BWithout%2BMemorization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=B4BoGeMEN2UJ&ei=hdhXYrfcEM6E6rQPz8uiuAc&json=", "num_citations": 111, "citedby_url": "/scholar?cites=7293303494980173831&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:B4BoGeMEN2UJ:scholar.google.com/&scioq=Meta-learning+Without+Memorization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.03820"}, "Learned Step Size Quantization": {"container_type": "Publication", "bib": {"title": "Learned step size quantization", "author": ["SK Esser", "JL McKinstry", "D Bablani"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "to learn the quantization mapping for each layer in a deep network, Learned Step Size  Quantization ( For this purpose, for a given layer we define the final step size learned by LSQ as s"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.08153", "author_id": ["-UrVa3AAAAAJ", "qVhCHyoAAAAJ", "qlh34HoAAAAJ"], "url_scholarbib": "/scholar?q=info:-HNnq9yy7iwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearned%2BStep%2BSize%2BQuantization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-HNnq9yy7iwJ&ei=idhXYsKwL5LeyQTms5KQBg&json=", "num_citations": 212, "citedby_url": "/scholar?cites=3237721842964198392&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-HNnq9yy7iwJ:scholar.google.com/&scioq=Learned+Step+Size+Quantization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.08153"}, "Self: Learning To Filter Noisy Labels With Self-ensembling": {"container_type": "Publication", "bib": {"title": "Self: Learning to filter noisy labels with self-ensembling", "author": ["DT Nguyen", "CK Mummadi", "TPN Ngo"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "-fit a dataset when being trained with noisy labels for a long enough time. To overcome this   method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01842", "author_id": ["m8wQuf0AAAAJ", "XJLtaG4AAAAJ", "g9XBr9MAAAAJ"], "url_scholarbib": "/scholar?q=info:QHPTL3zzCEoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf:%2BLearning%2BTo%2BFilter%2BNoisy%2BLabels%2BWith%2BSelf-ensembling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QHPTL3zzCEoJ&ei=ldhXYofeH46pywSdh6agAg&json=", "num_citations": 129, "citedby_url": "/scholar?cites=5334781473324233536&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QHPTL3zzCEoJ:scholar.google.com/&scioq=Self:+Learning+To+Filter+Noisy+Labels+With+Self-ensembling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01842.pdf?ref=https://githubhelp.com"}, "A Framework For Robustness Certification Of Smoothed Classifiers Using F-divergences": {"container_type": "Publication", "bib": {"title": "A framework for robustness certification of smoothed classifiers using f-divergences", "author": ["KD Dvijotham", "J Hayes", "B Balle", "Z Kolter", "C Qin"], "pub_year": "2020", "venue": "NA", "abstract": "Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far require knowledge of the architecture of the machine learning model and remain hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=64mbzX0SxnG", "author_id": ["1tOFY1IAAAAJ", "bMI2GckAAAAJ", "xa-6ZlkAAAAJ", "UXh1I6UAAAAJ", "D5ut5cMAAAAJ"], "url_scholarbib": "/scholar?q=info:tERPv8WBM6oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BFramework%2BFor%2BRobustness%2BCertification%2BOf%2BSmoothed%2BClassifiers%2BUsing%2BF-divergences%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tERPv8WBM6oJ&ei=mdhXYojpBs6E6rQPz8uiuAc&json=", "num_citations": 23, "citedby_url": "/scholar?cites=12264288896578176180&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tERPv8WBM6oJ:scholar.google.com/&scioq=A+Framework+For+Robustness+Certification+Of+Smoothed+Classifiers+Using+F-divergences&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=64mbzX0SxnG"}, "Mutual Mean-teaching: Pseudo Label Refinery For Unsupervised Domain Adaptation On Person Re-identification": {"container_type": "Publication", "bib": {"title": "Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification", "author": ["Y Ge", "D Chen", "H Li"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.01526", "abstract": "Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.01526", "author_id": ["TtU74NAAAAAJ", "-Wpd7FcAAAAJ", "BN2Ze-QAAAAJ"], "url_scholarbib": "/scholar?q=info:snUhe3csLVIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMutual%2BMean-teaching:%2BPseudo%2BLabel%2BRefinery%2BFor%2BUnsupervised%2BDomain%2BAdaptation%2BOn%2BPerson%2BRe-identification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=snUhe3csLVIJ&ei=ndhXYt3nAYvEmgH7846QCg&json=", "num_citations": 249, "citedby_url": "/scholar?cites=5921437976740591026&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:snUhe3csLVIJ:scholar.google.com/&scioq=Mutual+Mean-teaching:+Pseudo+Label+Refinery+For+Unsupervised+Domain+Adaptation+On+Person+Re-identification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.01526"}, "Depth-adaptive Transformer": {"container_type": "Publication", "bib": {"title": "Depth-adaptive transformer", "author": ["M Elbayad", "J Gu", "E Grave", "M Auli"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.10073", "abstract": "In this paper, we train Transformer models which can make  Unlike dynamic computation in  Universal Transformers, which  of a well tuned baseline Transformer while using less than a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.10073", "author_id": ["4MFYc7AAAAAJ", "cB1mFBsAAAAJ", "7UV4ET4AAAAJ", "KMcwQtcAAAAJ"], "url_scholarbib": "/scholar?q=info:OOW12E_IFMQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDepth-adaptive%2BTransformer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OOW12E_IFMQJ&ei=oNhXYr7jPJaM6rQPlISayA8&json=", "num_citations": 55, "citedby_url": "/scholar?cites=14129138176231859512&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OOW12E_IFMQJ:scholar.google.com/&scioq=Depth-adaptive+Transformer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.10073"}, "Ridge Regression: Structure, Cross-validation, And Sketching": {"container_type": "Publication", "bib": {"title": "Ridge regression: Structure, cross-validation, and sketching", "author": ["S Liu", "E Dobriban"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.02373", "abstract": "We study the following three fundamental problems about ridge regression:(1) what is the structure of the estimator?(2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. We study the bias of $ K $-fold cross"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02373", "author_id": ["K-fIESYAAAAJ", "aGvH4yMAAAAJ"], "url_scholarbib": "/scholar?q=info:GmbPEzrQ4OsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRidge%2BRegression:%2BStructure,%2BCross-validation,%2BAnd%2BSketching%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GmbPEzrQ4OsJ&ei=pthXYo2fF8mUywTMkLbABQ&json=", "num_citations": 21, "citedby_url": "/scholar?cites=16996813941555291674&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GmbPEzrQ4OsJ:scholar.google.com/&scioq=Ridge+Regression:+Structure,+Cross-validation,+And+Sketching&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02373"}, "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers": {"container_type": "Publication", "bib": {"title": "Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers", "author": ["J Liu", "Z Xu", "R Shi", "RCC Cheung", "HKH So"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly find the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds. These thresholds can have fine-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same number of training epochs as dense models. Dynamic"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.06870", "author_id": ["PpUkzhgAAAAJ", "", "wJ5ikqAAAAAJ", "crJhZNMAAAAJ", "bdwy3iIAAAAJ"], "url_scholarbib": "/scholar?q=info:tJbqOwkoiyEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BSparse%2BTraining:%2BFind%2BEfficient%2BSparse%2BNetwork%2BFrom%2BScratch%2BWith%2BTrainable%2BMasked%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tJbqOwkoiyEJ&ei=qNhXYqLcOM2Ny9YPqPyUgAs&json=", "num_citations": 35, "citedby_url": "/scholar?cites=2417069645139449524&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tJbqOwkoiyEJ:scholar.google.com/&scioq=Dynamic+Sparse+Training:+Find+Efficient+Sparse+Network+From+Scratch+With+Trainable+Masked+Layers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.06870"}, "Caql: Continuous Action Q-learning": {"container_type": "Publication", "bib": {"title": "CAQL: Continuous action Q-learning", "author": ["M Ryu", "Y Chow", "R Anderson", "C Tjandraatmadja"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": ", we propose Continuous Action Q-learning (CAQL), a Q-learning framework for continuous  actions in  First, we develop the CAQL framework, which minimizes the Bellman residual in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12397", "author_id": ["EEBuCJ8AAAAJ", "BFlpS-8AAAAJ", "8wAjOQkAAAAJ", "fMOQBJUAAAAJ"], "url_scholarbib": "/scholar?q=info:ezL8aGdqmZEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCaql:%2BContinuous%2BAction%2BQ-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ezL8aGdqmZEJ&ei=q9hXYpObM4ySyATlkbrQCA&json=", "num_citations": 25, "citedby_url": "/scholar?cites=10491533799311815291&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ezL8aGdqmZEJ:scholar.google.com/&scioq=Caql:+Continuous+Action+Q-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12397"}, "V-mpo: On-policy Maximum A Posteriori Policy Optimization For Discrete And Continuous Control": {"container_type": "Publication", "bib": {"title": "V-mpo: On-policy maximum a posteriori policy optimization for discrete and continuous control", "author": ["HF Song", "A Abdolmaleki", "JT Springenberg"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Some of the most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algorithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12238", "author_id": ["oVF9D6EAAAAJ", "cCYTVWQAAAAJ", "MGXJkIAAAAAJ"], "url_scholarbib": "/scholar?q=info:gFRAGUv2xgQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DV-mpo:%2BOn-policy%2BMaximum%2BA%2BPosteriori%2BPolicy%2BOptimization%2BFor%2BDiscrete%2BAnd%2BContinuous%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gFRAGUv2xgQJ&ei=rthXYuTBMJWMy9YPt8OamA0&json=", "num_citations": 51, "citedby_url": "/scholar?cites=344233223947048064&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gFRAGUv2xgQJ:scholar.google.com/&scioq=V-mpo:+On-policy+Maximum+A+Posteriori+Policy+Optimization+For+Discrete+And+Continuous+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12238.pdf?ref=https://githubhelp.com"}, "Variational Template Machine For Data-to-text Generation": {"container_type": "Publication", "bib": {"title": "Variational template machine for data-to-text generation", "author": ["R Ye", "W Shi", "H Zhou", "Z Wei", "L Li"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.01127", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations. Learning such templates is prohibitive since it often requires a large paired< table, description> corpus, which is seldom available. This paper explores the problem of automatically learning reusable\" templates\" from paired and non-paired data. We propose"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.01127", "author_id": ["UV4u5UQAAAAJ", "Q4R4KhgAAAAJ", "", "AjLDxxgAAAAJ", "1B0l7U8AAAAJ"], "url_scholarbib": "/scholar?q=info:1aJ1zw5FC2cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BTemplate%2BMachine%2BFor%2BData-to-text%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1aJ1zw5FC2cJ&ei=sdhXYrzKH5LeyQTms5KQBg&json=", "num_citations": 24, "citedby_url": "/scholar?cites=7425104340562846421&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1aJ1zw5FC2cJ:scholar.google.com/&scioq=Variational+Template+Machine+For+Data-to-text+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.01127"}, "Computation Reallocation For Object Detection": {"container_type": "Publication", "bib": {"title": "Computation reallocation for object detection", "author": ["F Liang", "C Lin", "R Guo", "M Sun", "W Wu", "J Yan"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.11234", "author_id": ["ecTFCUMAAAAJ", "rObgGWIAAAAJ", "DEf4GkcAAAAJ", "1b1jbCQAAAAJ", "9RBxtd8AAAAJ", "rEYarG0AAAAJ"], "url_scholarbib": "/scholar?q=info:41goIdVHSP8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComputation%2BReallocation%2BFor%2BObject%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=41goIdVHSP8J&ei=vNhXYsW0I46pywSdh6agAg&json=", "num_citations": 33, "citedby_url": "/scholar?cites=18395031658704689379&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:41goIdVHSP8J:scholar.google.com/&scioq=Computation+Reallocation+For+Object+Detection&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.11234"}, "Hyper-sagnn: A Self-attention Based Graph Neural Network For Hypergraphs": {"container_type": "Publication", "bib": {"title": "Hyper-SAGNN: a self-attention based graph neural network for hypergraphs", "author": ["R Zhang", "Y Zou", "J Ma"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.02613", "abstract": "Graph representation learning for hypergraphs can be used to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.02613", "author_id": ["TXir_iowD-oC", "K-eOE88AAAAJ", "nDw9v78AAAAJ"], "url_scholarbib": "/scholar?q=info:2xM3nJxW-5QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHyper-sagnn:%2BA%2BSelf-attention%2BBased%2BGraph%2BNeural%2BNetwork%2BFor%2BHypergraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2xM3nJxW-5QJ&ei=v9hXYpTFJ5WMy9YPt8OamA0&json=", "num_citations": 60, "citedby_url": "/scholar?cites=10735269367403451355&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2xM3nJxW-5QJ:scholar.google.com/&scioq=Hyper-sagnn:+A+Self-attention+Based+Graph+Neural+Network+For+Hypergraphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.02613"}, "Mixed Precision Dnns: All You Need Is A Good Parametrization": {"container_type": "Publication", "bib": {"title": "Mixed precision dnns: All you need is a good parametrization", "author": ["S Uhlich", "L Mauch", "F Cardinaux", "K Yoshiyama"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Efficient deep neural network (DNN) inference on mobile or embedded devices typically involves quantization of the network parameters and activations. In particular, mixed precision networks achieve better performance than networks with homogeneous bitwidth for the same size constraint. Since choosing the optimal bitwidths is not straight forward, training methods, which can learn them, are desirable. Differentiable quantization with straight-through gradients allows to learn the quantizer's parameters using gradient"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11452", "author_id": ["hja8ejYAAAAJ", "", "UFl8n4gAAAAJ", ""], "url_scholarbib": "/scholar?q=info:SchgLhjy2EIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMixed%2BPrecision%2BDnns:%2BAll%2BYou%2BNeed%2BIs%2BA%2BGood%2BParametrization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SchgLhjy2EIJ&ei=w9hXYpedJMS4ywTtzb_QDA&json=", "num_citations": 65, "citedby_url": "/scholar?cites=4816865987143977033&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SchgLhjy2EIJ:scholar.google.com/&scioq=Mixed+Precision+Dnns:+All+You+Need+Is+A+Good+Parametrization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11452.pdf?ref=https://githubhelp.com"}, "Provable Benefit Of Orthogonal Initialization In Optimizing Deep Linear Networks": {"container_type": "Publication", "bib": {"title": "Provable benefit of orthogonal initialization in optimizing deep linear networks", "author": ["W Hu", "L Xiao", "J Pennington"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.05992", "abstract": "The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.05992", "author_id": ["ZybgAqkAAAAJ", "fvwzUnIAAAAJ", "cn_FoswAAAAJ"], "url_scholarbib": "/scholar?q=info:5fn7diMCvxEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProvable%2BBenefit%2BOf%2BOrthogonal%2BInitialization%2BIn%2BOptimizing%2BDeep%2BLinear%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5fn7diMCvxEJ&ei=ydhXYqmYGeHDywTjooCQBQ&json=", "num_citations": 49, "citedby_url": "/scholar?cites=1278743170539846117&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5fn7diMCvxEJ:scholar.google.com/&scioq=Provable+Benefit+Of+Orthogonal+Initialization+In+Optimizing+Deep+Linear+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.05992"}, "Thinking While Moving: Deep Reinforcement Learning With Concurrent Control": {"container_type": "Publication", "bib": {"title": "Thinking while moving: Deep reinforcement learning with concurrent control", "author": ["T Xiao", "E Jang", "D Kalashnikov", "S Levine", "J Ibarz"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.06089", "author_id": ["LIJQ_ZYAAAAJ", "", "2DBmo-wAAAAJ", "8R35rCwAAAAJ", "l-la0GQAAAAJ"], "url_scholarbib": "/scholar?q=info:Ak-cZ5b-_WEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThinking%2BWhile%2BMoving:%2BDeep%2BReinforcement%2BLearning%2BWith%2BConcurrent%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ak-cZ5b-_WEJ&ei=zNhXYs7gMpGJmwGIxre4DA&json=", "num_citations": 15, "citedby_url": "/scholar?cites=7061079712723652354&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ak-cZ5b-_WEJ:scholar.google.com/&scioq=Thinking+While+Moving:+Deep+Reinforcement+Learning+With+Concurrent+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.06089"}, "Emergent Tool Use From Multi-agent Autocurricula": {"container_type": "Publication", "bib": {"title": "Emergent tool use from multi-agent autocurricula", "author": ["B Baker", "I Kanitscheider", "T Markov", "Y Wu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.07528", "author_id": ["bMfPYdYAAAAJ", "tf-wLXoAAAAJ", "", "dusV5HMAAAAJ"], "url_scholarbib": "/scholar?q=info:aDiKIMjt8gUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BTool%2BUse%2BFrom%2BMulti-agent%2BAutocurricula%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aDiKIMjt8gUJ&ei=0dhXYsXVKMLZmQHc1ovQAg&json=", "num_citations": 357, "citedby_url": "/scholar?cites=428666358348789864&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aDiKIMjt8gUJ:scholar.google.com/&scioq=Emergent+Tool+Use+From+Multi-agent+Autocurricula&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.07528"}, "I Am Going Mad: Maximum Discrepancy Competition For Comparing Classifiers Adaptively": {"container_type": "Publication", "bib": {"title": "I am going mad: Maximum discrepancy competition for comparing classifiers adaptively", "author": ["H Wang", "T Chen", "Z Wang", "K Ma"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.10648", "abstract": "The learning of hierarchical representations for image classification has experienced an impressive series of successes due in part to the availability of large-scale labeled data for training. On the other hand, the trained classifiers have traditionally been evaluated on small and fixed sets of test images, which are deemed to be extremely sparsely distributed in the space of all natural images. It is thus questionable whether recent performance improvements on the excessively re-used test sets generalize to real-world natural images"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10648", "author_id": ["aMIJhlEAAAAJ", "LE3ctn0AAAAJ", "pxFyKAIAAAAJ", "sfzOyFoAAAAJ"], "url_scholarbib": "/scholar?q=info:SYIU1XmJCm0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DI%2BAm%2BGoing%2BMad:%2BMaximum%2BDiscrepancy%2BCompetition%2BFor%2BComparing%2BClassifiers%2BAdaptively%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SYIU1XmJCm0J&ei=3thXYpSSD5mM6rQPjaOSEA&json=", "num_citations": 14, "citedby_url": "/scholar?cites=7857243656260190793&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SYIU1XmJCm0J:scholar.google.com/&scioq=I+Am+Going+Mad:+Maximum+Discrepancy+Competition+For+Comparing+Classifiers+Adaptively&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10648"}, "Ensemble Distribution Distillation": {"container_type": "Publication", "bib": {"title": "Ensemble distribution distillation", "author": ["A Malinin", "B Mlodozeniec", "M Gales"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.00076", "abstract": "In addition, we also consider Ensemble Distillation and Ensemble Distribution Distillation  on a transfer set that contains both the original C10/C100/TIM training data and auxiliary (AUX)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.00076", "author_id": ["PnSFqO0AAAAJ", "kGPBRy8AAAAJ", "RSFlmjIAAAAJ"], "url_scholarbib": "/scholar?q=info:96TsEps4BxkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnsemble%2BDistribution%2BDistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=96TsEps4BxkJ&ei=4thXYp3TGpWMy9YPt8OamA0&json=", "num_citations": 95, "citedby_url": "/scholar?cites=1803472414473757943&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:96TsEps4BxkJ:scholar.google.com/&scioq=Ensemble+Distribution+Distillation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.00076"}, "Drawing Early-bird Tickets: Toward More Efficient Training Of Deep Networks": {"container_type": "Publication", "bib": {"title": "Drawing early-bird tickets: Towards more efficient training of deep networks", "author": ["H You", "C Li", "P Xu", "Y Fu", "Y Wang", "X Chen"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "(Frankle & Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11957", "author_id": ["z5Eku1sAAAAJ", "HvEBdf4AAAAJ", "YFxiIA4AAAAJ", "dp3C7GgAAAAJ", "y0vQSXEAAAAJ", "fEehMTwAAAAJ"], "url_scholarbib": "/scholar?q=info:RrN0SOlckFgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDrawing%2BEarly-bird%2BTickets:%2BToward%2BMore%2BEfficient%2BTraining%2BOf%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RrN0SOlckFgJ&ei=5thXYvb4JM2Ny9YPqPyUgAs&json=", "num_citations": 109, "citedby_url": "/scholar?cites=6381702828996735814&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RrN0SOlckFgJ:scholar.google.com/&scioq=Drawing+Early-bird+Tickets:+Toward+More+Efficient+Training+Of+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11957"}, "Harnessing Structures For Value-based Planning And Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Harnessing structures for value-based planning and reinforcement learning", "author": ["Y Yang", "G Zhang", "Z Xu", "D Katabi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12255", "abstract": "Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, ie, Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low-rank structure, which widely exists for big data matrices. We verify"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12255", "author_id": ["0_bSbIoAAAAJ", "jfWZH3cAAAAJ", "CbGdL4cAAAAJ", "nst5fHgAAAAJ"], "url_scholarbib": "/scholar?q=info:76BqiCNWAUIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHarnessing%2BStructures%2BFor%2BValue-based%2BPlanning%2BAnd%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=76BqiCNWAUIJ&ei=6dhXYvHaI4vMsQK69Y7ABg&json=", "num_citations": 17, "citedby_url": "/scholar?cites=4756177392092487919&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:76BqiCNWAUIJ:scholar.google.com/&scioq=Harnessing+Structures+For+Value-based+Planning+And+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12255"}, "Double Neural Counterfactual Regret Minimization": {"container_type": "Publication", "bib": {"title": "Double neural counterfactual regret minimization", "author": ["H Li", "K Hu", "Z Ge", "T Jiang", "Y Qi", "L Song"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.10607", "abstract": "Counterfactual Regret Minimization (CRF) is a fundamental and effective technique for solving Imperfect Information Games (IIG). However, the original CRF algorithm only works for discrete state and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games and continuing to improve from a poor strategy profile. In this paper, we propose a double neural representation for the imperfect information games, where one"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.10607", "author_id": ["L-mvmpAAAAAJ", "", "", "", "", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:niXeW0p224IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDouble%2BNeural%2BCounterfactual%2BRegret%2BMinimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=niXeW0p224IJ&ei=7NhXYr7sI-HDywTjooCQBQ&json=", "num_citations": 36, "citedby_url": "/scholar?cites=9429260306571208094&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:niXeW0p224IJ:scholar.google.com/&scioq=Double+Neural+Counterfactual+Regret+Minimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.10607"}, "Understanding And Robustifying Differentiable Architecture Search": {"container_type": "Publication", "bib": {"title": "Understanding and robustifying differentiable architecture search", "author": ["A Zela", "T Elsken", "T Saikia", "Y Marrakchi", "T Brox"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.09656", "author_id": ["hD_6YioAAAAJ", "tzDC8FQAAAAJ", "HHv75fUAAAAJ", "-zCH-nwAAAAJ", "0VAe-TQAAAAJ"], "url_scholarbib": "/scholar?q=info:wS23pZ4fU-YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BAnd%2BRobustifying%2BDifferentiable%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wS23pZ4fU-YJ&ei=79hXYpOJIpLeyQTms5KQBg&json=", "num_citations": 204, "citedby_url": "/scholar?cites=16596643818035948993&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wS23pZ4fU-YJ:scholar.google.com/&scioq=Understanding+And+Robustifying+Differentiable+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.09656"}, "Detecting And Diagnosing Adversarial Images With Class-conditional Capsule Reconstructions": {"container_type": "Publication", "bib": {"title": "Detecting and diagnosing adversarial images with class-conditional capsule reconstructions", "author": ["Y Qin", "N Frosst", "S Sabour", "C Raffel", "G Cottrell"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.02957", "author_id": ["z2UlHHUAAAAJ", "1yVnaTgAAAAJ", "l8wQ39EAAAAJ", "I66ZBYwAAAAJ", "5Aut7EEAAAAJ"], "url_scholarbib": "/scholar?q=info:5hpROf_e7ssJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDetecting%2BAnd%2BDiagnosing%2BAdversarial%2BImages%2BWith%2BClass-conditional%2BCapsule%2BReconstructions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5hpROf_e7ssJ&ei=-thXYpT6PI6pywSdh6agAg&json=", "num_citations": 49, "citedby_url": "/scholar?cites=14694927821916150502&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5hpROf_e7ssJ:scholar.google.com/&scioq=Detecting+And+Diagnosing+Adversarial+Images+With+Class-conditional+Capsule+Reconstructions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.02957"}, "Don't Use Large Mini-batches, Use Local Sgd": {"container_type": "Publication", "bib": {"title": "Don't use large mini-batches, use local SGD", "author": ["T Lin", "SU Stich", "KK Patel", "M Jaggi"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1808.07217", "abstract": "Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. Drastic increases in the mini-batch sizes have lead to key efficiency and scalability gains in recent years. However, progress faces a major roadblock, as models trained with large batches often do not generalize well, ie they do not show good accuracy on new data. As a remedy, we propose a\\emph {post-local} SGD and show that it significantly improves the generalization performance compared to large-batch training on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1808.07217", "author_id": ["QE9pa_cAAAAJ", "8l-mDfQAAAAJ", "Okd0qN0AAAAJ", "r1TJBr8AAAAJ"], "url_scholarbib": "/scholar?q=info:jWhlpZzxRS8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDon%2527t%2BUse%2BLarge%2BMini-batches,%2BUse%2BLocal%2BSgd%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jWhlpZzxRS8J&ei=_thXYsyQGZLeyQTms5KQBg&json=", "num_citations": 253, "citedby_url": "/scholar?cites=3406394348267726989&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jWhlpZzxRS8J:scholar.google.com/&scioq=Don%27t+Use+Large+Mini-batches,+Use+Local+Sgd&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1808.07217"}, "Unbiased Contrastive Divergence Algorithm For Training Energy-based Latent Variable Models": {"container_type": "Publication", "bib": {"title": "Unbiased contrastive divergence algorithm for training energy-based latent variable models", "author": ["Y Qiu", "L Zhang", "X Wang"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "The contrastive divergence algorithm is a popular approach to training energy-based latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article we propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1eyceSYPr", "author_id": ["", "_VR254AAAAAJ", "Ti-2_wUAAAAJ"], "url_scholarbib": "/scholar?q=info:wjIMynMSxiAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnbiased%2BContrastive%2BDivergence%2BAlgorithm%2BFor%2BTraining%2BEnergy-based%2BLatent%2BVariable%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wjIMynMSxiAJ&ei=AtlXYobULMWemAHB5baIBQ&json=", "num_citations": 10, "citedby_url": "/scholar?cites=2361595343122739906&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wjIMynMSxiAJ:scholar.google.com/&scioq=Unbiased+Contrastive+Divergence+Algorithm+For+Training+Energy-based+Latent+Variable+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1eyceSYPr"}, "Pairnorm: Tackling Oversmoothing In Gnns": {"container_type": "Publication", "bib": {"title": "Pairnorm: Tackling oversmoothing in gnns", "author": ["L Zhao", "L Akoglu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12223", "abstract": "The performance of graph neural nets (GNNs) is known to gradually decrease with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. We take a closer look at two different interpretations, aiming to quantify oversmoothing. Our main contribution is PairNorm, a novel normalization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12223", "author_id": ["QKslW6EAAAAJ", "4ITkr_kAAAAJ"], "url_scholarbib": "/scholar?q=info:d23cJ0LZYwMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPairnorm:%2BTackling%2BOversmoothing%2BIn%2BGnns%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d23cJ0LZYwMJ&ei=BtlXYtTeAYvEmgH7846QCg&json=", "num_citations": 187, "citedby_url": "/scholar?cites=244277682967965047&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:d23cJ0LZYwMJ:scholar.google.com/&scioq=Pairnorm:+Tackling+Oversmoothing+In+Gnns&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12223"}, "Restricting The Flow: Information Bottlenecks For Attribution": {"container_type": "Publication", "bib": {"title": "Restricting the flow: Information bottlenecks for attribution", "author": ["K Schulz", "L Sixt", "F Tombari", "T Landgraf"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.00396", "abstract": "Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00396", "author_id": ["", "XtejLN8AAAAJ", "TFsE4BIAAAAJ", "ChX0opIAAAAJ"], "url_scholarbib": "/scholar?q=info:NpeRblmMeLsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRestricting%2BThe%2BFlow:%2BInformation%2BBottlenecks%2BFor%2BAttribution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NpeRblmMeLsJ&ei=CdlXYrTXF5WMy9YPt8OamA0&json=", "num_citations": 74, "citedby_url": "/scholar?cites=13508701398032815926&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NpeRblmMeLsJ:scholar.google.com/&scioq=Restricting+The+Flow:+Information+Bottlenecks+For+Attribution&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00396"}, "Quantum Algorithms For Deep Convolutional Neural Networks": {"container_type": "Publication", "bib": {"title": "Quantum algorithms for deep convolutional neural networks", "author": ["I Kerenidis", "J Landman", "A Prakash"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.01117", "abstract": "Quantum computing is a new computational paradigm that promises applications in several fields, including machine learning. In the last decade, deep learning, and in particular Convolutional neural networks (CNN), have become essential for applications in signal processing and image recognition. Quantum deep learning, however remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries. In this paper we propose a quantum algorithm for applying and training deep convolutional neural networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.01117", "author_id": ["poOHhXMAAAAJ", "itqTkSwAAAAJ", "UBlMT-AAAAAJ"], "url_scholarbib": "/scholar?q=info:qcQcMhtcL18J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuantum%2BAlgorithms%2BFor%2BDeep%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qcQcMhtcL18J&ei=D9lXYtz6DMmUywTMkLbABQ&json=", "num_citations": 66, "citedby_url": "/scholar?cites=6858802029383173289&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qcQcMhtcL18J:scholar.google.com/&scioq=Quantum+Algorithms+For+Deep+Convolutional+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.01117"}, "Explain Your Move: Understanding Agent Actions Using Focused Feature Saliency": {"container_type": "Publication", "bib": {"title": "Explain your move: Understanding agent actions using focused feature saliency", "author": ["P Gupta", "N Puri", "S Verma", "S Singh"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "As deep reinforcement learning (RL) is applied to more tasks, there is a need to visualize and understand the behavior of learned agents. Saliency maps explain agent behavior by highlighting the features of the input state that are most relevant for the agent in taking an action. Existing perturbation-based approaches to compute saliency often highlight regions of the input that are not relevant to the action taken by the agent. Our approach generates more focused saliency maps by balancing two aspects (specificity and relevance) that"}, "filled": false, "gsrank": 1, "pub_url": "https://www.researchgate.net/profile/Shripad-Deshmukh/publication/338065601_EXPLAIN_YOUR_MOVE_UNDERSTANDING_AGENT_ACTIONS_USING_FOCUSED_FEATURE_SALIENCY/links/5e0c4947a6fdcc28374d459d/EXPLAIN-YOUR-MOVE-UNDERSTANDING-AGENT-ACTIONS-USING-FOCUSED-FEATURE-SALIENCY.pdf", "author_id": ["mAOlb8IAAAAJ", "", "xUrD7DgAAAAJ", "-hGZC54AAAAJ"], "url_scholarbib": "/scholar?q=info:mMDvwRMMbL0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExplain%2BYour%2BMove:%2BUnderstanding%2BAgent%2BActions%2BUsing%2BFocused%2BFeature%2BSaliency%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mMDvwRMMbL0J&ei=FdlXYv-ZFOHDywTjooCQBQ&json=", "num_citations": 17, "citedby_url": "/scholar?cites=13649297849650757784&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mMDvwRMMbL0J:scholar.google.com/&scioq=Explain+Your+Move:+Understanding+Agent+Actions+Using+Focused+Feature+Saliency&hl=en&as_sdt=0,33", "eprint_url": "https://www.researchgate.net/profile/Shripad-Deshmukh/publication/338065601_EXPLAIN_YOUR_MOVE_UNDERSTANDING_AGENT_ACTIONS_USING_FOCUSED_FEATURE_SALIENCY/links/5e0c4947a6fdcc28374d459d/EXPLAIN-YOUR-MOVE-UNDERSTANDING-AGENT-ACTIONS-USING-FOCUSED-FEATURE-SALIENCY.pdf"}, "Implementation Matters In Deep Rl: A Case Study On Ppo And Trpo": {"container_type": "Publication", "bib": {"title": "Implementation matters in deep rl: A case study on ppo and trpo", "author": ["L Engstrom", "A Ilyas", "S Santurkar", "D Tsipras"], "pub_year": "2019", "venue": "International \u2026", "abstract": "We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of\" code-level optimizations:\" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1etN1rtPB", "author_id": ["IWPWNxkAAAAJ", "Dtw3YBoAAAAJ", "QMkbFp8AAAAJ", "26eh1jAAAAAJ"], "url_scholarbib": "/scholar?q=info:ZUuCZkZRnhIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImplementation%2BMatters%2BIn%2BDeep%2BRl:%2BA%2BCase%2BStudy%2BOn%2BPpo%2BAnd%2BTrpo%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZUuCZkZRnhIJ&ei=GdlXYo3CCIvEmgH7846QCg&json=", "num_citations": 106, "citedby_url": "/scholar?cites=1341599101812362085&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZUuCZkZRnhIJ:scholar.google.com/&scioq=Implementation+Matters+In+Deep+Rl:+A+Case+Study+On+Ppo+And+Trpo&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1etN1rtPB"}, "Keep Doing What Worked: Behavior Modelling Priors For Offline Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Keep doing what worked: Behavioral modelling priors for offline reinforcement learning", "author": ["NY Siegel", "JT Springenberg", "F Berkenkamp"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior--the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.08396", "author_id": ["l2E0LR4AAAAJ", "MGXJkIAAAAAJ", "N_tCEl8AAAAJ"], "url_scholarbib": "/scholar?q=info:6JmcrErfj9IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKeep%2BDoing%2BWhat%2BWorked:%2BBehavior%2BModelling%2BPriors%2BFor%2BOffline%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6JmcrErfj9IJ&ei=HtlXYrqrC4ySyATlkbrQCA&json=", "num_citations": 121, "citedby_url": "/scholar?cites=15172591181451008488&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6JmcrErfj9IJ:scholar.google.com/&scioq=Keep+Doing+What+Worked:+Behavior+Modelling+Priors+For+Offline+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.08396"}, "Order Learning And Its Application To Age Estimation": {"container_type": "Publication", "bib": {"title": "Order learning and its application to age estimation", "author": ["K Lim", "NH Shin", "YY Lee", "CS Kim"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "We propose order learning to determine the order graph of classes, representing ranks or priorities, and classify an object instance into one of the classes. To this end, we design a pairwise comparator to categorize the relationship between two instances into one of three cases: one instance isgreater than,'similar to,'orsmaller than'the other. Then, by comparing an input instance with reference instances and maximizing the consistency among the comparison results, the class of the input can be estimated reliably. We apply order learning"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HygsuaNFwr", "author_id": ["10pEgPkAAAAJ", "dLCMcXMAAAAJ", "LdA0DacAAAAJ", "KOdKwNsAAAAJ"], "url_scholarbib": "/scholar?q=info:ioi60-jNU94J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOrder%2BLearning%2BAnd%2BIts%2BApplication%2BTo%2BAge%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ioi60-jNU94J&ei=ItlXYojTDYvMsQK69Y7ABg&json=", "num_citations": 12, "citedby_url": "/scholar?cites=16020374699355310218&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ioi60-jNU94J:scholar.google.com/&scioq=Order+Learning+And+Its+Application+To+Age+Estimation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HygsuaNFwr"}, "Disentangling Neural Mechanisms For Perceptual Grouping": {"container_type": "Publication", "bib": {"title": "Disentangling neural mechanisms for perceptual grouping", "author": ["J Kim", "D Linsley", "K Thakkar", "T Serre"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.01558", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.01558", "author_id": ["JBGADIwAAAAJ", "cXZlAuQAAAAJ", "CoI7UtsAAAAJ", "kZlPW4wAAAAJ"], "url_scholarbib": "/scholar?q=info:9NYZGp57MEYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDisentangling%2BNeural%2BMechanisms%2BFor%2BPerceptual%2BGrouping%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9NYZGp57MEYJ&ei=JdlXYszVOpGJmwGIxre4DA&json=", "num_citations": 17, "citedby_url": "/scholar?cites=5057678300510017268&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9NYZGp57MEYJ:scholar.google.com/&scioq=Disentangling+Neural+Mechanisms+For+Perceptual+Grouping&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.01558"}, "Identifying Through Flows For Recovering Latent Representations": {"container_type": "Publication", "bib": {"title": "Identifying through flows for recovering latent representations", "author": ["S Li", "B Hooi", "GH Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12555", "abstract": "Identifiability, or recovery of the true latent representations from which the observed data originates, is de facto a fundamental goal of representation learning. Yet, most deep generative models do not address the question of identifiability, and thus fail to deliver on the promise of the recovery of the true latent sources that generate the observations. Recent work proposed identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. Due to the intractablity of KL divergence between variational"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12555", "author_id": ["", "ErEL3bgAAAAJ", "7hNKrPsAAAAJ"], "url_scholarbib": "/scholar?q=info:Ms4QMdgJ_bMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIdentifying%2BThrough%2BFlows%2BFor%2BRecovering%2BLatent%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ms4QMdgJ_bMJ&ei=KtlXYqXuCM6E6rQPz8uiuAc&json=", "num_citations": 5, "citedby_url": "/scholar?cites=12969533326037667378&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ms4QMdgJ_bMJ:scholar.google.com/&scioq=Identifying+Through+Flows+For+Recovering+Latent+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12555"}, "Imitation Learning Via Off-policy Distribution Matching": {"container_type": "Publication", "bib": {"title": "Imitation learning via off-policy distribution matching", "author": ["I Kostrikov", "O Nachum", "J Tompson"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.05032", "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data-inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.05032", "author_id": ["PTS2AOgAAAAJ", "C-ZlBWMAAAAJ", "U_Jw8DUAAAAJ"], "url_scholarbib": "/scholar?q=info:ZH794KXUJO8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImitation%2BLearning%2BVia%2BOff-policy%2BDistribution%2BMatching%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZH794KXUJO8J&ei=LdlXYoyNF42EmgH6u5u4BA&json=", "num_citations": 61, "citedby_url": "/scholar?cites=17232131883135762020&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZH794KXUJO8J:scholar.google.com/&scioq=Imitation+Learning+Via+Off-policy+Distribution+Matching&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.05032"}, "Inductive Matrix Completion Based On Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Inductive matrix completion based on graph neural networks", "author": ["M Zhang", "Y Chen"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.12058", "abstract": "We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.12058", "author_id": ["OBBqkosAAAAJ", "NByrsK0AAAAJ"], "url_scholarbib": "/scholar?q=info:UDfJ72VTieQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInductive%2BMatrix%2BCompletion%2BBased%2BOn%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UDfJ72VTieQJ&ei=MdlXYpnmBZGJmwGIxre4DA&json=", "num_citations": 86, "citedby_url": "/scholar?cites=16467785209736673104&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UDfJ72VTieQJ:scholar.google.com/&scioq=Inductive+Matrix+Completion+Based+On+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.12058"}, "Critical Initialisation In Continuous Approximations Of Binary Neural Networks": {"container_type": "Publication", "bib": {"title": "Critical initialisation in continuous approximations of binary neural networks", "author": ["G Stamatescu", "F Gerace", "C Lucibello", "I Fuss"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The training of stochastic neural network models with binary ($\\pm1 $) weights and activations via continuous surrogate networks is investigated. We derive new surrogates using a novel derivation based on writing the stochastic neural network as a Markov chain. This derivation also encompasses existing variants of the surrogates presented in the literature. Following this, we theoretically study the surrogates at initialisation. We derive, using mean field theory, a set of scalar equations describing how input signals propagate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.00177", "author_id": ["2hwiLEQAAAAJ", "dvDLaPkAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:-zo_G04eqqcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCritical%2BInitialisation%2BIn%2BContinuous%2BApproximations%2BOf%2BBinary%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-zo_G04eqqcJ&ei=NNlXYvGuLovMsQK69Y7ABg&json=", "num_citations": 1, "citedby_url": "/scholar?cites=12081502271188187899&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-zo_G04eqqcJ:scholar.google.com/&scioq=Critical+Initialisation+In+Continuous+Approximations+Of+Binary+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.00177"}, "Empirical Bayes Transductive Meta-learning With Synthetic Gradients": {"container_type": "Publication", "bib": {"title": "Empirical bayes transductive meta-learning with synthetic gradients", "author": ["SX Hu", "PG Moreno", "Y Xiao", "X Shen"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a meta-learning approach that learns from multiple tasks in a transductive setting, by leveraging the unlabeled query set in addition to the support set to generate a more powerful model for each task. To develop our framework, we revisit the empirical Bayes formulation for multi-task learning. The evidence lower bound of the marginal log-likelihood of empirical Bayes decomposes as a sum of local KL divergences between the variational posterior and the true posterior on the query set of each task. We derive a novel"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.12696", "author_id": ["jU7nGnEAAAAJ", "WnW5PmcAAAAJ", "HzIp5_YAAAAJ", "nKSXus4AAAAJ"], "url_scholarbib": "/scholar?q=info:Bn9m5m--9WEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmpirical%2BBayes%2BTransductive%2BMeta-learning%2BWith%2BSynthetic%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Bn9m5m--9WEJ&ei=O9lXYsPhFpWMy9YPt8OamA0&json=", "num_citations": 68, "citedby_url": "/scholar?cites=7058757378789244678&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Bn9m5m--9WEJ:scholar.google.com/&scioq=Empirical+Bayes+Transductive+Meta-learning+With+Synthetic+Gradients&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.12696"}, "Learning The Difference That Makes A Difference With Counterfactually-augmented Data": {"container_type": "Publication", "bib": {"title": "Learning the difference that makes a difference with counterfactually-augmented data", "author": ["D Kaushik", "E Hovy", "ZC Lipton"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12434", "abstract": "Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (eg, a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12434", "author_id": ["Sg3jtCgAAAAJ", "PUFxrroAAAAJ", "MN9Kfg8AAAAJ"], "url_scholarbib": "/scholar?q=info:AxBGkOTVF2kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BThe%2BDifference%2BThat%2BMakes%2BA%2BDifference%2BWith%2BCounterfactually-augmented%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AxBGkOTVF2kJ&ei=QdlXYqufD5LeyQTms5KQBg&json=", "num_citations": 228, "citedby_url": "/scholar?cites=7572756476096548867&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AxBGkOTVF2kJ:scholar.google.com/&scioq=Learning+The+Difference+That+Makes+A+Difference+With+Counterfactually-augmented+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12434"}, "Posterior Sampling For Multi-agent Reinforcement Learning: Solving Extensive Games With Imperfect Information": {"container_type": "Publication", "bib": {"title": "Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information", "author": ["Y Zhou", "J Li", "J Zhu"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "Posterior sampling for reinforcement learning (PSRL) is a useful framework for making decisions in an unknown environment. PSRL maintains a posterior distribution of the environment and then makes planning on the environment sampled from the posterior distribution. Though PSRL works well on single-agent reinforcement learning problems, how to apply PSRL to multi-agent reinforcement learning problems is relatively unexplored. In this work, we extend PSRL to two-player zero-sum extensive-games with imperfect"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Syg-ET4FPS", "author_id": ["", "", "axsP38wAAAAJ"], "url_scholarbib": "/scholar?q=info:hwb3E3wInP8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPosterior%2BSampling%2BFor%2BMulti-agent%2BReinforcement%2BLearning:%2BSolving%2BExtensive%2BGames%2BWith%2BImperfect%2BInformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hwb3E3wInP8J&ei=Q9lXYr-pNcLZmQHc1ovQAg&json=", "num_citations": 11, "citedby_url": "/scholar?cites=18418605905042409095&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hwb3E3wInP8J:scholar.google.com/&scioq=Posterior+Sampling+For+Multi-agent+Reinforcement+Learning:+Solving+Extensive+Games+With+Imperfect+Information&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Syg-ET4FPS"}, "Intriguing Properties Of Adversarial Training At Scale": {"container_type": "Publication", "bib": {"title": "Intriguing properties of adversarial training at scale", "author": ["C Xie", "A Yuille"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.03787", "abstract": "Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of adversarial training, which reveals two intriguing properties. First, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.03787", "author_id": ["X3vVZPcAAAAJ", "FJ-huxgAAAAJ"], "url_scholarbib": "/scholar?q=info:mboftkKcEe8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntriguing%2BProperties%2BOf%2BAdversarial%2BTraining%2BAt%2BScale%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mboftkKcEe8J&ei=RtlXYsS9L5GJmwGIxre4DA&json=", "num_citations": 104, "citedby_url": "/scholar?cites=17226721860006165145&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mboftkKcEe8J:scholar.google.com/&scioq=Intriguing+Properties+Of+Adversarial+Training+At+Scale&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.03787?ref=https://githubhelp.com"}, "Sadam: A Variant Of Adam For Strongly Convex Functions": {"container_type": "Publication", "bib": {"title": "Sadam: A variant of adam for strongly convex functions", "author": ["G Wang", "S Lu", "W Tu", "L Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.02957", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependant $ O (\\sqrt {T}) $ regret bound where $ T $ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependant $ O (\\log T) $ regret bound for strongly convex functions. The essential idea"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.02957", "author_id": ["oNgvRg4AAAAJ", "WFMoc84AAAAJ", "NrSit7IAAAAJ", "DC5FEjoAAAAJ"], "url_scholarbib": "/scholar?q=info:k-Wgikh75TgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSadam:%2BA%2BVariant%2BOf%2BAdam%2BFor%2BStrongly%2BConvex%2BFunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=k-Wgikh75TgJ&ei=TtlXYuHKGsWemAHB5baIBQ&json=", "num_citations": 23, "citedby_url": "/scholar?cites=4099818587284366739&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:k-Wgikh75TgJ:scholar.google.com/&scioq=Sadam:+A+Variant+Of+Adam+For+Strongly+Convex+Functions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.02957"}, "Efficient Transformer For Mobile Applications": {"container_type": "Publication", "bib": {"title": "Efficient transformer for mobile applicatoins", "author": ["Z Wu", "Z Liu", "J Lin", "Y Lin", "S Han"], "pub_year": "2020", "venue": "Proceedings of the International \u2026", "abstract": "\u2022 Our mobile transformer (MBT) with LSRA outperforms the basic transformer. \u2022 On IWSLT\u201914  De-En dataset with better trade-off for both Mult-Adds and the number of parameters.  \u2022  Our mobile transformer (MBT) also outperforms the basic transformer on both WMT\u201914 En-De  and WMT\u201914 En-Fr dataset on mobile settings. \u2022 Specialization is more effective with tighter  resource constraints.  The efficient natural language processing designed for mobile  settings is vital for the deployment of language related applications, such as machine"}, "filled": false, "gsrank": 1, "pub_url": "https://zhanghaowu.me/assets/pdf/Presentation_ENMT.pdf", "author_id": ["YfyMDFgAAAAJ", "3coYSTUAAAAJ", "dVtzVVAAAAAJ", "Drc8L5EAAAAJ", "E0iCaa4AAAAJ"], "url_scholarbib": "/scholar?q=info:CueA1PKgRzYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BTransformer%2BFor%2BMobile%2BApplications%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CueA1PKgRzYJ&ei=VNlXYovdDc2Ny9YPqPyUgAs&json=", "num_citations": 2, "citedby_url": "/scholar?cites=3911271766202312458&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CueA1PKgRzYJ:scholar.google.com/&scioq=Efficient+Transformer+For+Mobile+Applications&hl=en&as_sdt=0,33"}, "Prox-sgd: Training Structured Neural Networks Under Regularization And Constraints": {"container_type": "Publication", "bib": {"title": "Proxsgd: Training structured neural networks under regularization and constraints", "author": ["Y Yang", "Y Yuan", "A Chatzimichailidis"], "pub_year": "2019", "venue": "International \u2026", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HygpthEtvr", "author_id": ["QS1t3FwAAAAJ", "frmsx20AAAAJ", "6dUH7x8AAAAJ"], "url_scholarbib": "/scholar?q=info:G7WdVIpfvaoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProx-sgd:%2BTraining%2BStructured%2BNeural%2BNetworks%2BUnder%2BRegularization%2BAnd%2BConstraints%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G7WdVIpfvaoJ&ei=V9lXYsLWDZWMy9YPt8OamA0&json=", "num_citations": 10, "citedby_url": "/scholar?cites=12303094804775810331&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:G7WdVIpfvaoJ:scholar.google.com/&scioq=Prox-sgd:+Training+Structured+Neural+Networks+Under+Regularization+And+Constraints&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HygpthEtvr"}, "Bayesopt Adversarial Attack": {"container_type": "Publication", "bib": {"title": "Bayesopt adversarial attack", "author": ["B Ru", "A Cobb", "A Blaas", "Y Gal"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "robustness and their ability to detect such attacks. In this paper we focus on highly practical  adversarial attacks that fulfill the following two criteria. First, the attack is designed for a black-"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Hkem-lrtvH", "author_id": ["4piw-XMAAAAJ", "mO5ZTSEAAAAJ", "xMCznYUAAAAJ", "SIayDoQAAAAJ"], "url_scholarbib": "/scholar?q=info:9vH86x4oukkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesopt%2BAdversarial%2BAttack%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9vH86x4oukkJ&ei=WdlXYojdM5LeyQTms5KQBg&json=", "num_citations": 40, "citedby_url": "/scholar?cites=5312602823710274038&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9vH86x4oukkJ:scholar.google.com/&scioq=Bayesopt+Adversarial+Attack&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Hkem-lrtvH"}, "Word2ket: Space-efficient Word Embeddings Inspired By Quantum Entanglement": {"container_type": "Publication", "bib": {"title": "word2ket: Space-efficient word embeddings inspired by quantum entanglement", "author": ["A Panahi", "S Saeedi", "T Arodz"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.04975", "abstract": "Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.04975", "author_id": ["bKhC61wAAAAJ", "sYSm20IAAAAJ", "UXlsAbwAAAAJ"], "url_scholarbib": "/scholar?q=info:2Zthg0RaY0QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWord2ket:%2BSpace-efficient%2BWord%2BEmbeddings%2BInspired%2BBy%2BQuantum%2BEntanglement%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2Zthg0RaY0QJ&ei=ZdlXYt7IDciBy9YP18Gi8As&json=", "num_citations": 16, "citedby_url": "/scholar?cites=4927881667581942745&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2Zthg0RaY0QJ:scholar.google.com/&scioq=Word2ket:+Space-efficient+Word+Embeddings+Inspired+By+Quantum+Entanglement&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.04975"}, "Chameleon: Adaptive Code Optimization For Expedited Deep Neural Network Compilation": {"container_type": "Publication", "bib": {"title": "Chameleon: Adaptive code optimization for expedited deep neural network compilation", "author": ["BH Ahn", "P Pilligundla", "A Yazdanbakhsh"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.08743", "author_id": ["n1grp7EAAAAJ", "uIQ2vHQAAAAJ", "Vdu_sqwAAAAJ"], "url_scholarbib": "/scholar?q=info:aORzkPYZ0uAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DChameleon:%2BAdaptive%2BCode%2BOptimization%2BFor%2BExpedited%2BDeep%2BNeural%2BNetwork%2BCompilation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aORzkPYZ0uAJ&ei=atlXYtjmJovMsQK69Y7ABg&json=", "num_citations": 26, "citedby_url": "/scholar?cites=16200039356381258856&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aORzkPYZ0uAJ:scholar.google.com/&scioq=Chameleon:+Adaptive+Code+Optimization+For+Expedited+Deep+Neural+Network+Compilation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.08743"}, "Mixed-curvature Variational Autoencoders": {"container_type": "Publication", "bib": {"title": "Mixed-curvature variational autoencoders", "author": ["O Skopek", "OE Ganea", "G B\u00e9cigneul"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.08411", "abstract": "Consequently, generative models like Variational Autoencoders (VAEs) have been successfully   We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.08411", "author_id": ["CWSImZYAAAAJ", "0dYS0sMAAAAJ", "_tehCwgAAAAJ"], "url_scholarbib": "/scholar?q=info:7UqRmn_LhT8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMixed-curvature%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7UqRmn_LhT8J&ei=btlXYpryF4vMsQK69Y7ABg&json=", "num_citations": 13, "citedby_url": "/scholar?cites=4577288345206475501&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7UqRmn_LhT8J:scholar.google.com/&scioq=Mixed-curvature+Variational+Autoencoders&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.08411"}, "Symplectic Ode-net: Learning Hamiltonian Dynamics With Control": {"container_type": "Publication", "bib": {"title": "Symplectic ode-net: Learning hamiltonian dynamics with control", "author": ["YD Zhong", "B Dey", "A Chakraborty"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12077", "abstract": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12077", "author_id": ["Fv6IUjcAAAAJ", "jdLBoY8AAAAJ", "1wxreSIAAAAJ"], "url_scholarbib": "/scholar?q=info:VdlZWKvn_OAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSymplectic%2BOde-net:%2BLearning%2BHamiltonian%2BDynamics%2BWith%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VdlZWKvn_OAJ&ei=c9lXYtfbPIvEmgH7846QCg&json=", "num_citations": 118, "citedby_url": "/scholar?cites=16212087481734650197&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VdlZWKvn_OAJ:scholar.google.com/&scioq=Symplectic+Ode-net:+Learning+Hamiltonian+Dynamics+With+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12077"}, "Reducing Transformer Depth On Demand With Structured Dropout": {"container_type": "Publication", "bib": {"title": "Reducing transformer depth on demand with structured dropout", "author": ["A Fan", "E Grave", "A Joulin"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11556", "abstract": "Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11556", "author_id": ["TLZR9zgAAAAJ", "7UV4ET4AAAAJ", "kRJkDakAAAAJ"], "url_scholarbib": "/scholar?q=info:GI7tbA_nJ28J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReducing%2BTransformer%2BDepth%2BOn%2BDemand%2BWith%2BStructured%2BDropout%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GI7tbA_nJ28J&ei=etlXYqDNNpmM6rQPjaOSEA&json=", "num_citations": 234, "citedby_url": "/scholar?cites=8009624515739749912&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GI7tbA_nJ28J:scholar.google.com/&scioq=Reducing+Transformer+Depth+On+Demand+With+Structured+Dropout&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11556"}, "A Theoretical Analysis Of The Number Of Shots In Few-shot Learning": {"container_type": "Publication", "bib": {"title": "A theoretical analysis of the number of shots in few-shot learning", "author": ["T Cao", "M Law", "S Fidler"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11722", "abstract": "Few-shot classification is the task of predicting the category of an example from a set of few labeled examples. The number of labeled examples per category is called the number of shots (or shot number). Recent works tackle this task through meta-learning, where a meta-learner extracts information from observed tasks during meta-training to quickly adapt to new tasks during meta-testing. In this formulation, the number of shots exploited during meta-training has an impact on the recognition performance at meta-test time. Generally, the shot"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11722", "author_id": ["", "_7QgnUcAAAAJ", "CUlqK5EAAAAJ"], "url_scholarbib": "/scholar?q=info:RMRhTKavfHkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BTheoretical%2BAnalysis%2BOf%2BThe%2BNumber%2BOf%2BShots%2BIn%2BFew-shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RMRhTKavfHkJ&ei=f9lXYrGrEsWemAHB5baIBQ&json=", "num_citations": 35, "citedby_url": "/scholar?cites=8754064904482309188&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RMRhTKavfHkJ:scholar.google.com/&scioq=A+Theoretical+Analysis+Of+The+Number+Of+Shots+In+Few-shot+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11722"}, "Learning Representations For Binary-classification Without Backpropagation": {"container_type": "Publication", "bib": {"title": "Learning representations for binary-classification without backpropagation", "author": ["M Lechner"], "pub_year": "2019", "venue": "International Conference on Learning Representations", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains. While FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities. Here we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bke61krFvS", "author_id": ["fIupeSAAAAAJ"], "url_scholarbib": "/scholar?q=info:ymmRcsYKk-EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRepresentations%2BFor%2BBinary-classification%2BWithout%2BBackpropagation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ymmRcsYKk-EJ&ei=gtlXYrXaMMS4ywTtzb_QDA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=16254347327552186826&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ymmRcsYKk-EJ:scholar.google.com/&scioq=Learning+Representations+For+Binary-classification+Without+Backpropagation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bke61krFvS"}, "Deep Learning Of Determinantal Point Processes Via Proper Spectral Sub-gradient": {"container_type": "Publication", "bib": {"title": "Deep learning of determinantal point processes via proper spectral sub-gradient", "author": ["T Yu", "Y Li", "B Li"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "Determinantal point processes (DPPs) is an effective tool to deliver diversity on multiple machine learning and computer vision tasks. Under deep learning framework, DPP is typically optimized via approximation, which is not straightforward and has some conflict with diversity requirement. We note, however, there has been no deep learning paradigms to optimize DPP directly since it involves matrix inversion which may result in highly computational instability. This fact greatly hinders the wide use of DPP on some specific"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkeIq2VYPr", "author_id": ["MTHO7DsAAAAJ", "9UWlxzsAAAAJ", "8OOBH04AAAAJ"], "url_scholarbib": "/scholar?q=info:J2TyeqlC6NcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2BOf%2BDeterminantal%2BPoint%2BProcesses%2BVia%2BProper%2BSpectral%2BSub-gradient%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J2TyeqlC6NcJ&ei=jdlXYonYNsWemAHB5baIBQ&json=", "num_citations": 3, "citedby_url": "/scholar?cites=15557758208430990375&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:J2TyeqlC6NcJ:scholar.google.com/&scioq=Deep+Learning+Of+Determinantal+Point+Processes+Via+Proper+Spectral+Sub-gradient&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkeIq2VYPr"}, "On The Relationship Between Self-attention And Convolutional Layers": {"container_type": "Publication", "bib": {"title": "On the relationship between self-attention and convolutional layers", "author": ["JB Cordonnier", "A Loukas", "M Jaggi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.03584", "abstract": "Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al.(2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.03584", "author_id": ["3YUTuIUAAAAJ", "-XGXJbQAAAAJ", "r1TJBr8AAAAJ"], "url_scholarbib": "/scholar?q=info:PNb8EHNuOaYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BRelationship%2BBetween%2BSelf-attention%2BAnd%2BConvolutional%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PNb8EHNuOaYJ&ei=kNlXYoj8Lc6E6rQPz8uiuAc&json=", "num_citations": 199, "citedby_url": "/scholar?cites=11977726124453844540&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PNb8EHNuOaYJ:scholar.google.com/&scioq=On+The+Relationship+Between+Self-attention+And+Convolutional+Layers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.03584.pdf?ref=https://githubhelp.com"}, "A Closer Look At The Optimization Landscapes Of Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "A closer look at the optimization landscapes of generative adversarial networks", "author": ["H Berard", "G Gidel", "A Almahairi", "P Vincent"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative adversarial networks have been very successful in generative modeling, however they remain relatively challenging to train compared to standard deep neural networks. In this paper, we propose new visualization techniques for the optimization landscapes of GANs that enable us to study the game vector field resulting from the concatenation of the gradient of both players. Using these visualization techniques we try to bridge the gap between theory and practice by showing empirically that the training of GANs"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.04848", "author_id": ["P5d_140AAAAJ", "bDrXQPUAAAAJ", "WbYAa7IAAAAJ", "WBCKQMsAAAAJ"], "url_scholarbib": "/scholar?q=info:5SqLuiUns3gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCloser%2BLook%2BAt%2BThe%2BOptimization%2BLandscapes%2BOf%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5SqLuiUns3gJ&ei=k9lXYvGQJciBy9YP18Gi8As&json=", "num_citations": 42, "citedby_url": "/scholar?cites=8697338348379515621&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5SqLuiUns3gJ:scholar.google.com/&scioq=A+Closer+Look+At+The+Optimization+Landscapes+Of+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.04848"}, "Input Complexity And Out-of-distribution Detection With Likelihood-based Generative Models": {"container_type": "Publication", "bib": {"title": "Input complexity and out-of-distribution detection with likelihood-based generative models", "author": ["J Serr\u00e0", "D \u00c1lvarez", "V G\u00f3mez", "O Slizovskaia"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11480", "author_id": ["sZLj96sAAAAJ", "WK4V8VUAAAAJ", "TWuHi7EAAAAJ", "Kxh4-s8AAAAJ"], "url_scholarbib": "/scholar?q=info:S0VrEY6_gc8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInput%2BComplexity%2BAnd%2BOut-of-distribution%2BDetection%2BWith%2BLikelihood-based%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S0VrEY6_gc8J&ei=ltlXYtm2NsWemAHB5baIBQ&json=", "num_citations": 106, "citedby_url": "/scholar?cites=14952442854745261387&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:S0VrEY6_gc8J:scholar.google.com/&scioq=Input+Complexity+And+Out-of-distribution+Detection+With+Likelihood-based+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11480.pdf?ref=https://githubhelp.com"}, "Fsnet: Compression Of Deep Convolutional Neural Networks By Filter Summary": {"container_type": "Publication", "bib": {"title": "Fsnet: Compression of deep convolutional neural networks by filter summary", "author": ["Y Yang", "J Yu", "N Jojic", "J Huan", "TS Huang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.03264", "abstract": "We present a novel method of compression of deep Convolutional Neural Networks (CNNs) by weight sharing through a new representation of convolutional filters. The proposed method reduces the number of parameters of each convolutional layer by learning a 1D vector termed Filter Summary (FS). The convolutional filters are located in FS as overlapping 1D segments, and nearby filters in FS share weights in their overlapping regions in a natural way. The resultant neural network based on such weight sharing scheme, termed Filter"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.03264", "author_id": ["36vh2hgAAAAJ", "-CLCMk4AAAAJ", "iS5UrMkAAAAJ", "pu00nBoAAAAJ", "rGF6-WkAAAAJ"], "url_scholarbib": "/scholar?q=info:HNQOCP1arAwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFsnet:%2BCompression%2BOf%2BDeep%2BConvolutional%2BNeural%2BNetworks%2BBy%2BFilter%2BSummary%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HNQOCP1arAwJ&ei=mdlXYpTHGI2EmgH6u5u4BA&json=", "num_citations": 14, "citedby_url": "/scholar?cites=913204867257783324&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HNQOCP1arAwJ:scholar.google.com/&scioq=Fsnet:+Compression+Of+Deep+Convolutional+Neural+Networks+By+Filter+Summary&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.03264?ref=https://githubhelp.com"}, "Rapid Learning Or Feature Reuse? Towards Understanding The Effectiveness Of Maml": {"container_type": "Publication", "bib": {"title": "Rapid learning or feature reuse? towards understanding the effectiveness of maml", "author": ["A Raghu", "M Raghu", "S Bengio", "O Vinyals"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.09157", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains--is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.09157", "author_id": ["hvnqk7YAAAAJ", "xdwK2NsAAAAJ", "Vs-MdPcAAAAJ", "NkzyCvUAAAAJ"], "url_scholarbib": "/scholar?q=info:av79iZ1TjvAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRapid%2BLearning%2BOr%2BFeature%2BReuse%253F%2BTowards%2BUnderstanding%2BThe%2BEffectiveness%2BOf%2BMaml%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=av79iZ1TjvAJ&ei=o9lXYu_NHZGJmwGIxre4DA&json=", "num_citations": 242, "citedby_url": "/scholar?cites=17333883951885713002&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:av79iZ1TjvAJ:scholar.google.com/&scioq=Rapid+Learning+Or+Feature+Reuse%3F+Towards+Understanding+The+Effectiveness+Of+Maml&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.09157"}, "Unsupervised Clustering Using Pseudo-semi-supervised Learning": {"container_type": "Publication", "bib": {"title": "Unsupervised clustering using pseudo-semi-supervised learning", "author": ["D Gupta", "R Ramjee", "N Kwatra"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "In this paper, we propose a framework that leverages semi-supervised models to improve unsupervised clustering performance. To leverage semi-supervised models, we first need to automatically generate labels, called pseudo-labels. We find that prior approaches for generating pseudo-labels hurt clustering performance because of their low accuracy. Instead, we use an ensemble of deep networks to construct a similarity graph, from which we extract high accuracy pseudo-labels. The approach of finding high quality pseudo-labels"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJlnxkSYPS", "author_id": ["YNg9Sg8AAAAJ", "0P35aLUAAAAJ", "NKtRqvYAAAAJ"], "url_scholarbib": "/scholar?q=info:P11m-Ls_Fe0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BClustering%2BUsing%2BPseudo-semi-supervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=P11m-Ls_Fe0J&ei=ptlXYpOFNZaM6rQPlISayA8&json=", "num_citations": 11, "citedby_url": "/scholar?cites=17083630838058736959&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:P11m-Ls_Fe0J:scholar.google.com/&scioq=Unsupervised+Clustering+Using+Pseudo-semi-supervised+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJlnxkSYPS"}, "Learning Nearly Decomposable Value Functions Via Communication Minimization": {"container_type": "Publication", "bib": {"title": "Learning nearly decomposable value functions via communication minimization", "author": ["T Wang", "J Wang", "C Zheng", "C Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.05366", "abstract": "Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value functions, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable Q-functions (NDQ) via"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.05366", "author_id": ["-AR1yc4AAAAJ", "RpomSmsAAAAJ", "", "LjxqXycAAAAJ"], "url_scholarbib": "/scholar?q=info:8MTzAb2Jh4cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BNearly%2BDecomposable%2BValue%2BFunctions%2BVia%2BCommunication%2BMinimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8MTzAb2Jh4cJ&ei=qtlXYqDYGc6E6rQPz8uiuAc&json=", "num_citations": 36, "citedby_url": "/scholar?cites=9765925761850787056&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8MTzAb2Jh4cJ:scholar.google.com/&scioq=Learning+Nearly+Decomposable+Value+Functions+Via+Communication+Minimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.05366"}, "Deep Batch Active Learning By Diverse, Uncertain Gradient Lower Bounds": {"container_type": "Publication", "bib": {"title": "Deep batch active learning by diverse, uncertain gradient lower bounds", "author": ["JT Ash", "C Zhang", "A Krishnamurthy", "J Langford"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. We show that while other"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.03671", "author_id": ["bmRNH-UAAAAJ", "mho7MawAAAAJ", "K0kaNvkAAAAJ", "LFiqVpwAAAAJ"], "url_scholarbib": "/scholar?q=info:-otkxY__GUwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BBatch%2BActive%2BLearning%2BBy%2BDiverse,%2BUncertain%2BGradient%2BLower%2BBounds%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-otkxY__GUwJ&ei=rtlXYpfjCIySyATlkbrQCA&json=", "num_citations": 189, "citedby_url": "/scholar?cites=5483695014257396730&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-otkxY__GUwJ:scholar.google.com/&scioq=Deep+Batch+Active+Learning+By+Diverse,+Uncertain+Gradient+Lower+Bounds&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.03671"}, "Adversarial Autoaugment": {"container_type": "Publication", "bib": {"title": "Adversarial autoaugment", "author": ["X Zhang", "Q Wang", "J Zhang", "Z Zhong"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.11188", "abstract": "adversarial method to arrive at a computationally-affordable solution called Adversarial  AutoAugment,  \u2022 We propose an adversarial framework to jointly optimize target network training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.11188", "author_id": ["zGLVABAAAAAJ", "_bwYVa4AAAAJ", "lGCFM4gAAAAJ", "igtXP_kAAAAJ"], "url_scholarbib": "/scholar?q=info:Ck2mZ2kXmWEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BAutoaugment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ck2mZ2kXmWEJ&ei=sdlXYsKVIM6E6rQPz8uiuAc&json=", "num_citations": 91, "citedby_url": "/scholar?cites=7032678034593697034&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ck2mZ2kXmWEJ:scholar.google.com/&scioq=Adversarial+Autoaugment&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.11188.pdf?ref=https://githubhelp.com"}, "Deep Imitative Models For Flexible Inference, Planning, And Control": {"container_type": "Publication", "bib": {"title": "Deep imitative models for flexible inference, planning, and control", "author": ["N Rhinehart", "R McAllister", "S Levine"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.06544", "abstract": "Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.06544", "author_id": ["xUGZX_MAAAAJ", "6uIhh6MAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:0uonhF68UAgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BImitative%2BModels%2BFor%2BFlexible%2BInference,%2BPlanning,%2BAnd%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0uonhF68UAgJ&ei=vdlXYsGrH4ySyATlkbrQCA&json=", "num_citations": 89, "citedby_url": "/scholar?cites=599185864570432210&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0uonhF68UAgJ:scholar.google.com/&scioq=Deep+Imitative+Models+For+Flexible+Inference,+Planning,+And+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.06544"}, "Dynamic Model Pruning With Feedback": {"container_type": "Publication", "bib": {"title": "Dynamic model pruning with feedback", "author": ["T Lin", "SU Stich", "L Barba", "D Dmitriev", "M Jaggi"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2006.07253", "author_id": ["QE9pa_cAAAAJ", "8l-mDfQAAAAJ", "c-BgneYAAAAJ", "3_4gF8wAAAAJ", "r1TJBr8AAAAJ"], "url_scholarbib": "/scholar?q=info:MZRxFiuorN0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BModel%2BPruning%2BWith%2BFeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MZRxFiuorN0J&ei=wdlXYrtUi8yxArr1jsAG&json=", "num_citations": 77, "citedby_url": "/scholar?cites=15973326881389909041&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MZRxFiuorN0J:scholar.google.com/&scioq=Dynamic+Model+Pruning+With+Feedback&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2006.07253"}, "Difference-seeking Generative Adversarial Network--unseen Sample Generation": {"container_type": "Publication", "bib": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "author": ["YL Sung", "SH Hsieh", "SC Pei", "CS Lu"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Unseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications,({\\em eg,} novelty detection, semi-supervised learning, and adversarial training). In this paper, we introduce a general framework called\\textbf {d} ifference-\\textbf {s} eeking\\textbf {g} enerative\\textbf {a} dversarial\\textbf {n} etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rygjmpVFvB", "author_id": ["aW2XnF0AAAAJ", "", "-JiGrnAAAAAJ", "3iOHvUAAAAAJ"], "url_scholarbib": "/scholar?q=info:O3UGdNm8XlIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifference-seeking%2BGenerative%2BAdversarial%2BNetwork--unseen%2BSample%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O3UGdNm8XlIJ&ei=xNlXYu7LD82Ny9YPqPyUgAs&json=", "num_citations": 2, "citedby_url": "/scholar?cites=5935389001061397819&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:O3UGdNm8XlIJ:scholar.google.com/&scioq=Difference-seeking+Generative+Adversarial+Network--unseen+Sample+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rygjmpVFvB"}, "Meta Dropout: Learning To Perturb Latent Features For Generalization": {"container_type": "Publication", "bib": {"title": "Meta dropout: Learning to perturb features for generalization", "author": ["HB Lee", "T Nam", "E Yang", "SJ Hwang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.12914", "abstract": "A machine learning model that generalizes well should obtain low errors on unseen test examples. Thus, if we know how to optimally perturb training examples to account for test examples, we may achieve better generalization performance. However, obtaining such perturbation is not possible in standard machine learning frameworks as the distribution of the test data is unknown. To tackle this challenge, we propose a novel regularization method, meta-dropout, which learns to perturb the latent features of training examples for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.12914", "author_id": ["50_nxq0AAAAJ", "", "UWO1mloAAAAJ", "RP4Qx3QAAAAJ"], "url_scholarbib": "/scholar?q=info:soD9Xh-xPHQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta%2BDropout:%2BLearning%2BTo%2BPerturb%2BLatent%2BFeatures%2BFor%2BGeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=soD9Xh-xPHQJ&ei=x9lXYqfBJcWemAHB5baIBQ&json=", "num_citations": 31, "citedby_url": "/scholar?cites=8375764155298054322&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:soD9Xh-xPHQJ:scholar.google.com/&scioq=Meta+Dropout:+Learning+To+Perturb+Latent+Features+For+Generalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.12914"}, "Understanding The Limitations Of Variational Mutual Information Estimators": {"container_type": "Publication", "bib": {"title": "Understanding the limitations of variational mutual information estimators", "author": ["J Song", "S Ermon"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.06222", "abstract": "Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. We theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. We also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.06222", "author_id": ["6dP660cAAAAJ", "ogXTOZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:9pZuDqBtxT4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BThe%2BLimitations%2BOf%2BVariational%2BMutual%2BInformation%2BEstimators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9pZuDqBtxT4J&ei=ytlXYtXAIc2Ny9YPqPyUgAs&json=", "num_citations": 86, "citedby_url": "/scholar?cites=4523141934967854838&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9pZuDqBtxT4J:scholar.google.com/&scioq=Understanding+The+Limitations+Of+Variational+Mutual+Information+Estimators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.06222"}, "Nas-bench-102: Extending The Scope Of Reproducible Neural Architecture Search": {"container_type": "Publication", "bib": {"title": "Nas-bench-201: Extending the scope of reproducible neural architecture search", "author": ["X Dong", "Y Yang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.00326", "abstract": "Neural architecture search (NAS) has achieved breakthrough success in a great number of applications in the past few years. It could be time to take a step back and analyze the good and bad aspects in the field of NAS. A variety of algorithms search architectures under different search space. These searched architectures are trained using different setups, eg, hyper-parameters, data augmentation, regularization. This raises a comparability problem when comparing the performance of various NAS algorithms. NAS-Bench-101 has shown"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00326", "author_id": ["7zp9arUAAAAJ", "RMSuNFwAAAAJ"], "url_scholarbib": "/scholar?q=info:r6Ie1KKn_ScJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNas-bench-102:%2BExtending%2BThe%2BScope%2BOf%2BReproducible%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r6Ie1KKn_ScJ&ei=ztlXYrWoMOHDywTjooCQBQ&json=", "num_citations": 283, "citedby_url": "/scholar?cites=2881643654372303535&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r6Ie1KKn_ScJ:scholar.google.com/&scioq=Nas-bench-102:+Extending+The+Scope+Of+Reproducible+Neural+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00326"}, "Smooth Markets: A Basic Mechanism For Organizing Gradient-based Learners": {"container_type": "Publication", "bib": {"title": "Smooth markets: A basic mechanism for organizing gradient-based learners", "author": ["D Balduzzi", "WM Czarnecki", "TW Anthony"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "With the success of modern machine learning, it is becoming increasingly important to understand and control how learning algorithms interact. Unfortunately, negative results from game theory show there is little hope of understanding or controlling general n-player games. We therefore introduce smooth markets (SM-games), a class of n-player games with pairwise zero sum interactions. SM-games codify a common design pattern in machine learning that includes (some) GANs, adversarial training, and other recent algorithms. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.04678", "author_id": ["xA3Jd5gAAAAJ", "aOvr9eMAAAAJ", "Ksz7c7YAAAAJ"], "url_scholarbib": "/scholar?q=info:1I0tbMI13psJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmooth%2BMarkets:%2BA%2BBasic%2BMechanism%2BFor%2BOrganizing%2BGradient-based%2BLearners%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1I0tbMI13psJ&ei=0tlXYv7MJYySyATlkbrQCA&json=", "num_citations": 10, "citedby_url": "/scholar?cites=11231473629863448020&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1I0tbMI13psJ:scholar.google.com/&scioq=Smooth+Markets:+A+Basic+Mechanism+For+Organizing+Gradient-based+Learners&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.04678"}, "Biologically Inspired Sleep Algorithm For Increased Generalization And Adversarial Robustness In Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks", "author": ["T Tadros", "G Krishnan", "R Ramyaa"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network may perform well on similar testing data, inputs that differ even slightly from the training data may trigger unpredictable behavior. Due to this limitation, it is possible to design inputs with very small perturbations that can result in misclassification. These adversarial attacks present a security risk to deployed ANNs and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1xGnA4Kvr", "author_id": ["VB0XiVQAAAAJ", "IGsdszkAAAAJ", "IWAor20AAAAJ"], "url_scholarbib": "/scholar?q=info:2ux5kxXbJBAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBiologically%2BInspired%2BSleep%2BAlgorithm%2BFor%2BIncreased%2BGeneralization%2BAnd%2BAdversarial%2BRobustness%2BIn%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2ux5kxXbJBAJ&ei=1dlXYszgOpaM6rQPlISayA8&json=", "num_citations": 12, "citedby_url": "/scholar?cites=1163295489483467994&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2ux5kxXbJBAJ:scholar.google.com/&scioq=Biologically+Inspired+Sleep+Algorithm+For+Increased+Generalization+And+Adversarial+Robustness+In+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1xGnA4Kvr"}, "Learning Disentangled Representations For Counterfactual Regression": {"container_type": "Publication", "bib": {"title": "Learning disentangled representations for counterfactual regression", "author": ["N Hassanpour", "R Greiner"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "We consider the challenge of estimating treatment effects from observational data; and point out that, in general, only some factors based on the observed covariates X contribute to selection of the treatment T, and only some to determining the outcomes Y. We model this by considering three underlying sources of {X, T, Y} and show that explicitly modeling these sources offers great insight to guide designing models that better handle selection bias. This paper is an attempt to conceptualize this line of thought and provide a path to explore it"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkxBJT4YvB", "author_id": ["g7GMn3gAAAAJ", "Rn7APGIAAAAJ"], "url_scholarbib": "/scholar?q=info:lYSf89n9ADcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDisentangled%2BRepresentations%2BFor%2BCounterfactual%2BRegression%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lYSf89n9ADcJ&ei=2tlXYt3oI82Ny9YPqPyUgAs&json=", "num_citations": 37, "citedby_url": "/scholar?cites=3963446784623084693&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lYSf89n9ADcJ:scholar.google.com/&scioq=Learning+Disentangled+Representations+For+Counterfactual+Regression&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkxBJT4YvB"}, "Learning Deep Graph Matching With Channel-independent Embedding And Hungarian Attention": {"container_type": "Publication", "bib": {"title": "Learning deep graph matching with channel-independent embedding and hungarian attention", "author": ["T Yu", "R Wang", "J Yan", "B Li"], "pub_year": "2019", "venue": "International conference on learning \u2026", "abstract": "Graph matching aims to establishing node-wise correspondence between two graphs, which is a classic combinatorial problem and in general NP-complete. Until very recently, deep graph matching methods start to resort to deep networks to achieve unprecedented matching accuracy. Along this direction, this paper makes two complementary contributions which can also be reused as plugin in existing works: i) a novel node and edge embedding strategy which stimulates the multi-head strategy in attention models and allows the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJgBd2NYPH", "author_id": ["MTHO7DsAAAAJ", "uoM0g3cAAAAJ", "ga230VoAAAAJ", "8OOBH04AAAAJ"], "url_scholarbib": "/scholar?q=info:KG9UtKja6dsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDeep%2BGraph%2BMatching%2BWith%2BChannel-independent%2BEmbedding%2BAnd%2BHungarian%2BAttention%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KG9UtKja6dsJ&ei=7dlXYvPFGM6E6rQPz8uiuAc&json=", "num_citations": 35, "citedby_url": "/scholar?cites=15846437181994594088&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KG9UtKja6dsJ:scholar.google.com/&scioq=Learning+Deep+Graph+Matching+With+Channel-independent+Embedding+And+Hungarian+Attention&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJgBd2NYPH"}, "Functional Vs. Parametric Equivalence Of Relu Networks": {"container_type": "Publication", "bib": {"title": "Functional vs. parametric equivalence of ReLU networks", "author": ["P Bui Thi Mai", "C Lampert"], "pub_year": "2020", "venue": "8th International \u2026", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are"}, "filled": false, "gsrank": 1, "pub_url": "https://research-explorer.app.ist.ac.at/record/7481", "author_id": ["", "iCf3SwgAAAAJ"], "url_scholarbib": "/scholar?q=info:mJSWUhrgq2wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFunctional%2BVs.%2BParametric%2BEquivalence%2BOf%2BRelu%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mJSWUhrgq2wJ&ei=8NlXYuuSJo6pywSdh6agAg&json=", "num_citations": 12, "citedby_url": "/scholar?cites=7830598780773110936&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mJSWUhrgq2wJ:scholar.google.com/&scioq=Functional+Vs.+Parametric+Equivalence+Of+Relu+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://research-explorer.app.ist.ac.at/download/7481/7482/main.pdf"}, "Neural Policy Gradient Methods: Global Optimality And Rates Of Convergence": {"container_type": "Publication", "bib": {"title": "Neural policy gradient methods: Global optimality and rates of convergence", "author": ["L Wang", "Q Cai", "Z Yang", "Z Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.01150", "abstract": "Policy gradient methods with actor-critic schemes demonstrate tremendous empirical successes, especially when the actors and critics are parameterized by neural networks. However, it remains less clear whether such\" neural\" policy gradient methods converge to globally optimal policies and whether they even converge at all. We answer both the questions affirmatively in the overparameterized regime. In detail, we prove that neural natural policy gradient converges to a globally optimal policy at a sublinear rate. Also, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.01150", "author_id": ["u3aY8EsAAAAJ", "FX6bV4UAAAAJ", "k7NgVSUAAAAJ", "HSx0BgQAAAAJ"], "url_scholarbib": "/scholar?q=info:S9Q4H-XxfHEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BPolicy%2BGradient%2BMethods:%2BGlobal%2BOptimality%2BAnd%2BRates%2BOf%2BConvergence%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S9Q4H-XxfHEJ&ei=9NlXYrzvFcWemAHB5baIBQ&json=", "num_citations": 117, "citedby_url": "/scholar?cites=8177676989771600971&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:S9Q4H-XxfHEJ:scholar.google.com/&scioq=Neural+Policy+Gradient+Methods:+Global+Optimality+And+Rates+Of+Convergence&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.01150"}, "Cyclical Stochastic Gradient Mcmc For Bayesian Deep Learning": {"container_type": "Publication", "bib": {"title": "Cyclical stochastic gradient MCMC for Bayesian deep learning", "author": ["R Zhang", "C Li", "J Zhang", "C Chen", "AG Wilson"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We also prove non-asymptotic convergence of our proposed algorithm. Moreover, we provide extensive experimental"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.03932", "author_id": ["4ojpmc8AAAAJ", "Zd7WmXUAAAAJ", "5UA6Z5cAAAAJ", "LtEcKBcAAAAJ", "twWX2LIAAAAJ"], "url_scholarbib": "/scholar?q=info:HUraPqnavY4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCyclical%2BStochastic%2BGradient%2BMcmc%2BFor%2BBayesian%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HUraPqnavY4J&ei=99lXYo_2DsWemAHB5baIBQ&json=", "num_citations": 119, "citedby_url": "/scholar?cites=10285617544422902301&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HUraPqnavY4J:scholar.google.com/&scioq=Cyclical+Stochastic+Gradient+Mcmc+For+Bayesian+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.03932"}, "Varibad: A Very Good Method For Bayes-adaptive Deep Rl Via Meta-learning": {"container_type": "Publication", "bib": {"title": "Varibad: A very good method for bayes-adaptive deep rl via meta-learning", "author": ["L Zintgraf", "K Shiarlis", "M Igl", "S Schulze", "Y Gal"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We show that variBAD performs structured and online exploration as it  meta-learning  settings by employing on four MuJoCo continuous control tasks commonly used in the meta-RL"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.08348", "author_id": ["lEzcLFwAAAAJ", "FNiMmWoAAAAJ", "rFcdDJEAAAAJ", "", "SIayDoQAAAAJ"], "url_scholarbib": "/scholar?q=info:om3eW8ZGKUQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVaribad:%2BA%2BVery%2BGood%2BMethod%2BFor%2BBayes-adaptive%2BDeep%2BRl%2BVia%2BMeta-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=om3eW8ZGKUQJ&ei=-tlXYp2EFcLZmQHc1ovQAg&json=", "num_citations": 87, "citedby_url": "/scholar?cites=4911534686383009186&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:om3eW8ZGKUQJ:scholar.google.com/&scioq=Varibad:+A+Very+Good+Method+For+Bayes-adaptive+Deep+Rl+Via+Meta-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.08348"}, "Robust Anomaly Detection And Backdoor Attack Detection Via Differential Privacy": {"container_type": "Publication", "bib": {"title": "Robust anomaly detection and backdoor attack detection via differential privacy", "author": ["M Du", "R Jia", "D Song"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.07116", "abstract": "Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as\" outliers\" that are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.07116", "author_id": ["cfNEycgAAAAJ", "JCrug-YAAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:54ewcrid_eoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BAnomaly%2BDetection%2BAnd%2BBackdoor%2BAttack%2BDetection%2BVia%2BDifferential%2BPrivacy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=54ewcrid_eoJ&ei=_dlXYr61M82Ny9YPqPyUgAs&json=", "num_citations": 73, "citedby_url": "/scholar?cites=16932863589506648039&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:54ewcrid_eoJ:scholar.google.com/&scioq=Robust+Anomaly+Detection+And+Backdoor+Attack+Detection+Via+Differential+Privacy&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.07116.pdf?ref=https://githubhelp.com"}, "Deep Semi-supervised Anomaly Detection": {"container_type": "Publication", "bib": {"title": "Deep semi-supervised anomaly detection", "author": ["L Ruff", "RA Vandermeulen", "N G\u00f6rnitz", "A Binder"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We instead build upon principle (2) to motivate a deep method for general semi-supervised  AD, where we include the label information Y through a novel representation learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.02694", "author_id": ["40QzNXMAAAAJ", "eSjfzOUAAAAJ", "ZTF-LpIAAAAJ", "5B8CTlEAAAAJ"], "url_scholarbib": "/scholar?q=info:6CYSP97CyUYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BSemi-supervised%2BAnomaly%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6CYSP97CyUYJ&ei=ANpXYrP4Is2Ny9YPqPyUgAs&json=", "num_citations": 196, "citedby_url": "/scholar?cites=5100822312770479848&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6CYSP97CyUYJ:scholar.google.com/&scioq=Deep+Semi-supervised+Anomaly+Detection&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.02694"}, "Scaling Autoregressive Video Models": {"container_type": "Publication", "bib": {"title": "Scaling autoregressive video models", "author": ["D Weissenborn", "O T\u00e4ckstr\u00f6m", "J Uszkoreit"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In contrast, we show that conceptually simple autoregressive video generation models   our models on Kinetics, a large scale action recognition dataset comprised of YouTube videos"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.02634", "author_id": ["DSQ-9ZwAAAAJ", "J2SoC6YAAAAJ", "mOG0bwsAAAAJ"], "url_scholarbib": "/scholar?q=info:A39yE-1PHK0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScaling%2BAutoregressive%2BVideo%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=A39yE-1PHK0J&ei=DNpXYtvyM82Ny9YPqPyUgAs&json=", "num_citations": 72, "citedby_url": "/scholar?cites=12473932947561545475&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:A39yE-1PHK0J:scholar.google.com/&scioq=Scaling+Autoregressive+Video+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.02634"}, "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps": {"container_type": "Publication", "bib": {"title": "Kaleidoscope: An efficient, learnable representation for all structured linear maps", "author": ["T Dao", "NS Sohoni", "A Gu", "M Eichhorn", "A Blonder"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2012.14966", "author_id": ["NQRw0bQAAAAJ", "DI5UMWQAAAAJ", "DVCHv1kAAAAJ", "13yqSuIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:vKQpnbyeQR0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKaleidoscope:%2BAn%2BEfficient,%2BLearnable%2BRepresentation%2BFor%2BAll%2BStructured%2BLinear%2BMaps%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vKQpnbyeQR0J&ei=D9pXYo7jOZaM6rQPlISayA8&json=", "num_citations": 17, "citedby_url": "/scholar?cites=2108140633513895100&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vKQpnbyeQR0J:scholar.google.com/&scioq=Kaleidoscope:+An+Efficient,+Learnable+Representation+For+All+Structured+Linear+Maps&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2012.14966"}, "Pcmc-net: Feature-based Pairwise Choice Markov Chains": {"container_type": "Publication", "bib": {"title": "PCMC-Net: Feature-based pairwise choice Markov chains", "author": ["A Lh\u00e9ritier"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11553", "abstract": "Pairwise Choice Markov Chains (PCMC) have been recently introduced to overcome limitations of choice models based on traditional axioms unable to express empirical observations from modern behavior economics like context effects occurring when a choice between two options is altered by adding a third alternative. The inference approach that estimates the transition rates between each possible pair of alternatives via maximum likelihood suffers when the examples of each alternative are scarce and is inappropriate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11553", "author_id": ["pCkKuH0AAAAJ"], "url_scholarbib": "/scholar?q=info:ITP5ZR6RUlgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPcmc-net:%2BFeature-based%2BPairwise%2BChoice%2BMarkov%2BChains%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ITP5ZR6RUlgJ&ei=E9pXYvPJJY2EmgH6u5u4BA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=6364308783173808929&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ITP5ZR6RUlgJ:scholar.google.com/&scioq=Pcmc-net:+Feature-based+Pairwise+Choice+Markov+Chains&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11553"}, "Mathematical Reasoning In Latent Space": {"container_type": "Publication", "bib": {"title": "Mathematical reasoning in latent space", "author": ["D Lee", "C Szegedy", "MN Rabe", "SM Loos"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (ie transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11851", "author_id": ["vOLXDDAAAAAJ", "3QeF7mAAAAAJ", "gCp7X74AAAAJ", "9j79kA8AAAAJ"], "url_scholarbib": "/scholar?q=info:CFvhlCVavDUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMathematical%2BReasoning%2BIn%2BLatent%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CFvhlCVavDUJ&ei=JdpXYtenJo2EmgH6u5u4BA&json=", "num_citations": 21, "citedby_url": "/scholar?cites=3872068897089870600&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CFvhlCVavDUJ:scholar.google.com/&scioq=Mathematical+Reasoning+In+Latent+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11851"}, "Graph Convolutional Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Graph convolutional reinforcement learning", "author": ["J Jiang", "C Dun", "T Huang", "Z Lu"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.09202", "abstract": "This makes it hard to learn abstract representations of mutual  graph convolutional  reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.09202", "author_id": ["zRgok8IAAAAJ", "QbPWRIoAAAAJ", "knvEK4AAAAAJ", "k3IFtTYAAAAJ"], "url_scholarbib": "/scholar?q=info:NxEFiCP5114J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BConvolutional%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NxEFiCP5114J&ei=KNpXYtrgKJaM6rQPlISayA8&json=", "num_citations": 169, "citedby_url": "/scholar?cites=6834204890559222071&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NxEFiCP5114J:scholar.google.com/&scioq=Graph+Convolutional+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.09202"}, "State Alignment-based Imitation Learning": {"container_type": "Publication", "bib": {"title": "State alignment-based imitation learning", "author": ["F Liu", "Z Ling", "T Mu", "H Su"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.10947", "abstract": "Consider an imitation learning problem that the imitator and the expert have different dynamics models. Most of the current imitation learning methods fail because they focus on imitating actions. We propose a novel state alignment-based imitation learning method to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both local and global perspectives and we combine them into a reinforcement learning framework by a regularized policy update"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.10947", "author_id": ["mV8nbIQAAAAJ", "y0zdQmMAAAAJ", "uVsZydYAAAAJ", "1P8Zu04AAAAJ"], "url_scholarbib": "/scholar?q=info:eLto3RlVsS8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DState%2BAlignment-based%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eLto3RlVsS8J&ei=K9pXYo3gNsWemAHB5baIBQ&json=", "num_citations": 36, "citedby_url": "/scholar?cites=3436621560237570936&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eLto3RlVsS8J:scholar.google.com/&scioq=State+Alignment-based+Imitation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.10947"}, "Continual Learning With Adaptive Weights (claw)": {"container_type": "Publication", "bib": {"title": "Continual learning with adaptive weights (claw)", "author": ["T Adel", "H Zhao", "RE Turner"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.09514", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.09514", "author_id": ["IkhgmfEAAAAJ", "x942ipYAAAAJ", "DgLEyZgAAAAJ"], "url_scholarbib": "/scholar?q=info:FdWDIk9AhrEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContinual%2BLearning%2BWith%2BAdaptive%2BWeights%2B(claw)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FdWDIk9AhrEJ&ei=LtpXYuGgHcLZmQHc1ovQAg&json=", "num_citations": 39, "citedby_url": "/scholar?cites=12791982500218131733&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FdWDIk9AhrEJ:scholar.google.com/&scioq=Continual+Learning+With+Adaptive+Weights+(claw)&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.09514"}, "Amrl: Aggregated Memory For Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Amrl: aggregated memory for reinforcement learning", "author": ["J Beck", "K Ciosek", "S Devlin", "S Tschiatschek"], "pub_year": "2019", "venue": "International \u2026", "abstract": "In many partially observable scenarios, Reinforcement Learning (RL) agents must rely on long-term memory in order to learn an optimal policy. We demonstrate that using techniques from NLP and supervised learning fails at RL tasks due to stochasticity from the environment and from exploration. Utilizing our insights on the limitations of traditional memory methods in RL, we propose AMRL, a class of models that can learn better policies with greater sample efficiency and are resilient to noisy inputs. Specifically, our models use a standard"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bkl7bREtDr", "author_id": ["PrS_dHMAAAAJ", "", "rOnQvwoAAAAJ", ""], "url_scholarbib": "/scholar?q=info:57sBvEOMnecJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAmrl:%2BAggregated%2BMemory%2BFor%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=57sBvEOMnecJ&ei=MdpXYrKPKJGJmwGIxre4DA&json=", "num_citations": 8, "citedby_url": "/scholar?cites=16689650016649853927&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:57sBvEOMnecJ:scholar.google.com/&scioq=Amrl:+Aggregated+Memory+For+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bkl7bREtDr"}, "Robustness Verification For Transformers": {"container_type": "Publication", "bib": {"title": "Robustness verification for transformers", "author": ["Z Shi", "H Zhang", "KW Chang", "M Huang"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We aim to verify the robustness of a Transformer whose input is a sequence of frames X =  [x(1) Nevertheless, our method for verifying Transformers is general and can also be applied in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06622", "author_id": ["YFIr4PwAAAAJ", "LTa3GzEAAAAJ", "fqDBtzYAAAAJ", "P1jPSzMAAAAJ"], "url_scholarbib": "/scholar?q=info:tru_VHE4gCUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobustness%2BVerification%2BFor%2BTransformers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tru_VHE4gCUJ&ei=NNpXYo2vCsWemAHB5baIBQ&json=", "num_citations": 46, "citedby_url": "/scholar?cites=2702221835826609078&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tru_VHE4gCUJ:scholar.google.com/&scioq=Robustness+Verification+For+Transformers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06622"}, "The Function Of Contextual Illusions": {"container_type": "Publication", "bib": {"title": "The tilt illusion: Phenomenology and functional implications", "author": ["CWG Clifford"], "pub_year": "2014", "venue": "Vision research", "abstract": "In the context of gain control models of the tilt illusion, this is consistent with differences in  the relative strength of inhibitory and facilitatory interactions across experimental manipulations"}, "filled": false, "gsrank": 1, "pub_url": "https://www.sciencedirect.com/science/article/pii/S004269891400145X", "author_id": ["BqJm-fwAAAAJ"], "url_scholarbib": "/scholar?q=info:mO5fWLWydbcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BFunction%2BOf%2BContextual%2BIllusions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mO5fWLWydbcJ&ei=NtpXYo63LJGJmwGIxre4DA&json=", "num_citations": 72, "citedby_url": "/scholar?cites=13219668773157465752&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mO5fWLWydbcJ:scholar.google.com/&scioq=The+Function+Of+Contextual+Illusions&hl=en&as_sdt=0,33", "eprint_url": "https://www.sciencedirect.com/science/article/pii/S004269891400145X"}, "A Mutual Information Maximization Perspective Of Language Representation Learning": {"container_type": "Publication", "bib": {"title": "A mutual information maximization perspective of language representation learning", "author": ["L Kong", "CM d'Autume", "W Ling", "L Yu", "Z Dai"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (ie, a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (eg, Skip-gram) and modern contextual embeddings (eg, BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.08350", "author_id": ["f1hBi5wAAAAJ", "vcmTSYEAAAAJ", "gl0PhvEAAAAJ", "gX5JBc4AAAAJ", ""], "url_scholarbib": "/scholar?q=info:ZJJQz-sPU5AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BMutual%2BInformation%2BMaximization%2BPerspective%2BOf%2BLanguage%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZJJQz-sPU5AJ&ei=OdpXYvOLHY2EmgH6u5u4BA&json=", "num_citations": 25, "citedby_url": "/scholar?cites=10399673469998502500&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZJJQz-sPU5AJ:scholar.google.com/&scioq=A+Mutual+Information+Maximization+Perspective+Of+Language+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.08350"}, "Implementing Inductive Bias For Different Navigation Tasks Through Diverse Rnn Attrractors": {"container_type": "Publication", "bib": {"title": "Implementing Inductive bias for different navigation tasks through diverse RNN attractors", "author": ["T Xu", "O Barak"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.02496", "abstract": "Navigation is crucial for animal behavior and is assumed to require an internal representation of the external environment, termed a cognitive map. The precise form of this representation is often considered to be a metric representation of space. An internal representation, however, is judged by its contribution to performance on a given task, and may thus vary between different types of navigation tasks. Here we train a recurrent neural network that controls an agent performing several navigation tasks in a simple environment"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.02496", "author_id": ["", "6BrZ2isAAAAJ"], "url_scholarbib": "/scholar?q=info:2LMIDeuJv-cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImplementing%2BInductive%2BBias%2BFor%2BDifferent%2BNavigation%2BTasks%2BThrough%2BDiverse%2BRnn%2BAttrractors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2LMIDeuJv-cJ&ei=PtpXYv3zHpLeyQTms5KQBg&json=", "num_citations": 1, "citedby_url": "/scholar?cites=16699217585942082520&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2LMIDeuJv-cJ:scholar.google.com/&scioq=Implementing+Inductive+Bias+For+Different+Navigation+Tasks+Through+Diverse+Rnn+Attrractors&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.02496"}, "Accelerating Sgd With Momentum For Over-parameterized Learning": {"container_type": "Publication", "bib": {"title": "Accelerating sgd with momentum for over-parameterized learning", "author": ["C Liu", "M Belkin"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.13395", "abstract": "Nesterov SGD is widely used for training modern neural networks and other machine learning models. Yet, its advantages over SGD have not been theoretically clarified. Indeed, as we show in our paper, both theoretically and empirically, Nesterov SGD with any parameter selection does not in general provide acceleration over ordinary SGD. Furthermore, Nesterov SGD may diverge for step sizes that ensure convergence of ordinary SGD. This is in contrast to the classical results in the deterministic scenario, where the same"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.13395", "author_id": ["sRjoMX0AAAAJ", "Iwd9DdkAAAAJ"], "url_scholarbib": "/scholar?q=info:dRsivmi0-dgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccelerating%2BSgd%2BWith%2BMomentum%2BFor%2BOver-parameterized%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dRsivmi0-dgJ&ei=QdpXYoPfJJLeyQTms5KQBg&json=", "num_citations": 35, "citedby_url": "/scholar?cites=15634725943352892277&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dRsivmi0-dgJ:scholar.google.com/&scioq=Accelerating+Sgd+With+Momentum+For+Over-parameterized+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.13395"}, "Understanding The Limitations Of Conditional Generative Models": {"container_type": "Publication", "bib": {"title": "Understanding the limitations of conditional generative models", "author": ["E Fetaya", "JH Jacobsen", "W Grathwohl"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties:(1)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.01171", "author_id": ["zLuqh-0AAAAJ", "c1FYGAQAAAAJ", "ZbClz98AAAAJ"], "url_scholarbib": "/scholar?q=info:-iuke0y9G_UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BThe%2BLimitations%2BOf%2BConditional%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-iuke0y9G_UJ&ei=RNpXYujNNY6pywSdh6agAg&json=", "num_citations": 21, "citedby_url": "/scholar?cites=17661918499853052922&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-iuke0y9G_UJ:scholar.google.com/&scioq=Understanding+The+Limitations+Of+Conditional+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.01171"}, "Enhancing Transformation-based Defenses Against Adversarial Attacks With A Distribution Classifier": {"container_type": "Publication", "bib": {"title": "Enhancing transformation-based defenses against adversarial attacks with a distribution classifier", "author": ["C Kou", "HK Lee", "EC Chang", "TK Ng"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Adversarial attacks on convolutional neural networks (CNN) have gained significant attention and there have been active research efforts on defense mechanisms. Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus among the random samples. However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images. While it is intuitive"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkgWahEFvr", "author_id": ["sVoKBKUAAAAJ", "", "qZavFBcAAAAJ", "koHhid0AAAAJ"], "url_scholarbib": "/scholar?q=info:Fbx-NwN_dg0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnhancing%2BTransformation-based%2BDefenses%2BAgainst%2BAdversarial%2BAttacks%2BWith%2BA%2BDistribution%2BClassifier%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Fbx-NwN_dg0J&ei=StpXYuj0CJLeyQTms5KQBg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=970102421537602581&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Fbx-NwN_dg0J:scholar.google.com/&scioq=Enhancing+Transformation-based+Defenses+Against+Adversarial+Attacks+With+A+Distribution+Classifier&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkgWahEFvr"}, "Ddsp: Differentiable Digital Signal Processing": {"container_type": "Publication", "bib": {"title": "DDSP: Differentiable digital signal processing", "author": ["J Engel", "L Hantrakul", "C Gu", "A Roberts"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.04643", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.04643", "author_id": ["Sc7qOfcAAAAJ", "g91EXNcAAAAJ", "_4B6OTAAAAAJ", "U5UpKq8AAAAJ"], "url_scholarbib": "/scholar?q=info:evkiqTgd3gYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDdsp:%2BDifferentiable%2BDigital%2BSignal%2BProcessing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=evkiqTgd3gYJ&ei=T9pXYvG9EIySyATlkbrQCA&json=", "num_citations": 151, "citedby_url": "/scholar?cites=494865138250348922&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:evkiqTgd3gYJ:scholar.google.com/&scioq=Ddsp:+Differentiable+Digital+Signal+Processing&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.04643"}, "Neural Machine Translation With Universal Visual Representation": {"container_type": "Publication", "bib": {"title": "Neural machine translation with universal visual representation", "author": ["Z Zhang", "K Chen", "R Wang", "M Utiyama"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Byl8hhNYPS", "author_id": ["63LTQhgAAAAJ", "_M4Am0AAAAAJ", "oTU0v5IAAAAJ", "artIO6gAAAAJ"], "url_scholarbib": "/scholar?q=info:GAJPhb__gu0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BMachine%2BTranslation%2BWith%2BUniversal%2BVisual%2BRepresentation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GAJPhb__gu0J&ei=UtpXYpb2CoySyATlkbrQCA&json=", "num_citations": 34, "citedby_url": "/scholar?cites=17114522732001690136&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GAJPhb__gu0J:scholar.google.com/&scioq=Neural+Machine+Translation+With+Universal+Visual+Representation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Byl8hhNYPS"}, "Functional Regularisation For Continual Learning With Gaussian Processes": {"container_type": "Publication", "bib": {"title": "Functional regularisation for continual learning with gaussian processes", "author": ["MK Titsias", "J Schwarz", "AGG Matthews"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.11356", "author_id": ["B-SbkAwAAAAJ", "Efs3XxQAAAAJ", "3OFgQKcAAAAJ"], "url_scholarbib": "/scholar?q=info:tPfzTWAQbLkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFunctional%2BRegularisation%2BFor%2BContinual%2BLearning%2BWith%2BGaussian%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tPfzTWAQbLkJ&ei=VdpXYtacK4vEmgH7846QCg&json=", "num_citations": 52, "citedby_url": "/scholar?cites=13361072200312158132&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tPfzTWAQbLkJ:scholar.google.com/&scioq=Functional+Regularisation+For+Continual+Learning+With+Gaussian+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.11356"}, "Scalable Neural Methods For Reasoning With A Symbolic Knowledge Base": {"container_type": "Publication", "bib": {"title": "Scalable neural methods for reasoning with a symbolic knowledge base", "author": ["WW Cohen", "H Sun", "RA Hofer", "M Siegler"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06115", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB. This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations. The reified KB"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06115", "author_id": ["8ys-38kAAAAJ", "opSHsTQAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:wc8i9SNbqYcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScalable%2BNeural%2BMethods%2BFor%2BReasoning%2BWith%2BA%2BSymbolic%2BKnowledge%2BBase%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wc8i9SNbqYcJ&ei=WdpXYoLWApaM6rQPlISayA8&json=", "num_citations": 29, "citedby_url": "/scholar?cites=9775444676179054529&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wc8i9SNbqYcJ:scholar.google.com/&scioq=Scalable+Neural+Methods+For+Reasoning+With+A+Symbolic+Knowledge+Base&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06115"}, "On Mutual Information Maximization For Representation Learning": {"container_type": "Publication", "bib": {"title": "On mutual information maximization for representation learning", "author": ["M Tschannen", "J Djolonga", "PK Rubenstein"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.13625", "author_id": ["TSj_8nYAAAAJ", "4NdMn_MAAAAJ", "SUbSm9UAAAAJ"], "url_scholarbib": "/scholar?q=info:7p3vg_v4UbsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BMutual%2BInformation%2BMaximization%2BFor%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7p3vg_v4UbsJ&ei=XNpXYpCkN8LZmQHc1ovQAg&json=", "num_citations": 229, "citedby_url": "/scholar?cites=13497843317340085742&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7p3vg_v4UbsJ:scholar.google.com/&scioq=On+Mutual+Information+Maximization+For+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.13625.pdf?ref=https://githubhelp.com"}, "Graphaf: A Flow-based Autoregressive Model For Molecular Graph Generation": {"container_type": "Publication", "bib": {"title": "Graphaf: a flow-based autoregressive model for molecular graph generation", "author": ["C Shi", "M Xu", "Z Zhu", "W Zhang", "M Zhang"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys:(1)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.09382", "author_id": ["0Um1Kz0AAAAJ", "fKuiInUAAAAJ", "Qd8JumkAAAAJ", "Qzss0GEAAAAJ", "LbzoQBsAAAAJ"], "url_scholarbib": "/scholar?q=info:DhSQSEacQygJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraphaf:%2BA%2BFlow-based%2BAutoregressive%2BModel%2BFor%2BMolecular%2BGraph%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DhSQSEacQygJ&ei=YNpXYvS8GpaM6rQPlISayA8&json=", "num_citations": 122, "citedby_url": "/scholar?cites=2901334410635777038&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DhSQSEacQygJ:scholar.google.com/&scioq=Graphaf:+A+Flow-based+Autoregressive+Model+For+Molecular+Graph+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.09382"}, "Observational Overfitting In Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Observational overfitting in reinforcement learning", "author": ["X Song", "Y Jiang", "S Tu", "Y Du", "B Neyshabur"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02975", "author_id": ["GnpHmO8AAAAJ", "x9qzWg8AAAAJ", "JQcDmB8AAAAJ", "GRMMc_MAAAAJ", "e1ucbCYAAAAJ"], "url_scholarbib": "/scholar?q=info:u1GKFJdNbXEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DObservational%2BOverfitting%2BIn%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=u1GKFJdNbXEJ&ei=ZNpXYpySEcLZmQHc1ovQAg&json=", "num_citations": 47, "citedby_url": "/scholar?cites=8173274210027327931&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:u1GKFJdNbXEJ:scholar.google.com/&scioq=Observational+Overfitting+In+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02975"}, "Evaluating The Search Phase Of Neural Architecture Search": {"container_type": "Publication", "bib": {"title": "Evaluating the search phase of neural architecture search", "author": ["K Yu", "C Sciuto", "M Jaggi", "C Musat"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we propose to evaluate the NAS search phase. To this end, we compare the quality of the solutions obtained by NAS search"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.08142", "author_id": ["Jtmq_m0AAAAJ", "Pb74Fg4AAAAJ", "r1TJBr8AAAAJ", "n4bdAtIAAAAJ"], "url_scholarbib": "/scholar?q=info:QqIfT06kx8IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvaluating%2BThe%2BSearch%2BPhase%2BOf%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QqIfT06kx8IJ&ei=adpXYve6KI2EmgH6u5u4BA&json=", "num_citations": 148, "citedby_url": "/scholar?cites=14035367419965317698&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QqIfT06kx8IJ:scholar.google.com/&scioq=Evaluating+The+Search+Phase+Of+Neural+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.08142"}, "Adversarial Example Detection And Classification With Asymmetrical Adversarial Training": {"container_type": "Publication", "bib": {"title": "Adversarial example detection and classification with asymmetrical adversarial training", "author": ["X Yin", "S Kolouri", "GK Rohde"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.11475", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we first present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11475", "author_id": ["", "yREBSy0AAAAJ", "UnRdQPMAAAAJ"], "url_scholarbib": "/scholar?q=info:o_ZpjAnSd0AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BExample%2BDetection%2BAnd%2BClassification%2BWith%2BAsymmetrical%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o_ZpjAnSd0AJ&ei=bNpXYrGCFsLZmQHc1ovQAg&json=", "num_citations": 19, "citedby_url": "/scholar?cites=4645412479108249251&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o_ZpjAnSd0AJ:scholar.google.com/&scioq=Adversarial+Example+Detection+And+Classification+With+Asymmetrical+Adversarial+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11475"}, "Non-autoregressive Dialog State Tracking": {"container_type": "Publication", "bib": {"title": "Non-autoregressive dialog state tracking", "author": ["H Le", "R Socher", "SCH Hoi"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.08024", "abstract": "of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive   reduce the latency of DST for realtime dialogue response generation, but also detect"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.08024", "author_id": ["jnYI1UgAAAAJ", "FaOcyfMAAAAJ", "JoLjflYAAAAJ"], "url_scholarbib": "/scholar?q=info:RQkEUxhzqbsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNon-autoregressive%2BDialog%2BState%2BTracking%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RQkEUxhzqbsJ&ei=b9pXYtuaM4vEmgH7846QCg&json=", "num_citations": 28, "citedby_url": "/scholar?cites=13522465904465807685&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RQkEUxhzqbsJ:scholar.google.com/&scioq=Non-autoregressive+Dialog+State+Tracking&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.08024"}, "Adaptive Structural Fingerprints For Graph Attention Networks": {"container_type": "Publication", "bib": {"title": "Adaptive structural fingerprints for graph attention networks", "author": ["K Zhang", "Y Zhu", "J Wang", "J Zhang"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJxWx0NYPr", "author_id": ["I6ifR7YAAAAJ", "", "i0KkESEAAAAJ", "epTfECgAAAAJ"], "url_scholarbib": "/scholar?q=info:2E8L67OqVggJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BStructural%2BFingerprints%2BFor%2BGraph%2BAttention%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2E8L67OqVggJ&ei=c9pXYvifEcLZmQHc1ovQAg&json=", "num_citations": 40, "citedby_url": "/scholar?cites=600855290019794904&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2E8L67OqVggJ:scholar.google.com/&scioq=Adaptive+Structural+Fingerprints+For+Graph+Attention+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJxWx0NYPr"}, "Toward Evaluating Robustness Of Deep Reinforcement Learning With Continuous Control": {"container_type": "Publication", "bib": {"title": "Toward evaluating robustness of deep reinforcement learning with continuous control", "author": ["TW Weng", "KD Dvijotham", "J Uesato", "K Xiao"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Deep reinforcement learning has achieved great success in many previously difficult reinforcement learning tasks, yet recent studies show that deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classification tasks. Prior works mostly focus on model-free adversarial attacks and agents with discrete actions. In this work, we study the problem of continuous control agents in deep RL with adversarial attacks and propose the first two-step algorithm based on learned model"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SylL0krYPS", "author_id": ["v8GM4xoAAAAJ", "1tOFY1IAAAAJ", "", "xblGvQgAAAAJ"], "url_scholarbib": "/scholar?q=info:M03Vg3hEY68J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DToward%2BEvaluating%2BRobustness%2BOf%2BDeep%2BReinforcement%2BLearning%2BWith%2BContinuous%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M03Vg3hEY68J&ei=dtpXYtGZMMWemAHB5baIBQ&json=", "num_citations": 9, "citedby_url": "/scholar?cites=12638020263730302259&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:M03Vg3hEY68J:scholar.google.com/&scioq=Toward+Evaluating+Robustness+Of+Deep+Reinforcement+Learning+With+Continuous+Control&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SylL0krYPS"}, "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality": {"container_type": "Publication", "bib": {"title": "Economy statistical recurrent units for inferring nonlinear granger causality", "author": ["S Khanna", "VYF Tan"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.09879", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.09879", "author_id": ["Ok2hzUEAAAAJ", "dJoAVvAAAAAJ"], "url_scholarbib": "/scholar?q=info:j6mQ0CJUK4cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEconomy%2BStatistical%2BRecurrent%2BUnits%2BFor%2BInferring%2BNonlinear%2BGranger%2BCausality%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j6mQ0CJUK4cJ&ei=edpXYtnQK4vEmgH7846QCg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=9739971127623592335&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:j6mQ0CJUK4cJ:scholar.google.com/&scioq=Economy+Statistical+Recurrent+Units+For+Inferring+Nonlinear+Granger+Causality&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.09879"}, "Information Geometry Of Orthogonal Initializations And Training": {"container_type": "Publication", "bib": {"title": "Information geometry of orthogonal initializations and training", "author": ["PA Sokol", "IM Park"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.03785", "abstract": "Recently mean field theory has been successfully used to analyze properties of wide, random neural networks. It gave rise to a prescriptive theory for initializing feed-forward neural networks with orthogonal weights, which ensures that both the forward propagated activations and the backpropagated gradients are near $\\ell_2 $ isometries and as a consequence training is orders of magnitude faster. Despite strong empirical performance, the mechanisms by which critical initializations confer an advantage in the optimization of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1810.03785", "author_id": ["MwySeOEAAAAJ", "CsmltusAAAAJ"], "url_scholarbib": "/scholar?q=info:AnR1QI85Up4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation%2BGeometry%2BOf%2BOrthogonal%2BInitializations%2BAnd%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AnR1QI85Up4J&ei=fNpXYtLhJZLeyQTms5KQBg&json=", "num_citations": 10, "citedby_url": "/scholar?cites=11408244093507433474&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AnR1QI85Up4J:scholar.google.com/&scioq=Information+Geometry+Of+Orthogonal+Initializations+And+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1810.03785"}, "On Universal Equivariant Set Networks": {"container_type": "Publication", "bib": {"title": "On universal equivariant set networks", "author": ["N Segol", "Y Lipman"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.02421", "abstract": "Using deep neural networks that are either invariant or equivariant to permutations in order to learn functions on unordered sets has become prevalent. The most popular, basic models are DeepSets [Zaheer et al. 2017] and PointNet [Qi et al. 2017]. While known to be universal for approximating invariant functions, DeepSets and PointNet are not known to be universal when approximating\\emph {equivariant} set functions. On the other hand, several recent equivariant set architectures have been proven equivariant universal [Sannai et al"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02421", "author_id": ["dTuhEVsAAAAJ", "vyteiT4AAAAJ"], "url_scholarbib": "/scholar?q=info:D-DZEByX8_EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BUniversal%2BEquivariant%2BSet%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D-DZEByX8_EJ&ei=f9pXYu7iM4vMsQK69Y7ABg&json=", "num_citations": 28, "citedby_url": "/scholar?cites=17434444729278914575&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D-DZEByX8_EJ:scholar.google.com/&scioq=On+Universal+Equivariant+Set+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02421"}, "Batch-shaping For Learning Conditional Channel Gated Networks": {"container_type": "Publication", "bib": {"title": "Batch-shaping for learning conditional channel gated networks", "author": ["BE Bejnordi", "T Blankevoort", "M Welling"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.06627", "abstract": "We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool $ batch $-$ shaping $ that matches the marginal aggregate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.06627", "author_id": ["Qk-AMk0AAAAJ", "OGEyrG8AAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:Edot_75FQ_EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBatch-shaping%2BFor%2BLearning%2BConditional%2BChannel%2BGated%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Edot_75FQ_EJ&ei=g9pXYpnQMcmUywTMkLbABQ&json=", "num_citations": 34, "citedby_url": "/scholar?cites=17384815673207544337&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Edot_75FQ_EJ:scholar.google.com/&scioq=Batch-shaping+For+Learning+Conditional+Channel+Gated+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.06627"}, "Option Discovery Using Deep Skill Chaining": {"container_type": "Publication", "bib": {"title": "Option discovery using deep skill chaining", "author": ["A Bagaria", "G Konidaris"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "Autonomously discovering temporally extended actions, or skills, is a longstanding goal of hierarchical reinforcement learning. We propose a new algorithm that combines skill chaining with deep neural networks to autonomously discover skills in high-dimensional, continuous domains. The resulting algorithm, deep skill chaining, constructs skills with the property that executing one enables the agent to execute another. We demonstrate that deep skill chaining significantly outperforms both non-hierarchical agents and other state-of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1gqipNYwH", "author_id": ["obLF_-IAAAAJ", "9UERvVEAAAAJ"], "url_scholarbib": "/scholar?q=info:tSOSxLDME7gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOption%2BDiscovery%2BUsing%2BDeep%2BSkill%2BChaining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tSOSxLDME7gJ&ei=h9pXYoOeEo2EmgH6u5u4BA&json=", "num_citations": 35, "citedby_url": "/scholar?cites=13264170387120464821&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tSOSxLDME7gJ:scholar.google.com/&scioq=Option+Discovery+Using+Deep+Skill+Chaining&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1gqipNYwH"}, "Self-supervised Learning Of Appliance Usage": {"container_type": "Publication", "bib": {"title": "Self-supervised learning of appliance usage", "author": ["CY Hsu", "A Zeitoun", "GH Lee", "D Katabi"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Learning home appliance usage is important for understanding people's activities and optimizing energy consumption. The problem is modeled as an event detection task, where the objective is to learn when a user turns an appliance on, and which appliance it is (microwave, hair dryer, etc.). Ideally, we would like to solve the problem in an unsupervised way so that the method can be applied to new homes and new appliances without any labels. To this end, we introduce a new deep learning model that takes input from two home"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1lJzyStvS", "author_id": ["zDax7zYAAAAJ", "", "1mdLkSMAAAAJ", "nst5fHgAAAAJ"], "url_scholarbib": "/scholar?q=info:n5-7qdOjGxMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-supervised%2BLearning%2BOf%2BAppliance%2BUsage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n5-7qdOjGxMJ&ei=itpXYrqnDMLZmQHc1ovQAg&json=", "num_citations": 9, "citedby_url": "/scholar?cites=1376874240572891039&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:n5-7qdOjGxMJ:scholar.google.com/&scioq=Self-supervised+Learning+Of+Appliance+Usage&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1lJzyStvS"}, "Multi-scale Representation Learning For Spatial Feature Distributions Using Grid Cells": {"container_type": "Publication", "bib": {"title": "Multi-scale representation learning for spatial feature distributions using grid cells", "author": ["G Mai", "K Janowicz", "B Yan", "R Zhu", "L Cai"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.00824", "author_id": ["X2Wfl1UAAAAJ", "6B2Z9vAAAAAJ", "lL_-M7IAAAAJ", "7ZN8prIAAAAJ", "g3_HTWsAAAAJ"], "url_scholarbib": "/scholar?q=info:i3ih7OGiv1EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-scale%2BRepresentation%2BLearning%2BFor%2BSpatial%2BFeature%2BDistributions%2BUsing%2BGrid%2BCells%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i3ih7OGiv1EJ&ei=jtpXYuLpHpGJmwGIxre4DA&json=", "num_citations": 35, "citedby_url": "/scholar?cites=5890605928845244555&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i3ih7OGiv1EJ:scholar.google.com/&scioq=Multi-scale+Representation+Learning+For+Spatial+Feature+Distributions+Using+Grid+Cells&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.00824"}, "Unpaired Point Cloud Completion On Real Scans Using Adversarial Training": {"container_type": "Publication", "bib": {"title": "Unpaired point cloud completion on real scans using adversarial training", "author": ["X Chen", "B Chen", "NJ Mitra"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.00069", "abstract": "As 3D scanning solutions become increasingly popular, several deep learning setups have been developed geared towards that task of scan completion, ie, plausibly filling in regions there were missed in the raw scans. These methods, however, largely rely on supervision in the form of paired training data, ie, partial scans with corresponding desired completed scans. While these methods have been successfully demonstrated on synthetic data, the approaches cannot be directly used on real scans in absence of suitable paired training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.00069", "author_id": ["", "iHWtrEAAAAAJ", "dPrZJWMAAAAJ"], "url_scholarbib": "/scholar?q=info:4zKAF4tLs1cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnpaired%2BPoint%2BCloud%2BCompletion%2BOn%2BReal%2BScans%2BUsing%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4zKAF4tLs1cJ&ei=kdpXYvDdOZLeyQTms5KQBg&json=", "num_citations": 45, "citedby_url": "/scholar?cites=6319477762897752803&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4zKAF4tLs1cJ:scholar.google.com/&scioq=Unpaired+Point+Cloud+Completion+On+Real+Scans+Using+Adversarial+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.00069"}, "Learning To Guide Random Search": {"container_type": "Publication", "bib": {"title": "Learning to guide random search", "author": ["O Sener", "V Koltun"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.12214", "abstract": "We specifically consider random search methods and extend them to the nonlinear  Then  we perform random search on this random manifold. ii) No online learning. We collect an offline"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.12214", "author_id": ["BI8xFr4AAAAJ", "kg4bCpgAAAAJ"], "url_scholarbib": "/scholar?q=info:Gq__5p1pbYsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BGuide%2BRandom%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Gq__5p1pbYsJ&ei=ldpXYqPUA-HDywTjooCQBQ&json=", "num_citations": 9, "citedby_url": "/scholar?cites=10046802470639742746&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Gq__5p1pbYsJ:scholar.google.com/&scioq=Learning+To+Guide+Random+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.12214"}, "Finding And Visualizing Weaknesses Of Deep Reinforcement Learning Agents": {"container_type": "Publication", "bib": {"title": "Finding and visualizing weaknesses of deep reinforcement learning agents", "author": ["C Rupprecht", "C Ibrahim", "CJ Pal"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.01318", "abstract": "As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.01318", "author_id": ["IrYlproAAAAJ", "ORU2iekAAAAJ", "1ScWJOoAAAAJ"], "url_scholarbib": "/scholar?q=info:hOrM5sWn258J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFinding%2BAnd%2BVisualizing%2BWeaknesses%2BOf%2BDeep%2BReinforcement%2BLearning%2BAgents%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hOrM5sWn258J&ei=mNpXYo76M5aM6rQPlISayA8&json=", "num_citations": 19, "citedby_url": "/scholar?cites=11518984940352760452&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hOrM5sWn258J:scholar.google.com/&scioq=Finding+And+Visualizing+Weaknesses+Of+Deep+Reinforcement+Learning+Agents&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.01318"}, "Jelly Bean World: A Testbed For Never-ending Learning": {"container_type": "Publication", "bib": {"title": "Jelly bean world: A testbed for never-ending learning", "author": ["EA Platanios", "A Saparov", "T Mitchell"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06306", "abstract": "Machine learning has shown growing success in recent years. However, current machine learning systems are highly specialized, trained for particular problems or domains, and typically on a single narrow dataset. Human learning, on the other hand, is highly general and adaptable. Never-ending learning is a machine learning paradigm that aims to bridge this gap, with the goal of encouraging researchers to design machine learning systems that can learn to perform a wider variety of inter-related tasks in more complex environments. To"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06306", "author_id": ["EFVk-mwAAAAJ", "TVNS71sAAAAJ", "MnfzuPYAAAAJ"], "url_scholarbib": "/scholar?q=info:Fb5h4G1MMMEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DJelly%2BBean%2BWorld:%2BA%2BTestbed%2BFor%2BNever-ending%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Fb5h4G1MMMEJ&ei=nNpXYqiaB-HDywTjooCQBQ&json=", "num_citations": 12, "citedby_url": "/scholar?cites=13920710483001851413&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Fb5h4G1MMMEJ:scholar.google.com/&scioq=Jelly+Bean+World:+A+Testbed+For+Never-ending+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06306"}, "Learning Efficient Parameter Server Synchronization Policies For Distributed Sgd": {"container_type": "Publication", "bib": {"title": "Learning efficient parameter server synchronization policies for distributed sgd", "author": ["R Zhu", "S Yang", "A Pfadler", "Z Qian"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "We apply a reinforcement learning (RL) based approach to learning optimal synchronization policies used for Parameter Server-based distributed training of machine learning models with Stochastic Gradient Descent (SGD). Utilizing a formal synchronization policy description in the PS-setting, we are able to derive a suitable and compact description of states and actions, allowing us to efficiently use the standard off-the-shelf deep Q-learning algorithm. As a result, we are able to learn synchronization policies which generalize to different cluster"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJxX8T4Kvr", "author_id": ["i0cC60cAAAAJ", "JzVNL8YAAAAJ", "MnEuVhQAAAAJ", "-7MZXG0AAAAJ"], "url_scholarbib": "/scholar?q=info:SQafLmtaE58J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BEfficient%2BParameter%2BServer%2BSynchronization%2BPolicies%2BFor%2BDistributed%2BSgd%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SQafLmtaE58J&ei=ptpXYrOeFciBy9YP18Gi8As&json=", "num_citations": 3, "citedby_url": "/scholar?cites=11462604892978218569&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SQafLmtaE58J:scholar.google.com/&scioq=Learning+Efficient+Parameter+Server+Synchronization+Policies+For+Distributed+Sgd&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJxX8T4Kvr"}, "A Meta-transfer Objective For Learning To Disentangle Causal Mechanisms": {"container_type": "Publication", "bib": {"title": "A meta-transfer objective for learning to disentangle causal mechanisms", "author": ["Y Bengio", "T Deleu", "N Rahaman", "R Ke"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, eg due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.10912", "author_id": ["kukA0LcAAAAJ", "nLNwh-wAAAAJ", "iH9DuY0AAAAJ", "dxwPYhQAAAAJ"], "url_scholarbib": "/scholar?q=info:Y-sd8GW1x-8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BMeta-transfer%2BObjective%2BFor%2BLearning%2BTo%2BDisentangle%2BCausal%2BMechanisms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Y-sd8GW1x-8J&ei=qdpXYqCnOMLZmQHc1ovQAg&json=", "num_citations": 188, "citedby_url": "/scholar?cites=17277977944855014243&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Y-sd8GW1x-8J:scholar.google.com/&scioq=A+Meta-transfer+Objective+For+Learning+To+Disentangle+Causal+Mechanisms&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.10912"}, "Neural Oblivious Decision Ensembles For Deep Learning On Tabular Data": {"container_type": "Publication", "bib": {"title": "Neural oblivious decision ensembles for deep learning on tabular data", "author": ["S Popov", "S Morozov", "A Babenko"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.06312", "abstract": "Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.06312", "author_id": ["", "", "q885d1wAAAAJ"], "url_scholarbib": "/scholar?q=info:32aKSrppUsUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BOblivious%2BDecision%2BEnsembles%2BFor%2BDeep%2BLearning%2BOn%2BTabular%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=32aKSrppUsUJ&ei=rdpXYqDdFM2Ny9YPqPyUgAs&json=", "num_citations": 65, "citedby_url": "/scholar?cites=14218543222397495007&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:32aKSrppUsUJ:scholar.google.com/&scioq=Neural+Oblivious+Decision+Ensembles+For+Deep+Learning+On+Tabular+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.06312"}, "Understanding Why Neural Networks Generalize Well Through Gsnr Of Parameters": {"container_type": "Publication", "bib": {"title": "Understanding why neural networks generalize well through gsnr of parameters", "author": ["J Liu", "G Jiang", "Y Bai", "T Chen", "H Wang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.07384", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.07384", "author_id": ["", "kCBHjI4AAAAJ", "", "", ""], "url_scholarbib": "/scholar?q=info:s6TIYXbaXIUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BWhy%2BNeural%2BNetworks%2BGeneralize%2BWell%2BThrough%2BGsnr%2BOf%2BParameters%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s6TIYXbaXIUJ&ei=sNpXYqXxFMLZmQHc1ovQAg&json=", "num_citations": 22, "citedby_url": "/scholar?cites=9609795906883331251&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s6TIYXbaXIUJ:scholar.google.com/&scioq=Understanding+Why+Neural+Networks+Generalize+Well+Through+Gsnr+Of+Parameters&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.07384"}, "Smoothness And Stability In Gans": {"container_type": "Publication", "bib": {"title": "Smoothness and stability in gans", "author": ["C Chu", "K Minami", "K Fukumizu"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.04185", "abstract": "satisfied by GANs, which are known to frequently destabilize and diverge during training.  To diagnose this instability, we consider the smoothness of the GAN\u2019s loss function. GANs are"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.04185", "author_id": ["J8olvlcAAAAJ", "3QYOhDoAAAAJ", "Dav2k7cAAAAJ"], "url_scholarbib": "/scholar?q=info:EP8Ue0UYUOMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmoothness%2BAnd%2BStability%2BIn%2BGans%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EP8Ue0UYUOMJ&ei=tNpXYovTAYvEmgH7846QCg&json=", "num_citations": 33, "citedby_url": "/scholar?cites=16379618531443277584&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EP8Ue0UYUOMJ:scholar.google.com/&scioq=Smoothness+And+Stability+In+Gans&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.04185"}, "Model-augmented Actor-critic: Backpropagating Through Paths": {"container_type": "Publication", "bib": {"title": "Model-augmented actor-critic: Backpropagating through paths", "author": ["I Clavera", "V Fu", "P Abbeel"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2005.08068", "abstract": "Current model-based reinforcement learning approaches use the model simply as a learned black-box simulator to augment the data for policy optimization or value function learning. In this paper, we show how to make more effective use of the model by exploiting its differentiability. We construct a policy optimization algorithm that uses the pathwise derivative of the learned model and policy across future timesteps. Instabilities of learning across many timesteps are prevented by using a terminal value function, learning the policy"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.08068", "author_id": ["yABlzrsAAAAJ", "", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:6z5CCfSOz-kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel-augmented%2BActor-critic:%2BBackpropagating%2BThrough%2BPaths%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6z5CCfSOz-kJ&ei=t9pXYuX1F8S4ywTtzb_QDA&json=", "num_citations": 42, "citedby_url": "/scholar?cites=16847841909794815723&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6z5CCfSOz-kJ:scholar.google.com/&scioq=Model-augmented+Actor-critic:+Backpropagating+Through+Paths&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.08068"}, "Reinforced Active Learning For Image Segmentation": {"container_type": "Publication", "bib": {"title": "Reinforced active learning for image segmentation", "author": ["A Casanova", "PO Pinheiro", "N Rostamzadeh"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06583", "author_id": ["iFhSTbAAAAAJ", "BU6f7L4AAAAJ", "t5ak3j0AAAAJ"], "url_scholarbib": "/scholar?q=info:bsO_lX-boA4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforced%2BActive%2BLearning%2BFor%2BImage%2BSegmentation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bsO_lX-boA4J&ei=udpXYqjsNJWMy9YPt8OamA0&json=", "num_citations": 36, "citedby_url": "/scholar?cites=1054013285080220526&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bsO_lX-boA4J:scholar.google.com/&scioq=Reinforced+Active+Learning+For+Image+Segmentation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06583"}, "Query-efficient Meta Attack To Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Query-efficient meta attack to deep neural networks", "author": ["J Du", "H Zhang", "JT Zhou", "Y Yang", "J Feng"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.02398", "abstract": "Black-box attack methods aim to infer suitable attack patterns to targeted DNN models by only using output feedback of the models and the corresponding input queries. However, due to lack of prior and inefficiency in leveraging the query and feedback information, existing methods are mostly query-intensive for obtaining effective attack patterns. In this work, we propose a meta attack approach that is capable of attacking a targeted model with much fewer queries. Its high queryefficiency stems from effective utilization of meta learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.02398", "author_id": ["WrJKEzEAAAAJ", "DkAZJX4AAAAJ", "cYNqDokAAAAJ", "RMSuNFwAAAAJ", "Q8iay0gAAAAJ"], "url_scholarbib": "/scholar?q=info:7mbdDZ3gDbUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuery-efficient%2BMeta%2BAttack%2BTo%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7mbdDZ3gDbUJ&ei=vdpXYqyMA8WemAHB5baIBQ&json=", "num_citations": 33, "citedby_url": "/scholar?cites=13046330660709295854&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7mbdDZ3gDbUJ:scholar.google.com/&scioq=Query-efficient+Meta+Attack+To+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.02398"}, "Deep Probabilistic Subsampling For Task-adaptive Compressed Sensing": {"container_type": "Publication", "bib": {"title": "Deep probabilistic subsampling for task-adaptive compressed sensing", "author": ["IAM Huijben", "BS Veeling"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the deep learning paradigm can be extended to incorporate a subsampling scheme that is jointly optimized under a desired minimum sample rate. We present Deep Probabilistic Subsampling (DPS), a widely applicable framework for task-adaptive compressed sensing that enables end-to end optimization of an optimal subset of signal"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJeq9JBFvH", "author_id": ["1ReBr6sAAAAJ", "qStzdQsAAAAJ"], "url_scholarbib": "/scholar?q=info:NIc5EyueP5oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BProbabilistic%2BSubsampling%2BFor%2BTask-adaptive%2BCompressed%2BSensing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NIc5EyueP5oJ&ei=wNpXYpT2B8WemAHB5baIBQ&json=", "num_citations": 18, "citedby_url": "/scholar?cites=11114776313216993076&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NIc5EyueP5oJ:scholar.google.com/&scioq=Deep+Probabilistic+Subsampling+For+Task-adaptive+Compressed+Sensing&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJeq9JBFvH"}, "Spectral Embedding Of Regularized Block Models": {"container_type": "Publication", "bib": {"title": "Spectral embedding of regularized block models", "author": ["N De Lara", "T Bonald"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.10903", "abstract": "Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10903", "author_id": ["f3CdXKcAAAAJ", "gw-JPVEAAAAJ"], "url_scholarbib": "/scholar?q=info:VhWuq9GXyJsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpectral%2BEmbedding%2BOf%2BRegularized%2BBlock%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VhWuq9GXyJsJ&ei=w9pXYtzPFMiBy9YP18Gi8As&json=", "num_citations": 5, "citedby_url": "/scholar?cites=11225388998005232982&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VhWuq9GXyJsJ:scholar.google.com/&scioq=Spectral+Embedding+Of+Regularized+Block+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10903"}, "Mirror-generative Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Mirror-generative neural machine translation", "author": ["Z Zheng", "H Zhou", "S Huang", "L Li", "XY Dai"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HkxQRTNYPH", "author_id": ["JPSrehMAAAAJ", "", "HF3-E9kAAAAJ", "1B0l7U8AAAAJ", "zpWB1CgAAAAJ"], "url_scholarbib": "/scholar?q=info:Q0hUKa469f0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMirror-generative%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Q0hUKa469f0J&ei=xtpXYoqyHpWMy9YPt8OamA0&json=", "num_citations": 29, "citedby_url": "/scholar?cites=18299597180581988419&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Q0hUKa469f0J:scholar.google.com/&scioq=Mirror-generative+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HkxQRTNYPH"}, "Federated Learning With Matched Averaging": {"container_type": "Publication", "bib": {"title": "Federated learning with matched averaging", "author": ["H Wang", "M Yurochkin", "Y Sun", "D Papailiopoulos"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures eg convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (ie channels for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06440", "author_id": ["zYdZORsAAAAJ", "QjBF9sUAAAAJ", "6T1XtW8AAAAJ", "hYi6i9sAAAAJ"], "url_scholarbib": "/scholar?q=info:txoYVwjMp1IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFederated%2BLearning%2BWith%2BMatched%2BAveraging%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=txoYVwjMp1IJ&ei=ytpXYt-6Ds2Ny9YPqPyUgAs&json=", "num_citations": 255, "citedby_url": "/scholar?cites=5955953368413772471&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:txoYVwjMp1IJ:scholar.google.com/&scioq=Federated+Learning+With+Matched+Averaging&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06440.pdf?ref=https://githubhelp.com"}, "Residual Energy-based Models For Text Generation": {"container_type": "Publication", "bib": {"title": "Residual energy-based models for text generation", "author": ["Y Deng", "A Bakhtin", "M Ott", "A Szlam"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.11714", "author_id": ["tk0e5lYAAAAJ", "50O3v1MAAAAJ", "5LtyqOQAAAAJ", "u3-FxUgAAAAJ"], "url_scholarbib": "/scholar?q=info:lZ1jlYERe7gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DResidual%2BEnergy-based%2BModels%2BFor%2BText%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lZ1jlYERe7gJ&ei=zdpXYvH0J46pywSdh6agAg&json=", "num_citations": 56, "citedby_url": "/scholar?cites=13293237973368937877&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lZ1jlYERe7gJ:scholar.google.com/&scioq=Residual+Energy-based+Models+For+Text+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.11714"}, "Latent Normalizing Flows For Many-to-many Cross Domain Mappings": {"container_type": "Publication", "bib": {"title": "Latent normalizing flows for many-to-many cross-domain mappings", "author": ["S Mahajan", "I Gurevych", "S Roth"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06661", "abstract": "Learned joint representations of images and text form the backbone of several important cross-domain tasks such as image captioning. Prior work mostly maps both domains into a common latent representation in a purely supervised fashion. This is rather restrictive, however, as the two domains follow distinct generative processes. Therefore, we propose a novel semi-supervised framework, which models shared information between domains and domain-specific information separately. The information shared between the domains is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06661", "author_id": ["DUKzkPMAAAAJ", "t3A39e8AAAAJ", "0yDoR0AAAAAJ"], "url_scholarbib": "/scholar?q=info:y2_2cocBrjEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BNormalizing%2BFlows%2BFor%2BMany-to-many%2BCross%2BDomain%2BMappings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y2_2cocBrjEJ&ei=0dpXYo8XwtmZAdzWi9AC&json=", "num_citations": 14, "citedby_url": "/scholar?cites=3579800435067088843&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:y2_2cocBrjEJ:scholar.google.com/&scioq=Latent+Normalizing+Flows+For+Many-to-many+Cross+Domain+Mappings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06661"}, "State-only Imitation With Transition Dynamics Mismatch": {"container_type": "Publication", "bib": {"title": "State-only imitation with transition dynamics mismatch", "author": ["T Gangwani", "J Peng"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.11879", "abstract": "Imitation Learning (IL) is a popular paradigm for training agents to achieve complicated goals by leveraging expert behavior, rather than dealing with the hardships of designing a correct reward function. With the environment modeled as a Markov Decision Process (MDP), most of the existing IL algorithms are contingent on the availability of expert demonstrations in the same MDP as the one in which a new imitator policy is to be learned. This is uncharacteristic of many real-life scenarios where discrepancies between the expert"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.11879", "author_id": ["IUY5oVkAAAAJ", "H2JX-RQAAAAJ"], "url_scholarbib": "/scholar?q=info:gD4b2ehBnssJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DState-only%2BImitation%2BWith%2BTransition%2BDynamics%2BMismatch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gD4b2ehBnssJ&ei=1NpXYv6ZD8mUywTMkLbABQ&json=", "num_citations": 19, "citedby_url": "/scholar?cites=14672237104350314112&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gD4b2ehBnssJ:scholar.google.com/&scioq=State-only+Imitation+With+Transition+Dynamics+Mismatch&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.11879"}, "A Fair Comparison Of Graph Neural Networks For Graph Classification": {"container_type": "Publication", "bib": {"title": "A fair comparison of graph neural networks for graph classification", "author": ["F Errica", "M Podda", "D Bacciu", "A Micheli"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.09893", "abstract": "Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works. As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.09893", "author_id": ["VJ0n2gQAAAAJ", "pDMrAqgAAAAJ", "1d5n2WkAAAAJ", "rnaNixYAAAAJ"], "url_scholarbib": "/scholar?q=info:CL8e4hnySzUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BFair%2BComparison%2BOf%2BGraph%2BNeural%2BNetworks%2BFor%2BGraph%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CL8e4hnySzUJ&ei=2NpXYuCHJsLZmQHc1ovQAg&json=", "num_citations": 189, "citedby_url": "/scholar?cites=3840429300245249800&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CL8e4hnySzUJ:scholar.google.com/&scioq=A+Fair+Comparison+Of+Graph+Neural+Networks+For+Graph+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.09893"}, "Breaking Certified Defenses: Semantic Adversarial Examples With Spoofed Robustness Certificates": {"container_type": "Publication", "bib": {"title": "Breaking certified defenses: Semantic adversarial examples with spoofed robustness certificates", "author": ["A Ghiasi", "A Shafahi", "T Goldstein"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2003.08937", "abstract": "To deflect adversarial attacks, a range of\" certified\" classifiers have been proposed. In addition to labeling an image, certified classifiers produce (when possible) a certificate guaranteeing that the input image is not an $\\ell_p $-bounded adversarial example. We present a new attack that exploits not only the labelling function of a classifier, but also the certificate generator. The proposed method applies large perturbations that place images far from a class boundary while maintaining the imperceptibility property of adversarial"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.08937", "author_id": ["tNQWOxUAAAAJ", "5Jnk00MAAAAJ", "KmSuVtgAAAAJ"], "url_scholarbib": "/scholar?q=info:vryP05sorNMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBreaking%2BCertified%2BDefenses:%2BSemantic%2BAdversarial%2BExamples%2BWith%2BSpoofed%2BRobustness%2BCertificates%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vryP05sorNMJ&ei=29pXYvjYN86E6rQPz8uiuAc&json=", "num_citations": 31, "citedby_url": "/scholar?cites=15252610687731481790&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:vryP05sorNMJ:scholar.google.com/&scioq=Breaking+Certified+Defenses:+Semantic+Adversarial+Examples+With+Spoofed+Robustness+Certificates&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.08937"}, "Deep Graph Matching Consensus": {"container_type": "Publication", "bib": {"title": "Deep graph matching consensus", "author": ["M Fey", "JE Lenssen", "C Morris", "J Masci"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Here, we propose a fully-differentiable graph matching procedure which aims to reach a  data-driven neighborhood consensus between matched node pairs without the need to solve"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.09621", "author_id": ["5HaSBN0AAAAJ", "enXCzCgAAAAJ", "3f6_I8MAAAAJ", "HwDTzQEAAAAJ"], "url_scholarbib": "/scholar?q=info:whRpBcDb8b8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BGraph%2BMatching%2BConsensus%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=whRpBcDb8b8J&ei=39pXYuSOHs2Ny9YPqPyUgAs&json=", "num_citations": 106, "citedby_url": "/scholar?cites=13831077548402480322&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:whRpBcDb8b8J:scholar.google.com/&scioq=Deep+Graph+Matching+Consensus&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.09621"}, "Monotonic Multihead Attention": {"container_type": "Publication", "bib": {"title": "Monotonic multihead attention", "author": ["X Ma", "J Pino", "J Cross", "L Puzon", "J Gu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12406", "abstract": ", or a learnable monotonic attention on a weaker recurrent  attention mechanism, Monotonic  Multihead Attention (MMA), which extends the monotonic attention mechanism to multihead"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12406", "author_id": ["al5bfIwAAAAJ", "weU_-4IAAAAJ", "jsBUMOMAAAAJ", "", "cB1mFBsAAAAJ"], "url_scholarbib": "/scholar?q=info:Cqulgi4qud0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMonotonic%2BMultihead%2BAttention%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Cqulgi4qud0J&ei=4tpXYr7VNcLZmQHc1ovQAg&json=", "num_citations": 57, "citedby_url": "/scholar?cites=15976847532322302730&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Cqulgi4qud0J:scholar.google.com/&scioq=Monotonic+Multihead+Attention&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12406.pdf?ref=https://githubhelp.com"}, "Unsupervised Model Selection For Variational Disentangled Representation Learning": {"container_type": "Publication", "bib": {"title": "Unsupervised model selection for variational disentangled representation learning", "author": ["S Duan", "L Matthey", "A Saraiva", "N Watters"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.12614", "author_id": ["BT2OfOMAAAAJ", "f520HmwAAAAJ", "-_yUSOoAAAAJ", "2OSq4Q0AAAAJ"], "url_scholarbib": "/scholar?q=info:4Dc5lLY1Cv8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BModel%2BSelection%2BFor%2BVariational%2BDisentangled%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4Dc5lLY1Cv8J&ei=5tpXYsi5LpWMy9YPt8OamA0&json=", "num_citations": 31, "citedby_url": "/scholar?cites=18377560287725828064&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4Dc5lLY1Cv8J:scholar.google.com/&scioq=Unsupervised+Model+Selection+For+Variational+Disentangled+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.12614"}, "Macer: Attack-free And Scalable Robust Training Via Maximizing Certified Radius": {"container_type": "Publication", "bib": {"title": "Macer: Attack-free and scalable robust training via maximizing certified radius", "author": ["R Zhai", "C Dan", "D He", "H Zhang", "B Gong"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide a certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02378", "author_id": ["EXd0ES8AAAAJ", "hQQFfuwAAAAJ", "orVoz4IAAAAJ", "LTa3GzEAAAAJ", "lv9ZeVUAAAAJ"], "url_scholarbib": "/scholar?q=info:md7BNrKCh_UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMacer:%2BAttack-free%2BAnd%2BScalable%2BRobust%2BTraining%2BVia%2BMaximizing%2BCertified%2BRadius%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=md7BNrKCh_UJ&ei=6tpXYqKtJovMsQK69Y7ABg&json=", "num_citations": 66, "citedby_url": "/scholar?cites=17692253363082747545&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:md7BNrKCh_UJ:scholar.google.com/&scioq=Macer:+Attack-free+And+Scalable+Robust+Training+Via+Maximizing+Certified+Radius&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02378"}, "Convolutional Conditional Neural Processes": {"container_type": "Publication", "bib": {"title": "Convolutional conditional neural processes", "author": ["J Gordon", "WP Bruinsma", "AYK Foong"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "convolution to the context mask to form the density channel: h(0) = CONV\u03b8(Mc) (Figure 1c,  line 4). To all other channels, we apply a normalized convolution over existing neural process"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.13556", "author_id": ["IZGi3pEAAAAJ", "QRQwz3cAAAAJ", "2UOjgIUAAAAJ"], "url_scholarbib": "/scholar?q=info:sEIj9OZnw6wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConvolutional%2BConditional%2BNeural%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sEIj9OZnw6wJ&ei=7dpXYvnCF8WemAHB5baIBQ&json=", "num_citations": 47, "citedby_url": "/scholar?cites=12448908036618273456&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sEIj9OZnw6wJ:scholar.google.com/&scioq=Convolutional+Conditional+Neural+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.13556"}, "Plug And Play Language Model: A Simple Baseline For Controlled Language Generation": {"container_type": "Publication", "bib": {"title": "Plug and play language models: A simple approach to controlled text generation", "author": ["S Dathathri", "A Madotto", "J Lan", "J Hung", "E Frank"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable  language generation, which combines a pretrained LM  We evaluate WD as a baseline."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02164", "author_id": ["", "JBnyLicAAAAJ", "L5jDQS8AAAAJ", "BN7CL1AAAAAJ", "b1vTmkAAAAAJ"], "url_scholarbib": "/scholar?q=info:4FG95BNitYgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPlug%2BAnd%2BPlay%2BLanguage%2BModel:%2BA%2BSimple%2BBaseline%2BFor%2BControlled%2BLanguage%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4FG95BNitYgJ&ei=8NpXYuuSEo6pywSdh6agAg&json=", "num_citations": 239, "citedby_url": "/scholar?cites=9850887597524341216&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4FG95BNitYgJ:scholar.google.com/&scioq=Plug+And+Play+Language+Model:+A+Simple+Baseline+For+Controlled+Language+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02164"}, "Continual Learning With Hypernetworks": {"container_type": "Publication", "bib": {"title": "Continual learning with hypernetworks", "author": ["J Von Oswald", "C Henning", "J Sacramento"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Hypernetworks parameterize target models. The centerpiece of our approach to continual  learning is the hypernetwork, Fig. 1a. Instead of learning the parameters \u0398trgt of a particular"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.00695", "author_id": ["jdnL-PgAAAAJ", "u6QSFrsAAAAJ", "9hpcmYUAAAAJ"], "url_scholarbib": "/scholar?q=info:xNnh59eqh7IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContinual%2BLearning%2BWith%2BHypernetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xNnh59eqh7IJ&ei=89pXYqLjCM2Ny9YPqPyUgAs&json=", "num_citations": 113, "citedby_url": "/scholar?cites=12864438704892139972&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xNnh59eqh7IJ:scholar.google.com/&scioq=Continual+Learning+With+Hypernetworks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.00695"}, "Behaviour Suite For Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Behaviour suite for reinforcement learning", "author": ["I Osband", "Y Doron", "M Hessel", "J Aslanides"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github. com/deepmind/bsuite, which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.03568", "author_id": ["QA4o6eYAAAAJ", "0jxjiqMAAAAJ", "odVYodIAAAAJ", "jWIWqfcAAAAJ"], "url_scholarbib": "/scholar?q=info:PT6FZBQtUZEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBehaviour%2BSuite%2BFor%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PT6FZBQtUZEJ&ei=99pXYvCnLZmM6rQPjaOSEA&json=", "num_citations": 92, "citedby_url": "/scholar?cites=10471200174222163517&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PT6FZBQtUZEJ:scholar.google.com/&scioq=Behaviour+Suite+For+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.03568"}, "Decoding As Dynamic Programming For Recurrent Autoregressive Models": {"container_type": "Publication", "bib": {"title": "Decoding as dynamic programming for recurrent autoregressive models", "author": ["N Zaidi", "T Cohn", "G Haffari"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model. Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings. Our method"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HklOo0VFDH", "author_id": ["RV3idtYAAAAJ", "FCom398AAAAJ", "Perjx5EAAAAJ"], "url_scholarbib": "/scholar?q=info:MM7HQQia_BEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecoding%2BAs%2BDynamic%2BProgramming%2BFor%2BRecurrent%2BAutoregressive%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MM7HQQia_BEJ&ei=_NpXYvWPJY2EmgH6u5u4BA&json=", "num_citations": 1, "citedby_url": "/scholar?cites=1296080153029889584&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MM7HQQia_BEJ:scholar.google.com/&scioq=Decoding+As+Dynamic+Programming+For+Recurrent+Autoregressive+Models&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HklOo0VFDH"}, "A Constructive Prediction Of The Generalization Error Across Scales": {"container_type": "Publication", "bib": {"title": "A constructive prediction of the generalization error across scales", "author": ["JS Rosenfeld", "A Rosenfeld", "Y Belinkov"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (eg, width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12673", "author_id": ["8HYnMeYAAAAJ", "Rspq7eQAAAAJ", "K-6ujU4AAAAJ"], "url_scholarbib": "/scholar?q=info:outxwp-SrRsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BConstructive%2BPrediction%2BOf%2BThe%2BGeneralization%2BError%2BAcross%2BScales%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=outxwp-SrRsJ&ei=ANtXYreOB5LeyQTms5KQBg&json=", "num_citations": 52, "citedby_url": "/scholar?cites=1994411424854698914&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:outxwp-SrRsJ:scholar.google.com/&scioq=A+Constructive+Prediction+Of+The+Generalization+Error+Across+Scales&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12673"}, "Diverse Trajectory Forecasting With Determinantal Point Processes": {"container_type": "Publication", "bib": {"title": "Diverse trajectory forecasting with determinantal point processes", "author": ["Y Yuan", "K Kitani"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.04967", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (eg, future trajectories of a pedestrian) is essential for safety-critical perception systems (eg, autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a single outcome. While generative"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.04967", "author_id": ["EEp82sIAAAAJ", "yv3sH74AAAAJ"], "url_scholarbib": "/scholar?q=info:5jLEb0uamt8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiverse%2BTrajectory%2BForecasting%2BWith%2BDeterminantal%2BPoint%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5jLEb0uamt8J&ei=BttXYuCtGcWemAHB5baIBQ&json=", "num_citations": 62, "citedby_url": "/scholar?cites=16112360265659724518&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5jLEb0uamt8J:scholar.google.com/&scioq=Diverse+Trajectory+Forecasting+With+Determinantal+Point+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.04967"}, "Rotation-invariant Clustering Of Functional Cell Types In Primary Visual Cortex": {"container_type": "Publication", "bib": {"title": "Rotation-invariant clustering of neuronal responses in primary visual cortex", "author": ["I Ustyuzhaninov", "SA Cadena", "E Froudarakis"], "pub_year": "2019", "venue": "International \u2026", "abstract": "to clustering neurons into putative functional cell types invariant to location and orientation  of their receptive field. We find around 10\u201320 functional clusters,  the V1 functional cell types,"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rklr9kHFDB", "author_id": ["YGEMpYUAAAAJ", "_XJDXjEAAAAJ", "peGMCr8AAAAJ"], "url_scholarbib": "/scholar?q=info:WfVOOv1H2QUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRotation-invariant%2BClustering%2BOf%2BFunctional%2BCell%2BTypes%2BIn%2BPrimary%2BVisual%2BCortex%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WfVOOv1H2QUJ&ei=CdtXYt65C5GJmwGIxre4DA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=421447193066403161&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WfVOOv1H2QUJ:scholar.google.com/&scioq=Rotation-invariant+Clustering+Of+Functional+Cell+Types+In+Primary+Visual+Cortex&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rklr9kHFDB"}, "Variational Hetero-encoder Randomized Gans For Joint Image-text Modeling": {"container_type": "Publication", "bib": {"title": "Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling", "author": ["H Zhang", "B Chen", "L Tian", "Z Wang", "M Zhou"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "For bidirectional joint image-text modeling, we develop variational hetero-encoder (VHE) randomized generative adversarial network (GAN), a versatile deep generative model that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into a coherent end-to-end multi-modality learning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its associated text, and feeds the variational posterior as the source of randomness into the GAN image generator. We plug three off-the-shelf modules, including a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.08622", "author_id": ["Eo8e5icAAAAJ", "uv16_-UAAAAJ", "", "", "LXwCIisAAAAJ"], "url_scholarbib": "/scholar?q=info:kegvFw0JM1cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BHetero-encoder%2BRandomized%2BGans%2BFor%2BJoint%2BImage-text%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kegvFw0JM1cJ&ei=DNtXYu7oFc6E6rQPz8uiuAc&json=", "num_citations": 8, "citedby_url": "/scholar?cites=6283375856940214417&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kegvFw0JM1cJ:scholar.google.com/&scioq=Variational+Hetero-encoder+Randomized+Gans+For+Joint+Image-text+Modeling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.08622"}, "Slomo: Improving Communication-efficient Distributed Sgd With Slow Momentum": {"container_type": "Publication", "bib": {"title": "SlowMo: Improving communication-efficient distributed SGD with slow momentum", "author": ["J Wang", "V Tantia", "N Ballas", "M Rabbat"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.00643", "abstract": "Distributed optimization is essential for training large models on large datasets. Multiple approaches have been proposed to reduce the communication overhead in distributed training, such as synchronizing only after performing multiple local SGD steps, and decentralized methods (eg, using gossip algorithms) to decouple communications among workers. Although these methods run faster than AllReduce-based methods, which use blocking communication before every update, the resulting models may be less accurate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.00643", "author_id": ["5nrx1YwAAAAJ", "xL66xLAAAAAJ", "euUV4iUAAAAJ", "cMPKe9UAAAAJ"], "url_scholarbib": "/scholar?q=info:jmE8bo9YE9UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSlomo:%2BImproving%2BCommunication-efficient%2BDistributed%2BSgd%2BWith%2BSlow%2BMomentum%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jmE8bo9YE9UJ&ei=EdtXYsqhD4ySyATlkbrQCA&json=", "num_citations": 74, "citedby_url": "/scholar?cites=15353712927689171342&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jmE8bo9YE9UJ:scholar.google.com/&scioq=Slomo:+Improving+Communication-efficient+Distributed+Sgd+With+Slow+Momentum&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.00643"}, "The Asymptotic Spectrum Of The Hessian Of Dnn Throughout Training": {"container_type": "Publication", "bib": {"title": "The asymptotic spectrum of the hessian of dnn throughout training", "author": ["A Jacot", "F Gabriel", "C Hongler"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.02875", "abstract": "The dynamics of DNNs during gradient descent is described by the so-called Neural Tangent Kernel (NTK). In this article, we show that the NTK allows one to gain precise insight into the Hessian of the cost of DNNs. When the NTK is fixed during training, we obtain a full characterization of the asymptotics of the spectrum of the Hessian, at initialization and during training. In the so-called mean-field limit, where the NTK is not fixed during training, we describe the first two moments of the Hessian at initialization."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02875", "author_id": ["G6OhFawAAAAJ", "Y8mM04oAAAAJ", "p9B6eWEAAAAJ"], "url_scholarbib": "/scholar?q=info:_ZhMTW18T6kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BAsymptotic%2BSpectrum%2BOf%2BThe%2BHessian%2BOf%2BDnn%2BThroughout%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_ZhMTW18T6kJ&ei=FeJXYujTEY6pywSdh6agAg&json=", "num_citations": 18, "citedby_url": "/scholar?cites=12200106724460108029&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_ZhMTW18T6kJ:scholar.google.com/&scioq=The+Asymptotic+Spectrum+Of+The+Hessian+Of+Dnn+Throughout+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02875"}, "A Target-agnostic Attack On Deep Models: Exploiting Security Vulnerabilities Of Transfer Learning": {"container_type": "Publication", "bib": {"title": "A target-agnostic attack on deep models: Exploiting security vulnerabilities of transfer learning", "author": ["S Rezaei", "X Liu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.04334", "abstract": "Due to insufficient training data and the high computational cost to train a deep neural network from scratch, transfer learning has been extensively used in many deep-neural-network-based applications. A commonly used transfer learning approach involves taking a part of a pre-trained model, adding a few layers at the end, and re-training the new layers with a small dataset. This approach, while efficient and widely used, imposes a security vulnerability because the pre-trained model used in transfer learning is usually publicly"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.04334", "author_id": ["dSQ7ka8AAAAJ", "4MV5BkQAAAAJ"], "url_scholarbib": "/scholar?q=info:G8VvPcgfetwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BTarget-agnostic%2BAttack%2BOn%2BDeep%2BModels:%2BExploiting%2BSecurity%2BVulnerabilities%2BOf%2BTransfer%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G8VvPcgfetwJ&ei=IuJXYpfbBpaM6rQPlISayA8&json=", "num_citations": 26, "citedby_url": "/scholar?cites=15887045580387501339&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:G8VvPcgfetwJ:scholar.google.com/&scioq=A+Target-agnostic+Attack+On+Deep+Models:+Exploiting+Security+Vulnerabilities+Of+Transfer+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.04334"}, "Meta-learning Curiosity Algorithms": {"container_type": "Publication", "bib": {"title": "Meta-learning curiosity algorithms", "author": ["F Alet", "MF Schneider", "T Lozano-Perez"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "of generating curious behavior as one of meta-learning: an outer  a space of algorithms for  generating curious behavior by  In this meta-learning setting, our objective is to find a curiosity"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.05325", "author_id": ["1lmBq3QAAAAJ", "", "gQOKAggAAAAJ"], "url_scholarbib": "/scholar?q=info:QC6ODnMOSA0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BCuriosity%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QC6ODnMOSA0J&ei=KeJXYvQajYSaAfq7m7gE&json=", "num_citations": 28, "citedby_url": "/scholar?cites=957030808144457280&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QC6ODnMOSA0J:scholar.google.com/&scioq=Meta-learning+Curiosity+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.05325"}, "Learn To Explain Efficiently Via Neural Logic Inductive Learning": {"container_type": "Publication", "bib": {"title": "Learn to explain efficiently via neural logic inductive learning", "author": ["Y Yang", "L Song"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.02481", "abstract": "The capability of making interpretable and self-explanatory decisions is essential for developing responsible machine learning systems. In this work, we study the learning to explain problem in the scope of inductive logic programming (ILP). We propose Neural Logic Inductive Learning (NLIL), an efficient differentiable ILP framework that learns first-order logic rules that can explain the patterns in the data. In experiments, compared with the state-of-the-art methods, we find NLIL can search for rules that are x10 times longer while"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02481", "author_id": ["Lt4tmL8AAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:tfN28670Jz8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearn%2BTo%2BExplain%2BEfficiently%2BVia%2BNeural%2BLogic%2BInductive%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tfN28670Jz8J&ei=LOJXYqb6HpaM6rQPlISayA8&json=", "num_citations": 28, "citedby_url": "/scholar?cites=4550874980727321525&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tfN28670Jz8J:scholar.google.com/&scioq=Learn+To+Explain+Efficiently+Via+Neural+Logic+Inductive+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02481"}, "On Computation And Generalization Of Gener- Ative Adversarial Imitation Learning": {"container_type": "Publication", "bib": {"title": "On computation and generalization of generative adversarial imitation learning", "author": ["M Chen", "Y Wang", "T Liu", "Z Yang", "X Li", "Z Wang"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative Adversarial Imitation Learning (GAIL) is a powerful and practical approach for learning sequential decision-making policies. Different from Reinforcement Learning (RL), GAIL takes advantage of demonstration data by experts (eg, human), and learns both the policy and reward function of the unknown environment. Despite the significant empirical progresses, the theory behind GAIL is still largely unknown. The major difficulty comes from the underlying temporal dependency of the demonstration data and the minimax"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02792", "author_id": ["qU9WvTgAAAAJ", "H4kqV1MAAAAJ", "dw5f9yIAAAAJ", "", "MkLPs5EAAAAJ", "HSx0BgQAAAAJ"], "url_scholarbib": "/scholar?q=info:kC0vlE2dJwcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BComputation%2BAnd%2BGeneralization%2BOf%2BGener-%2BAtive%2BAdversarial%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kC0vlE2dJwcJ&ei=L-JXYuPFGZWMy9YPt8OamA0&json=", "num_citations": 17, "citedby_url": "/scholar?cites=515553638881373584&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kC0vlE2dJwcJ:scholar.google.com/&scioq=On+Computation+And+Generalization+Of+Gener-+Ative+Adversarial+Imitation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02792"}, "Improving Adversarial Robustness Requires Revisiting Misclassified Examples": {"container_type": "Publication", "bib": {"title": "Improving adversarial robustness requires revisiting misclassified examples", "author": ["Y Wang", "D Zou", "J Yi", "J Bailey", "X Ma"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rklOg6EFwS", "author_id": ["uMWPDboAAAAJ", "Cp4fcTQAAAAJ", "lZxRZ84AAAAJ", "ujsYC98AAAAJ", "XQViiyYAAAAJ"], "url_scholarbib": "/scholar?q=info:cg-hx_TMiWIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BAdversarial%2BRobustness%2BRequires%2BRevisiting%2BMisclassified%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cg-hx_TMiWIJ&ei=MuJXYrjSK5mM6rQPjaOSEA&json=", "num_citations": 215, "citedby_url": "/scholar?cites=7100431639219605362&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cg-hx_TMiWIJ:scholar.google.com/&scioq=Improving+Adversarial+Robustness+Requires+Revisiting+Misclassified+Examples&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rklOg6EFwS"}, "Unrestricted Adversarial Examples Via Semantic Manipulation": {"container_type": "Publication", "bib": {"title": "Unrestricted adversarial examples via semantic manipulation", "author": ["A Bhattad", "MJ Chong", "K Liang", "B Li"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Machine learning models, especially deep neural networks (DNNs), have been shown to be vulnerable against adversarial examples which are carefully crafted samples with a small magnitude of the perturbation. Such adversarial perturbations are usually restricted by bounding their $\\mathcal {L} _p $ norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact. In this paper, we instead introduce\" unrestricted\" perturbations that manipulate semantically meaningful"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.06347", "author_id": ["XUsauXIAAAAJ", "sSm_SkgAAAAJ", "_vRde4EAAAAJ", "K8vJkTcAAAAJ"], "url_scholarbib": "/scholar?q=info:bMAA5MnulQgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnrestricted%2BAdversarial%2BExamples%2BVia%2BSemantic%2BManipulation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bMAA5MnulQgJ&ei=NeJXYqvqAovMsQK69Y7ABg&json=", "num_citations": 73, "citedby_url": "/scholar?cites=618663074714402924&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bMAA5MnulQgJ:scholar.google.com/&scioq=Unrestricted+Adversarial+Examples+Via+Semantic+Manipulation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.06347"}, "Learning Expensive Coordination: An Event-based Deep Rl Approach": {"container_type": "Publication", "bib": {"title": "Learning expensive coordination: An event-based deep RL approach", "author": ["Z Shi", "R Yu", "X Wang", "R Wang", "Y Zhang"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, ie, the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryeG924twB", "author_id": ["", "", "ROANfPUAAAAJ", "JEVpgE8AAAAJ", "i2j5DmwAAAAJ"], "url_scholarbib": "/scholar?q=info:mYIajZwGJxAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BExpensive%2BCoordination:%2BAn%2BEvent-based%2BDeep%2BRl%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mYIajZwGJxAJ&ei=QeJXYpiUIJaM6rQPlISayA8&json=", "num_citations": 6, "citedby_url": "/scholar?cites=1163906298150552217&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mYIajZwGJxAJ:scholar.google.com/&scioq=Learning+Expensive+Coordination:+An+Event-based+Deep+Rl+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryeG924twB"}, "Shifted And Squeezed 8-bit Floating Point Format For Low-precision Training Of Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks", "author": ["L Cambier", "A Bhiwandiwalla", "T Gong", "M Nekuii"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Training with larger number of parameters while keeping fast iterations is an increasingly adopted strategy and trend for developing better performing Deep Neural Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit floating point (FP8) numbers. Reduced bit precision allows for a larger effective memory and increased computational speed. We name this method Shifted and Squeezed FP8"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.05674", "author_id": ["gx_sbUwAAAAJ", "N-Qoq1gAAAAJ", "22hlGRIAAAAJ", ""], "url_scholarbib": "/scholar?q=info:14ErYXYaeJ4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DShifted%2BAnd%2BSqueezed%2B8-bit%2BFloating%2BPoint%2BFormat%2BFor%2BLow-precision%2BTraining%2BOf%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=14ErYXYaeJ4J&ei=ReJXYvrHBY6pywSdh6agAg&json=", "num_citations": 22, "citedby_url": "/scholar?cites=11418905950936596951&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:14ErYXYaeJ4J:scholar.google.com/&scioq=Shifted+And+Squeezed+8-bit+Floating+Point+Format+For+Low-precision+Training+Of+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.05674"}, "On The Convergence Of Fedavg On Non-iid Data": {"container_type": "Publication", "bib": {"title": "On the convergence of fedavg on non-iid data", "author": ["X Li", "K Huang", "W Yang", "S Wang", "Z Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt {FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of\\texttt {FedAvg} on non-iid data and establish a convergence rate of $\\mathcal {O}(\\frac"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.02189", "author_id": ["UR7uLq8AAAAJ", "EfxwV6oAAAAJ", "-GQEMJ8AAAAJ", "HAf4pEoAAAAJ", "M5YT8IoAAAAJ"], "url_scholarbib": "/scholar?q=info:q6LivDSwijYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BConvergence%2BOf%2BFedavg%2BOn%2BNon-iid%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=q6LivDSwijYJ&ei=SOJXYs-NOo6pywSdh6agAg&json=", "num_citations": 616, "citedby_url": "/scholar?cites=3930147365387936427&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:q6LivDSwijYJ:scholar.google.com/&scioq=On+The+Convergence+Of+Fedavg+On+Non-iid+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.02189"}, "Sliced Cramer Synaptic Consolidation For Preserving Deeply Learned Representations": {"container_type": "Publication", "bib": {"title": "Sliced cramer synaptic consolidation for preserving deeply learned representations", "author": ["S Kolouri", "NA Ketz", "A Soltoggio"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (ie, catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training. Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJge3TNKwH", "author_id": ["yREBSy0AAAAJ", "VOvR0i8AAAAJ", "mlt7uksAAAAJ"], "url_scholarbib": "/scholar?q=info:w6qaM0mQfiYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSliced%2BCramer%2BSynaptic%2BConsolidation%2BFor%2BPreserving%2BDeeply%2BLearned%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w6qaM0mQfiYJ&ei=TOJXYs-YH82Ny9YPqPyUgAs&json=", "num_citations": 8, "citedby_url": "/scholar?cites=2773813064579590851&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:w6qaM0mQfiYJ:scholar.google.com/&scioq=Sliced+Cramer+Synaptic+Consolidation+For+Preserving+Deeply+Learned+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJge3TNKwH"}, "Robust Training With Ensemble Consensus": {"container_type": "Publication", "bib": {"title": "Robust training with ensemble consensus", "author": ["J Lee", "SY Chung"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.09792", "abstract": "Since deep neural networks are over-parameterized, they can memorize noisy examples. We address such a memorization issue in the presence of label noise. From the fact that deep neural networks cannot generalize to neighborhoods of memorized features, we hypothesize that noisy examples do not consistently incur small losses on the network under a certain perturbation. Based on this, we propose a novel training method called Learning with Ensemble Consensus (LEC) that prevents overfitting to noisy examples by removing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.09792", "author_id": ["fZEe4a0AAAAJ", "k-o3JBIAAAAJ"], "url_scholarbib": "/scholar?q=info:5Cjav4YlXu8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BTraining%2BWith%2BEnsemble%2BConsensus%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5Cjav4YlXu8J&ei=T-JXYryXF5GJmwGIxre4DA&json=", "num_citations": 9, "citedby_url": "/scholar?cites=17248264883550169316&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5Cjav4YlXu8J:scholar.google.com/&scioq=Robust+Training+With+Ensemble+Consensus&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.09792"}, "Towards Neural Networks That Provably Know When They Don't Know": {"container_type": "Publication", "bib": {"title": "Towards neural networks that provably know when they don't know", "author": ["A Meinke", "M Hein"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12180", "abstract": "It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the training data. Thus, ReLU networks do not know when they don't know. However, this is a highly important property in safety critical applications. In the context of out-of-distribution detection (OOD) there have been a number of proposals to mitigate this problem but none of them are able to make any mathematical guarantees. In this paper we propose a new approach to OOD which overcomes both problems. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12180", "author_id": ["PqHTP_AAAAAJ", "0ZAb3tsAAAAJ"], "url_scholarbib": "/scholar?q=info:kNg-ciaWODYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BNeural%2BNetworks%2BThat%2BProvably%2BKnow%2BWhen%2BThey%2BDon%2527t%2BKnow%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kNg-ciaWODYJ&ei=UuJXYt-OKpGJmwGIxre4DA&json=", "num_citations": 75, "citedby_url": "/scholar?cites=3907037768613550224&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kNg-ciaWODYJ:scholar.google.com/&scioq=Towards+Neural+Networks+That+Provably+Know+When+They+Don%27t+Know&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12180"}, "Classification-based Anomaly Detection For General Data": {"container_type": "Publication", "bib": {"title": "Classification-based anomaly detection for general data", "author": ["L Bergman", "Y Hoshen"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2005.02359", "abstract": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.02359", "author_id": ["YQ5czYAAAAAJ", "6y1-qS4AAAAJ"], "url_scholarbib": "/scholar?q=info:gdeEItEwPAAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DClassification-based%2BAnomaly%2BDetection%2BFor%2BGeneral%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gdeEItEwPAAJ&ei=VeJXYpWTJ4vEmgH7846QCg&json=", "num_citations": 127, "citedby_url": "/scholar?cites=16942173388068737&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gdeEItEwPAAJ:scholar.google.com/&scioq=Classification-based+Anomaly+Detection+For+General+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.02359"}, "Scalable Model Compression By Entropy Penalized Reparameterization": {"container_type": "Publication", "bib": {"title": "Scalable model compression by entropy penalized reparameterization", "author": ["D Oktay", "J Ball\u00e9", "S Singh", "A Shrivastava"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.06624", "abstract": "We describe a simple and general neural network weight compression approach, in which the network parameters (weights and biases) are represented in a\" latent\" space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate--accuracy trade-off"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.06624", "author_id": ["NQ1BBEwAAAAJ", "uKDe38UAAAAJ", "L7fTK1MAAAAJ", "mIF9BowAAAAJ"], "url_scholarbib": "/scholar?q=info:BpO2NAQJNK4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScalable%2BModel%2BCompression%2BBy%2BEntropy%2BPenalized%2BReparameterization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BpO2NAQJNK4J&ei=WeJXYu21HZLeyQTms5KQBg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=12552667975057314566&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BpO2NAQJNK4J:scholar.google.com/&scioq=Scalable+Model+Compression+By+Entropy+Penalized+Reparameterization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.06624.pdf?ref=https://githubhelp.com"}, "Exploration In Reinforcement Learning With Deep Covering Options": {"container_type": "Publication", "bib": {"title": "Exploration in reinforcement learning with deep covering options", "author": ["Y Jinnai", "JW Park", "MC Machado"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "While many option discovery methods have been proposed to accelerate exploration in reinforcement learning, they are often heuristic. Recently, covering options was proposed to discover a set of options that provably reduce the upper bound of the environment's cover time, a measure of the difficulty of exploration. Covering options are computed using the eigenvectors of the graph Laplacian, but they are constrained to tabular tasks and are not applicable to tasks with large or continuous state-spaces. We introduce deep covering"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkeIyaVtwB", "author_id": ["H0MaUNIAAAAJ", "64Gd5kgAAAAJ", "xf_n4xUAAAAJ"], "url_scholarbib": "/scholar?q=info:63Z5tnUP_pcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploration%2BIn%2BReinforcement%2BLearning%2BWith%2BDeep%2BCovering%2BOptions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=63Z5tnUP_pcJ&ei=XOJXYrSMGsLZmQHc1ovQAg&json=", "num_citations": 20, "citedby_url": "/scholar?cites=10952208342058628843&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:63Z5tnUP_pcJ:scholar.google.com/&scioq=Exploration+In+Reinforcement+Learning+With+Deep+Covering+Options&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkeIyaVtwB"}, "A Baseline For Few-shot Image Classification": {"container_type": "Publication", "bib": {"title": "A baseline for few-shot image classification", "author": ["GS Dhillon", "P Chaudhari", "A Ravichandran"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.02729", "author_id": ["m68VEtIAAAAJ", "c_z5hWEAAAAJ", "28p_eLYAAAAJ"], "url_scholarbib": "/scholar?q=info:suxyiAkVdGYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BBaseline%2BFor%2BFew-shot%2BImage%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=suxyiAkVdGYJ&ei=a-JXYtfYFpLeyQTms5KQBg&json=", "num_citations": 250, "citedby_url": "/scholar?cites=7382548819855207602&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:suxyiAkVdGYJ:scholar.google.com/&scioq=A+Baseline+For+Few-shot+Image+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.02729"}, "Permutation Equivariant Models For Compositional Generalization In Language": {"container_type": "Publication", "bib": {"title": "Permutation equivariant models for compositional generalization in language", "author": ["J Gordon", "D Lopez-Paz", "M Baroni"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Humans understand novel sentences by composing meanings and roles of core language components. In contrast, neural network models for natural language modeling fail when such compositional generalization is required. The main contribution of this paper is to hypothesize that language compositionality is a form of group-equivariance. Based on this hypothesis, we propose a set of tools for constructing equivariant sequence-to-sequence models. Throughout a variety of experiments on the SCAN tasks, we analyze the behavior of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SylVNerFvr", "author_id": ["IZGi3pEAAAAJ", "SiCHxTkAAAAJ", "l-xu2w0AAAAJ"], "url_scholarbib": "/scholar?q=info:nrHJ4taTm0AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPermutation%2BEquivariant%2BModels%2BFor%2BCompositional%2BGeneralization%2BIn%2BLanguage%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nrHJ4taTm0AJ&ei=dOJXYp63HsS4ywTtzb_QDA&json=", "num_citations": 47, "citedby_url": "/scholar?cites=4655477190954693022&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nrHJ4taTm0AJ:scholar.google.com/&scioq=Permutation+Equivariant+Models+For+Compositional+Generalization+In+Language&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SylVNerFvr"}, "How To 0wn The Nas In Your Spare Time": {"container_type": "Publication", "bib": {"title": "How to 0wn NAS in your spare time", "author": ["S Hong", "M Davinroy", "Y Kaya", "D Dachman-Soled"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The network architectures found from the NAS procedure commonly are wide and deep,   the NAS procedure\u2014this process factorizes the entire architecture into blocks by their functions."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06776", "author_id": ["664LW90AAAAJ", "sbxBaL4AAAAJ", "tPiXuV0AAAAJ", "Ss009KUAAAAJ"], "url_scholarbib": "/scholar?q=info:zi-0Z4tE7lsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BTo%2B0wn%2BThe%2BNas%2BIn%2BYour%2BSpare%2BTime%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zi-0Z4tE7lsJ&ei=fuJXYq3hKc6E6rQPz8uiuAc&json=", "num_citations": 17, "citedby_url": "/scholar?cites=6624307467439583182&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zi-0Z4tE7lsJ:scholar.google.com/&scioq=How+To+0wn+The+Nas+In+Your+Spare+Time&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06776"}, "Towards Fast Adaptation Of Neural Architectures With Meta Learning": {"container_type": "Publication", "bib": {"title": "Towards fast adaptation of neural architectures with meta learning", "author": ["D Lian", "Y Zheng", "Y Xu", "Y Lu", "L Lin", "P Zhao"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Recently, Neural Architecture Search (NAS) has been successfully applied to multiple artificial intelligence areas and shows better performance compared with hand-designed networks. However, the existing NAS methods only target a specific task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. Generally, the architecture for a new task is either searched from scratch, which is neither efficient nor flexible enough for practical application scenarios, or"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1eowANFvr", "author_id": ["q-C8LqsAAAAJ", "cibWNZIAAAAJ", "", "", "", "HPeX_YcAAAAJ"], "url_scholarbib": "/scholar?q=info:YoeECtk159wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BFast%2BAdaptation%2BOf%2BNeural%2BArchitectures%2BWith%2BMeta%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YoeECtk159wJ&ei=guJXYrOFCI6pywSdh6agAg&json=", "num_citations": 50, "citedby_url": "/scholar?cites=15917750614264940386&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YoeECtk159wJ:scholar.google.com/&scioq=Towards+Fast+Adaptation+Of+Neural+Architectures+With+Meta+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1eowANFvr"}, "Svqn: Sequential Variational Soft Q-learning Networks": {"container_type": "Publication", "bib": {"title": "SVQN: sequential variational soft Q-learning networks", "author": ["S Huang", "H Su", "J Zhu", "T Chen"], "pub_year": "2020", "venue": "NA", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=9_oTSv35eS3", "author_id": ["PK57vrQAAAAJ", "dxN1_X0AAAAJ", "axsP38wAAAAJ", "yE6LvhMAAAAJ"], "url_scholarbib": "/scholar?q=info:8IBi5z2fYqIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSvqn:%2BSequential%2BVariational%2BSoft%2BQ-learning%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8IBi5z2fYqIJ&ei=heJXYtyYLMWemAHB5baIBQ&json=", "num_citations": 5, "citedby_url": "/scholar?cites=11701089870085783792&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8IBi5z2fYqIJ:scholar.google.com/&scioq=Svqn:+Sequential+Variational+Soft+Q-learning+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=9_oTSv35eS3"}, "Domain Adaptive Multiflow Networks": {"container_type": "Publication", "bib": {"title": "SDN-enabled sliceable BVT based on multicarrier technology for multiflow rate/distance and grid adaptation", "author": ["MS Moreolo", "JM Fabrega", "L Nadal"], "pub_year": "2015", "venue": "Journal of Lightwave \u2026", "abstract": "the available network resources, programmable adaptive  in the electrical domain (at the  edge-node of the network) [9].  per subcarrier at the nodes of the network. In [19], it has been"}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/7360107/", "author_id": ["Agj9efoAAAAJ", "Wim5TOAAAAAJ", "H8dqTukAAAAJ"], "url_scholarbib": "/scholar?q=info:HShzCcXB1icJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDomain%2BAdaptive%2BMultiflow%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HShzCcXB1icJ&ei=ieJXYs-QB82Ny9YPqPyUgAs&json=", "num_citations": 81, "citedby_url": "/scholar?cites=2870694864506529821&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HShzCcXB1icJ:scholar.google.com/&scioq=Domain+Adaptive+Multiflow+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://www.researchgate.net/profile/Ricard-Vilalta/publication/291018888_SDN-enabled_Sliceable_BVT_Based_on_Multicarrier_Technology_for_Multi-Flow_RateDistance_and_Grid_Adaptation/links/573326bb08ae9ace84073212/SDN-enabled-Sliceable-BVT-Based-on-Multicarrier-Technology-for-Multi-Flow-Rate-Distance-and-Grid-Adaptation.pdf"}, "Evolutionary Population Curriculum For Scaling Multi-agent Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Evolutionary population curriculum for scaling multi-agent reinforcement learning", "author": ["Q Long", "Z Zhou", "A Gupta", "F Fang", "Y Wu"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.10423", "author_id": ["", "HwFGzZMAAAAJ", "bqL73OkAAAAJ", "R6jE0VEAAAAJ", "dusV5HMAAAAJ"], "url_scholarbib": "/scholar?q=info:SGSyo6N-kbcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvolutionary%2BPopulation%2BCurriculum%2BFor%2BScaling%2BMulti-agent%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SGSyo6N-kbcJ&ei=j-JXYsCDD8iBy9YP18Gi8As&json=", "num_citations": 28, "citedby_url": "/scholar?cites=13227492821855003720&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SGSyo6N-kbcJ:scholar.google.com/&scioq=Evolutionary+Population+Curriculum+For+Scaling+Multi-agent+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.10423"}, "Deep 3d Pan Via Local Adaptive \"t-shaped\" Convolutions With Global And Local Adaptive Dilations": {"container_type": "Publication", "bib": {"title": "Deep 3d pan via local adaptive\" t-shaped\" convolutions with global and local adaptive dilations", "author": ["JL Gonzalez Bello", "M Kim"], "pub_year": "2020", "venue": "International Conference on Learning \u2026", "abstract": "kernels equipped with globally and locally adaptive dilations. Our proposed  t-shaped  adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global"}, "filled": false, "gsrank": 1, "pub_url": "https://koasas.kaist.ac.kr/handle/10203/277832", "author_id": ["", "bGXte_4AAAAJ"], "url_scholarbib": "/scholar?q=info:1j0BtM_pfKIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2B3d%2BPan%2BVia%2BLocal%2BAdaptive%2B%2522t-shaped%2522%2BConvolutions%2BWith%2BGlobal%2BAnd%2BLocal%2BAdaptive%2BDilations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1j0BtM_pfKIJ&ei=muJXYsCjAovMsQK69Y7ABg&json=", "num_citations": 5, "citedby_url": "/scholar?cites=11708490209543929302&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1j0BtM_pfKIJ:scholar.google.com/&scioq=Deep+3d+Pan+Via+Local+Adaptive+%22t-shaped%22+Convolutions+With+Global+And+Local+Adaptive+Dilations&hl=en&as_sdt=0,33"}, "On The Interaction Between Supervision And Self-play In Emergent Communication": {"container_type": "Publication", "bib": {"title": "On the interaction between supervision and self-play in emergent communication", "author": ["R Lowe", "A Gupta", "J Foerster", "D Kiela"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.01093", "author_id": ["iRgYMuEAAAAJ", "bqL73OkAAAAJ", "6z4lQzMAAAAJ", "Q0piorUAAAAJ"], "url_scholarbib": "/scholar?q=info:r4mLy7-qqioJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BInteraction%2BBetween%2BSupervision%2BAnd%2BSelf-play%2BIn%2BEmergent%2BCommunication%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r4mLy7-qqioJ&ei=nOJXYubyJ42EmgH6u5u4BA&json=", "num_citations": 37, "citedby_url": "/scholar?cites=3074457436364179887&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r4mLy7-qqioJ:scholar.google.com/&scioq=On+The+Interaction+Between+Supervision+And+Self-play+In+Emergent+Communication&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.01093"}, "Efficient And Information-preserving Future Frame Prediction And Beyond": {"container_type": "Publication", "bib": {"title": "Efficient and information-preserving future frame prediction and beyond", "author": ["W Yu", "Y Lu", "S Easterbrook", "S Fidler"], "pub_year": "2020", "venue": "NA", "abstract": "Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly limits their application scenarios. We propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor. Our model enjoys the theoretically guaranteed property of no information loss during the feature extraction, much lower memory consumption and computational"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=F4e26c-K1DM", "author_id": ["smZffVEAAAAJ", "u45sF6kAAAAJ", "bD8DWiEAAAAJ", "CUlqK5EAAAAJ"], "url_scholarbib": "/scholar?q=info:X-h3Pww2y2MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BAnd%2BInformation-preserving%2BFuture%2BFrame%2BPrediction%2BAnd%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=X-h3Pww2y2MJ&ei=oOJXYrSfLovMsQK69Y7ABg&json=", "num_citations": 31, "citedby_url": "/scholar?cites=7190900656259459167&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:X-h3Pww2y2MJ:scholar.google.com/&scioq=Efficient+And+Information-preserving+Future+Frame+Prediction+And+Beyond&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=F4e26c-K1DM"}, "Episodic Reinforcement Learning With Associative Memory": {"container_type": "Publication", "bib": {"title": "Episodic reinforcement learning with associative memory", "author": ["G Zhu", "Z Lin", "G Yang", "C Zhang"], "pub_year": "2020", "venue": "NA", "abstract": "Sample efficiency has been one of the major challenges for deep reinforcement learning. Non-parametric episodic control has been proposed to speed up parametric reinforcement learning by rapidly latching on previously successful policies. However, previous work on episodic reinforcement learning neglects the relationship between states and only stored the experiences as unrelated items. To improve sample efficiency of reinforcement learning, we propose a novel framework, called Episodic Reinforcement Learning with Associative"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=xNIR3ac6J-4", "author_id": ["pTS7LTkAAAAJ", "Tlc4yaMAAAAJ", "", "LjxqXycAAAAJ"], "url_scholarbib": "/scholar?q=info:dog8pOpfFfoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEpisodic%2BReinforcement%2BLearning%2BWith%2BAssociative%2BMemory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dog8pOpfFfoJ&ei=pOJXYoXOAeHDywTjooCQBQ&json=", "num_citations": 24, "citedby_url": "/scholar?cites=18020414945375324278&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dog8pOpfFfoJ:scholar.google.com/&scioq=Episodic+Reinforcement+Learning+With+Associative+Memory&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=xNIR3ac6J-4"}, "Cross-domain Few-shot Classification Via Learned Feature-wise Transformation": {"container_type": "Publication", "bib": {"title": "Cross-domain few-shot classification via learned feature-wise transformation", "author": ["HY Tseng", "HY Lee", "JB Huang", "MH Yang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.08735", "abstract": "Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms predict categories by comparing the feature embeddings of query images with those from a few labeled images (support examples) using a learned metric function. While promising performance has been demonstrated, these methods often fail to generalize to unseen domains due to large discrepancy of the feature distribution across domains. In this work, we address the problem"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.08735", "author_id": ["hzOgd9MAAAAJ", "SeozinYAAAAJ", "pp848fYAAAAJ", "p9-ohHsAAAAJ"], "url_scholarbib": "/scholar?q=info:3wNIBh0nV2EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCross-domain%2BFew-shot%2BClassification%2BVia%2BLearned%2BFeature-wise%2BTransformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3wNIBh0nV2EJ&ei=qOJXYvXdLM2Ny9YPqPyUgAs&json=", "num_citations": 161, "citedby_url": "/scholar?cites=7014117950265754591&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3wNIBh0nV2EJ:scholar.google.com/&scioq=Cross-domain+Few-shot+Classification+Via+Learned+Feature-wise+Transformation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.08735"}, "Learning To Move With Affordance Maps": {"container_type": "Publication", "bib": {"title": "Learning to move with affordance maps", "author": ["W Qi", "RT Mullapudi", "S Gupta", "D Ramanan"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The ability to autonomously explore and navigate a physical space is a fundamental requirement for virtually any mobile autonomous agent, from household robotic vacuums to autonomous vehicles. Traditional SLAM-based approaches for exploration and navigation largely focus on leveraging scene geometry, but fail to model dynamic objects (such as other agents) or semantic constraints (such as wet floors or doorways). Learning-based RL agents are an attractive alternative because they can incorporate both semantic and geometric"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02364", "author_id": ["Z8-UfkUAAAAJ", "vh9KxEIAAAAJ", "1HO5UacAAAAJ", "9B8PoXUAAAAJ"], "url_scholarbib": "/scholar?q=info:uhv1l6RIdpMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BMove%2BWith%2BAffordance%2BMaps%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uhv1l6RIdpMJ&ei=reJXYuCNDMWemAHB5baIBQ&json=", "num_citations": 16, "citedby_url": "/scholar?cites=10625760242588523450&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uhv1l6RIdpMJ:scholar.google.com/&scioq=Learning+To+Move+With+Affordance+Maps&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02364"}, "Cm3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Cm3: Cooperative multi-goal multi-stage multi-agent reinforcement learning", "author": ["J Yang", "A Nakhaei", "D Isele", "K Fujimura"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1809.05188", "author_id": ["7spgYVAAAAAJ", "icszNxkAAAAJ", "", "1pj4IqIAAAAJ"], "url_scholarbib": "/scholar?q=info:_TSErp8pRpsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCm3:%2BCooperative%2BMulti-goal%2BMulti-stage%2BMulti-agent%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_TSErp8pRpsJ&ei=suJXYuemAcWemAHB5baIBQ&json=", "num_citations": 38, "citedby_url": "/scholar?cites=11188676090053014781&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_TSErp8pRpsJ:scholar.google.com/&scioq=Cm3:+Cooperative+Multi-goal+Multi-stage+Multi-agent+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1809.05188.pdf?ref=https://githubhelp.com"}, "To Relieve Your Headache Of Training An Mrf, Take Advil": {"container_type": "Publication", "bib": {"title": "To relieve your headache of training an mrf, take advil", "author": ["C Li", "C Du", "K Xu", "M Welling", "J Zhu", "B Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a black-box algorithm called {\\it Adversarial Variational Inference and Learning}(AdVIL) to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.08400", "author_id": ["UKMcQn4AAAAJ", "QOp7xW0AAAAJ", "gfl-HVYAAAAJ", "8200InoAAAAJ", "axsP38wAAAAJ", ""], "url_scholarbib": "/scholar?q=info:jR2PQer_vS0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTo%2BRelieve%2BYour%2BHeadache%2BOf%2BTraining%2BAn%2BMrf,%2BTake%2BAdvil%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jR2PQer_vS0J&ei=tOJXYvaWLJGJmwGIxre4DA&json=", "num_citations": 12, "citedby_url": "/scholar?cites=3296071883892399501&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jR2PQer_vS0J:scholar.google.com/&scioq=To+Relieve+Your+Headache+Of+Training+An+Mrf,+Take+Advil&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.08400"}, "Locality And Compositionality In Zero-shot Learning": {"container_type": "Publication", "bib": {"title": "Locality and compositionality in zero-shot learning", "author": ["T Sylvain", "L Petrini", "D Hjelm"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.12179", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (eg ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, ie how well can the learned representations be expressed as a function of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.12179", "author_id": ["Dg5qUb0AAAAJ", "a5siqi0AAAAJ", "68c5HfwAAAAJ"], "url_scholarbib": "/scholar?q=info:gcPPe9y5vVgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLocality%2BAnd%2BCompositionality%2BIn%2BZero-shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gcPPe9y5vVgJ&ei=t-JXYp7ZKYvEmgH7846QCg&json=", "num_citations": 32, "citedby_url": "/scholar?cites=6394471402557129601&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gcPPe9y5vVgJ:scholar.google.com/&scioq=Locality+And+Compositionality+In+Zero-shot+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.12179"}, "Analysis Of Video Feature Learning In Two-stream Cnns On The Example Of Zebrafish Swim Bout Classification": {"container_type": "Publication", "bib": {"title": "Analysis of video feature learning in two-stream CNNs on the example of zebrafish swim bout classification", "author": ["B Breier", "A Onken"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.09857", "abstract": "Semmelhack et al.(2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.09857", "author_id": ["RQIi2z0AAAAJ", "JQh31ekAAAAJ"], "url_scholarbib": "/scholar?q=info:YFnZcbQ7L2UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalysis%2BOf%2BVideo%2BFeature%2BLearning%2BIn%2BTwo-stream%2BCnns%2BOn%2BThe%2BExample%2BOf%2BZebrafish%2BSwim%2BBout%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YFnZcbQ7L2UJ&ei=u-JXYrScCM6E6rQPz8uiuAc&json=", "num_citations": 4, "citedby_url": "/scholar?cites=7291111967926344032&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YFnZcbQ7L2UJ:scholar.google.com/&scioq=Analysis+Of+Video+Feature+Learning+In+Two-stream+Cnns+On+The+Example+Of+Zebrafish+Swim+Bout+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.09857"}, "Distributed Bandit Learning: Near-optimal Regret With Efficient Communication": {"container_type": "Publication", "bib": {"title": "Distributed bandit learning: Near-optimal regret with efficient communication", "author": ["Y Wang", "J Hu", "X Chen", "L Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1904.06309", "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $ M $ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $ O (M\\log (MK)) $ communication cost, where $ K $ is the number of arms. The communication"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.06309", "author_id": ["yj2b7pgAAAAJ", "5GavKiQAAAAJ", "sioumZAAAAAJ", "VZHxoh8AAAAJ"], "url_scholarbib": "/scholar?q=info:28hUwLeYIPQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BBandit%2BLearning:%2BNear-optimal%2BRegret%2BWith%2BEfficient%2BCommunication%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=28hUwLeYIPQJ&ei=vuJXYqCBHs2Ny9YPqPyUgAs&json=", "num_citations": 29, "citedby_url": "/scholar?cites=17591228059482376411&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:28hUwLeYIPQJ:scholar.google.com/&scioq=Distributed+Bandit+Learning:+Near-optimal+Regret+With+Efficient+Communication&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.06309"}, "Beyond Linearization: On Quadratic And Higher-order Approximation Of Wide Neural Networks": {"container_type": "Publication", "bib": {"title": "Beyond linearization: On quadratic and higher-order approximation of wide neural networks", "author": ["Y Bai", "JD Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.01619", "abstract": "Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by he Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory. Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01619", "author_id": ["owqhKD8AAAAJ", "GR_DsT0AAAAJ"], "url_scholarbib": "/scholar?q=info:7uM_Maq_-gwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BLinearization:%2BOn%2BQuadratic%2BAnd%2BHigher-order%2BApproximation%2BOf%2BWide%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7uM_Maq_-gwJ&ei=weJXYs2UM5aM6rQPlISayA8&json=", "num_citations": 60, "citedby_url": "/scholar?cites=935270610324415470&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7uM_Maq_-gwJ:scholar.google.com/&scioq=Beyond+Linearization:+On+Quadratic+And+Higher-order+Approximation+Of+Wide+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01619"}, "Learning Robust Representations Via Multi-view Information Bottleneck": {"container_type": "Publication", "bib": {"title": "Learning robust representations via multi-view information bottleneck", "author": ["M Federici", "A Dutta", "P Forr\u00e9", "N Kushman"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07017", "author_id": ["TfInmkIAAAAJ", "1aKTzmIAAAAJ", "fWbf74cAAAAJ", "I_YIc0YAAAAJ"], "url_scholarbib": "/scholar?q=info:0g4plRZrR54J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRobust%2BRepresentations%2BVia%2BMulti-view%2BInformation%2BBottleneck%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0g4plRZrR54J&ei=xOJXYtnBN8WemAHB5baIBQ&json=", "num_citations": 58, "citedby_url": "/scholar?cites=11405202326075018962&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0g4plRZrR54J:scholar.google.com/&scioq=Learning+Robust+Representations+Via+Multi-view+Information+Bottleneck&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07017"}, "Conditional Learning Of Fair Representations": {"container_type": "Publication", "bib": {"title": "Conditional learning of fair representations", "author": ["H Zhao", "A Coston", "T Adel", "GJ Gordon"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.07162", "abstract": "We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups in the classification setting. Two key components underpinning the design of our algorithm are balanced error rate and conditional alignment of representations. We show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity. Furthermore, we also demonstrate"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.07162", "author_id": ["x942ipYAAAAJ", "8U7d-_MAAAAJ", "IkhgmfEAAAAJ", "8LcYFjEAAAAJ"], "url_scholarbib": "/scholar?q=info:VihdmO1MWWwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConditional%2BLearning%2BOf%2BFair%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VihdmO1MWWwJ&ei=x-JXYuPWJcWemAHB5baIBQ&json=", "num_citations": 37, "citedby_url": "/scholar?cites=7807356012370667606&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VihdmO1MWWwJ:scholar.google.com/&scioq=Conditional+Learning+Of+Fair+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.07162"}, "Higher-order Function Networks For Learning Composable 3d Object Representations": {"container_type": "Publication", "bib": {"title": "Higher-order function networks for learning composable 3d object representations", "author": ["E Mitchell", "S Engin", "V Isler", "DD Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.10388", "abstract": "We present a new approach to 3D object representation where a neural network encodes the geometry of an object directly into the weights and biases of a second'mapping'network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. We study the effectiveness of our method through various experiments on subsets of the ShapeNet dataset. We find that the proposed approach can reconstruct encoded objects"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.10388", "author_id": ["q77J4fgAAAAJ", "DJOri7EAAAAJ", "Q5KT-hEAAAAJ", "J0l7wWwAAAAJ"], "url_scholarbib": "/scholar?q=info:FFcZTnXwhCIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHigher-order%2BFunction%2BNetworks%2BFor%2BLearning%2BComposable%2B3d%2BObject%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FFcZTnXwhCIJ&ei=y-JXYsiHO4ySyATlkbrQCA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=2487377280827479828&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FFcZTnXwhCIJ:scholar.google.com/&scioq=Higher-order+Function+Networks+For+Learning+Composable+3d+Object+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.10388"}, "Fast Is Better Than Free: Revisiting Adversarial Training": {"container_type": "Publication", "bib": {"title": "Fast is better than free: Revisiting adversarial training", "author": ["E Wong", "L Rice", "JZ Kolter"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.03994", "abstract": "Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.03994", "author_id": ["pWnTMRkAAAAJ", "HpT4p-UAAAAJ", "UXh1I6UAAAAJ"], "url_scholarbib": "/scholar?q=info:79lxjtIDKQMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFast%2BIs%2BBetter%2BThan%2BFree:%2BRevisiting%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=79lxjtIDKQMJ&ei=0OJXYqXlDs2Ny9YPqPyUgAs&json=", "num_citations": 447, "citedby_url": "/scholar?cites=227717459026762223&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:79lxjtIDKQMJ:scholar.google.com/&scioq=Fast+Is+Better+Than+Free:+Revisiting+Adversarial+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.03994.pdf?ref=https://githubhelp.com"}, "Poly-encoders: Architectures And Pre-training Strategies For Fast And Accurate Multi-sentence Scoring": {"container_type": "Publication", "bib": {"title": "Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring", "author": ["S Humeau", "K Shuster", "MA Lachaux"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The use of deep pre-trained bidirectional transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.01969", "author_id": ["gkUVnO8AAAAJ", "k8eeP8EAAAAJ", "dSEMIJ8AAAAJ"], "url_scholarbib": "/scholar?q=info:hFhDdZbqyREJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPoly-encoders:%2BArchitectures%2BAnd%2BPre-training%2BStrategies%2BFor%2BFast%2BAnd%2BAccurate%2BMulti-sentence%2BScoring%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hFhDdZbqyREJ&ei=0-JXYvPfGc6E6rQPz8uiuAc&json=", "num_citations": 206, "citedby_url": "/scholar?cites=1281813500896958596&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hFhDdZbqyREJ:scholar.google.com/&scioq=Poly-encoders:+Architectures+And+Pre-training+Strategies+For+Fast+And+Accurate+Multi-sentence+Scoring&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.01969"}, "Towards Stabilizing Batch Statistics In Backward Propagation Of Batch Normalization": {"container_type": "Publication", "bib": {"title": "Towards stabilizing batch statistics in backward propagation of batch normalization", "author": ["J Yan", "R Wan", "X Zhang", "W Zhang", "Y Wei"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.06838", "author_id": ["", "TSlNpN4AAAAJ", "yuB-cfoAAAAJ", "umFdSjQAAAAJ", "O7A6nYMAAAAJ"], "url_scholarbib": "/scholar?q=info:GN1waV6zPiIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BStabilizing%2BBatch%2BStatistics%2BIn%2BBackward%2BPropagation%2BOf%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GN1waV6zPiIJ&ei=1eJXYpGbM4ySyATlkbrQCA&json=", "num_citations": 23, "citedby_url": "/scholar?cites=2467606863922912536&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GN1waV6zPiIJ:scholar.google.com/&scioq=Towards+Stabilizing+Batch+Statistics+In+Backward+Propagation+Of+Batch+Normalization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.06838"}, "Extreme Tensoring For Low-memory Preconditioning": {"container_type": "Publication", "bib": {"title": "Extreme tensoring for low-memory preconditioning", "author": ["X Chen", "N Agarwal", "E Hazan", "C Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose extreme tensoring for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.04620", "author_id": ["JIUz890AAAAJ", "sEMrGicAAAAJ", "LnhCGNMAAAAJ", "sXtjq8IAAAAJ"], "url_scholarbib": "/scholar?q=info:NDyjDKOJC34J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExtreme%2BTensoring%2BFor%2BLow-memory%2BPreconditioning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NDyjDKOJC34J&ei=2OJXYpzLBpWMy9YPt8OamA0&json=", "num_citations": 7, "citedby_url": "/scholar?cites=9082504406907436084&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NDyjDKOJC34J:scholar.google.com/&scioq=Extreme+Tensoring+For+Low-memory+Preconditioning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.04620"}, "Scale-equivariant Steerable Networks": {"container_type": "Publication", "bib": {"title": "Scale-equivariant steerable networks", "author": ["I Sosnovik", "M Szmaja", "A Smeulders"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.11093", "abstract": "we develop a theory of scale-equivariant networks. We demonstrate the concept of steerable  filter  Then we derive scale-equivariant convolution and demonstrate a fast algorithm for its"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.11093", "author_id": ["brUsNccAAAAJ", "", "aa5Ou7gAAAAJ"], "url_scholarbib": "/scholar?q=info:Eg70gUD08rkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScale-equivariant%2BSteerable%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Eg70gUD08rkJ&ei=3eJXYrSwKoySyATlkbrQCA&json=", "num_citations": 66, "citedby_url": "/scholar?cites=13399040399275986450&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Eg70gUD08rkJ:scholar.google.com/&scioq=Scale-equivariant+Steerable+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.11093"}, "Distributionally Robust Neural Networks": {"container_type": "Publication", "bib": {"title": "Distributionally robust neural networks", "author": ["S Sagawa", "PW Koh", "TB Hashimoto"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Overparameterized neural networks can be highly accurate  Distributionally robust optimization  (DRO) allows us to learn  group DRO to overparameterized neural networks fails: these"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryxGuJrFvS", "author_id": ["9EnJFEEAAAAJ", "Nn990CkAAAAJ", "5ygiTwsAAAAJ"], "url_scholarbib": "/scholar?q=info:N7awwYFYCH4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributionally%2BRobust%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N7awwYFYCH4J&ei=5OJXYrarNoySyATlkbrQCA&json=", "num_citations": 79, "citedby_url": "/scholar?cites=9081605962916214327&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:N7awwYFYCH4J:scholar.google.com/&scioq=Distributionally+Robust+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryxGuJrFvS"}, "Training Binary Neural Networks With Real-to-binary Convolutions": {"container_type": "Publication", "bib": {"title": "Training binary neural networks with real-to-binary convolutions", "author": ["B Martinez", "J Yang", "A Bulat", "G Tzimiropoulos"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper shows how to train binary networks to within a few percent points ($\\sim 3-5\\% $) of the full precision counterpart. We first show how to build a strong baseline, which already achieves state-of-the-art accuracy, by combining recently proposed advances and carefully adjusting the optimization procedure. Secondly, we show that by attempting to minimize the discrepancy between the output of the binary and the corresponding real-valued convolution, additional significant accuracy gains can be obtained. We materialize this idea"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.11535", "author_id": ["-62MApgAAAAJ", "a0HJYXcAAAAJ", "5sKcsg0AAAAJ", "D4JkWxf-8fwC"], "url_scholarbib": "/scholar?q=info:CdlrVVSu1GAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BBinary%2BNeural%2BNetworks%2BWith%2BReal-to-binary%2BConvolutions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CdlrVVSu1GAJ&ei=6OJXYvqCC8WemAHB5baIBQ&json=", "num_citations": 84, "citedby_url": "/scholar?cites=6977393399937358089&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CdlrVVSu1GAJ:scholar.google.com/&scioq=Training+Binary+Neural+Networks+With+Real-to-binary+Convolutions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.11535"}, "Curvature Graph Network": {"container_type": "Publication", "bib": {"title": "Curvature graph network", "author": ["Z Ye", "KS Liu", "T Ma", "J Gao", "C Chen"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Next we present how Ricci curvature is used in our graph convolutional network. Intuitively,  curvature measures how easily a message flows through an edge, and could be used to"}, "filled": false, "gsrank": 1, "pub_url": "https://drive.google.com/file/d/1xJdFQ7V5-rzI6eQDmvBY53t02Bn5YGte/view", "author_id": ["_RoXw30AAAAJ", "IsmX7BoAAAAJ", "9OvNakkAAAAJ", "P1CMmgEAAAAJ", "J-iIIFAAAAAJ"], "url_scholarbib": "/scholar?q=info:wk-AiH9etu8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCurvature%2BGraph%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wk-AiH9etu8J&ei=8-JXYsf8F5WMy9YPt8OamA0&json=", "num_citations": 13, "citedby_url": "/scholar?cites=17273097322670084034&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wk-AiH9etu8J:scholar.google.com/&scioq=Curvature+Graph+Network&hl=en&as_sdt=0,33", "eprint_url": "https://drive.google.com/file/d/1xJdFQ7V5-rzI6eQDmvBY53t02Bn5YGte/view"}, "Minimizing Flops To Learn Efficient Sparse Representations": {"container_type": "Publication", "bib": {"title": "Minimizing flops to learn efficient sparse representations", "author": ["B Paria", "CK Yeh", "IEH Yen", "N Xu", "P Ravikumar"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.05665", "author_id": ["8tgfu84AAAAJ", "tikMtMsAAAAJ", "VH1mq2AAAAAJ", "dRDZBoEAAAAJ", "Q4DTPw4AAAAJ"], "url_scholarbib": "/scholar?q=info:1Svulr_peOMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimizing%2BFlops%2BTo%2BLearn%2BEfficient%2BSparse%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1Svulr_peOMJ&ei=9uJXYvWdIcS4ywTtzb_QDA&json=", "num_citations": 7, "citedby_url": "/scholar?cites=16391107852895136725&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1Svulr_peOMJ:scholar.google.com/&scioq=Minimizing+Flops+To+Learn+Efficient+Sparse+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.05665"}, "Language Gans Falling Short": {"container_type": "Publication", "bib": {"title": "Language gans falling short", "author": ["M Caccia", "L Caccia", "W Fedus", "H Larochelle"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We compare MLE to GANs using recently proposed global metrics, the Language Model  score (quality) and Reverse Language Model score (diversity+quality) (C\u0131fka et al., 2018;"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.02549", "author_id": ["dW7WXMwAAAAJ", "fuvIITUAAAAJ", "-ZfwQOkAAAAJ", "U89FHq4AAAAJ"], "url_scholarbib": "/scholar?q=info:dYLDRq5cE04J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLanguage%2BGans%2BFalling%2BShort%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dYLDRq5cE04J&ei=-eJXYo2SCcmUywTMkLbABQ&json=", "num_citations": 128, "citedby_url": "/scholar?cites=5625942263097164405&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dYLDRq5cE04J:scholar.google.com/&scioq=Language+Gans+Falling+Short&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.02549"}, "Learning To Group: A Bottom-up Framework For 3d Part Discovery In Unseen Categories": {"container_type": "Publication", "bib": {"title": "Learning to group: A bottom-up framework for 3d part discovery in unseen categories", "author": ["T Luo", "K Mo", "Z Huang", "J Xu", "S Hu", "L Wang"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We address the problem of discovering 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learning-based agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06478", "author_id": ["fbVyT0QAAAAJ", "pL7JsOsAAAAJ", "Xz8d1RQAAAAJ", "2GKLw94AAAAJ", "", "VZHxoh8AAAAJ"], "url_scholarbib": "/scholar?q=info:i_dkWOZGXqAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BGroup:%2BA%2BBottom-up%2BFramework%2BFor%2B3d%2BPart%2BDiscovery%2BIn%2BUnseen%2BCategories%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i_dkWOZGXqAJ&ei=BONXYsWHOZLeyQTms5KQBg&json=", "num_citations": 20, "citedby_url": "/scholar?cites=11555751649018705803&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i_dkWOZGXqAJ:scholar.google.com/&scioq=Learning+To+Group:+A+Bottom-up+Framework+For+3d+Part+Discovery+In+Unseen+Categories&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06478.pdf?ref=https://githubhelp.com"}, "Dynamic Time Lag Regression: Predicting What & When": {"container_type": "Publication", "bib": {"title": "Dynamic time lag regression: predicting what & when", "author": ["M Chandorkar", "C Furtlehner", "B Poduval"], "pub_year": "2019", "venue": "International \u2026", "abstract": "This paper tackles a new regression problem, called Dynamic Time-Lag Regression (DTLR), where a cause signal drives an effect signal with an unknown time delay. The motivating application, pertaining to space weather modelling, aims to predict the near-Earth solar wind speed based on estimates of the Sun's coronal magnetic field. DTLR differs from mainstream regression and from sequence-to-sequence learning in two respects: firstly, no ground truth (eg, pairs of associated sub-sequences) is available; secondly, the cause"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SkxybANtDB", "author_id": ["ZGG4LxQAAAAJ", "38kE8ygAAAAJ", ""], "url_scholarbib": "/scholar?q=info:6DGftjaodxQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BTime%2BLag%2BRegression:%2BPredicting%2BWhat%2B%2526%2BWhen%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6DGftjaodxQJ&ei=B-NXYsToJ82Ny9YPqPyUgAs&json=", "num_citations": 9, "citedby_url": "/scholar?cites=1474832355932713448&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6DGftjaodxQJ:scholar.google.com/&scioq=Dynamic+Time+Lag+Regression:+Predicting+What+%26+When&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SkxybANtDB"}, "Pruned Graph Scattering Transforms": {"container_type": "Publication", "bib": {"title": "Pruned graph scattering transforms", "author": ["VN Ioannidis", "S Chen", "GB Giannakis"], "pub_year": "2020", "venue": "International Conference on \u2026", "abstract": "This paper developed a novel approach to pruning the graph scattering transform. The  proposed pGST relies on a graph-spectrum-based data-adaptive criterion to prune non-"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/servlets/purl/10182240", "author_id": ["mjmiI4sAAAAJ", "W_Q33RMAAAAJ", "Nu_6R8sAAAAJ"], "url_scholarbib": "/scholar?q=info:mngwpaSrab4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPruned%2BGraph%2BScattering%2BTransforms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mngwpaSrab4J&ei=CuNXYtq-JY2EmgH6u5u4BA&json=", "num_citations": 6, "citedby_url": "/scholar?cites=13720686463395330202&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mngwpaSrab4J:scholar.google.com/&scioq=Pruned+Graph+Scattering+Transforms&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10182240"}, "The Curious Case Of Neural Text Degeneration": {"container_type": "Publication", "bib": {"title": "The curious case of neural text degeneration", "author": ["A Holtzman", "J Buys", "L Du", "M Forbes", "Y Choi"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Work on neural dialog systems have proposed methods for  to address the problem of neural  text degeneration through an \u201c Our focus is on exposing neural text degeneration and provid"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.09751", "author_id": ["8veBOSIAAAAJ", "TlqDbGYAAAAJ", "efDU43kAAAAJ", "sF1vZ0oAAAAJ", "vhP-tlcAAAAJ"], "url_scholarbib": "/scholar?q=info:nmdRulIjrrUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BCurious%2BCase%2BOf%2BNeural%2BText%2BDegeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nmdRulIjrrUJ&ei=FeNXYrOuK8WemAHB5baIBQ&json=", "num_citations": 801, "citedby_url": "/scholar?cites=13091440005032798110&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nmdRulIjrrUJ:scholar.google.com/&scioq=The+Curious+Case+Of+Neural+Text+Degeneration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.09751"}, "Query2box: Reasoning Over Knowledge Graphs In Vector Space Using Box Embeddings": {"container_type": "Publication", "bib": {"title": "Query2box: Reasoning over knowledge graphs in vector space using box embeddings", "author": ["H Ren", "W Hu", "J Leskovec"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05969", "abstract": "Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05969", "author_id": ["", "wAFMjfkAAAAJ", "Q_kKkIUAAAAJ"], "url_scholarbib": "/scholar?q=info:OFDUZrWCyKgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuery2box:%2BReasoning%2BOver%2BKnowledge%2BGraphs%2BIn%2BVector%2BSpace%2BUsing%2BBox%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OFDUZrWCyKgJ&ei=GeNXYujrOsWemAHB5baIBQ&json=", "num_citations": 80, "citedby_url": "/scholar?cites=12162114509339906104&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OFDUZrWCyKgJ:scholar.google.com/&scioq=Query2box:+Reasoning+Over+Knowledge+Graphs+In+Vector+Space+Using+Box+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05969.pdf?ref=https://githubhelp.com"}, "Consistency Regularization For Generative Adversarial Networks": {"container_type": "Publication", "bib": {"title": "Consistency regularization for generative adversarial networks", "author": ["H Zhang", "Z Zhang", "A Odena", "H Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.12027", "abstract": "Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.12027", "author_id": ["cxEoVL4AAAAJ", "lGrbH60AAAAJ", "EHQHNdEAAAAJ", "fmSHtE8AAAAJ"], "url_scholarbib": "/scholar?q=info:2Y2VuJacLZYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConsistency%2BRegularization%2BFor%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2Y2VuJacLZYJ&ei=HONXYsOPMMmUywTMkLbABQ&json=", "num_citations": 123, "citedby_url": "/scholar?cites=10821477650797006297&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2Y2VuJacLZYJ:scholar.google.com/&scioq=Consistency+Regularization+For+Generative+Adversarial+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.12027"}, "Geometric Analysis Of Nonconvex Optimization Landscapes For Overcomplete Learning": {"container_type": "Publication", "bib": {"title": "Geometric analysis of nonconvex optimization landscapes for overcomplete learning", "author": ["Q Qu", "Y Zhai", "X Li", "Y Zhang", "Z Zhu"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Learning overcomplete representations finds many applications in machine learning and data analytics. In the past decade, despite the empirical success of heuristic methods, theoretical understandings and explanations of these algorithms are still far from satisfactory. In this work, we provide new theoretical insights for several important representation learning problems: learning (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries. We formulate these problems as $\\ell^ 4$-norm optimization"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rygixkHKDH", "author_id": ["JfblW3MAAAAJ", "78WTKm4AAAAJ", "WkRojboAAAAJ", "usawh5oAAAAJ", "gmSwszcAAAAJ"], "url_scholarbib": "/scholar?q=info:VTTv4Kz7iGoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeometric%2BAnalysis%2BOf%2BNonconvex%2BOptimization%2BLandscapes%2BFor%2BOvercomplete%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VTTv4Kz7iGoJ&ei=J-NXYquDJ8WemAHB5baIBQ&json=", "num_citations": 16, "citedby_url": "/scholar?cites=7676662284779730005&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VTTv4Kz7iGoJ:scholar.google.com/&scioq=Geometric+Analysis+Of+Nonconvex+Optimization+Landscapes+For+Overcomplete+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rygixkHKDH"}, "Vq-wav2vec: Self-supervised Learning Of Discrete Speech Representations": {"container_type": "Publication", "bib": {"title": "vq-wav2vec: Self-supervised learning of discrete speech representations", "author": ["A Baevski", "S Schneider", "M Auli"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.05453", "abstract": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.05453", "author_id": ["i7sxIX8AAAAJ", "KR5dj44AAAAJ", "KMcwQtcAAAAJ"], "url_scholarbib": "/scholar?q=info:GH8aGJ963A8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVq-wav2vec:%2BSelf-supervised%2BLearning%2BOf%2BDiscrete%2BSpeech%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GH8aGJ963A8J&ei=KuNXYpmJGZGJmwGIxre4DA&json=", "num_citations": 259, "citedby_url": "/scholar?cites=1142923229168041752&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GH8aGJ963A8J:scholar.google.com/&scioq=Vq-wav2vec:+Self-supervised+Learning+Of+Discrete+Speech+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.05453"}, "The Early Phase Of Neural Network Training": {"container_type": "Publication", "bib": {"title": "The early phase of neural network training", "author": ["J Frankle", "DJ Schwab", "AS Morcos"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.10365", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here, we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10365", "author_id": ["MlLJapIAAAAJ", "xRtvC50AAAAJ", "v-A_7UsAAAAJ"], "url_scholarbib": "/scholar?q=info:i8_XEeOE-9kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BEarly%2BPhase%2BOf%2BNeural%2BNetwork%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i8_XEeOE-9kJ&ei=LeNXYrzKLI2EmgH6u5u4BA&json=", "num_citations": 73, "citedby_url": "/scholar?cites=15707294236176535435&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i8_XEeOE-9kJ:scholar.google.com/&scioq=The+Early+Phase+Of+Neural+Network+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10365"}, "Extreme Classification Via Adversarial Softmax Approximation": {"container_type": "Publication", "bib": {"title": "Extreme classification via adversarial softmax approximation", "author": ["R Bamler", "S Mandt"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06298", "abstract": "Training a classifier over a large number of classes, known as' extreme classification', has become a topic of major interest with applications in technology, science, and e-commerce. Traditional softmax regression induces a gradient cost proportional to the number of classes $ C $, which often is prohibitively expensive. A popular scalable softmax approximation relies on uniform negative sampling, which suffers from slow convergence due a poor signal-to-noise ratio. In this paper, we propose a simple training method for drastically enhancing"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06298", "author_id": ["LwvdNAgAAAAJ", "HOrGe7wAAAAJ"], "url_scholarbib": "/scholar?q=info:t9z4-Ga9zMoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExtreme%2BClassification%2BVia%2BAdversarial%2BSoftmax%2BApproximation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=t9z4-Ga9zMoJ&ei=MONXYovqNo6pywSdh6agAg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=14613263140871789751&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:t9z4-Ga9zMoJ:scholar.google.com/&scioq=Extreme+Classification+Via+Adversarial+Softmax+Approximation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06298"}, "Simplified Action Decoder For Deep Multi-agent Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Simplified action decoder for deep multi-agent reinforcement learning", "author": ["H Hu", "JN Foerster"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.02288", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02288", "author_id": ["sJwwn54AAAAJ", "6z4lQzMAAAAJ"], "url_scholarbib": "/scholar?q=info:EioHqxGU4vgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimplified%2BAction%2BDecoder%2BFor%2BDeep%2BMulti-agent%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EioHqxGU4vgJ&ei=NONXYtPzCZaM6rQPlISayA8&json=", "num_citations": 44, "citedby_url": "/scholar?cites=17934059469747464722&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EioHqxGU4vgJ:scholar.google.com/&scioq=Simplified+Action+Decoder+For+Deep+Multi-agent+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02288"}, "Making Sense Of Reinforcement Learning And Probabilistic Inference": {"container_type": "Publication", "bib": {"title": "Making sense of reinforcement learning and probabilistic inference", "author": ["B O'Donoghue", "I Osband", "C Ionescu"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.00805", "abstract": "Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research castsRL as inference'and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00805", "author_id": ["0Pzjj-cAAAAJ", "QA4o6eYAAAAJ", "hOl-5zcAAAAJ"], "url_scholarbib": "/scholar?q=info:trdQWFsgbjUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaking%2BSense%2BOf%2BReinforcement%2BLearning%2BAnd%2BProbabilistic%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=trdQWFsgbjUJ&ei=N-NXYru2IMS4ywTtzb_QDA&json=", "num_citations": 12, "citedby_url": "/scholar?cites=3850050308144150454&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:trdQWFsgbjUJ:scholar.google.com/&scioq=Making+Sense+Of+Reinforcement+Learning+And+Probabilistic+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00805"}, "Graphsaint: Graph Sampling Based Inductive Learning Method": {"container_type": "Publication", "bib": {"title": "Graphsaint: Graph sampling based inductive learning method", "author": ["H Zeng", "H Zhou", "A Srivastava", "R Kannan"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the\" neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.04931", "author_id": ["ubUx3R0AAAAJ", "2SVpxq0AAAAJ", "5NtH-JcAAAAJ", "uXxUv0IAAAAJ"], "url_scholarbib": "/scholar?q=info:ex2upFxYVUEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraphsaint:%2BGraph%2BSampling%2BBased%2BInductive%2BLearning%2BMethod%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ex2upFxYVUEJ&ei=OuNXYt7ZO5LeyQTms5KQBg&json=", "num_citations": 304, "citedby_url": "/scholar?cites=4707766140408831355&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ex2upFxYVUEJ:scholar.google.com/&scioq=Graphsaint:+Graph+Sampling+Based+Inductive+Learning+Method&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.04931.pdf?ref=https://githubhelp.com"}, "Gendice: Generalized Offline Estimation Of Stationary Values": {"container_type": "Publication", "bib": {"title": "Gendice: Generalized offline estimation of stationary values", "author": ["R Zhang", "B Dai", "L Li", "D Schuurmans"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.09072", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this challenging scenario, and that effective estimation can still be achieved in important applications. Our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.09072", "author_id": ["-seCWbAAAAAJ", "TIKl_foAAAAJ", "Rqy5KDEAAAAJ", "xaQuPloAAAAJ"], "url_scholarbib": "/scholar?q=info:5S3oTf9W7dUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGendice:%2BGeneralized%2BOffline%2BEstimation%2BOf%2BStationary%2BValues%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5S3oTf9W7dUJ&ei=PeNXYsHbIY6pywSdh6agAg&json=", "num_citations": 85, "citedby_url": "/scholar?cites=15415072754082786789&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5S3oTf9W7dUJ:scholar.google.com/&scioq=Gendice:+Generalized+Offline+Estimation+Of+Stationary+Values&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.09072"}, "Learning To Coordinate Manipulation Skills Via Skill Behavior Diversification": {"container_type": "Publication", "bib": {"title": "Learning to coordinate manipulation skills via skill behavior diversification", "author": ["Y Lee", "J Yang", "JJ Lim"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "When mastering a complex manipulation task, humans often decompose the task into sub-skills of their body parts, practice the sub-skills independently, and then execute the sub-skills together. Similarly, a robot with multiple end-effectors can perform complex tasks by coordinating sub-skills of each end-effector. To realize temporal and behavioral coordination of skills, we propose a modular framework that first individually trains sub-skills of each end-effector with skill behavior diversification, and then learns to coordinate end-effectors using"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryxB2lBtvH", "author_id": ["CDPa3AgAAAAJ", "7XBAa2QAAAAJ", "jTnQTBoAAAAJ"], "url_scholarbib": "/scholar?q=info:-HQTu7M1Og0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BCoordinate%2BManipulation%2BSkills%2BVia%2BSkill%2BBehavior%2BDiversification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-HQTu7M1Og0J&ei=QeNXYsKPFZGJmwGIxre4DA&json=", "num_citations": 22, "citedby_url": "/scholar?cites=953133317196313848&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-HQTu7M1Og0J:scholar.google.com/&scioq=Learning+To+Coordinate+Manipulation+Skills+Via+Skill+Behavior+Diversification&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryxB2lBtvH"}, "Your Classifier Is Secretly An Energy Based Model And You Should Treat It Like One": {"container_type": "Publication", "bib": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "author": ["W Grathwohl", "KC Wang", "JH Jacobsen"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based  model for the  In this setting, the standard class probabilities can be easily computed as well as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.03263", "author_id": ["ZbClz98AAAAJ", "LgMuT6IAAAAJ", "c1FYGAQAAAAJ"], "url_scholarbib": "/scholar?q=info:JkkLyW20oLUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DYour%2BClassifier%2BIs%2BSecretly%2BAn%2BEnergy%2BBased%2BModel%2BAnd%2BYou%2BShould%2BTreat%2BIt%2BLike%2BOne%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JkkLyW20oLUJ&ei=RONXYu2kJsmUywTMkLbABQ&json=", "num_citations": 208, "citedby_url": "/scholar?cites=13087658900756056358&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JkkLyW20oLUJ:scholar.google.com/&scioq=Your+Classifier+Is+Secretly+An+Energy+Based+Model+And+You+Should+Treat+It+Like+One&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.03263"}, "Data-dependent Gaussian Prior Objective For Language Generation": {"container_type": "Publication", "bib": {"title": "Data-dependent gaussian prior objective for language generation", "author": ["Z Li", "R Wang", "K Chen", "M Utiyama", "E Sumita", "Z Zhang"], "pub_year": "2020", "venue": "NA", "abstract": "For typical sequence prediction problems such as language generation, maximum likelihood estimation (MLE) has commonly been adopted as it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE focuses on once-to-all matching between the predicted sequence and gold-standard, consequently treating all incorrect predictions as being equally incorrect. We refer to this drawback as negative diversity ignorance in this paper. Treating all incorrect"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=skGRylpqFp4", "author_id": ["PyzBf5oAAAAJ", "oTU0v5IAAAAJ", "_M4Am0AAAAAJ", "artIO6gAAAAJ", "AgC7dTQAAAAJ", "63LTQhgAAAAJ"], "url_scholarbib": "/scholar?q=info:51Yvl5q2AvgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DData-dependent%2BGaussian%2BPrior%2BObjective%2BFor%2BLanguage%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=51Yvl5q2AvgJ&ei=SONXYs2QJsmUywTMkLbABQ&json=", "num_citations": 26, "citedby_url": "/scholar?cites=17871047046437230311&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:51Yvl5q2AvgJ:scholar.google.com/&scioq=Data-dependent+Gaussian+Prior+Objective+For+Language+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=skGRylpqFp4"}, "Lookahead: A Far-sighted Alternative Of Magnitude-based Pruning": {"container_type": "Publication", "bib": {"title": "Lookahead: a far-sighted alternative of magnitude-based pruning", "author": ["S Park", "J Lee", "S Mo", "J Shin"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.04809", "abstract": "Magnitude-based pruning is one of the simplest methods for pruning neural networks. Despite its simplicity, magnitude-based pruning and its variants demonstrated remarkable performances for pruning modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we develop a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.04809", "author_id": ["", "t91zoQMAAAAJ", "Sq9y3NMAAAAJ", "m3eDp7kAAAAJ"], "url_scholarbib": "/scholar?q=info:gmRovYzXbh0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLookahead:%2BA%2BFar-sighted%2BAlternative%2BOf%2BMagnitude-based%2BPruning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gmRovYzXbh0J&ei=TONXYuaYL8WemAHB5baIBQ&json=", "num_citations": 38, "citedby_url": "/scholar?cites=2120869474011210882&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gmRovYzXbh0J:scholar.google.com/&scioq=Lookahead:+A+Far-sighted+Alternative+Of+Magnitude-based+Pruning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.04809"}, "Deepv2d: Video To Depth With Differentiable Structure From Motion": {"container_type": "Publication", "bib": {"title": "Deepv2d: Video to depth with differentiable structure from motion", "author": ["Z Teed", "J Deng"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.04605", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video. DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.04605", "author_id": ["28u7JMQAAAAJ", "U3Eub-EAAAAJ"], "url_scholarbib": "/scholar?q=info:1FAk_HTk0wcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeepv2d:%2BVideo%2BTo%2BDepth%2BWith%2BDifferentiable%2BStructure%2BFrom%2BMotion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1FAk_HTk0wcJ&ei=UONXYtSGA5aM6rQPlISayA8&json=", "num_citations": 72, "citedby_url": "/scholar?cites=564045569449021652&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1FAk_HTk0wcJ:scholar.google.com/&scioq=Deepv2d:+Video+To+Depth+With+Differentiable+Structure+From+Motion&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.04605"}, "Reclor: A Reading Comprehension Dataset Requiring Logical Reasoning": {"container_type": "Publication", "bib": {"title": "Reclor: A reading comprehension dataset requiring logical reasoning", "author": ["W Yu", "Z Jiang", "Y Dong", "J Feng"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.04326", "abstract": "Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.04326", "author_id": ["LYxjt1QAAAAJ", "Wo8tMSMAAAAJ", "", "Q8iay0gAAAAJ"], "url_scholarbib": "/scholar?q=info:K_Jj5ezyzz8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReclor:%2BA%2BReading%2BComprehension%2BDataset%2BRequiring%2BLogical%2BReasoning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=K_Jj5ezyzz8J&ei=U-NXYoClMovEmgH7846QCg&json=", "num_citations": 40, "citedby_url": "/scholar?cites=4598160843843301931&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:K_Jj5ezyzz8J:scholar.google.com/&scioq=Reclor:+A+Reading+Comprehension+Dataset+Requiring+Logical+Reasoning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.04326"}, "Expected Information Maximization: Using The I-projection For Mixture Density Estimation": {"container_type": "Publication", "bib": {"title": "Expected information maximization: Using the i-projection for mixture density estimation", "author": ["P Becker", "O Arenz", "G Neumann"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.08682", "abstract": "Modelling highly multi-modal data is a challenging problem in machine learning. Most algorithms are based on maximizing the likelihood, which corresponds to the M (oment)-projection of the data distribution to the model distribution. The M-projection forces the model to average over modes it cannot represent. In contrast, the I (information)-projection ignores such modes in the data and concentrates on the modes the model can represent. Such behavior is appealing whenever we deal with highly multi-modal data where"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.08682", "author_id": ["jXx-LuQAAAAJ", "fSyGIDwAAAAJ", "GL360kMAAAAJ"], "url_scholarbib": "/scholar?q=info:tmZKT7J4QI8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExpected%2BInformation%2BMaximization:%2BUsing%2BThe%2BI-projection%2BFor%2BMixture%2BDensity%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tmZKT7J4QI8J&ei=V-NXYsOpGcWemAHB5baIBQ&json=", "num_citations": 5, "citedby_url": "/scholar?cites=10322383053162964662&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tmZKT7J4QI8J:scholar.google.com/&scioq=Expected+Information+Maximization:+Using+The+I-projection+For+Mixture+Density+Estimation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.08682"}, "Optimal Strategies Against Generative Attacks": {"container_type": "Publication", "bib": {"title": "Optimal strategies against generative attacks", "author": ["R Mor", "E Peterfreund", "M Gavish"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Generative neural models have improved dramatically recently. With this progress comes the risk that such models will be used to attack systems that rely on sensor data for authentication and anomaly detection. Many such learning systems are installed worldwide, protecting critical infrastructure or private data against malfunction and cyber attacks. We formulate the scenario of such an authentication system facing generative impersonation attacks, characterize it from a theoretical perspective and explore its practical implications. In"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkgzMCVtPB", "author_id": ["jac_HrgAAAAJ", "AEr-DWcAAAAJ", "j0nK_CsAAAAJ"], "url_scholarbib": "/scholar?q=info:CN7KtYmD4J0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BStrategies%2BAgainst%2BGenerative%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CN7KtYmD4J0J&ei=WuNXYsjMC4vEmgH7846QCg&json=", "num_citations": 4, "citedby_url": "/scholar?cites=11376237286221602312&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CN7KtYmD4J0J:scholar.google.com/&scioq=Optimal+Strategies+Against+Generative+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkgzMCVtPB"}, "Deep Learning For Symbolic Mathematics": {"container_type": "Publication", "bib": {"title": "Deep learning for symbolic mathematics", "author": ["G Lample", "F Charton"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.01412", "abstract": "In the rest of the paper, we focus on two problems of symbolic mathematics: function  integration and solving ordinary differential equations (ODE) of the first and second order."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.01412", "author_id": ["H7sVDmIAAAAJ", "1tMnd-4AAAAJ"], "url_scholarbib": "/scholar?q=info:V-amH39wa6gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2BFor%2BSymbolic%2BMathematics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=V-amH39wa6gJ&ei=XeNXYuKCIpWMy9YPt8OamA0&json=", "num_citations": 181, "citedby_url": "/scholar?cites=12135917312174122583&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:V-amH39wa6gJ:scholar.google.com/&scioq=Deep+Learning+For+Symbolic+Mathematics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.01412"}, "Generative Ratio Matching Networks": {"container_type": "Publication", "bib": {"title": "Generative ratio matching networks", "author": ["A Srivastava", "K Xu", "MU Gutmann", "C Sutton"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "training deep generative models that does not involve saddlepoint optimization. We call our   generative ratio matching or GRAM for short. In GRAM, the generator and the critic networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1806.00101", "author_id": ["2h6SZeEAAAAJ", "kf3C60wAAAAJ", "CPhI2jYAAAAJ", "hYtGXD0AAAAJ"], "url_scholarbib": "/scholar?q=info:7VmuqUeCrAkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BRatio%2BMatching%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7VmuqUeCrAkJ&ei=aONXYvWuF82Ny9YPqPyUgAs&json=", "num_citations": 6, "citedby_url": "/scholar?cites=697075286636648941&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7VmuqUeCrAkJ:scholar.google.com/&scioq=Generative+Ratio+Matching+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1806.00101"}, "Semi-supervised Generative Modeling For Controllable Speech Synthesis": {"container_type": "Publication", "bib": {"title": "Semi-supervised generative modeling for controllable speech synthesis", "author": ["R Habib", "S Mariooryad", "M Shannon"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a novel generative model that combines state-of-the-art neural text-to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn't been possible with purely unsupervised TTS models. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01709", "author_id": ["S1y3RpYAAAAJ", "wLsZd9wAAAAJ", "8Lew0HEAAAAJ"], "url_scholarbib": "/scholar?q=info:PiMI8i1ctiAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-supervised%2BGenerative%2BModeling%2BFor%2BControllable%2BSpeech%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PiMI8i1ctiAJ&ei=a-NXYr2yMcLZmQHc1ovQAg&json=", "num_citations": 29, "citedby_url": "/scholar?cites=2357172807378936638&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:PiMI8i1ctiAJ:scholar.google.com/&scioq=Semi-supervised+Generative+Modeling+For+Controllable+Speech+Synthesis&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01709"}, "Atomnas: Fine-grained End-to-end Neural Architecture Search": {"container_type": "Publication", "bib": {"title": "Atomnas: Fine-grained end-to-end neural architecture search", "author": ["J Mei", "Y Li", "X Lian", "X Jin", "L Yang", "A Yuille"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Search space design is very critical to neural architecture search (NAS) algorithms. We propose a fine-grained search space comprised of atomic blocks, a minimal search unit that is much smaller than the ones used in recent NAS algorithms. This search space allows a mix of operations by composing different types of atomic blocks, while the search space in previous methods only allows homogeneous operations. Based on this search space, we propose a resource-aware architecture search framework which automatically assigns the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.09640", "author_id": ["nHKExN0AAAAJ", "phWmJeIAAAAJ", "-jNTDU0AAAAJ", "OEZ816YAAAAJ", "XptEO8oAAAAJ", "FJ-huxgAAAAJ"], "url_scholarbib": "/scholar?q=info:KuV7kcwN-OEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAtomnas:%2BFine-grained%2BEnd-to-end%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KuV7kcwN-OEJ&ei=buNXYtDULoySyATlkbrQCA&json=", "num_citations": 74, "citedby_url": "/scholar?cites=16282779625023333674&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KuV7kcwN-OEJ:scholar.google.com/&scioq=Atomnas:+Fine-grained+End-to-end+Neural+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.09640.pdf?ref=https://codemonkey.link"}, "Progressive Learning And Disentanglement Of Hierarchical Representations": {"container_type": "Publication", "bib": {"title": "Progressive learning and disentanglement of hierarchical representations", "author": ["Z Li", "JV Murkute", "PK Gyawali", "L Wang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.10549", "abstract": "Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of\" starting small\", we present a strategy to progressively learn independent hierarchical representations from high-to low-levels of abstractions. The model starts with learning the most abstract representation, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10549", "author_id": ["T0yPXRwAAAAJ", "", "mZ_KzxcAAAAJ", "CG56DzcAAAAJ"], "url_scholarbib": "/scholar?q=info:BtA8WZAGpAYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgressive%2BLearning%2BAnd%2BDisentanglement%2BOf%2BHierarchical%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BtA8WZAGpAYJ&ei=dONXYoqKEI2EmgH6u5u4BA&json=", "num_citations": 13, "citedby_url": "/scholar?cites=478514677450330118&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BtA8WZAGpAYJ:scholar.google.com/&scioq=Progressive+Learning+And+Disentanglement+Of+Hierarchical+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10549"}, "Energy-based Models For Atomic-resolution Protein Conformations": {"container_type": "Publication", "bib": {"title": "Energy-based models for atomic-resolution protein conformations", "author": ["Y Du", "J Meier", "J Ma", "R Fergus", "A Rives"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.13167", "abstract": "We propose an energy-based model (EBM) of protein conformations that operates at atomic scale. The model is trained solely on crystallized protein data. By contrast, existing approaches for scoring conformations use energy functions that incorporate knowledge of physical principles and features that are the complex product of several decades of research and tuning. To evaluate the model, we benchmark on the rotamer recovery task, the problem of predicting the conformation of a side chain from its context within a protein structure"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.13167", "author_id": ["GRMMc_MAAAAJ", "2M0OltAAAAAJ", "qukcWBAAAAAJ", "GgQ9GEkAAAAJ", "vqb78-gAAAAJ"], "url_scholarbib": "/scholar?q=info:ixxtexMp4xcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnergy-based%2BModels%2BFor%2BAtomic-resolution%2BProtein%2BConformations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ixxtexMp4xcJ&ei=d-NXYs6vEciBy9YP18Gi8As&json=", "num_citations": 30, "citedby_url": "/scholar?cites=1721264646237527179&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ixxtexMp4xcJ:scholar.google.com/&scioq=Energy-based+Models+For+Atomic-resolution+Protein+Conformations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.13167"}, "Reinforced Genetic Algorithm Learning For Optimizing Computation Graphs": {"container_type": "Publication", "bib": {"title": "Reinforced genetic algorithm learning for optimizing computation graphs", "author": ["A Paliwal", "F Gimeno", "V Nair", "Y Li", "M Lubin"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.02494", "author_id": ["tFlJbiEAAAAJ", "fgWuGoAAAAAJ", "RnoIxUwAAAAJ", "", "jCoWyqUAAAAJ"], "url_scholarbib": "/scholar?q=info:kHLsbPQ0newJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforced%2BGenetic%2BAlgorithm%2BLearning%2BFor%2BOptimizing%2BComputation%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kHLsbPQ0newJ&ei=euNXYoi3BpGJmwGIxre4DA&json=", "num_citations": 34, "citedby_url": "/scholar?cites=17049841988698665616&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kHLsbPQ0newJ:scholar.google.com/&scioq=Reinforced+Genetic+Algorithm+Learning+For+Optimizing+Computation+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.02494"}, "Lambdanet: Probabilistic Type Inference Using Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Lambdanet: Probabilistic type inference using graph neural networks", "author": ["J Wei", "M Goyal", "G Durrett", "I Dillig"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2005.02161", "abstract": "As gradual typing becomes increasingly popular in languages like Python and TypeScript, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully determined by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for TypeScript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.02161", "author_id": ["fTJ8pY8AAAAJ", "IOf2274AAAAJ", "EpQ_sDEAAAAJ", "Wlq5rZEAAAAJ"], "url_scholarbib": "/scholar?q=info:Ck3ET7fUAckJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLambdanet:%2BProbabilistic%2BType%2BInference%2BUsing%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ck3ET7fUAckJ&ei=fuNXYq6qE8WemAHB5baIBQ&json=", "num_citations": 52, "citedby_url": "/scholar?cites=14484091760382594314&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ck3ET7fUAckJ:scholar.google.com/&scioq=Lambdanet:+Probabilistic+Type+Inference+Using+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.02161"}, "Composing Task-agnostic Policies With Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Composing task-agnostic policies with deep reinforcement learning", "author": ["AH Qureshi", "JJ Johnson", "Y Qin", "T Henderson"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent's primitive policies to solve unseen tasks. We evaluate our method in difficult cases"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10681", "author_id": ["Lkrx2SkAAAAJ", "WTj1Vn4AAAAJ", "3KF3AIMAAAAJ", "3r2qfaoAAAAJ"], "url_scholarbib": "/scholar?q=info:__4rMtDjqkEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComposing%2BTask-agnostic%2BPolicies%2BWith%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=__4rMtDjqkEJ&ei=geNXYp3sEpmM6rQPjaOSEA&json=", "num_citations": 15, "citedby_url": "/scholar?cites=4731844841840574207&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:__4rMtDjqkEJ:scholar.google.com/&scioq=Composing+Task-agnostic+Policies+With+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10681"}, "Convergence Behaviour Of Some Gradient-based Methods On Bilinear Zero-sum Games": {"container_type": "Publication", "bib": {"title": "Convergence behaviour of some gradient-based methods on bilinear zero-sum games", "author": ["G Zhang", "Y Yu"], "pub_year": "2019", "venue": "arXiv, pp. arXiv\u20131908", "abstract": "Min-max formulations have attracted much attention in the ML community due to the rise of deep generative models and adversarial methods, and understanding the dynamics of (stochastic) gradient algorithms for solving such formulations has been a grand challenge. As a first step, we restrict to bilinear zero-sum games and give a systematic analysis of popular gradient updates, for both simultaneous and alternating versions. We provide exact conditions for their convergence and find the optimal parameter setup and convergence"}, "filled": false, "gsrank": 1, "pub_url": "https://sgo-workshop.github.io/CameraReady2019/24.pdf", "author_id": ["p8Y0xJEAAAAJ", "zbXIQMsAAAAJ"], "url_scholarbib": "/scholar?q=info:5gxCdszmiR0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConvergence%2BBehaviour%2BOf%2BSome%2BGradient-based%2BMethods%2BOn%2BBilinear%2BZero-sum%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5gxCdszmiR0J&ei=heNXYozeIYvMsQK69Y7ABg&json=", "num_citations": 2, "citedby_url": "/scholar?cites=2128486064741027046&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5gxCdszmiR0J:scholar.google.com/&scioq=Convergence+Behaviour+Of+Some+Gradient-based+Methods+On+Bilinear+Zero-sum+Games&hl=en&as_sdt=0,33", "eprint_url": "https://sgo-workshop.github.io/CameraReady2019/24.pdf"}, "Learning Space Partitions For Nearest Neighbor Search": {"container_type": "Publication", "bib": {"title": "Learning space partitions for nearest neighbor search", "author": ["Y Dong", "P Indyk", "I Razenshteyn", "T Wagner"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Space partitions of $\\mathbb {R}^ d $ underlie a vast and important class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn, Waingarten STOC 2018, FOCS 2018], we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.08544", "author_id": ["AjX6hisAAAAJ", "oOwNKsAAAAAJ", "CMZAKkEAAAAJ", "gV4dPToAAAAJ"], "url_scholarbib": "/scholar?q=info:rUIpPp6iT5IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BSpace%2BPartitions%2BFor%2BNearest%2BNeighbor%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rUIpPp6iT5IJ&ei=iONXYrq-BIvMsQK69Y7ABg&json=", "num_citations": 43, "citedby_url": "/scholar?cites=10542824053229044397&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rUIpPp6iT5IJ:scholar.google.com/&scioq=Learning+Space+Partitions+For+Nearest+Neighbor+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.08544"}, "Selection Via Proxy: Efficient Data Selection For Deep Learning": {"container_type": "Publication", "bib": {"title": "Selection via proxy: Efficient data selection for deep learning", "author": ["C Coleman", "C Yeh", "S Mussmann"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Data selection methods, such as active learning and core-set selection, are useful tools for machine learning on large datasets. However, they can be prohibitively expensive to apply in deep learning because they depend on feature representations that need to be learned. In this work, we show that we can greatly improve the computational efficiency by using a small proxy model to perform data selection (eg, selecting data points to label for active learning). By removing hidden layers from the target model, using smaller architectures, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.11829", "author_id": ["OtJC70cAAAAJ", "6Omq2pcAAAAJ", "oGah6EgAAAAJ"], "url_scholarbib": "/scholar?q=info:dJlG-8twMpMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelection%2BVia%2BProxy:%2BEfficient%2BData%2BSelection%2BFor%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dJlG-8twMpMJ&ei=iuNXYoTyKsS4ywTtzb_QDA&json=", "num_citations": 51, "citedby_url": "/scholar?cites=10606664093807319412&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dJlG-8twMpMJ:scholar.google.com/&scioq=Selection+Via+Proxy:+Efficient+Data+Selection+For+Deep+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.11829"}, "Sumo: Unbiased Estimation Of Log Marginal Probability For Latent Variable Models": {"container_type": "Publication", "bib": {"title": "Sumo: Unbiased estimation of log marginal probability for latent variable models", "author": ["Y Luo", "A Beatson", "M Norouzi", "J Zhu"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Standard variational lower bounds used to train latent variable models produce biased estimates of most quantities of interest. We introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series. If parameterized by an encoder-decoder architecture, the parameters of the encoder can be optimized to minimize its variance of this estimator. We show that models trained using our estimator give better test-set likelihoods than a standard"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.00353", "author_id": ["__wMZSYAAAAJ", "xCjAOsgAAAAJ", "Lncr-VoAAAAJ", "axsP38wAAAAJ"], "url_scholarbib": "/scholar?q=info:BKTutLVLTWAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSumo:%2BUnbiased%2BEstimation%2BOf%2BLog%2BMarginal%2BProbability%2BFor%2BLatent%2BVariable%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BKTutLVLTWAJ&ei=juNXYov-HM6E6rQPz8uiuAc&json=", "num_citations": 18, "citedby_url": "/scholar?cites=6939285844644504580&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BKTutLVLTWAJ:scholar.google.com/&scioq=Sumo:+Unbiased+Estimation+Of+Log+Marginal+Probability+For+Latent+Variable+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.00353"}, "Gradients As Features For Deep Representation Learning": {"container_type": "Publication", "bib": {"title": "Gradients as features for deep representation learning", "author": ["F Mu", "Y Liang", "Y Li"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.05529", "abstract": "We address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We show that our model provides a local linear approximation to an underlying deep model, and discuss"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.05529", "author_id": ["OOymFJsAAAAJ", "_RVvnS4AAAAJ", "_y-8nrcAAAAJ"], "url_scholarbib": "/scholar?q=info:NiaM1LYLIm8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradients%2BAs%2BFeatures%2BFor%2BDeep%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NiaM1LYLIm8J&ei=m-NXYpXxAs2Ny9YPqPyUgAs&json=", "num_citations": 18, "citedby_url": "/scholar?cites=8007975967296071222&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:NiaM1LYLIm8J:scholar.google.com/&scioq=Gradients+As+Features+For+Deep+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.05529"}, "Frequency-based Search-control In Dyna": {"container_type": "Publication", "bib": {"title": "Frequency-based Search-control in Dyna", "author": ["Y Pan", "J Mei", "A Farahmand"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05822", "abstract": "Model-based reinforcement learning has been empirically demonstrated as a successful strategy to improve sample efficiency. In particular, Dyna is an elegant model-based architecture integrating learning and planning that provides huge flexibility of using a model. One of the most important components in Dyna is called search-control, which refers to the process of generating state or state-action pairs from which we query the model to acquire simulated experiences. Search-control is critical in improving learning efficiency. In this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05822", "author_id": ["QyAsyYEAAAAJ", "g2uX6_gAAAAJ", "G5SAV7gAAAAJ"], "url_scholarbib": "/scholar?q=info:dP23wTu7jCcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrequency-based%2BSearch-control%2BIn%2BDyna%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dP23wTu7jCcJ&ei=neNXYvjyN86E6rQPz8uiuAc&json=", "num_citations": 6, "citedby_url": "/scholar?cites=2849858529546206580&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dP23wTu7jCcJ:scholar.google.com/&scioq=Frequency-based+Search-control+In+Dyna&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05822"}, "Demystifying Inter-class Disentanglement": {"container_type": "Publication", "bib": {"title": "Demystifying inter-class disentanglement", "author": ["A Gabbay", "Y Hoshen"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.11796", "abstract": "to as disentanglement. In this paper, we present a new method for achieving disentanglement   There are multiple settings for disentanglement. The simplest is fully supervised - for each"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.11796", "author_id": ["fv_z0ewAAAAJ", "6y1-qS4AAAAJ"], "url_scholarbib": "/scholar?q=info:dkpcUk0gW0UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDemystifying%2BInter-class%2BDisentanglement%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dkpcUk0gW0UJ&ei=oeNXYtnuC4vMsQK69Y7ABg&json=", "num_citations": 33, "citedby_url": "/scholar?cites=4997623727964047990&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dkpcUk0gW0UJ:scholar.google.com/&scioq=Demystifying+Inter-class+Disentanglement&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.11796"}, "The Intriguing Role Of Module Criticality In The Generalization Of Deep Networks": {"container_type": "Publication", "bib": {"title": "The intriguing role of module criticality in the generalization of deep networks", "author": ["NS Chatterji", "B Neyshabur", "H Sedghi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.00528", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connects the initial and final values of the module parameters. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.00528", "author_id": ["oWcFfZcAAAAJ", "e1ucbCYAAAAJ", "_9GX96fDWAMC"], "url_scholarbib": "/scholar?q=info:Uu97WoAuFRYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BIntriguing%2BRole%2BOf%2BModule%2BCriticality%2BIn%2BThe%2BGeneralization%2BOf%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Uu97WoAuFRYJ&ei=pONXYqrNGY6pywSdh6agAg&json=", "num_citations": 20, "citedby_url": "/scholar?cites=1591229172154101586&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Uu97WoAuFRYJ:scholar.google.com/&scioq=The+Intriguing+Role+Of+Module+Criticality+In+The+Generalization+Of+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.00528"}, "Training Individually Fair Ml Models With Sensitive Subspace Robustness": {"container_type": "Publication", "bib": {"title": "Training individually fair ML models with sensitive subspace robustness", "author": ["M Yurochkin", "A Bower", "Y Sun"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.00020", "abstract": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.00020", "author_id": ["QjBF9sUAAAAJ", "J3r0-xIAAAAJ", "6T1XtW8AAAAJ"], "url_scholarbib": "/scholar?q=info:On82a51wOfsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BIndividually%2BFair%2BMl%2BModels%2BWith%2BSensitive%2BSubspace%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=On82a51wOfsJ&ei=qONXYum9HI2EmgH6u5u4BA&json=", "num_citations": 40, "citedby_url": "/scholar?cites=18102623998603329338&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:On82a51wOfsJ:scholar.google.com/&scioq=Training+Individually+Fair+Ml+Models+With+Sensitive+Subspace+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.00020"}, "Causal Discovery With Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Causal discovery with reinforcement learning", "author": ["S Zhu", "I Ng", "Z Chen"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.04477", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, eg, greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are usually less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.04477", "author_id": ["", "", ""], "url_scholarbib": "/scholar?q=info:LIyoqa9yiNoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCausal%2BDiscovery%2BWith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LIyoqa9yiNoJ&ei=teNXYrWfC46pywSdh6agAg&json=", "num_citations": 75, "citedby_url": "/scholar?cites=15746962195892177964&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LIyoqa9yiNoJ:scholar.google.com/&scioq=Causal+Discovery+With+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.04477"}, "Q-learning With Ucb Exploration Is Sample Efficient For Infinite-horizon Mdp": {"container_type": "Publication", "bib": {"title": "Q-learning with ucb exploration is sample efficient for infinite-horizon mdp", "author": ["K Dong", "Y Wang", "X Chen", "L Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1901.09311", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al.\\cite {jin2018q} proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards\\emph {without} accessing a generative model. We show that the\\textit {sample complexity of exploration} of our algorithm is bounded by $\\tilde"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1901.09311", "author_id": ["XalUZEoAAAAJ", "yj2b7pgAAAAJ", "sioumZAAAAAJ", "VZHxoh8AAAAJ"], "url_scholarbib": "/scholar?q=info:Q5TDlhvbbJEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQ-learning%2BWith%2BUcb%2BExploration%2BIs%2BSample%2BEfficient%2BFor%2BInfinite-horizon%2BMdp%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Q5TDlhvbbJEJ&ei=ueNXYvuOB42EmgH6u5u4BA&json=", "num_citations": 61, "citedby_url": "/scholar?cites=10478991344524301379&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Q5TDlhvbbJEJ:scholar.google.com/&scioq=Q-learning+With+Ucb+Exploration+Is+Sample+Efficient+For+Infinite-horizon+Mdp&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1901.09311"}, "Few-shot Learning On Graphs Via Super-classes Based On Graph Spectral Measures": {"container_type": "Publication", "bib": {"title": "Few-shot learning on graphs via super-classes based on graph spectral measures", "author": ["J Chauhan", "D Nathani", "M Kaul"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.12815", "abstract": "We propose to study the problem of few shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graphs"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.12815", "author_id": ["kTiFFPcAAAAJ", "HZSadHkAAAAJ", "jNroyK4AAAAJ"], "url_scholarbib": "/scholar?q=info:_sTO9nKf1cYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFew-shot%2BLearning%2BOn%2BGraphs%2BVia%2BSuper-classes%2BBased%2BOn%2BGraph%2BSpectral%2BMeasures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_sTO9nKf1cYJ&ei=vONXYoScO82Ny9YPqPyUgAs&json=", "num_citations": 23, "citedby_url": "/scholar?cites=14327533105664935166&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_sTO9nKf1cYJ:scholar.google.com/&scioq=Few-shot+Learning+On+Graphs+Via+Super-classes+Based+On+Graph+Spectral+Measures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.12815"}, "Glad: Learning Sparse Graph Recovery": {"container_type": "Publication", "bib": {"title": "GLAD: Learning sparse graph recovery", "author": ["H Shrivastava", "X Chen", "B Chen", "G Lan", "S Aluru"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an $\\ell_1 $ regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.00271", "author_id": ["8ZDK7c8AAAAJ", "irg46EUAAAAJ", "6Px5HxsAAAAJ", "", "YOGOScoAAAAJ"], "url_scholarbib": "/scholar?q=info:7mJh3-Eva_AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGlad:%2BLearning%2BSparse%2BGraph%2BRecovery%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7mJh3-Eva_AJ&ei=wONXYrnkDpWMy9YPt8OamA0&json=", "num_citations": 10, "citedby_url": "/scholar?cites=17323993038772593390&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7mJh3-Eva_AJ:scholar.google.com/&scioq=Glad:+Learning+Sparse+Graph+Recovery&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.00271"}, "Continual Learning With Bayesian Neural Networks For Non-stationary Data": {"container_type": "Publication", "bib": {"title": "Continual learning with bayesian neural networks for non-stationary data", "author": ["R Kurle", "B Cseke", "A Klushyn"], "pub_year": "2019", "venue": "International \u2026", "abstract": "This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes. We represent the posterior approximation of the network weights by a diagonal Gaussian distribution and a complementary memory of raw data. This raw data corresponds to likelihood terms that cannot be well approximated by the Gaussian. We introduce a novel method for sequentially updating both components of the posterior approximation. Furthermore, we propose"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJlsFpVtDB", "author_id": ["q2YBN34AAAAJ", "v23xgC0AAAAJ", "4xT5H2oAAAAJ"], "url_scholarbib": "/scholar?q=info:VxOIuV8LhUEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContinual%2BLearning%2BWith%2BBayesian%2BNeural%2BNetworks%2BFor%2BNon-stationary%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VxOIuV8LhUEJ&ei=wuNXYouyJZGJmwGIxre4DA&json=", "num_citations": 31, "citedby_url": "/scholar?cites=4721192290130334551&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VxOIuV8LhUEJ:scholar.google.com/&scioq=Continual+Learning+With+Bayesian+Neural+Networks+For+Non-stationary+Data&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJlsFpVtDB"}, "Tabfact: A Large-scale Dataset For Table-based Fact Verification": {"container_type": "Publication", "bib": {"title": "Tabfact: A large-scale dataset for table-based fact verification", "author": ["W Chen", "H Wang", "J Chen", "Y Zhang", "H Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (eg, natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.02164", "author_id": ["U8ShbhUAAAAJ", "zp8BaywAAAAJ", "jQeFWdoAAAAJ", "J48boCIAAAAJ", "M9uQHIUAAAAJ"], "url_scholarbib": "/scholar?q=info:cnplYtilhewJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTabfact:%2BA%2BLarge-scale%2BDataset%2BFor%2BTable-based%2BFact%2BVerification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cnplYtilhewJ&ei=xeNXYsfDMZWMy9YPt8OamA0&json=", "num_citations": 115, "citedby_url": "/scholar?cites=17043210713635846770&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cnplYtilhewJ:scholar.google.com/&scioq=Tabfact:+A+Large-scale+Dataset+For+Table-based+Fact+Verification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.02164.pdf?ref=https://githubhelp.com"}, "Snow: Subscribing To Knowledge Via Channel Pooling For Transfer & Lifelong Learning": {"container_type": "Publication", "bib": {"title": "SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks", "author": ["C Yoo", "B Kang", "M Cho"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "SNOW is an efficient learning method to improve training/serving throughput as well as accuracy for transfer and lifelong learning of convolutional neural networks based on knowledge subscription. SNOW selects the top-K useful intermediate feature maps for a target task from a pre-trained and frozen source model through a novel channel pooling scheme, and utilizes them in the task-specific delta model. The source model is responsible for generating a large number of generic feature maps. Meanwhile, the delta model"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rJxtgJBKDr", "author_id": ["eNbpQSsAAAAJ", "BGdKqCoAAAAJ", "_AZys7EAAAAJ"], "url_scholarbib": "/scholar?q=info:Fa_XNMHIuZwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSnow:%2BSubscribing%2BTo%2BKnowledge%2BVia%2BChannel%2BPooling%2BFor%2BTransfer%2B%2526%2BLifelong%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Fa_XNMHIuZwJ&ei=yeNXYsu5AsWemAHB5baIBQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=11293278272749022997&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Fa_XNMHIuZwJ:scholar.google.com/&scioq=Snow:+Subscribing+To+Knowledge+Via+Channel+Pooling+For+Transfer+%26+Lifelong+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rJxtgJBKDr"}, "Fair Resource Allocation In Federated Learning": {"container_type": "Publication", "bib": {"title": "Fair resource allocation in federated learning", "author": ["T Li", "M Sanjabi", "A Beirami", "V Smith"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.10497", "abstract": "Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, we propose q-Fair Federated Learning (q-FFL), a novel optimization objective inspired by fair resource allocation in wireless networks that encourages a more fair (specifically, a more uniform) accuracy distribution across devices in federated networks. To solve q-FFL, we devise a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10497", "author_id": ["8JWoJrAAAAAJ", "bc_N2-oAAAAJ", "VuKWbMMAAAAJ", "bldHpWIAAAAJ"], "url_scholarbib": "/scholar?q=info:Lnl0QFZEstwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFair%2BResource%2BAllocation%2BIn%2BFederated%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Lnl0QFZEstwJ&ei=zONXYuWMK5aM6rQPlISayA8&json=", "num_citations": 283, "citedby_url": "/scholar?cites=15902848371437893934&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Lnl0QFZEstwJ:scholar.google.com/&scioq=Fair+Resource+Allocation+In+Federated+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10497.pdf?ref=https://githubhelp.com"}, "Decentralized Distributed Ppo: Mastering Pointgoal Navigation": {"container_type": "Publication", "bib": {"title": "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames", "author": ["E Wijmans", "A Kadian", "A Morcos", "S Lee", "I Essa"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Finally, we show that the scene understanding and navigation policies learned can be   PPO that runs on one GPU we create a decentralized distributed variant by adding gradient syn"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.00357", "author_id": ["9v86038AAAAJ", "wjJnkcAAAAAJ", "v-A_7UsAAAAJ", "8j3t5HsAAAAJ", "XM97iScAAAAJ"], "url_scholarbib": "/scholar?q=info:iUY9-I3iykMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecentralized%2BDistributed%2BPpo:%2BMastering%2BPointgoal%2BNavigation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iUY9-I3iykMJ&ei=0ONXYtDbGsS4ywTtzb_QDA&json=", "num_citations": 134, "citedby_url": "/scholar?cites=4884965845219755657&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iUY9-I3iykMJ:scholar.google.com/&scioq=Decentralized+Distributed+Ppo:+Mastering+Pointgoal+Navigation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.00357"}, "Capsules With Inverted Dot-product Attention Routing": {"container_type": "Publication", "bib": {"title": "Capsules with inverted dot-product attention routing", "author": ["YHH Tsai", "N Srivastava", "H Goh"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing. When compared to previously proposed routing algorithms, our method improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100, and it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.04764", "author_id": ["TMimDRoAAAAJ", "s1PgoeUAAAAJ", "B_5DFeMAAAAJ"], "url_scholarbib": "/scholar?q=info:_-NEQQ8dUe4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCapsules%2BWith%2BInverted%2BDot-product%2BAttention%2BRouting%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_-NEQQ8dUe4J&ei=1ONXYo3aMZWMy9YPt8OamA0&json=", "num_citations": 45, "citedby_url": "/scholar?cites=17172538805497160703&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_-NEQQ8dUe4J:scholar.google.com/&scioq=Capsules+With+Inverted+Dot-product+Attention+Routing&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.04764"}, "Rethinking Softmax Cross-entropy Loss For Adversarial Robustness": {"container_type": "Publication", "bib": {"title": "Rethinking softmax cross-entropy loss for adversarial robustness", "author": ["T Pang", "K Xu", "Y Dong", "C Du", "N Chen", "J Zhu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Previous work shows that adversarially robust generalization requires larger sample complexity, and the same dataset, eg, CIFAR-10, which enables good standard accuracy may not suffice to train robust models. Since collecting new training data could be costly, we focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. We first formally show that the softmax cross-entropy (SCE) loss and its variants convey"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10626", "author_id": ["wYDbtFsAAAAJ", "gfl-HVYAAAAJ", "6_4ad84AAAAJ", "QOp7xW0AAAAJ", "cSxeVz0AAAAJ", "axsP38wAAAAJ"], "url_scholarbib": "/scholar?q=info:Q8I1KweaHLQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2BSoftmax%2BCross-entropy%2BLoss%2BFor%2BAdversarial%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Q8I1KweaHLQJ&ei=2ONXYprHD4vMsQK69Y7ABg&json=", "num_citations": 81, "citedby_url": "/scholar?cites=12978417581755318851&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Q8I1KweaHLQJ:scholar.google.com/&scioq=Rethinking+Softmax+Cross-entropy+Loss+For+Adversarial+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10626"}, "Snode: Spectral Discretization Of Neural Odes For System Identification": {"container_type": "Publication", "bib": {"title": "Snode: Spectral discretization of neural odes for system identification", "author": ["A Quaglino", "M Gallieri", "J Masci", "J Koutn\u00edk"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper proposes the use of spectral element methods\\citep {canuto_spectral_1988} for fast and accurate training of Neural Ordinary Differential Equations (ODE-Nets;\\citealp {Chen2018NeuralOD}) for system identification. This is achieved by expressing their dynamics as a truncated series of Legendre polynomials. The series coefficients, as well as the network weights, are computed by minimizing the weighted sum of the loss function and the violation of the ODE-Net dynamics. The problem is solved by coordinate descent that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.07038", "author_id": ["a_KA_MkAAAAJ", "moNjsXoAAAAJ", "HwDTzQEAAAAJ", ""], "url_scholarbib": "/scholar?q=info:2Muc-S0vHYAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSnode:%2BSpectral%2BDiscretization%2BOf%2BNeural%2BOdes%2BFor%2BSystem%2BIdentification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2Muc-S0vHYAJ&ei=4uNXYu_5N5GJmwGIxre4DA&json=", "num_citations": 26, "citedby_url": "/scholar?cites=9231586685687221208&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2Muc-S0vHYAJ:scholar.google.com/&scioq=Snode:+Spectral+Discretization+Of+Neural+Odes+For+System+Identification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.07038"}, "Program Guided Agent": {"container_type": "Publication", "bib": {"title": "Program guided agent", "author": ["SH Sun", "TL Wu", "JJ Lim"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "obtain program execution results for evaluating program synthesis frameworks when no  program  our problem formulation where the agent is asked to follow a given program/procedure."}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BkxUvnEYDH", "author_id": ["uXsfnaQAAAAJ", "Q5aezXQAAAAJ", "jTnQTBoAAAAJ"], "url_scholarbib": "/scholar?q=info:7-TqjiGFMqsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgram%2BGuided%2BAgent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7-TqjiGFMqsJ&ei=5eNXYrWsOs2Ny9YPqPyUgAs&json=", "num_citations": 16, "citedby_url": "/scholar?cites=12336068708499383535&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7-TqjiGFMqsJ:scholar.google.com/&scioq=Program+Guided+Agent&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BkxUvnEYDH"}, "Inductive And Unsupervised Representation Learning On Graph Structured Objects": {"container_type": "Publication", "bib": {"title": "Inductive and unsupervised representation learning on graph structured objects", "author": ["L Wang", "B Zong", "Q Ma", "W Cheng", "J Ni"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Inductive and unsupervised graph learning is a critical technique for predictive or information retrieval tasks where label information is difficult to obtain. It is also challenging to make graph learning inductive and unsupervised at the same time, as learning processes guided by reconstruction error based loss functions inevitably demand graph similarity evaluation that is usually computationally intractable. In this paper, we propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkem91rtDB", "author_id": ["cE25iX4AAAAJ", "GZKO2WoAAAAJ", "qmIm4K4AAAAJ", "PRrGVmoAAAAJ", "31RY2zYAAAAJ"], "url_scholarbib": "/scholar?q=info:IioDiZNMhioJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInductive%2BAnd%2BUnsupervised%2BRepresentation%2BLearning%2BOn%2BGraph%2BStructured%2BObjects%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IioDiZNMhioJ&ei=6eNXYs2dN5aM6rQPlISayA8&json=", "num_citations": 13, "citedby_url": "/scholar?cites=3064220793014790690&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IioDiZNMhioJ:scholar.google.com/&scioq=Inductive+And+Unsupervised+Representation+Learning+On+Graph+Structured+Objects&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkem91rtDB"}, "Logic And The 2-simplicial Transformer": {"container_type": "Publication", "bib": {"title": "Logic and the -Simplicial Transformer", "author": ["J Clift", "D Doryn", "D Murfet", "J Wallbridge"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.00668", "abstract": "We introduce the $2 $-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. We show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.00668", "author_id": ["", "", "ilWvLq0AAAAJ", ""], "url_scholarbib": "/scholar?q=info:yes0pTLAwyoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLogic%2BAnd%2BThe%2B2-simplicial%2BTransformer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yes0pTLAwyoJ&ei=7uNXYuj1NsiBy9YP18Gi8As&json=", "num_citations": 1, "citedby_url": "/scholar?cites=3081517893804157897&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:yes0pTLAwyoJ:scholar.google.com/&scioq=Logic+And+The+2-simplicial+Transformer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.00668"}, "Implicit Bias Of Gradient Descent Based Adversarial Training On Separable Data": {"container_type": "Publication", "bib": {"title": "Implicit bias of gradient descent based adversarial training on separable data", "author": ["Y Li", "EX Fang", "H Xu", "T Zhao"], "pub_year": "2020", "venue": "NA", "abstract": "Adversarial training is a principled approach for training robust neural networks. Despite of tremendous successes in practice, its theoretical properties still remain largely unexplored. In this paper, we provide new theoretical insights of gradient descent based adversarial training by studying its computational properties, specifically on its implicit bias. We take the binary classification task on linearly separable data as an illustrative example, where the loss asymptotically attains its infimum as the parameter diverges to infinity along certain"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=qUtDupanTMy", "author_id": ["wLfoeakAAAAJ", "uglffdcAAAAJ", "", "EJXN6tYAAAAJ"], "url_scholarbib": "/scholar?q=info:dlj-P7ugajkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImplicit%2BBias%2BOf%2BGradient%2BDescent%2BBased%2BAdversarial%2BTraining%2BOn%2BSeparable%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dlj-P7ugajkJ&ei=9ONXYtvKIMS4ywTtzb_QDA&json=", "num_citations": 16, "citedby_url": "/scholar?cites=4137295933786183798&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dlj-P7ugajkJ:scholar.google.com/&scioq=Implicit+Bias+Of+Gradient+Descent+Based+Adversarial+Training+On+Separable+Data&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=qUtDupanTMy"}, "Graph Neural Networks Exponentially Lose Expressive Power For Node Classification": {"container_type": "Publication", "bib": {"title": "Graph neural networks exponentially lose expressive power for node classification", "author": ["K Oono", "T Suzuki"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.10947", "abstract": "Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10947", "author_id": ["CYAdEBkAAAAJ", "x8osrBsAAAAJ"], "url_scholarbib": "/scholar?q=info:OK8s70YHMdQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BNeural%2BNetworks%2BExponentially%2BLose%2BExpressive%2BPower%2BFor%2BNode%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OK8s70YHMdQJ&ei=9-NXYvKNM8S4ywTtzb_QDA&json=", "num_citations": 182, "citedby_url": "/scholar?cites=15290010211141332792&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OK8s70YHMdQJ:scholar.google.com/&scioq=Graph+Neural+Networks+Exponentially+Lose+Expressive+Power+For+Node+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10947.pdf?ref=https://codemonkey.link"}, "Understanding Architectures Learnt By Cell-based Neural Architecture Search": {"container_type": "Publication", "bib": {"title": "Understanding architectures learnt by cell-based neural architecture search", "author": ["Y Shu", "W Wang", "S Cai"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.09569", "abstract": "Neural architecture search (NAS) searches architectures automatically for given tasks, eg, image classification and language modeling. Improving the search efficiency and effectiveness have attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures. In this paper, we first reveal that existing NAS algorithms (eg, DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.09569", "author_id": ["qb9STggAAAAJ", "46Dd4v4AAAAJ", "Nzr-hIoAAAAJ"], "url_scholarbib": "/scholar?q=info:ugte7pdxqmkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BArchitectures%2BLearnt%2BBy%2BCell-based%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ugte7pdxqmkJ&ei=B-RXYt3YGIvEmgH7846QCg&json=", "num_citations": 60, "citedby_url": "/scholar?cites=7614023017376385978&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ugte7pdxqmkJ:scholar.google.com/&scioq=Understanding+Architectures+Learnt+By+Cell-based+Neural+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.09569"}, "Picking Winning Tickets Before Training By Preserving Gradient Flow": {"container_type": "Publication", "bib": {"title": "Picking winning tickets before training by preserving gradient flow", "author": ["C Wang", "G Zhang", "R Grosse"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.07376", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07376", "author_id": ["yN2iRpwAAAAJ", "B_TZBtwAAAAJ", "xgQd1qgAAAAJ"], "url_scholarbib": "/scholar?q=info:2aXPdXWiX4MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPicking%2BWinning%2BTickets%2BBefore%2BTraining%2BBy%2BPreserving%2BGradient%2BFlow%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2aXPdXWiX4MJ&ei=CuRXYufWMI2EmgH6u5u4BA&json=", "num_citations": 185, "citedby_url": "/scholar?cites=9466463567127487961&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2aXPdXWiX4MJ:scholar.google.com/&scioq=Picking+Winning+Tickets+Before+Training+By+Preserving+Gradient+Flow&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07376"}, "Clevrer: Collision Events For Video Representation And Reasoning": {"container_type": "Publication", "bib": {"title": "Clevrer: Collision events for video representation and reasoning", "author": ["K Yi", "C Gan", "Y Li", "P Kohli", "J Wu", "A Torralba"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER), a diagnostic video dataset for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01442", "author_id": ["SwxS_JkAAAAJ", "PTeSCbIAAAAJ", "WlA92lcAAAAJ", "3pyzQQ8AAAAJ", "2efgcS0AAAAJ", "8cxDHS4AAAAJ"], "url_scholarbib": "/scholar?q=info:4jX8_ZGjZTwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DClevrer:%2BCollision%2BEvents%2BFor%2BVideo%2BRepresentation%2BAnd%2BReasoning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4jX8_ZGjZTwJ&ei=FuRXYqilLsS4ywTtzb_QDA&json=", "num_citations": 151, "citedby_url": "/scholar?cites=4352064462350202338&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4jX8_ZGjZTwJ:scholar.google.com/&scioq=Clevrer:+Collision+Events+For+Video+Representation+And+Reasoning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01442"}, "Dropedge: Towards Deep Graph Convolutional Networks On Node Classification": {"container_type": "Publication", "bib": {"title": "Dropedge: Towards deep graph convolutional networks on node classification", "author": ["Y Rong", "W Huang", "T Xu", "J Huang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.10903", "abstract": "\\emph {Over-fitting} and\\emph {over-smoothing} are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.10903", "author_id": ["itezhEMAAAAJ", "0yNkmO4AAAAJ", "6gIs5YMAAAAJ", "X7KrguAAAAAJ"], "url_scholarbib": "/scholar?q=info:07WW29TW0N8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDropedge:%2BTowards%2BDeep%2BGraph%2BConvolutional%2BNetworks%2BOn%2BNode%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=07WW29TW0N8J&ei=GeRXYuCWD5GJmwGIxre4DA&json=", "num_citations": 420, "citedby_url": "/scholar?cites=16127626475319244243&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:07WW29TW0N8J:scholar.google.com/&scioq=Dropedge:+Towards+Deep+Graph+Convolutional+Networks+On+Node+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.10903"}, "On Bonus Based Exploration Methods In The Arcade Learning Environment": {"container_type": "Publication", "bib": {"title": "On Bonus-Based Exploration Methods in the Arcade Learning Environment", "author": ["AA Taiga", "W Fedus", "MC Machado", "A Courville"], "pub_year": "2021", "venue": "arXiv preprint arXiv \u2026", "abstract": "Research on exploration in reinforcement learning, as applied to Atari 2600 game-playing, has emphasized tackling difficult exploration problems such as Montezuma's Revenge (Bellemare et al., 2016). Recently, bonus-based exploration methods, which explore by augmenting the environment reward, have reached above-human average performance on such domains. In this paper we reassess popular bonus-based exploration methods within a common evaluation framework. We combine Rainbow (Hessel et al., 2018) with different"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2109.11052", "author_id": ["rRbCqtoAAAAJ", "-ZfwQOkAAAAJ", "xf_n4xUAAAAJ", "km6CP8cAAAAJ"], "url_scholarbib": "/scholar?q=info:CjbVjDRWTvYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BBonus%2BBased%2BExploration%2BMethods%2BIn%2BThe%2BArcade%2BLearning%2BEnvironment%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CjbVjDRWTvYJ&ei=HORXYv_OJ5WMy9YPt8OamA0&json=", "num_citations": 31, "citedby_url": "/scholar?cites=17748217965214774794&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CjbVjDRWTvYJ:scholar.google.com/&scioq=On+Bonus+Based+Exploration+Methods+In+The+Arcade+Learning+Environment&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2109.11052"}, "Certified Defenses For Adversarial Patches": {"container_type": "Publication", "bib": {"title": "Certified defenses for adversarial patches", "author": ["PY Chiang", "R Ni", "A Abdelkader", "C Zhu"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial patch attacks are among one of the most practical threat models against real-world computer vision systems. This paper studies certified and empirical defenses against patch attacks. We begin with a set of experiments showing that most existing defenses, which work by pre-processing input images to mitigate adversarial patches, are easily broken by simple white-box adversaries. Motivated by this finding, we propose the first certified defense against patch attacks, and propose faster methods for its training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.06693", "author_id": ["WUoMq1IAAAAJ", "RYWzdAQAAAAJ", "Dr_TwpwAAAAJ", "m-om5O8AAAAJ"], "url_scholarbib": "/scholar?q=info:xgIFPsn0JCkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCertified%2BDefenses%2BFor%2BAdversarial%2BPatches%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xgIFPsn0JCkJ&ei=KORXYtLRCJWMy9YPt8OamA0&json=", "num_citations": 68, "citedby_url": "/scholar?cites=2964763599882748614&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xgIFPsn0JCkJ:scholar.google.com/&scioq=Certified+Defenses+For+Adversarial+Patches&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.06693"}, "Multilingual Alignment Of Contextual Word Representations": {"container_type": "Publication", "bib": {"title": "Multilingual alignment of contextual word representations", "author": ["S Cao", "N Kitaev", "D Klein"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.03518", "abstract": "We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.03518", "author_id": ["teCqOtsAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:HyLK3ABg-wYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultilingual%2BAlignment%2BOf%2BContextual%2BWord%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HyLK3ABg-wYJ&ei=K-RXYuWOGIvEmgH7846QCg&json=", "num_citations": 85, "citedby_url": "/scholar?cites=503101340202443295&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HyLK3ABg-wYJ:scholar.google.com/&scioq=Multilingual+Alignment+Of+Contextual+Word+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.03518"}, "Nonlinearities In Activations Substantially Shape The Loss Surfaces Of Neural Networks": {"container_type": "Publication", "bib": {"title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "author": ["F He", "B Wang", "D Tao"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2003.12236", "abstract": "piecewise linear activation functions substantially shape the loss surfaces of neural networks.  We  This paper reports that the nonlinearities in activations substantially shape the loss"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.12236", "author_id": ["QSx-Yu0AAAAJ", "LfkHCEUAAAAJ", "RwlJNLcAAAAJ"], "url_scholarbib": "/scholar?q=info:THZ1DCeAlVIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNonlinearities%2BIn%2BActivations%2BSubstantially%2BShape%2BThe%2BLoss%2BSurfaces%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=THZ1DCeAlVIJ&ei=LuRXYqDiMovMsQK69Y7ABg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=5950803387841082956&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:THZ1DCeAlVIJ:scholar.google.com/&scioq=Nonlinearities+In+Activations+Substantially+Shape+The+Loss+Surfaces+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.12236"}, "Global Relational Models Of Source Code": {"container_type": "Publication", "bib": {"title": "Global relational models of source code", "author": ["VJ Hellendoorn", "C Sutton", "R Singh"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (eg data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1lnbRNtwr", "author_id": ["PfYrc5kAAAAJ", "hYtGXD0AAAAJ", "5kVcNS4AAAAJ"], "url_scholarbib": "/scholar?q=info:t67usfwnlUwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGlobal%2BRelational%2BModels%2BOf%2BSource%2BCode%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=t67usfwnlUwJ&ei=MuRXYpTgK5GJmwGIxre4DA&json=", "num_citations": 88, "citedby_url": "/scholar?cites=5518360884682862263&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:t67usfwnlUwJ:scholar.google.com/&scioq=Global+Relational+Models+Of+Source+Code&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1lnbRNtwr"}, "Never Give Up: Learning Directed Exploration Strategies": {"container_type": "Publication", "bib": {"title": "Never give up: Learning directed exploration strategies", "author": ["AP Badia", "P Sprechmann", "A Vitvitskyi", "D Guo"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06038", "author_id": ["DcWRJW4AAAAJ", "YCPycGAAAAAJ", "uebYofUAAAAJ", "fxr_9oQAAAAJ"], "url_scholarbib": "/scholar?q=info:iPdsRFm8_H8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNever%2BGive%2BUp:%2BLearning%2BDirected%2BExploration%2BStrategies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iPdsRFm8_H8J&ei=NeRXYvWaFo6pywSdh6agAg&json=", "num_citations": 97, "citedby_url": "/scholar?cites=9222453228534036360&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:iPdsRFm8_H8J:scholar.google.com/&scioq=Never+Give+Up:+Learning+Directed+Exploration+Strategies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06038"}, "Black-box Off-policy Estimation For Infinite-horizon Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Black-box off-policy estimation for infinite-horizon reinforcement learning", "author": ["A Mousavi", "L Li", "Q Liu", "D Zhou"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2003.11126", "abstract": "Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible. Recently,\\cite {liu18breaking} proposed an approach that avoids the\\emph {curse of horizon} suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data be drawn from the\\emph {stationary distribution} of a\\emph {known} behavior"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.11126", "author_id": ["Yuxd6uYAAAAJ", "Rqy5KDEAAAAJ", "XEx1fZkAAAAJ", "UwLsYw8AAAAJ"], "url_scholarbib": "/scholar?q=info:LbVdBoTFxyAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBlack-box%2BOff-policy%2BEstimation%2BFor%2BInfinite-horizon%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LbVdBoTFxyAJ&ei=N-RXYtfgOpWMy9YPt8OamA0&json=", "num_citations": 24, "citedby_url": "/scholar?cites=2362073700412273965&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LbVdBoTFxyAJ:scholar.google.com/&scioq=Black-box+Off-policy+Estimation+For+Infinite-horizon+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.11126"}, "Maxmin Q-learning: Controlling The Estimation Bias Of Q-learning": {"container_type": "Publication", "bib": {"title": "Maxmin q-learning: Controlling the estimation bias of q-learning", "author": ["Q Lan", "Y Pan", "A Fyshe", "M White"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06487", "abstract": "Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called\\emph {Maxmin Q-learning}, which provides a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06487", "author_id": ["jKD9jpoAAAAJ", "QyAsyYEAAAAJ", "Vw8z7qwAAAAJ", "t5zdD_IAAAAJ"], "url_scholarbib": "/scholar?q=info:dnQwOjQCJWwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaxmin%2BQ-learning:%2BControlling%2BThe%2BEstimation%2BBias%2BOf%2BQ-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dnQwOjQCJWwJ&ei=OuRXYqC1F5GJmwGIxre4DA&json=", "num_citations": 59, "citedby_url": "/scholar?cites=7792637153572320374&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dnQwOjQCJWwJ:scholar.google.com/&scioq=Maxmin+Q-learning:+Controlling+The+Estimation+Bias+Of+Q-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06487"}, "Improving Neural Language Generation With Spectrum Control": {"container_type": "Publication", "bib": {"title": "Improving neural language generation with spectrum control", "author": ["L Wang", "J Huang", "K Huang", "Z Hu"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ByxY8CNtvr", "author_id": ["VPyxd6kAAAAJ", "ocPXoIkAAAAJ", "3pu8FE0AAAAJ", "x6ct1CsAAAAJ"], "url_scholarbib": "/scholar?q=info:if3baJsxOasJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BNeural%2BLanguage%2BGeneration%2BWith%2BSpectrum%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=if3baJsxOasJ&ei=PeRXYrMHkYmbAYjGt7gM&json=", "num_citations": 19, "citedby_url": "/scholar?cites=12337947197707124105&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:if3baJsxOasJ:scholar.google.com/&scioq=Improving+Neural+Language+Generation+With+Spectrum+Control&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ByxY8CNtvr"}, "Rna Secondary Structure Prediction By Learning Unrolled Algorithms": {"container_type": "Publication", "bib": {"title": "RNA secondary structure prediction by learning unrolled algorithms", "author": ["X Chen", "Y Li", "R Umarov", "X Gao", "L Song"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05810", "abstract": "In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05810", "author_id": ["irg46EUAAAAJ", "8YHZx-AAAAAJ", "421B5LYAAAAJ", "wqdK8ugAAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:wGiTC9NBgQ0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRna%2BSecondary%2BStructure%2BPrediction%2BBy%2BLearning%2BUnrolled%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wGiTC9NBgQ0J&ei=QORXYpjxEo6pywSdh6agAg&json=", "num_citations": 30, "citedby_url": "/scholar?cites=973131369176852672&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wGiTC9NBgQ0J:scholar.google.com/&scioq=Rna+Secondary+Structure+Prediction+By+Learning+Unrolled+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05810"}, "Batchensemble: An Alternative Approach To Efficient Ensemble And Lifelong Learning": {"container_type": "Publication", "bib": {"title": "Batchensemble: an alternative approach to efficient ensemble and lifelong learning", "author": ["Y Wen", "D Tran", "J Ba"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06715", "abstract": "Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble's cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable. In this paper, we propose BatchEnsemble, an ensemble method whose computational and memory costs are significantly lower than typical ensembles"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06715", "author_id": ["J2GzNAkAAAAJ", "wVazIm8AAAAJ", "ymzxRhAAAAAJ"], "url_scholarbib": "/scholar?q=info:om5NLYSJCQAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBatchensemble:%2BAn%2BAlternative%2BApproach%2BTo%2BEfficient%2BEnsemble%2BAnd%2BLifelong%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=om5NLYSJCQAJ&ei=RORXYubpIpaM6rQPlISayA8&json=", "num_citations": 132, "citedby_url": "/scholar?cites=2684475579133602&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:om5NLYSJCQAJ:scholar.google.com/&scioq=Batchensemble:+An+Alternative+Approach+To+Efficient+Ensemble+And+Lifelong+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06715"}, "Robust Reinforcement Learning For Continuous Control With Model Misspecification": {"container_type": "Publication", "bib": {"title": "Robust reinforcement learning for continuous control with model misspecification", "author": ["DJ Mankowitz", "N Levine", "R Jeong", "Y Shi", "J Kay"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We provide a framework for incorporating robustness--to perturbations in the transition dynamics which we refer to as model misspecification--into continuous control Reinforcement Learning (RL) algorithms. We specifically focus on incorporating robustness into a state-of-the-art continuous control RL algorithm called Maximum a-posteriori Policy Optimization (MPO). We achieve this by learning a policy that optimizes for a worst case expected return objective and derive a corresponding robust entropy-regularized Bellman"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.07516", "author_id": ["v84tWxsAAAAJ", "p1HAb2QAAAAJ", "p9R5_aIAAAAJ", "kQyQ_vwAAAAJ", "wuss22UAAAAJ"], "url_scholarbib": "/scholar?q=info:-ayPLxlbRgMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BReinforcement%2BLearning%2BFor%2BContinuous%2BControl%2BWith%2BModel%2BMisspecification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-ayPLxlbRgMJ&ei=SORXYvz7BsiBy9YP18Gi8As&json=", "num_citations": 49, "citedby_url": "/scholar?cites=235976194213784825&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-ayPLxlbRgMJ:scholar.google.com/&scioq=Robust+Reinforcement+Learning+For+Continuous+Control+With+Model+Misspecification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.07516"}, "Differentiable Programming For Physical Simulation": {"container_type": "Publication", "bib": {"title": "Difftaichi: Differentiable programming for physical simulation", "author": ["Y Hu", "L Anderson", "TM Li", "Q Sun", "N Carr"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "differentiable programming language tailored for building high-performance differentiable  physical simulators We present DiffTaichi, a new differentiable programming language for high"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.00935", "author_id": ["MEHV_tIAAAAJ", "FdxS6OEAAAAJ", "Y7MCOdYAAAAJ", "oN7gaqMAAAAJ", "xEFh0AgAAAAJ"], "url_scholarbib": "/scholar?q=info:m3AgD1SuUeIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BProgramming%2BFor%2BPhysical%2BSimulation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m3AgD1SuUeIJ&ei=S-RXYp3-DIvEmgH7846QCg&json=", "num_citations": 145, "citedby_url": "/scholar?cites=16308007401739546779&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m3AgD1SuUeIJ:scholar.google.com/&scioq=Differentiable+Programming+For+Physical+Simulation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.00935"}, "A Closer Look At Deep Policy Gradients": {"container_type": "Publication", "bib": {"title": "A closer look at deep policy gradients", "author": ["A Ilyas", "L Engstrom", "S Santurkar", "D Tsipras"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.02553", "author_id": ["Dtw3YBoAAAAJ", "IWPWNxkAAAAJ", "QMkbFp8AAAAJ", "26eh1jAAAAAJ"], "url_scholarbib": "/scholar?q=info:5KJ7ZALUoNMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCloser%2BLook%2BAt%2BDeep%2BPolicy%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5KJ7ZALUoNMJ&ei=TuRXYoS-HZaM6rQPlISayA8&json=", "num_citations": 33, "citedby_url": "/scholar?cites=15249421445017346788&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5KJ7ZALUoNMJ:scholar.google.com/&scioq=A+Closer+Look+At+Deep+Policy+Gradients&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.02553"}, "Neural Module Networks For Reasoning Over Text": {"container_type": "Publication", "bib": {"title": "Neural module networks for reasoning over text", "author": ["N Gupta", "K Lin", "D Roth", "S Singh", "M Gardner"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.04971", "author_id": ["STiAua8AAAAJ", "InQnNGIAAAAJ", "E-bpPWgAAAAJ", "-hGZC54AAAAJ", "SfKdzrUAAAAJ"], "url_scholarbib": "/scholar?q=info:Wklhpq--ZhwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BModule%2BNetworks%2BFor%2BReasoning%2BOver%2BText%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Wklhpq--ZhwJ&ei=UeRXYoX2LcWemAHB5baIBQ&json=", "num_citations": 68, "citedby_url": "/scholar?cites=2046532742306416986&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Wklhpq--ZhwJ:scholar.google.com/&scioq=Neural+Module+Networks+For+Reasoning+Over+Text&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.04971.pdf?ref=https://githubhelp.com"}, "Dream To Control: Learning Behaviors By Latent Imagination": {"container_type": "Publication", "bib": {"title": "Dream to control: Learning behaviors by latent imagination", "author": ["D Hafner", "T Lillicrap", "J Ba", "M Norouzi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.01603", "abstract": ": \u2022 Learning long-horizon behaviors by latent imagination Model-based agents can be  shortsighted if they use a finite imagination  by imagination in a latent space lets us efficiently learn"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.01603", "author_id": ["VINmGpYAAAAJ", "htPVdRMAAAAJ", "ymzxRhAAAAAJ", "Lncr-VoAAAAJ"], "url_scholarbib": "/scholar?q=info:sePnIg_T0M8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDream%2BTo%2BControl:%2BLearning%2BBehaviors%2BBy%2BLatent%2BImagination%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sePnIg_T0M8J&ei=VORXYtqXEovEmgH7846QCg&json=", "num_citations": 401, "citedby_url": "/scholar?cites=14974700822970491825&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:sePnIg_T0M8J:scholar.google.com/&scioq=Dream+To+Control:+Learning+Behaviors+By+Latent+Imagination&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.01603"}, "Sign Bits Are All You Need For Black-box Attacks": {"container_type": "Publication", "bib": {"title": "Sign bits are all you need for black-box attacks", "author": ["A Al-Dujaili", "UM O'Reilly"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "We present a novel black-box adversarial attack algorithm with state-of-the-art model evasion  rates for query efficiency under l\u221e and l2 metrics. It exploits a sign- 3) We evaluate our"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SygW0TEFwH", "author_id": ["r7g5r7cAAAAJ", "TgQM-B0AAAAJ"], "url_scholarbib": "/scholar?q=info:bB-2onQzrjMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSign%2BBits%2BAre%2BAll%2BYou%2BNeed%2BFor%2BBlack-box%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bB-2onQzrjMJ&ei=VuRXYoPYM5LeyQTms5KQBg&json=", "num_citations": 21, "citedby_url": "/scholar?cites=3723970517921046380&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bB-2onQzrjMJ:scholar.google.com/&scioq=Sign+Bits+Are+All+You+Need+For+Black-box+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SygW0TEFwH"}, "Compositional Languages Emerge In A Neural Iterated Learning Model": {"container_type": "Publication", "bib": {"title": "Compositional languages emerge in a neural iterated learning model", "author": ["Y Ren", "S Guo", "M Labeau", "SB Cohen", "S Kirby"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games. In this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.01365", "author_id": ["5QNce38AAAAJ", "cpOrbSoAAAAJ", "t7K9mKgAAAAJ", "Q4oVM7IAAAAJ", "5_9v0CIAAAAJ"], "url_scholarbib": "/scholar?q=info:5gcs97JkJqoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompositional%2BLanguages%2BEmerge%2BIn%2BA%2BNeural%2BIterated%2BLearning%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5gcs97JkJqoJ&ei=W-RXYuuyOI2EmgH6u5u4BA&json=", "num_citations": 35, "citedby_url": "/scholar?cites=12260597755376568294&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5gcs97JkJqoJ:scholar.google.com/&scioq=Compositional+Languages+Emerge+In+A+Neural+Iterated+Learning+Model&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.01365"}, "Learning To Learn By Zeroth-order Oracle": {"container_type": "Publication", "bib": {"title": "Learning to learn by zeroth-order oracle", "author": ["Y Ruan", "Y Xiong", "S Reddi", "S Kumar"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In the learning to learn (L2L) framework, we cast the design of optimization algorithms as a machine learning problem and use deep neural networks to learn the update rules. In this paper, we extend the L2L framework to zeroth-order (ZO) optimization setting, where no explicit gradient information is available. Our learned optimizer, modeled as recurrent neural network (RNN), first approximates gradient by ZO gradient estimator and then produces parameter update utilizing the knowledge of previous iterations. To reduce high variance"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.09464", "author_id": ["9AdCSywAAAAJ", "DVKxiMkAAAAJ", "70lgwYwAAAAJ", "08CNqrYAAAAJ"], "url_scholarbib": "/scholar?q=info:RNwd92aoRXwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BLearn%2BBy%2BZeroth-order%2BOracle%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RNwd92aoRXwJ&ei=X-RXYrHIDc2Ny9YPqPyUgAs&json=", "num_citations": 9, "citedby_url": "/scholar?cites=8954748594282159172&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:RNwd92aoRXwJ:scholar.google.com/&scioq=Learning+To+Learn+By+Zeroth-order+Oracle&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.09464"}, "Duration-of-stay Storage Assignment Under Uncertainty": {"container_type": "Publication", "bib": {"title": "Duration-of-Stay Storage Assignment under Uncertainty", "author": ["ML Li", "E Wolf", "D Wintz"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1903.05063", "abstract": "Optimizing storage assignment is a central problem in warehousing. Past literature has shown the superiority of the Duration-of-Stay (DoS) method in assigning pallets, but the methodology requires perfect prior knowledge of DoS for each pallet, which is unknown and uncertain under realistic conditions. The dynamic nature of a warehouse further complicates the validity of synthetic data testing that is often conducted for algorithms. In this paper, in collaboration with a large cold storage company, we release the first publicly available set of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.05063", "author_id": ["gnZVifUAAAAJ", "9rbXCNMAAAAJ", "UHsF5ykAAAAJ"], "url_scholarbib": "/scholar?q=info:LOAmKpe5WBAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDuration-of-stay%2BStorage%2BAssignment%2BUnder%2BUncertainty%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LOAmKpe5WBAJ&ei=YuRXYqL_D4vMsQK69Y7ABg&json=", "num_citations": 1, "citedby_url": "/scholar?cites=1177895361455775788&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LOAmKpe5WBAJ:scholar.google.com/&scioq=Duration-of-stay+Storage+Assignment+Under+Uncertainty&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.05063"}, "Rethinking The Hyperparameters For Fine-tuning": {"container_type": "Publication", "bib": {"title": "Rethinking the hyperparameters for fine-tuning", "author": ["H Li", "P Chaudhari", "H Yang", "M Lam"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks.(1) While prior works have thoroughly investigated"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.11770", "author_id": ["BNEeEosAAAAJ", "c_z5hWEAAAAJ", "Y-I1X9QAAAAJ", "dRShpScAAAAJ"], "url_scholarbib": "/scholar?q=info:bMAF47WUs8IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2BThe%2BHyperparameters%2BFor%2BFine-tuning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bMAF47WUs8IJ&ei=Z-RXYsflMovEmgH7846QCg&json=", "num_citations": 47, "citedby_url": "/scholar?cites=14029720773108023404&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bMAF47WUs8IJ:scholar.google.com/&scioq=Rethinking+The+Hyperparameters+For+Fine-tuning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.11770"}, "Cln2inv: Learning Loop Invariants With Continuous Logic Networks": {"container_type": "Publication", "bib": {"title": "CLN2INV: learning loop invariants with continuous logic networks", "author": ["G Ryan", "J Wong", "J Yao", "R Gu", "S Jana"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11542", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11542", "author_id": ["6mW4eCcAAAAJ", "l49M8zUAAAAJ", "I5M0hHsAAAAJ", "5WLG7j8AAAAJ", "SDY9FwUAAAAJ"], "url_scholarbib": "/scholar?q=info:UMzLwmsfcTkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCln2inv:%2BLearning%2BLoop%2BInvariants%2BWith%2BContinuous%2BLogic%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UMzLwmsfcTkJ&ei=bORXYqqrAc6E6rQPz8uiuAc&json=", "num_citations": 12, "citedby_url": "/scholar?cites=4139124080220294224&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UMzLwmsfcTkJ:scholar.google.com/&scioq=Cln2inv:+Learning+Loop+Invariants+With+Continuous+Logic+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11542"}, "Synthesizing Programmatic Policies That Inductively Generalize": {"container_type": "Publication", "bib": {"title": "Synthesizing programmatic policies that inductively generalize", "author": ["JP Inala", "O Bastani", "Z Tavares"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Deep reinforcement learning has successfully solved a number of challenging control tasks. However, learned policies typically have difficulty generalizing to novel environments. We propose an algorithm for learning programmatic state machine policies that can capture repeating behaviors. By doing so, they have the ability to generalize to instances requiring an arbitrary number of repetitions, a property we call inductive generalization. However, state machine policies are hard to learn since they consist of a combination of continuous"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=S1l8oANFDH", "author_id": ["", "", ""], "url_scholarbib": "/scholar?q=info:28M38IYw9hUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSynthesizing%2BProgrammatic%2BPolicies%2BThat%2BInductively%2BGeneralize%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=28M38IYw9hUJ&ei=b-RXYqrFFciBy9YP18Gi8As&json=", "num_citations": 15, "citedby_url": "/scholar?cites=1582505675181245403&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:28M38IYw9hUJ:scholar.google.com/&scioq=Synthesizing+Programmatic+Policies+That+Inductively+Generalize&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=S1l8oANFDH"}, "Fast Task Inference With Variational Intrinsic Successor Features": {"container_type": "Publication", "bib": {"title": "Fast task inference with variational intrinsic successor features", "author": ["S Hansen", "W Dabney", "A Barreto"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "It has been established that diverse behaviors spanning the controllable subspace of an Markov decision process can be trained by rewarding a policy for being distinguishable from other policies\\citep {gregor2016variational, eysenbach2018diversity, warde2018unsupervised}. However, one limitation of this formulation is generalizing behaviors beyond the finite set being explicitly learned, as is needed for use on subsequent tasks. Successor features\\citep {dayan93improving, barreto2017successor} provide an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.05030", "author_id": ["hIOEWsEAAAAJ", "dR-7QW8AAAAJ", "H-xtdV4AAAAJ"], "url_scholarbib": "/scholar?q=info:Gw4irD6lqSgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFast%2BTask%2BInference%2BWith%2BVariational%2BIntrinsic%2BSuccessor%2BFeatures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Gw4irD6lqSgJ&ei=eORXYo3WGJmM6rQPjaOSEA&json=", "num_citations": 61, "citedby_url": "/scholar?cites=2930054721175686683&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Gw4irD6lqSgJ:scholar.google.com/&scioq=Fast+Task+Inference+With+Variational+Intrinsic+Successor+Features&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.05030.pdf?ref=https://githubhelp.com"}, "Generalization Of Two-layer Neural Networks: An Asymptotic Viewpoint": {"container_type": "Publication", "bib": {"title": "Generalization of two-layer neural networks: An asymptotic viewpoint", "author": ["J Ba", "M Erdogdu", "T Suzuki", "D Wu"], "pub_year": "2019", "venue": "\u2026 conference on learning \u2026", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, ie when the number of samples $ n $, features $ d $, and neurons $ h $ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the\\textit {double descent}"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1gBsgBYwH", "author_id": ["ymzxRhAAAAAJ", "Lqc4cdAAAAAJ", "x8osrBsAAAAJ", "x7Q3zj4AAAAJ"], "url_scholarbib": "/scholar?q=info:Gl3elxW_VssJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralization%2BOf%2BTwo-layer%2BNeural%2BNetworks:%2BAn%2BAsymptotic%2BViewpoint%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Gl3elxW_VssJ&ei=fORXYp7AOOHDywTjooCQBQ&json=", "num_citations": 42, "citedby_url": "/scholar?cites=14652108537159638298&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Gl3elxW_VssJ:scholar.google.com/&scioq=Generalization+Of+Two-layer+Neural+Networks:+An+Asymptotic+Viewpoint&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1gBsgBYwH"}, "Overlearning Reveals Sensitive Attributes": {"container_type": "Publication", "bib": {"title": "Overlearning reveals sensitive attributes", "author": ["C Song", "V Shmatikov"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.11742", "abstract": "We demonstrate overlearning in several vision and NLP models and analyze its harmful   We also show that overlearned representations enable recognition of sensitive attributes"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11742", "author_id": ["lkPKfjgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:VIlEIlmPtNYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOverlearning%2BReveals%2BSensitive%2BAttributes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VIlEIlmPtNYJ&ei=gORXYpGbCcWemAHB5baIBQ&json=", "num_citations": 64, "citedby_url": "/scholar?cites=15471148232914274644&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VIlEIlmPtNYJ:scholar.google.com/&scioq=Overlearning+Reveals+Sensitive+Attributes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11742"}, "The Shape Of Data: Intrinsic Distance For Data Distributions": {"container_type": "Publication", "bib": {"title": "The shape of data: Intrinsic distance for data distributions", "author": ["A Tsitsulin", "M Munkhoeva", "D Mottin", "P Karras"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The ability to represent and compare machine learning models is crucial in order to quantify subtle model changes, evaluate generative models, and gather insights on neural network architectures. Existing techniques for comparing data distributions focus on global data properties such as mean and covariance; in that sense, they are extrinsic and uni-scale. We develop a first-of-its-kind intrinsic and multi-scale method for characterizing and comparing data manifolds, using a lower-bound of the spectral variant of the Gromov-Wasserstein inter"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11141", "author_id": ["hssTYQMAAAAJ", "pvjdOmkAAAAJ", "evZ9Q9EAAAAJ", "B6C4aBoAAAAJ"], "url_scholarbib": "/scholar?q=info:bS2kcRdzK4QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BShape%2BOf%2BData:%2BIntrinsic%2BDistance%2BFor%2BData%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bS2kcRdzK4QJ&ei=i-RXYtvqEYvEmgH7846QCg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=9523832381533072749&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bS2kcRdzK4QJ:scholar.google.com/&scioq=The+Shape+Of+Data:+Intrinsic+Distance+For+Data+Distributions&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11141"}, "Locally Constant Networks": {"container_type": "Publication", "bib": {"title": "Sparse and locally constant Gaussian graphical models", "author": ["J Honorio", "D Samaras", "N Paragios"], "pub_year": "2009", "venue": "Advances in \u2026", "abstract": "of variables as in [9], or block structures as in [10] in the context of Bayesian networks.   neighboring variables, which allows obtaining locally constant models that preserve sparseness,"}, "filled": false, "gsrank": 1, "pub_url": "https://proceedings.neurips.cc/paper/2009/hash/37693cfc748049e45d87b8c7d8b9aacd-Abstract.html", "author_id": ["8OW3TMMAAAAJ", "BxbKTYkAAAAJ", "7edhlaQAAAAJ"], "url_scholarbib": "/scholar?q=info:oLqb3u7A3KAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLocally%2BConstant%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oLqb3u7A3KAJ&ei=juRXYuquGJmM6rQPjaOSEA&json=", "num_citations": 37, "citedby_url": "/scholar?cites=11591351673114311328&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oLqb3u7A3KAJ:scholar.google.com/&scioq=Locally+Constant+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://proceedings.neurips.cc/paper/2009/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf"}, "Electra: Pre-training Text Encoders As Discriminators Rather Than Generators": {"container_type": "Publication", "bib": {"title": "Electra: Pre-training text encoders as discriminators rather than generators", "author": ["K Clark", "MT Luong", "QV Le", "CD Manning"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2003.10555", "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.10555", "author_id": ["Trk_R8wAAAAJ", "Bmbkv6sAAAAJ", "vfT6-XIAAAAJ", "1zmDOdwAAAAJ"], "url_scholarbib": "/scholar?q=info:K3_BYC8al_0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DElectra:%2BPre-training%2BText%2BEncoders%2BAs%2BDiscriminators%2BRather%2BThan%2BGenerators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=K3_BYC8al_0J&ei=nuRXYoOWB8WemAHB5baIBQ&json=", "num_citations": 1214, "citedby_url": "/scholar?cites=18273102803868155691&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:K3_BYC8al_0J:scholar.google.com/&scioq=Electra:+Pre-training+Text+Encoders+As+Discriminators+Rather+Than+Generators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.10555.pdf%3C/p%3E"}, "On The Global Convergence Of Training Deep Linear Resnets": {"container_type": "Publication", "bib": {"title": "On the global convergence of training deep linear ResNets", "author": ["D Zou", "PM Long", "Q Gu"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2003.01094", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $ L $-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.01094", "author_id": ["Cp4fcTQAAAAJ", "PVty8PUAAAAJ", "GU9HgNAAAAAJ"], "url_scholarbib": "/scholar?q=info:LVUw2JnzU88J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BGlobal%2BConvergence%2BOf%2BTraining%2BDeep%2BLinear%2BResnets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LVUw2JnzU88J&ei=oORXYsvgLcWemAHB5baIBQ&json=", "num_citations": 12, "citedby_url": "/scholar?cites=14939552231000659245&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LVUw2JnzU88J:scholar.google.com/&scioq=On+The+Global+Convergence+Of+Training+Deep+Linear+Resnets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.01094"}, "Action Semantics Network: Considering The Effects Of Actions In Multiagent Systems": {"container_type": "Publication", "bib": {"title": "Action semantics network: Considering the effects of actions in multiagent systems", "author": ["W Wang", "T Yang", "Y Liu", "J Hao", "X Hao", "Y Hu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In multiagent systems (MASs), each agent makes individual decisions but all of them contribute globally to the system evolution. Learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents. Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multiagent coordination. However, none of them"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.11461", "author_id": ["pG1-T4QAAAAJ", "WPRygosAAAAJ", "2ejuK8UAAAAJ", "", "xgk9NPwAAAAJ", ""], "url_scholarbib": "/scholar?q=info:CsIf7zpxVbMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAction%2BSemantics%2BNetwork:%2BConsidering%2BThe%2BEffects%2BOf%2BActions%2BIn%2BMultiagent%2BSystems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CsIf7zpxVbMJ&ei=pORXYqGgJ4vMsQK69Y7ABg&json=", "num_citations": 9, "citedby_url": "/scholar?cites=12922359203743384074&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CsIf7zpxVbMJ:scholar.google.com/&scioq=Action+Semantics+Network:+Considering+The+Effects+Of+Actions+In+Multiagent+Systems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.11461"}, "Curriculum Loss: Robust Learning And Generalization Against Label Corruption": {"container_type": "Publication", "bib": {"title": "Curriculum loss: Robust learning and generalization against label corruption", "author": ["Y Lyu", "IW Tsang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.10045", "abstract": "Deep neural networks (DNNs) have great expressive power, which can even memorize samples with wrong labels. It is vitally important to reiterate robustness and generalization in DNNs against label corruption. To this end, this paper studies the 0-1 loss, which has a monotonic relationship with an empirical adversary (reweighted) risk~\\citep {hu2016does}. Although the 0-1 loss has some robust properties, it is difficult to optimize. To efficiently optimize the 0-1 loss while keeping its robust properties, we propose a very simple and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10045", "author_id": ["uQXB6-oAAAAJ", "rJMOlVsAAAAJ"], "url_scholarbib": "/scholar?q=info:uCKTUR71sz4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCurriculum%2BLoss:%2BRobust%2BLearning%2BAnd%2BGeneralization%2BAgainst%2BLabel%2BCorruption%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uCKTUR71sz4J&ei=p-RXYtjnOovMsQK69Y7ABg&json=", "num_citations": 61, "citedby_url": "/scholar?cites=4518224361749160632&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uCKTUR71sz4J:scholar.google.com/&scioq=Curriculum+Loss:+Robust+Learning+And+Generalization+Against+Label+Corruption&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10045.pdf?ref=https://githubhelp.com"}, "Hamiltonian Generative Networks": {"container_type": "Publication", "bib": {"title": "Hamiltonian generative networks", "author": ["P Toth", "DJ Rezende", "A Jaegle", "S Racani\u00e8re"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems-from sequence prediction to reinforcement learning and density modelling-but are not typically provided out of the box by standard tools such as recurrent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.13789", "author_id": ["suIpQjYAAAAJ", "UGlyhFMAAAAJ", "2iBYdwEAAAAJ", "o-h0vrQAAAAJ"], "url_scholarbib": "/scholar?q=info:OVwb69tfpkwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHamiltonian%2BGenerative%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OVwb69tfpkwJ&ei=q-RXYtyZHpmM6rQPjaOSEA&json=", "num_citations": 116, "citedby_url": "/scholar?cites=5523207391163407417&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OVwb69tfpkwJ:scholar.google.com/&scioq=Hamiltonian+Generative+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.13789"}, "Kernel Of Cyclegan As A Principal Homogeneous Space": {"container_type": "Publication", "bib": {"title": "Kernel of cycleGAN as a principle homogeneous space", "author": ["N Moriakov", "J Adler", "J Teuwen"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.09061", "abstract": "Unpaired image-to-image translation has attracted significant interest due to the invention of CycleGAN, a method which utilizes a combination of adversarial and cycle consistency losses to avoid the need for paired data. It is known that the CycleGAN problem might admit multiple solutions, and our goal in this paper is to analyze the space of exact solutions and to give perturbation bounds for approximate solutions. We show theoretically that the exact solution space is invariant with respect to automorphisms of the underlying probability"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.09061", "author_id": ["OtjD3XwAAAAJ", "196_CDcAAAAJ", ""], "url_scholarbib": "/scholar?q=info:taSylG2zmuYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKernel%2BOf%2BCyclegan%2BAs%2BA%2BPrincipal%2BHomogeneous%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=taSylG2zmuYJ&ei=ruRXYrvACpLeyQTms5KQBg&json=", "num_citations": 2, "citedby_url": "/scholar?cites=16616791058364409013&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:taSylG2zmuYJ:scholar.google.com/&scioq=Kernel+Of+Cyclegan+As+A+Principal+Homogeneous+Space&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.09061"}, "Prediction, Consistency, Curvature: Representation Learning For Locally-linear Control": {"container_type": "Publication", "bib": {"title": "Prediction, consistency, curvature: Representation learning for locally-linear control", "author": ["N Levine", "Y Chow", "R Shu", "A Li"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.01506", "author_id": ["p1HAb2QAAAAJ", "BFlpS-8AAAAJ", "UB7UZEYAAAAJ", "6bRXWXEAAAAJ"], "url_scholarbib": "/scholar?q=info:rAiblYfaZ0oJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPrediction,%2BConsistency,%2BCurvature:%2BRepresentation%2BLearning%2BFor%2BLocally-linear%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rAiblYfaZ0oJ&ei=sORXYvi2OYvEmgH7846QCg&json=", "num_citations": 17, "citedby_url": "/scholar?cites=5361494157273270444&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rAiblYfaZ0oJ:scholar.google.com/&scioq=Prediction,+Consistency,+Curvature:+Representation+Learning+For+Locally-linear+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.01506"}, "Network Deconvolution": {"container_type": "Publication", "bib": {"title": "Network deconvolution", "author": ["C Ye", "M Evanusa", "H He", "A Mitrokhin"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "network deconvolution is indeed a deconvolution operation. \u2022 We show that network  deconvolution reduces  in the data, leading to sparse representations at each layer of the network."}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11926", "author_id": ["UhnKSmYAAAAJ", "B9VfE1cAAAAJ", "meN5Qv4AAAAJ", "bnw1vgwAAAAJ"], "url_scholarbib": "/scholar?q=info:KwfBOqaSzxsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNetwork%2BDeconvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KwfBOqaSzxsJ&ei=s-RXYrTdEpWMy9YPt8OamA0&json=", "num_citations": 22, "citedby_url": "/scholar?cites=2003981601851115307&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KwfBOqaSzxsJ:scholar.google.com/&scioq=Network+Deconvolution&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11926"}, "Tensor Decompositions For Temporal Knowledge Base Completion": {"container_type": "Publication", "bib": {"title": "Tensor decompositions for temporal knowledge base completion", "author": ["T Lacroix", "G Obozinski", "N Usunier"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.04926", "abstract": "Most algorithms for representation learning and link prediction in relational data have been designed for static data. However, the data they are applied to usually evolves with time, such as friend graphs in social networks or user interactions with items in recommender systems. This is also the case for knowledge bases, which contain facts such as (US, has president, B. Obama,[2009-2017]) that are valid only at certain points in time. For the problem of link prediction under temporal constraints, ie, answering queries such as (US"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.04926", "author_id": ["tZGS6dIAAAAJ", "8jAftjUAAAAJ", "tYro5N8AAAAJ"], "url_scholarbib": "/scholar?q=info:2e4OwpKpDv0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTensor%2BDecompositions%2BFor%2BTemporal%2BKnowledge%2BBase%2BCompletion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2e4OwpKpDv0J&ei=uuRXYoRnkt7JBOazkpAG&json=", "num_citations": 56, "citedby_url": "/scholar?cites=18234698389055794905&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2e4OwpKpDv0J:scholar.google.com/&scioq=Tensor+Decompositions+For+Temporal+Knowledge+Base+Completion&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.04926"}, "A Function Space View Of Bounded Norm Infinite Width Relu Nets: The Multivariate Case": {"container_type": "Publication", "bib": {"title": "A function space view of bounded norm infinite width ReLU nets: The multivariate case", "author": ["G Ongie", "R Willett", "D Soudry", "N Srebro"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.01635", "abstract": "A key element of understanding the efficacy of overparameterized neural networks is characterizing how they represent functions as the number of weights in the network approaches infinity. In this paper, we characterize the norm required to realize a function $ f:\\mathbb {R}^ d\\rightarrow\\mathbb {R} $ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), but where the Euclidean norm of the weights is bounded, including precisely characterizing which functions can be realized with finite norm"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01635", "author_id": ["uhhWy2YAAAAJ", "bGRVPl8AAAAJ", "AEBWEm8AAAAJ", "ZnT-QpMAAAAJ"], "url_scholarbib": "/scholar?q=info:96N3NssoOO8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BFunction%2BSpace%2BView%2BOf%2BBounded%2BNorm%2BInfinite%2BWidth%2BRelu%2BNets:%2BThe%2BMultivariate%2BCase%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=96N3NssoOO8J&ei=veRXYsuvKIvMsQK69Y7ABg&json=", "num_citations": 61, "citedby_url": "/scholar?cites=17237572427017855991&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:96N3NssoOO8J:scholar.google.com/&scioq=A+Function+Space+View+Of+Bounded+Norm+Infinite+Width+Relu+Nets:+The+Multivariate+Case&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01635"}, "Intensity-free Learning Of Temporal Point Processes": {"container_type": "Publication", "bib": {"title": "Intensity-free learning of temporal point processes", "author": ["O Shchur", "M Bilo\u0161", "S G\u00fcnnemann"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12127", "abstract": "Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. The standard way of learning in such models is by estimating the conditional intensity function. However, parameterizing the intensity function usually incurs several trade-offs. We show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times. We draw on the literature on normalizing flows to design models that are flexible and efficient"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12127", "author_id": ["np39q6IAAAAJ", "2WDzslAAAAAJ", "npqoAWwAAAAJ"], "url_scholarbib": "/scholar?q=info:f3GGG1tVN1QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntensity-free%2BLearning%2BOf%2BTemporal%2BPoint%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=f3GGG1tVN1QJ&ei=yeRXYqL1FsiBy9YP18Gi8As&json=", "num_citations": 58, "citedby_url": "/scholar?cites=6068412872697213311&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:f3GGG1tVN1QJ:scholar.google.com/&scioq=Intensity-free+Learning+Of+Temporal+Point+Processes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12127"}, "Provable Filter Pruning For Efficient Neural Networks": {"container_type": "Publication", "bib": {"title": "Provable filter pruning for efficient neural networks", "author": ["L Liebenwein", "C Baykal", "H Lang", "D Feldman"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs) by identifying and removing redundant filters from an over-parameterized network. Our algorithm uses a small batch of input data points to assign a saliency score to each filter and constructs an importance sampling distribution where filters that highly affect the output are sampled with correspondingly high probability. In contrast to existing filter pruning approaches, our method is simultaneously data-informed, exhibits"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.07412", "author_id": ["e7ab8u4AAAAJ", "lRxoOlwAAAAJ", "", "67QZN0gAAAAJ"], "url_scholarbib": "/scholar?q=info:2KjXp5Cb6X8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProvable%2BFilter%2BPruning%2BFor%2BEfficient%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2KjXp5Cb6X8J&ei=1eRXYq-BGJWMy9YPt8OamA0&json=", "num_citations": 61, "citedby_url": "/scholar?cites=9217069157983955160&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2KjXp5Cb6X8J:scholar.google.com/&scioq=Provable+Filter+Pruning+For+Efficient+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.07412"}, "How Much Position Information Do Convolutional Neural Networks Encode?": {"container_type": "Publication", "bib": {"title": "How much position information do convolutional neural networks encode?", "author": ["MA Islam", "S Jia", "NDB Bruce"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.08248", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.08248", "author_id": ["AeibrqUAAAAJ", "WOsy1foAAAAJ", "Gnezf-4AAAAJ"], "url_scholarbib": "/scholar?q=info:74fvV3pT28IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BMuch%2BPosition%2BInformation%2BDo%2BConvolutional%2BNeural%2BNetworks%2BEncode%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=74fvV3pT28IJ&ei=2eRXYpvkGM6E6rQPz8uiuAc&json=", "num_citations": 109, "citedby_url": "/scholar?cites=14040908048184084463&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:74fvV3pT28IJ:scholar.google.com/&scioq=How+Much+Position+Information+Do+Convolutional+Neural+Networks+Encode%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.08248"}, "Towards Better Understanding Of Adaptive Gradient Algorithms In Generative Adversarial Nets": {"container_type": "Publication", "bib": {"title": "Towards better understanding of adaptive gradient algorithms in generative adversarial nets", "author": ["M Liu", "Y Mroueh", "J Ross", "W Zhang", "X Cui"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep {daskalakis2017training}"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.11940", "author_id": ["KFoEnFQAAAAJ", "6F90JHgAAAAJ", "", "DJMSA3YAAAAJ", "wzNVJQsAAAAJ"], "url_scholarbib": "/scholar?q=info:HN4XloVwAbIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BBetter%2BUnderstanding%2BOf%2BAdaptive%2BGradient%2BAlgorithms%2BIn%2BGenerative%2BAdversarial%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HN4XloVwAbIJ&ei=3ORXYu2rF5LeyQTms5KQBg&json=", "num_citations": 34, "citedby_url": "/scholar?cites=12826656932778991132&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HN4XloVwAbIJ:scholar.google.com/&scioq=Towards+Better+Understanding+Of+Adaptive+Gradient+Algorithms+In+Generative+Adversarial+Nets&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.11940"}, "Depth-width Trade-offs For Relu Networks Via Sharkovsky's Theorem": {"container_type": "Publication", "bib": {"title": "Depth-width trade-offs for relu networks via sharkovsky's theorem", "author": ["V Chatziafratis", "SG Nagarajan", "I Panageas"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (eg, depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky highlighted the benefits of depth by presenting a family of functions (based on simple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.04378", "author_id": ["FjCVRKkAAAAJ", "VoaosL4AAAAJ", "5NiFWuwAAAAJ"], "url_scholarbib": "/scholar?q=info:oVZSEEy_vmEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDepth-width%2BTrade-offs%2BFor%2BRelu%2BNetworks%2BVia%2BSharkovsky%2527s%2BTheorem%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oVZSEEy_vmEJ&ei=4eRXYv23CcLZmQHc1ovQAg&json=", "num_citations": 18, "citedby_url": "/scholar?cites=7043277200666285729&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oVZSEEy_vmEJ:scholar.google.com/&scioq=Depth-width+Trade-offs+For+Relu+Networks+Via+Sharkovsky%27s+Theorem&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.04378"}, "Hilloc: Lossless Image Compression With Hierarchical Latent Variable Models": {"container_type": "Publication", "bib": {"title": "Hilloc: Lossless image compression with hierarchical latent variable models", "author": ["J Townsend", "T Bird", "J Kunze", "D Barber"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.09953", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based'Bits-Back with ANS'algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.09953", "author_id": ["fhYBZTcAAAAJ", "2dkA4KwAAAAJ", "Nte3grUAAAAJ", "dqJPZHEAAAAJ"], "url_scholarbib": "/scholar?q=info:xtYZBnU_WHkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHilloc:%2BLossless%2BImage%2BCompression%2BWith%2BHierarchical%2BLatent%2BVariable%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xtYZBnU_WHkJ&ei=7ORXYu2NBovMsQK69Y7ABg&json=", "num_citations": 35, "citedby_url": "/scholar?cites=8743808448385898182&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xtYZBnU_WHkJ:scholar.google.com/&scioq=Hilloc:+Lossless+Image+Compression+With+Hierarchical+Latent+Variable+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.09953"}, "Geom-gcn: Geometric Graph Convolutional Networks": {"container_type": "Publication", "bib": {"title": "Geom-gcn: Geometric graph convolutional networks", "author": ["H Pei", "B Wei", "KCC Chang", "Y Lei", "B Yang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05287", "abstract": "Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05287", "author_id": ["FxVmazsAAAAJ", "", "sugWZ6MAAAAJ", "nHlrzV0AAAAJ", ""], "url_scholarbib": "/scholar?q=info:ObSSF2yUsJAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeom-gcn:%2BGeometric%2BGraph%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ObSSF2yUsJAJ&ei=7-RXYobOAZaM6rQPlISayA8&json=", "num_citations": 238, "citedby_url": "/scholar?cites=10425996329335567417&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ObSSF2yUsJAJ:scholar.google.com/&scioq=Geom-gcn:+Geometric+Graph+Convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05287.pdf?ref=https://githubhelp.com"}, "Neural Execution Of Graph Algorithms": {"container_type": "Publication", "bib": {"title": "Neural execution of graph algorithms", "author": ["P Veli\u010dkovi\u0107", "R Ying", "M Padovano", "R Hadsell"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.10593", "author_id": ["kcTK_FAAAAAJ", "6fqNXooAAAAJ", "iQCa4vUAAAAJ", "Q0YEc-QAAAAJ"], "url_scholarbib": "/scholar?q=info:oSbggA4OM68J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BExecution%2BOf%2BGraph%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oSbggA4OM68J&ei=8uRXYr3UKcLZmQHc1ovQAg&json=", "num_citations": 80, "citedby_url": "/scholar?cites=12624449635904136865&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oSbggA4OM68J:scholar.google.com/&scioq=Neural+Execution+Of+Graph+Algorithms&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.10593"}, "Fspool: Learning Set Representations With Featurewise Sort Pooling": {"container_type": "Publication", "bib": {"title": "Fspool: Learning set representations with featurewise sort pooling", "author": ["Y Zhang", "J Hare", "A Pr\u00fcgel-Bennett"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.02795", "abstract": "Traditional set prediction models can struggle with simple datasets due to an issue we call the responsibility problem. We introduce a pooling method for sets of feature vectors based on sorting features across elements of the set. This can be used to construct a permutation-equivariant auto-encoder that avoids this responsibility problem. On a toy dataset of polygons and a set version of MNIST, we show that such an auto-encoder produces considerably better reconstructions and representations. Replacing the pooling function in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.02795", "author_id": ["XtCqbfEAAAAJ", "UFeON5oAAAAJ", "oQgxYjkAAAAJ"], "url_scholarbib": "/scholar?q=info:F1unKc7ZiDYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFspool:%2BLearning%2BSet%2BRepresentations%2BWith%2BFeaturewise%2BSort%2BPooling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F1unKc7ZiDYJ&ei=9eRXYvyNL5WMy9YPt8OamA0&json=", "num_citations": 45, "citedby_url": "/scholar?cites=3929630154366081815&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:F1unKc7ZiDYJ:scholar.google.com/&scioq=Fspool:+Learning+Set+Representations+With+Featurewise+Sort+Pooling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.02795"}, "Why Not To Use Zero Imputation? Correcting Sparsity Bias In Training Neural Networks": {"container_type": "Publication", "bib": {"title": "Why not to use zero imputation? correcting sparsity bias in training neural networks", "author": ["J Yi", "J Lee", "KJ Kim", "SJ Hwang", "E Yang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.00150", "abstract": "Handling missing data is one of the most fundamental problems in machine learning. Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.00150", "author_id": ["DfmoSNwAAAAJ", "InhSbJAAAAAJ", "Wvwaw94AAAAJ", "RP4Qx3QAAAAJ", "UWO1mloAAAAJ"], "url_scholarbib": "/scholar?q=info:e9Cm_ZNZCwUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhy%2BNot%2BTo%2BUse%2BZero%2BImputation%253F%2BCorrecting%2BSparsity%2BBias%2BIn%2BTraining%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e9Cm_ZNZCwUJ&ei=-eRXYoGJJsS4ywTtzb_QDA&json=", "num_citations": 19, "citedby_url": "/scholar?cites=363482687084089467&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e9Cm_ZNZCwUJ:scholar.google.com/&scioq=Why+Not+To+Use+Zero+Imputation%3F+Correcting+Sparsity+Bias+In+Training+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.00150"}, "Playing The Lottery With Rewards And Multiple Languages: Lottery Tickets In Rl And Nlp": {"container_type": "Publication", "bib": {"title": "Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp", "author": ["H Yu", "S Edunov", "Y Tian", "AS Morcos"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.02768", "abstract": "The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a\" lucky\" sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether\""}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.02768", "author_id": ["Army5cEAAAAJ", "5w7uYrIAAAAJ", "0mgEF28AAAAJ", "v-A_7UsAAAAJ"], "url_scholarbib": "/scholar?q=info:KV0F5nv4LMYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPlaying%2BThe%2BLottery%2BWith%2BRewards%2BAnd%2BMultiple%2BLanguages:%2BLottery%2BTickets%2BIn%2BRl%2BAnd%2BNlp%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KV0F5nv4LMYJ&ei=_eRXYqDUEMiBy9YP18Gi8As&json=", "num_citations": 73, "citedby_url": "/scholar?cites=14280061729508777257&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KV0F5nv4LMYJ:scholar.google.com/&scioq=Playing+The+Lottery+With+Rewards+And+Multiple+Languages:+Lottery+Tickets+In+Rl+And+Nlp&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.02768"}, "Massively Multilingual Sparse Word Representations": {"container_type": "Publication", "bib": {"title": "Massively Multilingual Sparse Word Representations", "author": ["G Berend"], "pub_year": "2019", "venue": "International Conference on Learning Representations", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyeYTgrFPB", "author_id": ["JMafrTEAAAAJ"], "url_scholarbib": "/scholar?q=info:IgDBnSPwF5kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMassively%2BMultilingual%2BSparse%2BWord%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IgDBnSPwF5kJ&ei=AeVXYpi4DJGJmwGIxre4DA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:IgDBnSPwF5kJ:scholar.google.com/&scioq=Massively+Multilingual+Sparse+Word+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyeYTgrFPB"}, "Augmix: A Simple Data Processing Method To Improve Robustness And Uncertainty": {"container_type": "Publication", "bib": {"title": "Augmix: A simple data processing method to improve robustness and uncertainty", "author": ["D Hendrycks", "N Mu", "ED Cubuk", "B Zoph"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02781", "author_id": ["czyretsAAAAJ", "UFlWdvUAAAAJ", "Mu_8iOEAAAAJ", "H-BnRI0AAAAJ"], "url_scholarbib": "/scholar?q=info:DCJ2e5FrKZYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAugmix:%2BA%2BSimple%2BData%2BProcessing%2BMethod%2BTo%2BImprove%2BRobustness%2BAnd%2BUncertainty%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DCJ2e5FrKZYJ&ei=A-VXYpjfOIySyATlkbrQCA&json=", "num_citations": 374, "citedby_url": "/scholar?cites=10820297852320096780&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DCJ2e5FrKZYJ:scholar.google.com/&scioq=Augmix:+A+Simple+Data+Processing+Method+To+Improve+Robustness+And+Uncertainty&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02781"}, "Adaptive Correlated Monte Carlo For Contextual Categorical Sequence Generation": {"container_type": "Publication", "bib": {"title": "Adaptive correlated Monte Carlo for contextual categorical sequence generation", "author": ["X Fan", "Y Zhang", "Z Wang", "M Zhou"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.13151", "abstract": "Sequence generation models are commonly refined with reinforcement learning over user-defined metrics. However, high gradient variance hinders the practical use of this method. To stabilize this method, we adapt to contextual generation of categorical sequences a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. Due to the correlation, the number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.13151", "author_id": ["", "WDVMfggAAAAJ", "lRiIjhcAAAAJ", "LXwCIisAAAAJ"], "url_scholarbib": "/scholar?q=info:1CbdjBX-izQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BCorrelated%2BMonte%2BCarlo%2BFor%2BContextual%2BCategorical%2BSequence%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1CbdjBX-izQJ&ei=EOVXYrPmDYvEmgH7846QCg&json=", "num_citations": 2, "citedby_url": "/scholar?cites=3786399280246105812&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1CbdjBX-izQJ:scholar.google.com/&scioq=Adaptive+Correlated+Monte+Carlo+For+Contextual+Categorical+Sequence+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.13151"}, "Dynamics-aware Embeddings": {"container_type": "Publication", "bib": {"title": "Dynamics-aware embeddings", "author": ["W Whitney", "R Agarwal", "K Cho", "A Gupta"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.09357", "abstract": "method, Dynamics-aware Embedding (DynE) embeddings lead to more efficient exploration,  resulting in more sample efficient learning on complex tasks, while DynE state embeddings"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.09357", "author_id": ["aQcYWDMAAAAJ", "", "0RAmmIAAAAAJ", "bqL73OkAAAAJ"], "url_scholarbib": "/scholar?q=info:zbFdR5xV8nMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamics-aware%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zbFdR5xV8nMJ&ei=E-VXYqubGsLZmQHc1ovQAg&json=", "num_citations": 26, "citedby_url": "/scholar?cites=8354834388426273229&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zbFdR5xV8nMJ:scholar.google.com/&scioq=Dynamics-aware+Embeddings&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.09357"}, "Pc-darts: Partial Channel Connections For Memory-efficient Architecture Search": {"container_type": "Publication", "bib": {"title": "Pc-darts: Partial channel connections for memory-efficient architecture search", "author": ["Y Xu", "L Xie", "X Zhang", "X Chen", "GJ Qi", "Q Tian"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-network and searching for an optimal architecture. In this paper, we present a novel approach, namely, Partially-Connected DARTS, by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.05737", "author_id": ["42DgoIMAAAAJ", "EEMm7hwAAAAJ", "Ud6aBAcAAAAJ", "oqv81a0AAAAJ", "Nut-uvoAAAAJ", "61b6eYkAAAAJ"], "url_scholarbib": "/scholar?q=info:-8CjwqR4mhEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPc-darts:%2BPartial%2BChannel%2BConnections%2BFor%2BMemory-efficient%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-8CjwqR4mhEJ&ei=GOVXYrCZHIySyATlkbrQCA&json=", "num_citations": 381, "citedby_url": "/scholar?cites=1268458894093697275&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-8CjwqR4mhEJ:scholar.google.com/&scioq=Pc-darts:+Partial+Channel+Connections+For+Memory-efficient+Architecture+Search&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.05737"}, "Counterfactuals Uncover The Modular Structure Of Deep Generative Models": {"container_type": "Publication", "bib": {"title": "Counterfactuals uncover the modular structure of deep generative models", "author": ["M Besserve", "A Mehrjou", "R Sun", "B Sch\u00f6lkopf"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement is too restrictive and propose instead a non-statistical framework that relies on counterfactual manipulations"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.03253", "author_id": ["Nbq6kI0AAAAJ", "pnypNygAAAAJ", "bZI8pc8AAAAJ", "DZ-fHPgAAAAJ"], "url_scholarbib": "/scholar?q=info:r7zlyfU4WoIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCounterfactuals%2BUncover%2BThe%2BModular%2BStructure%2BOf%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=r7zlyfU4WoIJ&ei=H-VXYsmxH8WemAHB5baIBQ&json=", "num_citations": 52, "citedby_url": "/scholar?cites=9392882601140010159&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:r7zlyfU4WoIJ:scholar.google.com/&scioq=Counterfactuals+Uncover+The+Modular+Structure+Of+Deep+Generative+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.03253"}, "Enabling Deep Spiking Neural Networks With Hybrid Conversion And Spike Timing Dependent Backpropagation": {"container_type": "Publication", "bib": {"title": "Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation", "author": ["N Rathi", "G Srinivasan", "P Panda", "K Roy"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2005.01807", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.01807", "author_id": ["N4aDtWUAAAAJ", "ynJrDpAAAAAJ", "qA5WsYUAAAAJ", "to4P8KgAAAAJ"], "url_scholarbib": "/scholar?q=info:lJyKudGwbiAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnabling%2BDeep%2BSpiking%2BNeural%2BNetworks%2BWith%2BHybrid%2BConversion%2BAnd%2BSpike%2BTiming%2BDependent%2BBackpropagation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lJyKudGwbiAJ&ei=IuVXYtnoBOHDywTjooCQBQ&json=", "num_citations": 84, "citedby_url": "/scholar?cites=2336999671459388564&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lJyKudGwbiAJ:scholar.google.com/&scioq=Enabling+Deep+Spiking+Neural+Networks+With+Hybrid+Conversion+And+Spike+Timing+Dependent+Backpropagation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.01807"}, "On Need For Topology-aware Generative Models For Manifold-based Defenses": {"container_type": "Publication", "bib": {"title": "On the need for topology-aware generative models for manifold-based defenses", "author": ["U Jang", "S Jha", "S Jha"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.03334", "abstract": "Machine-learning (ML) algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, researchers have recently demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause misclassification). The existence of adversarial examples has hindered the deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.03334", "author_id": ["VHLGVWUAAAAJ", "wPNgzO4AAAAJ", "BaI7l8QAAAAJ"], "url_scholarbib": "/scholar?q=info:TrFleeW16NUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BNeed%2BFor%2BTopology-aware%2BGenerative%2BModels%2BFor%2BManifold-based%2BDefenses%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TrFleeW16NUJ&ei=JOVXYp2vKs2Ny9YPqPyUgAs&json=", "num_citations": 4, "citedby_url": "/scholar?cites=15413769721864368462&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TrFleeW16NUJ:scholar.google.com/&scioq=On+Need+For+Topology-aware+Generative+Models+For+Manifold-based+Defenses&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.03334"}, "Finite Depth And Width Corrections To The Neural Tangent Kernel": {"container_type": "Publication", "bib": {"title": "Finite depth and width corrections to the neural tangent kernel", "author": ["B Hanin", "M Nica"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.05989", "abstract": "We prove the precise scaling, at finite depth and width, for the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The standard deviation is exponential in the ratio of network depth to width. Thus, even in the limit of infinite overparameterization, the NTK is not deterministic if depth and width simultaneously tend to infinity. Moreover, we prove that for such deep and wide networks, the NTK has a non-trivial evolution during training by showing that the mean of its first SGD update is also exponential"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.05989", "author_id": ["Nt9r1_IAAAAJ", "R9L0aqAAAAAJ"], "url_scholarbib": "/scholar?q=info:Ylc0ew9i3DEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFinite%2BDepth%2BAnd%2BWidth%2BCorrections%2BTo%2BThe%2BNeural%2BTangent%2BKernel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ylc0ew9i3DEJ&ei=J-VXYrHqKOHDywTjooCQBQ&json=", "num_citations": 64, "citedby_url": "/scholar?cites=3592854421365872482&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ylc0ew9i3DEJ:scholar.google.com/&scioq=Finite+Depth+And+Width+Corrections+To+The+Neural+Tangent+Kernel&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.05989"}, "Blockswap: Fisher-guided Block Substitution For Network Compression On A Budget": {"container_type": "Publication", "bib": {"title": "Blockswap: Fisher-guided block substitution for network compression on a budget", "author": ["J Turner", "EJ Crowley", "M O'Boyle", "A Storkey"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The desire to map neural networks to varying-capacity devices has led to the development of a wealth of compression techniques, many of which involve replacing standard convolutional blocks in a large network with cheap alternative blocks. However, not all blocks are created equally; for a required compute budget there may exist a potent combination of many different cheap blocks, though exhaustively searching for such a combination is prohibitively expensive. In this work, we develop BlockSwap: a fast algorithm"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.04113", "author_id": ["FPGHMRgAAAAJ", "RyKtqiQAAAAJ", "T-JW3vwAAAAJ", "3Rlc8EAAAAAJ"], "url_scholarbib": "/scholar?q=info:e9EVN9BhESUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBlockswap:%2BFisher-guided%2BBlock%2BSubstitution%2BFor%2BNetwork%2BCompression%2BOn%2BA%2BBudget%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e9EVN9BhESUJ&ei=KuVXYu3-G8iBy9YP18Gi8As&json=", "num_citations": 13, "citedby_url": "/scholar?cites=2671023600912683387&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e9EVN9BhESUJ:scholar.google.com/&scioq=Blockswap:+Fisher-guided+Block+Substitution+For+Network+Compression+On+A+Budget&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.04113?ref=https://githubhelp.com"}, "Deep Audio Priors Emerge From Harmonic Convolutional Networks": {"container_type": "Publication", "bib": {"title": "Deep audio priors emerge from harmonic convolutional networks", "author": ["Z Zhang", "Y Wang", "C Gan", "J Wu"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Convolutional neural networks (CNNs) excel in image recognition and generation. Among many efforts to explain their effectiveness, experiments show that CNNs carry strong inductive biases that capture natural image priors. Do deep networks also have inductive biases for audio signals? In this paper, we empirically show that current network architectures for audio processing do not show strong evidence in capturing such priors. We propose Harmonic Convolution, an operation that helps deep networks distill priors in audio"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rygjHxrYDB", "author_id": ["WYx0LHwAAAAJ", "", "PTeSCbIAAAAJ", "2efgcS0AAAAJ"], "url_scholarbib": "/scholar?q=info:FEvv7OYo14EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BAudio%2BPriors%2BEmerge%2BFrom%2BHarmonic%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FEvv7OYo14EJ&ei=LeVXYv_LBs6E6rQPz8uiuAc&json=", "num_citations": 17, "citedby_url": "/scholar?cites=9355991723168189204&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FEvv7OYo14EJ:scholar.google.com/&scioq=Deep+Audio+Priors+Emerge+From+Harmonic+Convolutional+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rygjHxrYDB"}, "Binaryduo: Reducing Gradient Mismatch In Binary Activation Network By Coupling Binary Activations": {"container_type": "Publication", "bib": {"title": "Binaryduo: Reducing gradient mismatch in binary activation network by coupling binary activations", "author": ["H Kim", "K Kim", "J Kim", "JJ Kim"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06517", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06517", "author_id": ["ZWOTfbwAAAAJ", "bCMZWFIAAAAJ", "2H5yN4MAAAAJ", "Ee994T0AAAAJ"], "url_scholarbib": "/scholar?q=info:Bl6SmJfV68gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBinaryduo:%2BReducing%2BGradient%2BMismatch%2BIn%2BBinary%2BActivation%2BNetwork%2BBy%2BCoupling%2BBinary%2BActivations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Bl6SmJfV68gJ&ei=L-VXYp7DNpLeyQTms5KQBg&json=", "num_citations": 25, "citedby_url": "/scholar?cites=14477900274189098502&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Bl6SmJfV68gJ:scholar.google.com/&scioq=Binaryduo:+Reducing+Gradient+Mismatch+In+Binary+Activation+Network+By+Coupling+Binary+Activations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06517"}, "Universal Approximation With Certified Networks": {"container_type": "Publication", "bib": {"title": "Universal approximation with certified networks", "author": ["M Baader", "M Mirman", "M Vechev"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.13846", "abstract": "Training neural networks to be certifiably robust is critical to ensure their safety against adversarial attacks. However, it is currently very difficult to train a neural network that is both accurate and certifiably robust. In this work we take a step towards addressing this challenge. We prove that for every continuous function $ f $, there exists a network $ n $ such that:(i) $ n $ approximates $ f $ arbitrarily close, and (ii) simple interval bound propagation of a region $ B $ through $ n $ yields a result that is arbitrarily close to the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.13846", "author_id": ["LKqCkWoAAAAJ", "ovm4iLwAAAAJ", "aZ1Rh50AAAAJ"], "url_scholarbib": "/scholar?q=info:lNVAozfjNXMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BApproximation%2BWith%2BCertified%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lNVAozfjNXMJ&ei=M-VXYr2pJ86E6rQPz8uiuAc&json=", "num_citations": 16, "citedby_url": "/scholar?cites=8301791316229019028&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lNVAozfjNXMJ:scholar.google.com/&scioq=Universal+Approximation+With+Certified+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.13846"}, "Pseudo-lidar++: Accurate Depth For 3d Object Detection In Autonomous Driving": {"container_type": "Publication", "bib": {"title": "Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving", "author": ["Y You", "Y Wang", "WL Chao", "D Garg", "G Pleiss"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.06310", "author_id": ["rdwkreIAAAAJ", "nZsD8XwAAAAJ", "PGKakWwAAAAJ", "ZvnesrkAAAAJ", "XO8T-Y4AAAAJ"], "url_scholarbib": "/scholar?q=info:qx0SJih_VJcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPseudo-lidar%252B%252B:%2BAccurate%2BDepth%2BFor%2B3d%2BObject%2BDetection%2BIn%2BAutonomous%2BDriving%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qx0SJih_VJcJ&ei=NuVXYqj4HovMsQK69Y7ABg&json=", "num_citations": 171, "citedby_url": "/scholar?cites=10904480408184954283&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qx0SJih_VJcJ:scholar.google.com/&scioq=Pseudo-lidar%2B%2B:+Accurate+Depth+For+3d+Object+Detection+In+Autonomous+Driving&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.06310.pdf?ref=https://githubhelp.com"}, "Improving Generalization In Meta Reinforcement Learning Using Neural Objectives": {"container_type": "Publication", "bib": {"title": "Improving generalization in meta reinforcement learning using learned objectives", "author": ["L Kirsch", "S van Steenkiste", "J Schmidhuber"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.04098", "author_id": ["w8AkOEAAAAAJ", "i-AStBYAAAAJ", "gLnCTgIAAAAJ"], "url_scholarbib": "/scholar?q=info:eaXHOX_zgnEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGeneralization%2BIn%2BMeta%2BReinforcement%2BLearning%2BUsing%2BNeural%2BObjectives%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eaXHOX_zgnEJ&ei=OeVXYv7-KI2EmgH6u5u4BA&json=", "num_citations": 60, "citedby_url": "/scholar?cites=8179367601014023545&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eaXHOX_zgnEJ:scholar.google.com/&scioq=Improving+Generalization+In+Meta+Reinforcement+Learning+Using+Neural+Objectives&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.04098"}, "Gradient Descent Maximizes The Margin Of Homogeneous Neural Networks": {"container_type": "Publication", "bib": {"title": "Gradient descent maximizes the margin of homogeneous neural networks", "author": ["K Lyu", "J Li"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.05890", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (ie, gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.05890", "author_id": ["843JJtgAAAAJ", "zX7i1EkAAAAJ"], "url_scholarbib": "/scholar?q=info:v4uXwjlsUgUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient%2BDescent%2BMaximizes%2BThe%2BMargin%2BOf%2BHomogeneous%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v4uXwjlsUgUJ&ei=PeVXYuGKCI2EmgH6u5u4BA&json=", "num_citations": 116, "citedby_url": "/scholar?cites=383487913613560767&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v4uXwjlsUgUJ:scholar.google.com/&scioq=Gradient+Descent+Maximizes+The+Margin+Of+Homogeneous+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.05890"}, "Adjustable Real-time Style Transfer": {"container_type": "Publication", "bib": {"title": "Adjustable real-time style transfer", "author": ["M Babaeizadeh", "G Ghiasi"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1811.08560", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying\" optimum\" for different input images. Therefore, if the stylized output is not appealing to the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.08560", "author_id": ["3Y4egcYAAAAJ", "9pNIbGkAAAAJ"], "url_scholarbib": "/scholar?q=info:8YVDrJ2ZmeoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdjustable%2BReal-time%2BStyle%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8YVDrJ2ZmeoJ&ei=QOVXYqXVAo2EmgH6u5u4BA&json=", "num_citations": 11, "citedby_url": "/scholar?cites=16904711578790888945&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8YVDrJ2ZmeoJ:scholar.google.com/&scioq=Adjustable+Real-time+Style+Transfer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.08560"}, "You Only Train Once: Loss-conditional Training Of Deep Networks": {"container_type": "Publication", "bib": {"title": "You only train once: Loss-conditional training of deep networks", "author": ["A Dosovitskiy", "J Djolonga"], "pub_year": "2019", "venue": "International conference on learning \u2026", "abstract": "In many machine learning problems, loss functions are weighted sums of several terms. A typical approach to dealing with these is to train multiple separate models with different selections of weights and then either choose the best one according to some criterion or keep multiple models if it is desirable to maintain a diverse set of solutions. This is inefficient both at training and at inference time. We propose a method that allows replacing multiple models trained on one loss function each by a single model trained on a distribution of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyxY6JHKwr&fbclid=IwAR3cArgo7ZEWTDY4vUqQAxPKqFjYWfRfd8hW08w2J-GB2c5DTDqhHuv-0AI", "author_id": ["FXNJRDoAAAAJ", "4NdMn_MAAAAJ"], "url_scholarbib": "/scholar?q=info:djUJY2ODuncJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DYou%2BOnly%2BTrain%2BOnce:%2BLoss-conditional%2BTraining%2BOf%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=djUJY2ODuncJ&ei=RuVXYtD8KciBy9YP18Gi8As&json=", "num_citations": 29, "citedby_url": "/scholar?cites=8627352499068155254&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:djUJY2ODuncJ:scholar.google.com/&scioq=You+Only+Train+Once:+Loss-conditional+Training+Of+Deep+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyxY6JHKwr"}, "Building Deep Equivariant Capsule Networks": {"container_type": "Publication", "bib": {"title": "Building deep, equivariant capsule networks", "author": ["S Venkatraman", "S Balasubramanian"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Capsule networks are constrained by the parameter-expensive nature of their layers, and the general lack of provable equivariance guarantees. We present a variation of capsule networks that aims to remedy this. We identify that learning all pair-wise part-whole relationships between capsules of successive layers is inefficient. Further, we also realise that the choice of prediction networks and the routing mechanism are both key to equivariance. Based on these, we propose an alternative framework for capsule networks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.01300", "author_id": ["", ""], "url_scholarbib": "/scholar?q=info:bwu5y6hPNAAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBuilding%2BDeep%2BEquivariant%2BCapsule%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bwu5y6hPNAAJ&ei=SeVXYrHuLo2EmgH6u5u4BA&json=", "num_citations": 21, "citedby_url": "/scholar?cites=14724285179956079&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bwu5y6hPNAAJ:scholar.google.com/&scioq=Building+Deep+Equivariant+Capsule+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.01300"}, "Span Recovery For Deep Neural Networks With Applications To Input Obfuscation": {"container_type": "Publication", "bib": {"title": "Span Recovery for Deep Neural Networks with Applications to Input Obfuscation", "author": ["R Jayaram", "DP Woodruff", "Q Zhang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.08202", "abstract": "The tremendous success of deep neural networks has motivated the need to better understand the fundamental properties of these networks, but many of the theoretical results proposed have only been for shallow networks. In this paper, we study an important primitive for understanding the meaningful input space of a deep network: span recovery. For $ k< n $, let $\\mathbf {A}\\in\\mathbb {R}^{k\\times n} $ be the innermost weight matrix of an arbitrary feed forward neural network $ M:\\mathbb {R}^ n\\to\\mathbb {R} $, so $ M (x) $ can be written"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.08202", "author_id": ["Cerc8UYAAAAJ", "kMmxbbIAAAAJ", "mE11hO8AAAAJ"], "url_scholarbib": "/scholar?q=info:bc2-uvRIxZoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpan%2BRecovery%2BFor%2BDeep%2BNeural%2BNetworks%2BWith%2BApplications%2BTo%2BInput%2BObfuscation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bc2-uvRIxZoJ&ei=TeVXYtrrAs6E6rQPz8uiuAc&json=", "num_citations": 1, "citedby_url": "/scholar?cites=11152400268195188077&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:bc2-uvRIxZoJ:scholar.google.com/&scioq=Span+Recovery+For+Deep+Neural+Networks+With+Applications+To+Input+Obfuscation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.08202"}, "Sequential Latent Knowledge Selection For Knowledge-grounded Dialogue": {"container_type": "Publication", "bib": {"title": "Sequential latent knowledge selection for knowledge-grounded dialogue", "author": ["B Kim", "J Ahn", "G Kim"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.07510", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07510", "author_id": ["O23D9m8AAAAJ", "", "CiSdOV0AAAAJ"], "url_scholarbib": "/scholar?q=info:IkazQ_sgKmkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSequential%2BLatent%2BKnowledge%2BSelection%2BFor%2BKnowledge-grounded%2BDialogue%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IkazQ_sgKmkJ&ei=UOVXYoWEH4ySyATlkbrQCA&json=", "num_citations": 62, "citedby_url": "/scholar?cites=7577905586548983330&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:IkazQ_sgKmkJ:scholar.google.com/&scioq=Sequential+Latent+Knowledge+Selection+For+Knowledge-grounded+Dialogue&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07510"}, "Short And Sparse Deconvolution --- A Geometric Approach": {"container_type": "Publication", "bib": {"title": "Short-and-Sparse Deconvolution--A Geometric Approach", "author": ["Y Lau", "Q Qu", "HW Kuo", "P Zhou", "Y Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Short-and-sparse deconvolution (SaSD) is the problem of extracting localized, recurring motifs in signals with spatial or temporal structure. Variants of this problem arise in applications such as image deblurring, microscopy, neural spike sorting, and more. The problem is challenging in both theory and practice, as natural optimization formulations are nonconvex. Moreover, practical deconvolution problems involve smooth motifs (kernels) whose spectra decay rapidly, resulting in poor conditioning and numerical challenges. This"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.10959", "author_id": ["", "JfblW3MAAAAJ", "hIElC_kAAAAJ", "hw0JvzAAAAAJ", "usawh5oAAAAJ"], "url_scholarbib": "/scholar?q=info:idIXZa5avloJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DShort%2BAnd%2BSparse%2BDeconvolution%2B---%2BA%2BGeometric%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=idIXZa5avloJ&ei=UuVXYunfPJaM6rQPlISayA8&json=", "num_citations": 22, "citedby_url": "/scholar?cites=6538763414055408265&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:idIXZa5avloJ:scholar.google.com/&scioq=Short+And+Sparse+Deconvolution+---+A+Geometric+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.10959"}, "Progressive Memory Banks For Incremental Domain Adaptation": {"container_type": "Publication", "bib": {"title": "Progressive memory banks for incremental domain adaptation", "author": ["N Asghar", "L Mou", "KA Selby", "KD Pantasdo"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1811.00239", "author_id": ["xYFNj60AAAAJ", "32UrMrQAAAAJ", "_IAre6MAAAAJ", "85VyYBIAAAAJ"], "url_scholarbib": "/scholar?q=info:shB40KVna-AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgressive%2BMemory%2BBanks%2BFor%2BIncremental%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=shB40KVna-AJ&ei=VuVXYsnJB4vEmgH7846QCg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=16171132848868692146&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:shB40KVna-AJ:scholar.google.com/&scioq=Progressive+Memory+Banks+For+Incremental+Domain+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1811.00239"}, "Why Gradient Clipping Accelerates Training: A Theoretical Justification For Adaptivity": {"container_type": "Publication", "bib": {"title": "Why gradient clipping accelerates training: A theoretical justification for adaptivity", "author": ["J Zhang", "T He", "S Sra", "A Jadbabaie"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.11881", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11881", "author_id": ["8NudxYsAAAAJ", "egmfjjwAAAAJ", "eyCw9goAAAAJ", "ZBc_WwYAAAAJ"], "url_scholarbib": "/scholar?q=info:AhUQo3t9cCkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhy%2BGradient%2BClipping%2BAccelerates%2BTraining:%2BA%2BTheoretical%2BJustification%2BFor%2BAdaptivity%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AhUQo3t9cCkJ&ei=WeVXYrvxEZmM6rQPjaOSEA&json=", "num_citations": 121, "citedby_url": "/scholar?cites=2986024522916828418&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AhUQo3t9cCkJ:scholar.google.com/&scioq=Why+Gradient+Clipping+Accelerates+Training:+A+Theoretical+Justification+For+Adaptivity&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11881"}, "Compression Based Bound For Non-compressed Network: Unified Generalization Error Analysis Of Large Compressible Deep Neural Network": {"container_type": "Publication", "bib": {"title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "author": ["T Suzuki", "H Abe", "T Nishimura"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11274", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. The classical learning theory suggests that overparameterized models cause overfitting. However, practically used large deep models avoid overfitting, which is not well explained by the classical approaches. To resolve this issue, several attempts have been made. Among them, the compression based bound is one of the promising approaches. However, the compression based bound can be applied only to a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11274", "author_id": ["x8osrBsAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:4uc3LXTid5MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompression%2BBased%2BBound%2BFor%2BNon-compressed%2BNetwork:%2BUnified%2BGeneralization%2BError%2BAnalysis%2BOf%2BLarge%2BCompressible%2BDeep%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4uc3LXTid5MJ&ei=Y-VXYtPwMo2EmgH6u5u4BA&json=", "num_citations": 26, "citedby_url": "/scholar?cites=10626210834406696930&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4uc3LXTid5MJ:scholar.google.com/&scioq=Compression+Based+Bound+For+Non-compressed+Network:+Unified+Generalization+Error+Analysis+Of+Large+Compressible+Deep+Neural+Network&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11274"}, "Learning To Plan In High Dimensions Via Neural Exploration-exploitation Trees": {"container_type": "Publication", "bib": {"title": "Learning to plan in high dimensions via neural exploration-exploitation trees", "author": ["B Chen", "B Dai", "Q Lin", "G Ye", "H Liu", "L Song"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a meta path planning algorithm named\\emph {Neural Exploration-Exploitation Trees~(NEXT)} for learning from prior experience for solving new path planning problems in high dimensional continuous state and action spaces. Compared to more classical sampling-based methods like RRT, our approach achieves much better sample efficiency in high-dimensions and can benefit from prior experience of planning in similar environments. More specifically, NEXT exploits a novel neural architecture which can learn promising search"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1903.00070", "author_id": ["6Px5HxsAAAAJ", "TIKl_foAAAAJ", "yIbFM0EAAAAJ", "lZt-Ij8AAAAJ", "", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:rgahC4tWF7gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BPlan%2BIn%2BHigh%2BDimensions%2BVia%2BNeural%2BExploration-exploitation%2BTrees%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rgahC4tWF7gJ&ei=aeVXYsHVMYvMsQK69Y7ABg&json=", "num_citations": 21, "citedby_url": "/scholar?cites=13265166382638630574&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:rgahC4tWF7gJ:scholar.google.com/&scioq=Learning+To+Plan+In+High+Dimensions+Via+Neural+Exploration-exploitation+Trees&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1903.00070"}, "Discovering Motor Programs By Recomposing Demonstrations": {"container_type": "Publication", "bib": {"title": "Discovering motor programs by recomposing demonstrations", "author": ["T Shankar", "S Tulsiani", "L Pinto"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "In this paper, we present an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. Current approaches to decomposing demonstrations into primitives often assume manually defined primitives and bypass the difficulty of discovering these primitives. On the other hand, approaches in primitive discovery put restrictive assumptions on the complexity of a primitive, which limit applicability to narrow tasks. Our approach attempts to circumvent these challenges by jointly learning"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkgHY0NYwr", "author_id": ["0k1qcvgAAAAJ", "06rffEkAAAAJ", "pmVPj94AAAAJ"], "url_scholarbib": "/scholar?q=info:mLAGJwKVjowJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscovering%2BMotor%2BPrograms%2BBy%2BRecomposing%2BDemonstrations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mLAGJwKVjowJ&ei=bOVXYsLOCM6E6rQPz8uiuAc&json=", "num_citations": 20, "citedby_url": "/scholar?cites=10128196448480047256&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mLAGJwKVjowJ:scholar.google.com/&scioq=Discovering+Motor+Programs+By+Recomposing+Demonstrations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkgHY0NYwr"}, "Certified Robustness For Top-k Predictions Against Adversarial Perturbations Via Randomized Smoothing": {"container_type": "Publication", "bib": {"title": "Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing", "author": ["J Jia", "X Cao", "B Wang", "NZ Gong"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.09899", "abstract": "It is well-known that classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$ k $ predictions are more relevant. In this work, we aim to derive certified robustness for top-$ k $ predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.09899", "author_id": ["iyg4ytkAAAAJ", "X5qafxAAAAAJ", "SoOztcEAAAAJ", "t6uCsYoAAAAJ"], "url_scholarbib": "/scholar?q=info:beUmRWkJV64J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCertified%2BRobustness%2BFor%2BTop-k%2BPredictions%2BAgainst%2BAdversarial%2BPerturbations%2BVia%2BRandomized%2BSmoothing%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=beUmRWkJV64J&ei=b-VXYq-3KcS4ywTtzb_QDA&json=", "num_citations": 41, "citedby_url": "/scholar?cites=12562520033309681005&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:beUmRWkJV64J:scholar.google.com/&scioq=Certified+Robustness+For+Top-k+Predictions+Against+Adversarial+Perturbations+Via+Randomized+Smoothing&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.09899"}, "Hierarchical Foresight: Self-supervised Learning Of Long-horizon Tasks Via Visual Subgoal Generation": {"container_type": "Publication", "bib": {"title": "Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation", "author": ["S Nair", "C Finn"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.05829", "abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.05829", "author_id": ["EHSuFcwAAAAJ", "vfPE6hgAAAAJ"], "url_scholarbib": "/scholar?q=info:wKUvoL_SJhYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BForesight:%2BSelf-supervised%2BLearning%2BOf%2BLong-horizon%2BTasks%2BVia%2BVisual%2BSubgoal%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wKUvoL_SJhYJ&ei=c-VXYu67DJLeyQTms5KQBg&json=", "num_citations": 57, "citedby_url": "/scholar?cites=1596194838417483200&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wKUvoL_SJhYJ:scholar.google.com/&scioq=Hierarchical+Foresight:+Self-supervised+Learning+Of+Long-horizon+Tasks+Via+Visual+Subgoal+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.05829"}, "Vl-bert: Pre-training Of Generic Visual-linguistic Representations": {"container_type": "Publication", "bib": {"title": "Vl-bert: Pre-training of generic visual-linguistic representations", "author": ["W Su", "X Zhu", "Y Cao", "B Li", "L Lu", "F Wei", "J Dai"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.08530", "author_id": ["ECDe6IIAAAAJ", "02RXI00AAAAJ", "iRUO1ckAAAAJ", "", "zdgKJXIAAAAJ", "G-V1VpwAAAAJ", "SH_-B_AAAAAJ"], "url_scholarbib": "/scholar?q=info:oxggY7KzzWsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVl-bert:%2BPre-training%2BOf%2BGeneric%2BVisual-linguistic%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oxggY7KzzWsJ&ei=fuVXYtY_kYmbAYjGt7gM&json=", "num_citations": 629, "citedby_url": "/scholar?cites=7768062511032572067&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oxggY7KzzWsJ:scholar.google.com/&scioq=Vl-bert:+Pre-training+Of+Generic+Visual-linguistic+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.08530"}, "White Noise Analysis Of Neural Networks": {"container_type": "Publication", "bib": {"title": "White noise attenuation of seismic trace by integrating variational mode decomposition with convolutional neural network", "author": ["H Wu", "B Zhang", "T Lin", "F Li", "N Liu"], "pub_year": "2019", "venue": "Geophysics", "abstract": "-generated white noise to simulate the white noise contained in the seismic data. Considering  that the white noise contained in the seismic data is band-limited and the noise level is"}, "filled": false, "gsrank": 1, "pub_url": "https://library.seg.org/doi/abs/10.1190/geo2018-0635.1", "author_id": ["7TBobacAAAAJ", "Pu0C8qUAAAAJ", "wmC4pbMAAAAJ", "f1wyd2wAAAAJ", "y31KYP8AAAAJ"], "url_scholarbib": "/scholar?q=info:v5UGPbb7vdYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhite%2BNoise%2BAnalysis%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v5UGPbb7vdYJ&ei=guVXYpPEAsiBy9YP18Gi8As&json=", "num_citations": 27, "citedby_url": "/scholar?cites=15473800654841353663&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v5UGPbb7vdYJ:scholar.google.com/&scioq=White+Noise+Analysis+Of+Neural+Networks&hl=en&as_sdt=0,33"}, "Deep Symbolic Superoptimization Without Human Knowledge": {"container_type": "Publication", "bib": {"title": "Deep symbolic superoptimization without human knowledge", "author": ["H Shi", "Y Zhang", "X Chen", "Y Tian"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Deep symbolic superoptimization refers to the task of applying deep learning methods to simplify symbolic expressions. Existing approaches either perform supervised training on human-constructed datasets that defines equivalent expression pairs, or apply reinforcement learning with human-defined equivalent trans-formation actions. In short, almost all existing methods rely on human knowledge to define equivalence, which suffers from large labeling cost and learning bias, because it is almost impossible to define and"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=r1egIyBFPS", "author_id": ["fvlDLzAAAAAJ", "_-5PSgQAAAAJ", "d4W1UT0AAAAJ", "0mgEF28AAAAJ"], "url_scholarbib": "/scholar?q=info:aSVGhniX_j4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BSymbolic%2BSuperoptimization%2BWithout%2BHuman%2BKnowledge%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aSVGhniX_j4J&ei=h-VXYoLgOpWMy9YPt8OamA0&json=", "num_citations": 3, "citedby_url": "/scholar?cites=4539232018340652393&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aSVGhniX_j4J:scholar.google.com/&scioq=Deep+Symbolic+Superoptimization+Without+Human+Knowledge&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=r1egIyBFPS"}, "Automatically Discovering And Learning New Visual Categories With Ranking Statistics": {"container_type": "Publication", "bib": {"title": "Automatically discovering and learning new visual categories with ranking statistics", "author": ["K Han", "SA Rebuffi", "S Ehrhardt", "A Vedaldi"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work we address this problem by combining three ideas:(1) we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05714", "author_id": ["tG8S_vMAAAAJ", "swP3h24AAAAJ", "vllAwPsAAAAJ", "bRT7t28AAAAJ"], "url_scholarbib": "/scholar?q=info:fiy7zp-y6lMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomatically%2BDiscovering%2BAnd%2BLearning%2BNew%2BVisual%2BCategories%2BWith%2BRanking%2BStatistics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fiy7zp-y6lMJ&ei=i-VXYsWGE42EmgH6u5u4BA&json=", "num_citations": 44, "citedby_url": "/scholar?cites=6046841849136229502&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fiy7zp-y6lMJ:scholar.google.com/&scioq=Automatically+Discovering+And+Learning+New+Visual+Categories+With+Ranking+Statistics&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05714"}, "Hoppity: Learning Graph Transformations To Detect And Fix Bugs In Programs": {"container_type": "Publication", "bib": {"title": "Hoppity: Learning graph transformations to detect and fix bugs in programs", "author": ["E Dinella", "H Dai", "Z Li", "M Naik", "L Song"], "pub_year": "2020", "venue": "\u2026 Conference on Learning \u2026", "abstract": "We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy program modeled by a graph structure, our model makes a sequence of predictions including the position of bug nodes and corresponding graph edits to produce a fix. Unlike previous works built upon deep neural networks, our approach targets bugs that are more diverse and complex in nature (ie bugs that require adding or deleting statements"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/servlets/purl/10210341", "author_id": ["DxQ4wV0AAAAJ", "obpl7GQAAAAJ", "aAQ9abEAAAAJ", "fmsV6nEAAAAJ", "Xl4E0CsAAAAJ"], "url_scholarbib": "/scholar?q=info:-yh5PKCUGDEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHoppity:%2BLearning%2BGraph%2BTransformations%2BTo%2BDetect%2BAnd%2BFix%2BBugs%2BIn%2BPrograms%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-yh5PKCUGDEJ&ei=juVXYv_PHoySyATlkbrQCA&json=", "num_citations": 85, "citedby_url": "/scholar?cites=3537740923229776123&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-yh5PKCUGDEJ:scholar.google.com/&scioq=Hoppity:+Learning+Graph+Transformations+To+Detect+And+Fix+Bugs+In+Programs&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10210341"}, "Semantically-guided Representation Learning For Self-supervised Monocular Depth": {"container_type": "Publication", "bib": {"title": "Semantically-guided representation learning for self-supervised monocular depth", "author": ["V Guizilini", "R Hou", "J Li", "R Ambrus", "A Gaidon"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.12319", "author_id": ["UH9tP6QAAAAJ", "PKHKqX0AAAAJ", "_I3COxAAAAAJ", "2xjjS3oAAAAJ", "2StUgf4AAAAJ"], "url_scholarbib": "/scholar?q=info:ge7SuBW0D-0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemantically-guided%2BRepresentation%2BLearning%2BFor%2BSelf-supervised%2BMonocular%2BDepth%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ge7SuBW0D-0J&ei=kuVXYom4NpWMy9YPt8OamA0&json=", "num_citations": 96, "citedby_url": "/scholar?cites=17082069917027724929&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ge7SuBW0D-0J:scholar.google.com/&scioq=Semantically-guided+Representation+Learning+For+Self-supervised+Monocular+Depth&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.12319"}, "Real Or Not Real, That Is The Question": {"container_type": "Publication", "bib": {"title": "CD133: to be or not to be, is this the real question?", "author": ["E Irollo", "G Pirozzi"], "pub_year": "2013", "venue": "American journal of translational research", "abstract": "Therefore, the open question remains the enumeration of CTCs with current protocols that  usually involve only the use of epithelial markers and not stem or EMT markers. In fact, CTC"}, "filled": false, "gsrank": 1, "pub_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3786264/", "author_id": ["LnkJ2DYAAAAJ", "mjFp2MMAAAAJ"], "url_scholarbib": "/scholar?q=info:JoXYRlqaT_cJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReal%2BOr%2BNot%2BReal,%2BThat%2BIs%2BThe%2BQuestion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JoXYRlqaT_cJ&ei=neVXYveCHsS4ywTtzb_QDA&json=", "num_citations": 152, "citedby_url": "/scholar?cites=17820631963054671142&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JoXYRlqaT_cJ:scholar.google.com/&scioq=Real+Or+Not+Real,+That+Is+The+Question&hl=en&as_sdt=0,33", "eprint_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3786264/"}, "Hypermodels For Exploration": {"container_type": "Publication", "bib": {"title": "Hypermodels for exploration", "author": ["V Dwaracherla", "X Lu", "M Ibrahimi", "I Osband"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the use of hypermodels to represent epistemic uncertainty and guide exploration.  This  We show that alternative hypermodels can enjoy dramatic efficiency gains, enabling"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2006.07464", "author_id": ["", "", "pgcjVaYAAAAJ", "QA4o6eYAAAAJ"], "url_scholarbib": "/scholar?q=info:gHoBVp2QJYsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHypermodels%2BFor%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gHoBVp2QJYsJ&ei=o-VXYqX8KZmM6rQPjaOSEA&json=", "num_citations": 16, "citedby_url": "/scholar?cites=10026579150837480064&as_sdt=5,33&sciodt=0,33&hl=en", "eprint_url": "https://arxiv.org/pdf/2006.07464"}, "Albert: A Lite Bert For Self-supervised Learning Of Language Representations": {"container_type": "Publication", "bib": {"title": "Albert: A lite bert for self-supervised learning of language representations", "author": ["Z Lan", "M Chen", "S Goodman", "K Gimpel"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11942", "author_id": ["tlDABkgAAAAJ", "aRncxakAAAAJ", "", "kDHs7DYAAAAJ"], "url_scholarbib": "/scholar?q=info:wzWRMzbJr1sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAlbert:%2BA%2BLite%2BBert%2BFor%2BSelf-supervised%2BLearning%2BOf%2BLanguage%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wzWRMzbJr1sJ&ei=q-VXYoS-Gc6E6rQPz8uiuAc&json=", "num_citations": 2736, "citedby_url": "/scholar?cites=6606720413006378435&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wzWRMzbJr1sJ:scholar.google.com/&scioq=Albert:+A+Lite+Bert+For+Self-supervised+Learning+Of+Language+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11942.pdf?ref=https://githubhelp.com"}, "Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping": {"container_type": "Publication", "bib": {"title": "Learning from unlabelled videos using contrastive predictive neural 3d mapping", "author": ["AW Harley", "SK Lakshmikanth", "F Li", "X Zhou"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our retinas. This paper explores the role of view prediction in the development of 3D"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.03764", "author_id": ["OB6vAtkAAAAJ", "UjpM6IAAAAAJ", "UxEw4icAAAAJ", "Kbi2t9sAAAAJ"], "url_scholarbib": "/scholar?q=info:ostd00zFN2YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bfrom%2BUnlabelled%2BVideos%2BUsing%2BContrastive%2BPredictive%2BNeural%2B3D%2BMapping%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ostd00zFN2YJ&ei=0epXYtPSEMiBy9YP18Gi8As&json=", "num_citations": 13, "citedby_url": "/scholar?cites=7365572649342061474&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ostd00zFN2YJ:scholar.google.com/&scioq=Learning+from+Unlabelled+Videos+Using+Contrastive+Predictive+Neural+3D+Mapping&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.03764"}, "On The Variance Of The Adaptive Learning Rate And Beyond": {"container_type": "Publication", "bib": {"title": "On the variance of the adaptive learning rate and beyond", "author": ["L Liu", "H Jiang", "P He", "W Chen", "X Liu", "J Gao"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (ie, it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.03265", "author_id": ["RmvbkzYAAAAJ", "XaFhuG8AAAAJ", "TS1RoxAAAAAJ", "LG_E-4EAAAAJ", "NIewcxMAAAAJ", "CQ1cqKkAAAAJ"], "url_scholarbib": "/scholar?q=info:tTLLKZi0NB4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BThe%2BVariance%2BOf%2BThe%2BAdaptive%2BLearning%2BRate%2BAnd%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tTLLKZi0NB4J&ei=B9hXYsnLPMWemAHB5baIBQ&json=", "num_citations": 902, "citedby_url": "/scholar?cites=2176563085556003509&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tTLLKZi0NB4J:scholar.google.com/&scioq=On+The+Variance+Of+The+Adaptive+Learning+Rate+And+Beyond&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.03265"}, "Emergent Systematic Generalization In A Situated Agent": {"container_type": "Publication", "bib": {"title": "Emergent Systematic Generalization in a Situated Agent.", "author": ["F Hill", "AK Lampinen", "R Schneider", "S Clark", "M Botvinick"], "pub_year": "2019", "venue": "NA", "abstract": "2048-like games are games that have similar properties with 2048, a single-player stochastic sliding puzzle game. 2048-like games are highly suitable for educational purposes due to 2048's relatively simple rules and its popularity. When using 2048-l"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=0YWEJMDcrR3", "author_id": ["4HLUnhIAAAAJ", "_N44XxAAAAAJ", "Ysx_wZgAAAAJ", "bBnvK8cAAAAJ", "eM916YMAAAAJ"], "url_scholarbib": "/scholar?q=info:Zq6zcRFHigoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BSystematic%2BGeneralization%2BIn%2BA%2BSituated%2BAgent%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Zq6zcRFHigoJ&ei=CthXYvjPMMS4ywTtzb_QDA&json=", "num_citations": 21, "citedby_url": "/scholar?cites=759497627412967014&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Zq6zcRFHigoJ:scholar.google.com/&scioq=Emergent+Systematic+Generalization+In+A+Situated+Agent&hl=en&as_sdt=0,33"}, "Sparse Coding With Gated Learned Ista": {"container_type": "Publication", "bib": {"title": "Sparse coding with gated learned ISTA", "author": ["K Wu", "Y Guo", "Z Li", "C Zhang"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "In this paper, we study the learned iterative shrinkage thresholding algorithm (LISTA) for solving sparse coding problems. Following assumptions made by prior works, we first discover that the code components in its estimations may be lower than expected, ie, require gains, and to address this problem, a gated mechanism amenable to theoretical analysis is then introduced. Specific design of the gates is inspired by convergence analyses of the mechanism and hence its effectiveness can be formally guaranteed. In addition to the gain"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BygPO2VKPH", "author_id": ["", "oi_lEwYAAAAJ", "", "GL9M37YAAAAJ"], "url_scholarbib": "/scholar?q=info:QX1C4oGRKkAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSparse%2BCoding%2BWith%2BGated%2BLearned%2BIsta%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QX1C4oGRKkAJ&ei=DdhXYs3eLMmUywTMkLbABQ&json=", "num_citations": 29, "citedby_url": "/scholar?cites=4623667954482052417&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QX1C4oGRKkAJ:scholar.google.com/&scioq=Sparse+Coding+With+Gated+Learned+Ista&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BygPO2VKPH"}, "Dynamics-aware Unsupervised Skill Discovery": {"container_type": "Publication", "bib": {"title": "Dynamics-aware unsupervised skill discovery", "author": ["A Sharma", "S Gu", "S Levine", "V Kumar"], "pub_year": "2019", "venue": "Proceedings of the \u2026", "abstract": "Model-based Reinforcement Learning (MBRL) shows the potential to facilitate learning of many different tasks by taking advantage of the learned model of the world. Learning a global model that works in every part of state-space, however, can be exceedingly challenging, especially in complex, dynamical environments. To overcome this problem, we present a method that is able to discover skills together with their 'skill dynamics models' in an unsupervised fashion. By finding the skills whose dynamics are easier to"}, "filled": false, "gsrank": 1, "pub_url": "https://tarl2019.github.io/assets/papers/sharma2019dynamicsaware.pdf", "author_id": ["_0IIzxgAAAAJ", "B8wslVsAAAAJ", "8R35rCwAAAAJ", "lPFzIaUAAAAJ"], "url_scholarbib": "/scholar?q=info:a40XzmzmmZgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamics-aware%2BUnsupervised%2BSkill%2BDiscovery%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a40XzmzmmZgJ&ei=EdhXYvOGIcmUywTMkLbABQ&json=", "num_citations": 4, "citedby_url": "/scholar?cites=10996073320190283115&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a40XzmzmmZgJ:scholar.google.com/&scioq=Dynamics-aware+Unsupervised+Skill+Discovery&hl=en&as_sdt=0,33", "eprint_url": "https://tarl2019.github.io/assets/papers/sharma2019dynamicsaware.pdf"}, "Quantifying The Cost Of Reliable Photo Authentication Via High-performance Learned Lossy Representations": {"container_type": "Publication", "bib": {"title": "Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations", "author": ["P Korus", "N Memon"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "Detection of photo manipulation relies on subtle statistical traces, notoriously removed by aggressive lossy compression employed online. We demonstrate that end-to-end modeling of complex photo dissemination channels allows for codec optimization with explicit provenance objectives. We design a lightweight trainable lossy image codec, that delivers competitive rate-distortion performance, on par with best hand-engineered alternatives, but has lower computational footprint on modern GPU-enabled platforms. Our results show that"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HyxG3p4twS", "author_id": ["UDEEvd0AAAAJ", "Pz0YttAAAAAJ"], "url_scholarbib": "/scholar?q=info:vgVIAPGG2FcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuantifying%2BThe%2BCost%2BOf%2BReliable%2BPhoto%2BAuthentication%2BVia%2BHigh-performance%2BLearned%2BLossy%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vgVIAPGG2FcJ&ei=FNhXYtTMKY2EmgH6u5u4BA&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:vgVIAPGG2FcJ:scholar.google.com/&scioq=Quantifying+The+Cost+Of+Reliable+Photo+Authentication+Via+High-performance+Learned+Lossy+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HyxG3p4twS"}, "Disentanglement Through Nonlinear Ica With General Incompressible-flow Networks (gin)": {"container_type": "Publication", "bib": {"title": "Disentanglement by nonlinear ica with general incompressible-flow networks (gin)", "author": ["P Sorrenson", "C Rother", "U K\u00f6the"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.04872", "abstract": "A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al.(2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.04872", "author_id": ["894uGm4AAAAJ", "N_YNMIMAAAAJ", "gt-yaNMAAAAJ"], "url_scholarbib": "/scholar?q=info:qUKi_-sjgecJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDisentanglement%2BThrough%2BNonlinear%2BIca%2BWith%2BGeneral%2BIncompressible-flow%2BNetworks%2B(gin)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qUKi_-sjgecJ&ei=GdhXYv2jG4vMsQK69Y7ABg&json=", "num_citations": 56, "citedby_url": "/scholar?cites=16681653991270138537&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qUKi_-sjgecJ:scholar.google.com/&scioq=Disentanglement+Through+Nonlinear+Ica+With+General+Incompressible-flow+Networks+(gin)&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.04872"}, "Fooling Detection Alone Is Not Enough: Adversarial Attack Against Multiple Object Tracking": {"container_type": "Publication", "bib": {"title": "Fooling detection alone is not enough: Adversarial attack against multiple object tracking", "author": ["YJ Jia", "Y Lu", "J Shen", "QA Chen", "H Chen"], "pub_year": "2020", "venue": "International \u2026", "abstract": "Recent work in adversarial machine learning started to focus on the visual perception in autonomous driving and studied Adversarial Examples (AEs) for object detection models. However, in such visual perception pipeline the detected objects must also be tracked, in a process called Multiple Object Tracking (MOT), to build the moving trajectories of surrounding obstacles. Since MOT is designed to be robust against errors in object detection, it poses a general challenge to existing attack techniques that blindly target"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/servlets/purl/10197165", "author_id": ["", "kBUsho4AAAAJ", "Ift_GAQAAAAJ", "lcsu7m8AAAAJ", "1Aa3qxIAAAAJ"], "url_scholarbib": "/scholar?q=info:a-exk8uMUNcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFooling%2BDetection%2BAlone%2BIs%2BNot%2BEnough:%2BAdversarial%2BAttack%2BAgainst%2BMultiple%2BObject%2BTracking%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a-exk8uMUNcJ&ei=HNhXYvaLPIvMsQK69Y7ABg&json=", "num_citations": 34, "citedby_url": "/scholar?cites=15515055522275518315&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:a-exk8uMUNcJ:scholar.google.com/&scioq=Fooling+Detection+Alone+Is+Not+Enough:+Adversarial+Attack+Against+Multiple+Object+Tracking&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10197165"}, "Automated Curriculum Generation Through Setter-solver Interactions": {"container_type": "Publication", "bib": {"title": "Automated curriculum generation through setter-solver interactions", "author": ["S Racaniere", "A Lampinen", "A Santoro"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula\u2013the breakdown of tasks into simpler, static challenges with dense rewards\u2013to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1e0Wp4KvH", "author_id": ["o-h0vrQAAAAJ", "_N44XxAAAAAJ", "evIkDWoAAAAJ"], "url_scholarbib": "/scholar?q=info:Uq8VlWr8Ng4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomated%2BCurriculum%2BGeneration%2BThrough%2BSetter-solver%2BInteractions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Uq8VlWr8Ng4J&ei=H9hXYqKFIsiBy9YP18Gi8As&json=", "num_citations": 22, "citedby_url": "/scholar?cites=1024283499971325778&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Uq8VlWr8Ng4J:scholar.google.com/&scioq=Automated+Curriculum+Generation+Through+Setter-solver+Interactions&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1e0Wp4KvH"}, "From Inference To Generation: End-to-end Fully Self-supervised Generation Of Human Face From Speech": {"container_type": "Publication", "bib": {"title": "From inference to generation: End-to-end fully self-supervised generation of human face from speech", "author": ["HS Choi", "C Park", "K Lee"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.05830", "abstract": "This work seeks the possibility of generating the human face from voice solely based on the audio-visual data without any human-labeled annotations. To this end, we propose a multi-modal learning framework that links the inference stage and generation stage. First, the inference networks are trained to match the speaker identity between the two different modalities. Then the trained inference networks cooperate with the generation network by giving conditional information about the voice. The proposed method exploits the recent"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.05830", "author_id": ["YhtK34kAAAAJ", "", "Fk4jQFEAAAAJ"], "url_scholarbib": "/scholar?q=info:lX0OPWZNAIwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrom%2BInference%2BTo%2BGeneration:%2BEnd-to-end%2BFully%2BSelf-supervised%2BGeneration%2BOf%2BHuman%2BFace%2BFrom%2BSpeech%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lX0OPWZNAIwJ&ei=IthXYr-DF5LeyQTms5KQBg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=10088148266816273813&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lX0OPWZNAIwJ:scholar.google.com/&scioq=From+Inference+To+Generation:+End-to-end+Fully+Self-supervised+Generation+Of+Human+Face+From+Speech&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.05830.pdf?ref=https://githubhelp.com"}, "Cater: A Diagnostic Dataset For Compositional Actions & Temporal Reasoning": {"container_type": "Publication", "bib": {"title": "CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning", "author": ["R Girdhar", "D Ramanan"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.04744", "abstract": "Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.04744", "author_id": ["7cuwdr8AAAAJ", "9B8PoXUAAAAJ"], "url_scholarbib": "/scholar?q=info:qygqiEQnpbkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCater:%2BA%2BDiagnostic%2BDataset%2BFor%2BCompositional%2BActions%2B%2526%2BTemporal%2BReasoning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qygqiEQnpbkJ&ei=JdhXYpHbH-HDywTjooCQBQ&json=", "num_citations": 67, "citedby_url": "/scholar?cites=13377141443469650091&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:qygqiEQnpbkJ:scholar.google.com/&scioq=Cater:+A+Diagnostic+Dataset+For+Compositional+Actions+%26+Temporal+Reasoning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.04744"}, "V4d: 4d Convonlutional Neural Networks For Video-level Representation Learning": {"container_type": "Publication", "bib": {"title": "V4d: 4d convolutional neural networks for video-level representation learning", "author": ["S Zhang", "S Guo", "W Huang", "MR Scott"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Most existing 3D CNNs for video representation learning are clip-based methods, and thus do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, referred as V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, and at the same time, to preserve strong 3D spatio-temporal representation with residual connections. Specifically, we design a new 4D residual block able to capture inter-clip interactions, which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07442", "author_id": ["", "mbpgOmEAAAAJ", "78vU1IUAAAAJ", "0isiW-4AAAAJ"], "url_scholarbib": "/scholar?q=info:JEONhM3QAOsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DV4d:%2B4d%2BConvonlutional%2BNeural%2BNetworks%2BFor%2BVideo-level%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JEONhM3QAOsJ&ei=J9hXYtShOsmUywTMkLbABQ&json=", "num_citations": 37, "citedby_url": "/scholar?cites=16933764180023788324&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JEONhM3QAOsJ:scholar.google.com/&scioq=V4d:+4d+Convonlutional+Neural+Networks+For+Video-level+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07442"}, "Ranking Policy Gradient": {"container_type": "Publication", "bib": {"title": "Ranking policy gradient", "author": ["K Lin", "J Zhou"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.09674", "abstract": "stochastic policies exist since the pairwise ranking policy takes extra efforts to construct a   policy, we introduce Listwise Policy Gradient (LPG) that optimizes the probability of ranking a"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.09674", "author_id": ["egq785sAAAAJ", "yQKlLTQAAAAJ"], "url_scholarbib": "/scholar?q=info:3YTAOIW069AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRanking%2BPolicy%2BGradient%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3YTAOIW069AJ&ei=KthXYoHALYvMsQK69Y7ABg&json=", "num_citations": 10, "citedby_url": "/scholar?cites=15054324663691805917&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:3YTAOIW069AJ:scholar.google.com/&scioq=Ranking+Policy+Gradient&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.09674"}, "Regularizing Activations In Neural Networks Via Distribution Matching With The Wassertein Metric": {"container_type": "Publication", "bib": {"title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "author": ["T Joo", "D Kang", "B Kim"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05366", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $ L"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05366", "author_id": ["ESo1UqMAAAAJ", "", "MF7RSKAAAAAJ"], "url_scholarbib": "/scholar?q=info:YzhaWo-K81MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegularizing%2BActivations%2BIn%2BNeural%2BNetworks%2BVia%2BDistribution%2BMatching%2BWith%2BThe%2BWassertein%2BMetric%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YzhaWo-K81MJ&ei=LdhXYrmdIM2Ny9YPqPyUgAs&json=", "num_citations": 4, "citedby_url": "/scholar?cites=6049331072789526627&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:YzhaWo-K81MJ:scholar.google.com/&scioq=Regularizing+Activations+In+Neural+Networks+Via+Distribution+Matching+With+The+Wassertein+Metric&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05366"}, "In Search For A Sat-friendly Binarized Neural Network Architecture": {"container_type": "Publication", "bib": {"title": "In search for a SAT-friendly binarized neural network architecture", "author": ["N Narodytska", "H Zhang", "A Gupta"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Analyzing the behavior of neural networks is one of the most pressing challenges in deep learning. Binarized Neural Networks are an important class of networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. Such tools can be used to answer existential and probabilistic queries about the network, perform explanation generation, etc. However, the main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, we analyze"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJx-j64FDr", "author_id": ["pQw6xK4AAAAJ", "C0Fiv5oAAAAJ", ""], "url_scholarbib": "/scholar?q=info:fLQQ-CpXXrAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIn%2BSearch%2BFor%2BA%2BSat-friendly%2BBinarized%2BNeural%2BNetwork%2BArchitecture%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fLQQ-CpXXrAJ&ei=NNhXYq_7CYvMsQK69Y7ABg&json=", "num_citations": 13, "citedby_url": "/scholar?cites=12708691040548205692&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fLQQ-CpXXrAJ:scholar.google.com/&scioq=In+Search+For+A+Sat-friendly+Binarized+Neural+Network+Architecture&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJx-j64FDr"}, "Is A Good Representation Sufficient For Sample Efficient Reinforcement Learning?": {"container_type": "Publication", "bib": {"title": "Is a good representation sufficient for sample efficient reinforcement learning?", "author": ["SS Du", "SM Kakade", "R Wang", "LF Yang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.03016", "abstract": "Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.03016", "author_id": ["OttawxUAAAAJ", "wb-DKCIAAAAJ", "n8ZpnWMAAAAJ", "umivlPQAAAAJ"], "url_scholarbib": "/scholar?q=info:ZBUZ-FxytbUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIs%2BA%2BGood%2BRepresentation%2BSufficient%2BFor%2BSample%2BEfficient%2BReinforcement%2BLearning%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZBUZ-FxytbUJ&ei=N9hXYuu2CJGJmwGIxre4DA&json=", "num_citations": 104, "citedby_url": "/scholar?cites=13093497235274536292&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ZBUZ-FxytbUJ:scholar.google.com/&scioq=Is+A+Good+Representation+Sufficient+For+Sample+Efficient+Reinforcement+Learning%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.03016"}, "An Exponential Learning Rate Schedule For Batch Normalized Networks": {"container_type": "Publication", "bib": {"title": "An exponential learning rate schedule for batch normalized networks", "author": ["Z Li", "S Arora"], "venue": "NA", "pub_year": "NA", "abstract": ""}, "filled": false, "gsrank": 1, "author_id": ["", ""], "url_scholarbib": "/scholar?q=info:KFLe-nAYJoAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BExponential%2BLearning%2BRate%2BSchedule%2BFor%2BBatch%2BNormalized%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KFLe-nAYJoAJ&ei=OthXYrvpJsS4ywTtzb_QDA&json=", "num_citations": 2, "citedby_url": "/scholar?cites=9234094959494058536&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KFLe-nAYJoAJ:scholar.google.com/&scioq=An+Exponential+Learning+Rate+Schedule+For+Batch+Normalized+Networks&hl=en&as_sdt=0,33"}, "Dba: Distributed Backdoor Attacks Against Federated Learning": {"container_type": "Publication", "bib": {"title": "Dba: Distributed backdoor attacks against federated learning", "author": ["C Xie", "K Huang", "PY Chen", "B Li"], "pub_year": "2019", "venue": "International Conference on \u2026", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkgyS0VFvr", "author_id": ["WeJnzAgAAAAJ", "5IPQzykAAAAJ", "jxwlCUUAAAAJ", "K8vJkTcAAAAJ"], "url_scholarbib": "/scholar?q=info:S6kHgvgPGdUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDba:%2BDistributed%2BBackdoor%2BAttacks%2BAgainst%2BFederated%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S6kHgvgPGdUJ&ei=PthXYoiuIMWemAHB5baIBQ&json=", "num_citations": 160, "citedby_url": "/scholar?cites=15355321964504262987&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:S6kHgvgPGdUJ:scholar.google.com/&scioq=Dba:+Distributed+Backdoor+Attacks+Against+Federated+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkgyS0VFvr"}, "Optimistic Exploration Even With A Pessimistic Initialisation": {"container_type": "Publication", "bib": {"title": "Optimistic exploration even with a pessimistic initialisation", "author": ["T Rashid", "B Peng", "W Boehmer", "S Whiteson"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.12174", "author_id": ["d4BeWwcAAAAJ", "I1r7hQcAAAAJ", "wI5MV8IAAAAJ", "9zeEI-cAAAAJ"], "url_scholarbib": "/scholar?q=info:VdYsS6AEfzIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimistic%2BExploration%2BEven%2BWith%2BA%2BPessimistic%2BInitialisation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VdYsS6AEfzIJ&ei=QthXYq6qC82Ny9YPqPyUgAs&json=", "num_citations": 21, "citedby_url": "/scholar?cites=3638632110441158229&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VdYsS6AEfzIJ:scholar.google.com/&scioq=Optimistic+Exploration+Even+With+A+Pessimistic+Initialisation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.12174"}, "Multi-agent Reinforcement Learning For Networked System Control": {"container_type": "Publication", "bib": {"title": "Multi-agent reinforcement learning for networked system control", "author": ["T Chu", "S Chinchali", "S Katti"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2004.01339", "abstract": "This paper considers multi-agent reinforcement learning (MARL) in networked system control. Specifically, each agent learns a decentralized control policy based on local observations and messages from connected neighbors. We formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, we propose a new differentiable communication protocol, called NeurComm, to reduce information loss and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.01339", "author_id": ["Bi9EXmUAAAAJ", "262ASa4AAAAJ", "cc4Qi_IAAAAJ"], "url_scholarbib": "/scholar?q=info:mABapyQrqXQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-agent%2BReinforcement%2BLearning%2BFor%2BNetworked%2BSystem%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mABapyQrqXQJ&ei=RdhXYo2rGMiBy9YP18Gi8As&json=", "num_citations": 34, "citedby_url": "/scholar?cites=8406297615890251928&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mABapyQrqXQJ:scholar.google.com/&scioq=Multi-agent+Reinforcement+Learning+For+Networked+System+Control&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.01339"}, "Physics-as-inverse-graphics: Unsupervised Physical Parameter Estimation From Video": {"container_type": "Publication", "bib": {"title": "Physics-as-inverse-graphics: Unsupervised physical parameter estimation from video", "author": ["M Jaques", "M Burke", "T Hospedales"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.11169", "abstract": "We propose a model that is able to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a physics-as-inverse-graphics approach that brings together vision-as"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11169", "author_id": ["UymGb6BvT74C", "Abz56f4AAAAJ", "nHhtvqkAAAAJ"], "url_scholarbib": "/scholar?q=info:trUJNsuoqm4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPhysics-as-inverse-graphics:%2BUnsupervised%2BPhysical%2BParameter%2BEstimation%2BFrom%2BVideo%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=trUJNsuoqm4J&ei=SNhXYt6LHsS4ywTtzb_QDA&json=", "num_citations": 19, "citedby_url": "/scholar?cites=7974371680951317942&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:trUJNsuoqm4J:scholar.google.com/&scioq=Physics-as-inverse-graphics:+Unsupervised+Physical+Parameter+Estimation+From+Video&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11169"}, "On Solving Minimax Optimization Locally: A Follow-the-ridge Approach": {"container_type": "Publication", "bib": {"title": "On solving minimax optimization locally: A follow-the-ridge approach", "author": ["Y Wang", "G Zhang", "J Ba"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.07512", "abstract": "Many tasks in modern machine learning can be formulated as finding equilibria in\\emph {sequential} games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.07512", "author_id": ["yj2b7pgAAAAJ", "B_TZBtwAAAAJ", "ymzxRhAAAAAJ"], "url_scholarbib": "/scholar?q=info:i0PZ7GQSWXQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BSolving%2BMinimax%2BOptimization%2BLocally:%2BA%2BFollow-the-ridge%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=i0PZ7GQSWXQJ&ei=S9hXYsf4KsiBy9YP18Gi8As&json=", "num_citations": 63, "citedby_url": "/scholar?cites=8383752406006580107&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:i0PZ7GQSWXQJ:scholar.google.com/&scioq=On+Solving+Minimax+Optimization+Locally:+A+Follow-the-ridge+Approach&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.07512"}, "A Theory Of Usable Information Under Computational Constraints": {"container_type": "Publication", "bib": {"title": "A theory of usable information under computational constraints", "author": ["Y Xu", "S Zhao", "J Song", "R Stewart", "S Ermon"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting\\emph {predictive $\\mathcal {V} $-information} encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, $\\mathcal"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10689", "author_id": ["MU6vjxQAAAAJ", "bMoauM4AAAAJ", "6dP660cAAAAJ", "", "ogXTOZ4AAAAJ"], "url_scholarbib": "/scholar?q=info:_s7-o7cnWRIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BTheory%2BOf%2BUsable%2BInformation%2BUnder%2BComputational%2BConstraints%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_s7-o7cnWRIJ&ei=TthXYurxHIvMsQK69Y7ABg&json=", "num_citations": 37, "citedby_url": "/scholar?cites=1322131635293835006&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_s7-o7cnWRIJ:scholar.google.com/&scioq=A+Theory+Of+Usable+Information+Under+Computational+Constraints&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10689"}, "Variance Reduction With Sparse Gradients": {"container_type": "Publication", "bib": {"title": "Variance reduction with sparse gradients", "author": ["M Elibol", "L Lei", "MI Jordan"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.09623", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.09623", "author_id": ["MvYTnPoAAAAJ", "-lKb3XwAAAAJ", "yxUduqMAAAAJ"], "url_scholarbib": "/scholar?q=info:EUdDIirBhP4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance%2BReduction%2BWith%2BSparse%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EUdDIirBhP4J&ei=UdhXYse_Bc2Ny9YPqPyUgAs&json=", "num_citations": 11, "citedby_url": "/scholar?cites=18339995969267123985&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:EUdDIirBhP4J:scholar.google.com/&scioq=Variance+Reduction+With+Sparse+Gradients&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.09623"}, "Genesis: Generative Scene Inference And Sampling With Object-centric Latent Representations": {"container_type": "Publication", "bib": {"title": "Genesis: Generative scene inference and sampling with object-centric latent representations", "author": ["M Engelcke", "AR Kosiorek", "OP Jones"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.13052", "author_id": ["MDaD5mEAAAAJ", "i7eVfzwAAAAJ", "zXJwucQAAAAJ"], "url_scholarbib": "/scholar?q=info:hAKaCPmCyq4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenesis:%2BGenerative%2BScene%2BInference%2BAnd%2BSampling%2BWith%2BObject-centric%2BLatent%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hAKaCPmCyq4J&ei=VNhXYoyGCY6pywSdh6agAg&json=", "num_citations": 122, "citedby_url": "/scholar?cites=12595023313997791876&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hAKaCPmCyq4J:scholar.google.com/&scioq=Genesis:+Generative+Scene+Inference+And+Sampling+With+Object-centric+Latent+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.13052"}, "Few-shot Text Classification With Distributional Signatures": {"container_type": "Publication", "bib": {"title": "Few-shot text classification with distributional signatures", "author": ["Y Bao", "M Wu", "S Chang", "R Barzilay"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.06039", "abstract": "In this paper, we explore meta-learning for few-shot text classification. Meta-learning has shown strong performance in computer vision, where low-level patterns are transferable across learning tasks. However, directly applying this approach to text is challenging--lexical features highly informative for one task may be insignificant for another. Thus, rather than learning solely from words, our model also leverages their distributional signatures, which encode pertinent word occurrence patterns. Our model is trained within a meta-learning"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.06039", "author_id": ["Ee4Peu4AAAAJ", "MdA5FEAAAAAJ", "r21asW4AAAAJ", ""], "url_scholarbib": "/scholar?q=info:2JFD2VbrnkMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFew-shot%2BText%2BClassification%2BWith%2BDistributional%2BSignatures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2JFD2VbrnkMJ&ei=V9hXYuu4NMWemAHB5baIBQ&json=", "num_citations": 66, "citedby_url": "/scholar?cites=4872590605106254296&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2JFD2VbrnkMJ:scholar.google.com/&scioq=Few-shot+Text+Classification+With+Distributional+Signatures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.06039"}, "A Learning-based Iterative Method For Solving Vehicle Routing Problems": {"container_type": "Publication", "bib": {"title": "A learning-based iterative method for solving vehicle routing problems", "author": ["H Lu", "X Zhang", "S Yang"], "pub_year": "2019", "venue": "International conference on learning \u2026", "abstract": "This paper is concerned with solving combinatorial optimization problems, in particular, the capacitated vehicle routing problems (CVRP). Classical Operations Research (OR) algorithms such as LKH3\\citep {helsgaun2017extension} are inefficient and difficult to scale to larger-size problems. Machine learning based approaches have recently shown to be promising, partly because of their efficiency (once trained, they can perform solving within minutes or even seconds). However, there is still a considerable gap between the quality of"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=BJe1334YDH", "author_id": ["", "", "K8jZJxsAAAAJ"], "url_scholarbib": "/scholar?q=info:z6HBNf4ft30J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BLearning-based%2BIterative%2BMethod%2BFor%2BSolving%2BVehicle%2BRouting%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=z6HBNf4ft30J&ei=Y9hXYsr2Es2Ny9YPqPyUgAs&json=", "num_citations": 76, "citedby_url": "/scholar?cites=9058744352163078607&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:z6HBNf4ft30J:scholar.google.com/&scioq=A+Learning-based+Iterative+Method+For+Solving+Vehicle+Routing+Problems&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=BJe1334YDH"}, "Measuring Compositional Generalization: A Comprehensive Method On Realistic Data": {"container_type": "Publication", "bib": {"title": "Measuring compositional generalization: A comprehensive method on realistic data", "author": ["D Keysers", "N Sch\u00e4rli", "N Scales", "H Buisman"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.09713", "author_id": ["nZO3qCcAAAAJ", "64RoFnUAAAAJ", "zIop5SgAAAAJ", "-ropVHQAAAAJ"], "url_scholarbib": "/scholar?q=info:cV4iYUvF4q8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2BCompositional%2BGeneralization:%2BA%2BComprehensive%2BMethod%2BOn%2BRealistic%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cV4iYUvF4q8J&ei=ZdhXYvmFM5WMy9YPt8OamA0&json=", "num_citations": 119, "citedby_url": "/scholar?cites=12673909228916858481&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cV4iYUvF4q8J:scholar.google.com/&scioq=Measuring+Compositional+Generalization:+A+Comprehensive+Method+On+Realistic+Data&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.09713"}, "Learning To Balance: Bayesian Meta-learning For Imbalanced And Out-of-distribution Tasks": {"container_type": "Publication", "bib": {"title": "Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks", "author": ["HB Lee", "H Lee", "D Na", "S Kim", "M Park", "E Yang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that the number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.12917", "author_id": ["50_nxq0AAAAJ", "5DaLgBUAAAAJ", "", "_ZfueMIAAAAJ", "hZRH7hoAAAAJ", "UWO1mloAAAAJ"], "url_scholarbib": "/scholar?q=info:tixWPDPocU0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BBalance:%2BBayesian%2BMeta-learning%2BFor%2BImbalanced%2BAnd%2BOut-of-distribution%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tixWPDPocU0J&ei=aNhXYqGIM5WMy9YPt8OamA0&json=", "num_citations": 61, "citedby_url": "/scholar?cites=5580496720042011830&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tixWPDPocU0J:scholar.google.com/&scioq=Learning+To+Balance:+Bayesian+Meta-learning+For+Imbalanced+And+Out-of-distribution+Tasks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.12917"}, "Large Batch Optimization For Deep Learning: Training Bert In 76 Minutes": {"container_type": "Publication", "bib": {"title": "Large batch optimization for deep learning: Training bert in 76 minutes", "author": ["Y You", "J Li", "S Reddi", "J Hseu", "S Kumar"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1904.00962", "author_id": ["jF4dPZwAAAAJ", "", "70lgwYwAAAAJ", "2gPPf14AAAAJ", "08CNqrYAAAAJ"], "url_scholarbib": "/scholar?q=info:6m7pkbcWkJAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2BBatch%2BOptimization%2BFor%2BDeep%2BLearning:%2BTraining%2BBert%2BIn%2B76%2BMinutes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6m7pkbcWkJAJ&ei=bNhXYpKGE8mUywTMkLbABQ&json=", "num_citations": 353, "citedby_url": "/scholar?cites=10416850915790778090&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6m7pkbcWkJAJ:scholar.google.com/&scioq=Large+Batch+Optimization+For+Deep+Learning:+Training+Bert+In+76+Minutes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1904.00962.pdf)"}, "Understanding Knowledge Distillation In Non-autoregressive Machine Translation": {"container_type": "Publication", "bib": {"title": "Understanding knowledge distillation in non-autoregressive machine translation", "author": ["C Zhou", "G Neubig", "J Gu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.02727", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.02727", "author_id": ["mR5W7EgAAAAJ", "wlosgkoAAAAJ", "cB1mFBsAAAAJ"], "url_scholarbib": "/scholar?q=info:oJFo3KE794gJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BKnowledge%2BDistillation%2BIn%2BNon-autoregressive%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oJFo3KE794gJ&ei=b9hXYraaHJmM6rQPjaOSEA&json=", "num_citations": 90, "citedby_url": "/scholar?cites=9869422674779345312&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oJFo3KE794gJ:scholar.google.com/&scioq=Understanding+Knowledge+Distillation+In+Non-autoregressive+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.02727"}, "Deephoyer: Learning Sparser Neural Network With Differentiable Scale-invariant Sparsity Measures": {"container_type": "Publication", "bib": {"title": "Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures", "author": ["H Yang", "W Wen", "H Li"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.09979", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values, but it cannot provide useful gradients, and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant, causing the same"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.09979", "author_id": ["bjNCUt8AAAAJ", "JYD36ocAAAAJ", "E6Tpfq8AAAAJ"], "url_scholarbib": "/scholar?q=info:0RhEyQOy3YEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeephoyer:%2BLearning%2BSparser%2BNeural%2BNetwork%2BWith%2BDifferentiable%2BScale-invariant%2BSparsity%2BMeasures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0RhEyQOy3YEJ&ei=c9hXYoyyB46pywSdh6agAg&json=", "num_citations": 48, "citedby_url": "/scholar?cites=9357831330077087953&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0RhEyQOy3YEJ:scholar.google.com/&scioq=Deephoyer:+Learning+Sparser+Neural+Network+With+Differentiable+Scale-invariant+Sparsity+Measures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.09979"}, "Robust Local Features For Improving The Generalization Of Adversarial Training": {"container_type": "Publication", "bib": {"title": "Robust local features for improving the generalization of adversarial training", "author": ["C Song", "K He", "J Lin", "L Wang", "JE Hopcroft"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Adversarial training has been demonstrated as one of the most effective methods for training robust models to defend against adversarial examples. However, adversarially trained models often lack adversarially robust generalization on unseen testing data. Recent works show that adversarially trained models are more biased towards global structure features. Instead, in this work, we would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.10147", "author_id": ["el17bJoAAAAJ", "YTQnGJsAAAAJ", "UqIG9yAAAAAJ", "VZHxoh8AAAAJ", "4Z6vo5QAAAAJ"], "url_scholarbib": "/scholar?q=info:HvKn84dIT6IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BLocal%2BFeatures%2BFor%2BImproving%2BThe%2BGeneralization%2BOf%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HvKn84dIT6IJ&ei=dthXYpnDBpWMy9YPt8OamA0&json=", "num_citations": 43, "citedby_url": "/scholar?cites=11695646506050122270&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HvKn84dIT6IJ:scholar.google.com/&scioq=Robust+Local+Features+For+Improving+The+Generalization+Of+Adversarial+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.10147"}, "Adversarial Training And Provable Defenses: Bridging The Gap": {"container_type": "Publication", "bib": {"title": "Adversarial training and provable defenses: Bridging the gap", "author": ["M Balunovic", "M Vechev"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "We present COLT, a new method to train neural networks based on a novel combination of adversarial training and provable defenses. The key idea is to model neural network training as a procedure which includes both, the verifier and the adversary. In every iteration, the verifier aims to certify the network using convex relaxation while the adversary tries to find inputs inside that convex relaxation which cause verification to fail. We experimentally show that this training method, named convex layerwise adversarial training (COLT), is promising"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJxSDxrKDr", "author_id": ["fxkgmGwAAAAJ", "aZ1Rh50AAAAJ"], "url_scholarbib": "/scholar?q=info:VJNUww2hDu4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BTraining%2BAnd%2BProvable%2BDefenses:%2BBridging%2BThe%2BGap%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VJNUww2hDu4J&ei=eNhXYtuPLoySyATlkbrQCA&json=", "num_citations": 84, "citedby_url": "/scholar?cites=17153825111184544596&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VJNUww2hDu4J:scholar.google.com/&scioq=Adversarial+Training+And+Provable+Defenses:+Bridging+The+Gap&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJxSDxrKDr"}, "Lagrangian Fluid Simulation With Continuous Convolutions": {"container_type": "Publication", "bib": {"title": "Lagrangian fluid simulation with continuous convolutions", "author": ["B Ummenhofer", "L Prantl", "N Thuerey"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of ND convolutions to the continuous domain. We show that our network architecture can"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=B1lDoJSYDH", "author_id": ["QGlp5ywAAAAJ", "oKLBsnsAAAAJ", "GEehwv8AAAAJ"], "url_scholarbib": "/scholar?q=info:-nPar-yMWc8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLagrangian%2BFluid%2BSimulation%2BWith%2BContinuous%2BConvolutions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-nPar-yMWc8J&ei=fdhXYojgFYvEmgH7846QCg&json=", "num_citations": 68, "citedby_url": "/scholar?cites=14941128186968830970&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-nPar-yMWc8J:scholar.google.com/&scioq=Lagrangian+Fluid+Simulation+With+Continuous+Convolutions&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=B1lDoJSYDH"}, "Probabilistic Connection Importance Inference And Lossless Compression Of Deep Neural Networks": {"container_type": "Publication", "bib": {"title": "Probabilistic connection importance inference and lossless compression of deep neural networks", "author": ["X Xing", "L Sha"], "pub_year": "2020", "venue": "International Conference on Learning Representations", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels. We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN's outputs using a nonparemtric scoring testand keep only those significant ones. Experimental"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/biblio/10297632", "author_id": ["R0reBKEAAAAJ", "14dEKaQAAAAJ"], "url_scholarbib": "/scholar?q=info:oylJLOeI0_kJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbabilistic%2BConnection%2BImportance%2BInference%2BAnd%2BLossless%2BCompression%2BOf%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oylJLOeI0_kJ&ei=gthXYv28CZmM6rQPjaOSEA&json=", "num_citations": 4, "citedby_url": "/scholar?cites=18001882661991819683&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:oylJLOeI0_kJ:scholar.google.com/&scioq=Probabilistic+Connection+Importance+Inference+And+Lossless+Compression+Of+Deep+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10297632"}, "An Inductive Bias For Distances: Neural Nets That Respect The Triangle Inequality": {"container_type": "Publication", "bib": {"title": "An inductive bias for distances: Neural nets that respect the triangle inequality", "author": ["S Pitis", "H Chan", "K Jamali", "J Ba"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05825", "abstract": "Distances are pervasive in machine learning. They serve as similarity measures, loss functions, and learning targets; it is said that a good distance measure solves a task. When defining distances, the triangle inequality has proven to be a useful constraint, both theoretically--to prove convergence and optimality guarantees--and empirically--as an inductive bias. Deep metric learning architectures that respect the triangle inequality rely, almost exclusively, on Euclidean distance in the latent space. Though effective, this fails to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05825", "author_id": ["oYlo1ycAAAAJ", "0tLCTHYAAAAJ", "qoyhWcYAAAAJ", "ymzxRhAAAAAJ"], "url_scholarbib": "/scholar?q=info:GWUyO16Tiu8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BInductive%2BBias%2BFor%2BDistances:%2BNeural%2BNets%2BThat%2BRespect%2BThe%2BTriangle%2BInequality%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GWUyO16Tiu8J&ei=hthXYuvyNJGJmwGIxre4DA&json=", "num_citations": 7, "citedby_url": "/scholar?cites=17260770554780214553&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GWUyO16Tiu8J:scholar.google.com/&scioq=An+Inductive+Bias+For+Distances:+Neural+Nets+That+Respect+The+Triangle+Inequality&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05825"}, "Learning To Represent Programs With Property Signatures": {"container_type": "Publication", "bib": {"title": "Learning to represent programs with property signatures", "author": ["A Odena", "C Sutton"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.09030", "abstract": "We introduce the notion of property signatures, a representation for programs and program specifications meant for consumption by machine learning algorithms. Given a function with input type $\\tau_ {in} $ and output type $\\tau_ {out} $, a property is a function of type: $(\\tau_ {in},\\tau_ {out})\\rightarrow\\texttt {Bool} $ that (informally) describes some simple property of the function under consideration. For instance, if $\\tau_ {in} $ and $\\tau_ {out} $ are both lists of the same type, one property might askis the input list the same length as the output list?'. If"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.09030", "author_id": ["EHQHNdEAAAAJ", "hYtGXD0AAAAJ"], "url_scholarbib": "/scholar?q=info:eivJAwOAVX8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BRepresent%2BPrograms%2BWith%2BProperty%2BSignatures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eivJAwOAVX8J&ei=idhXYoncOc2Ny9YPqPyUgAs&json=", "num_citations": 14, "citedby_url": "/scholar?cites=9175380566274026362&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eivJAwOAVX8J:scholar.google.com/&scioq=Learning+To+Represent+Programs+With+Property+Signatures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.09030"}, "Cross-lingual Ability Of Multilingual Bert: An Empirical Study": {"container_type": "Publication", "bib": {"title": "Cross-lingual ability of multilingual bert: An empirical study", "author": ["Z Wang", "S Mayhew", "D Roth"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.07840", "abstract": "Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT)--surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages--Spanish"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.07840", "author_id": ["6UWtYZQAAAAJ", "-uLaJq4AAAAJ", "E-bpPWgAAAAJ"], "url_scholarbib": "/scholar?q=info:xqWnOITI0FsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCross-lingual%2BAbility%2BOf%2BMultilingual%2BBert:%2BAn%2BEmpirical%2BStudy%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xqWnOITI0FsJ&ei=jNhXYr-FM5mM6rQPjaOSEA&json=", "num_citations": 159, "citedby_url": "/scholar?cites=6616008322819007942&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xqWnOITI0FsJ:scholar.google.com/&scioq=Cross-lingual+Ability+Of+Multilingual+Bert:+An+Empirical+Study&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.07840"}, "Making Efficient Use Of Demonstrations To Solve Hard Exploration Problems": {"container_type": "Publication", "bib": {"title": "Making efficient use of demonstrations to solve hard exploration problems", "author": ["TL Paine", "C Gulcehre", "B Shahriari", "M Denil"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "the problem of learning from demonstrations in hard exploration  These three aspects together  conspire to make learning  Sparse rewards induce a difficult exploration problem, which is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.01387", "author_id": ["oFIvUSQAAAAJ", "7hwJ2ckAAAAJ", "Vwas7kAAAAAJ", "XrKLUO0AAAAJ"], "url_scholarbib": "/scholar?q=info:hDxEf56pF94J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaking%2BEfficient%2BUse%2BOf%2BDemonstrations%2BTo%2BSolve%2BHard%2BExploration%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hDxEf56pF94J&ei=kdhXYqPzIJaM6rQPlISayA8&json=", "num_citations": 21, "citedby_url": "/scholar?cites=16003446299089452164&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:hDxEf56pF94J:scholar.google.com/&scioq=Making+Efficient+Use+Of+Demonstrations+To+Solve+Hard+Exploration+Problems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.01387.pdf?ref=https://githubhelp.com"}, "Low-resource Knowledge-grounded Dialogue Generation": {"container_type": "Publication", "bib": {"title": "Low-resource knowledge-grounded dialogue generation", "author": ["X Zhao", "W Wu", "C Tao", "C Xu", "D Zhao", "R Yan"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, we consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a disentangled response decoder in order to isolate parameters that"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.10348", "author_id": ["h-87C9cAAAAJ", "YtqXSzMAAAAJ", "x_cOKuwAAAAJ", "5aiE_NcAAAAJ", "lhR8-68AAAAJ", "eLw6g-UAAAAJ"], "url_scholarbib": "/scholar?q=info:jEsSnfuB2YEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLow-resource%2BKnowledge-grounded%2BDialogue%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jEsSnfuB2YEJ&ei=lNhXYubIOYvEmgH7846QCg&json=", "num_citations": 48, "citedby_url": "/scholar?cites=9356652618510912396&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jEsSnfuB2YEJ:scholar.google.com/&scioq=Low-resource+Knowledge-grounded+Dialogue+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.10348"}, "Self-labelling Via Simultaneous Clustering And Representation Learning": {"container_type": "Publication", "bib": {"title": "Self-labelling via simultaneous clustering and representation learning", "author": ["YM Asano", "C Rupprecht", "A Vedaldi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.05371", "abstract": "Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard crossentropy minimization to an optimal transport problem, which"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.05371", "author_id": ["CdpLhlgAAAAJ", "IrYlproAAAAJ", "bRT7t28AAAAJ"], "url_scholarbib": "/scholar?q=info:DDYCGAHoC_AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-labelling%2BVia%2BSimultaneous%2BClustering%2BAnd%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DDYCGAHoC_AJ&ei=l9hXYrucOMLZmQHc1ovQAg&json=", "num_citations": 266, "citedby_url": "/scholar?cites=17297173885241931276&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:DDYCGAHoC_AJ:scholar.google.com/&scioq=Self-labelling+Via+Simultaneous+Clustering+And+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.05371"}, "Intrinsic Motivation For Encouraging Synergistic Behavior": {"container_type": "Publication", "bib": {"title": "Intrinsic motivation for encouraging synergistic behavior", "author": ["R Chitnis", "S Tulsiani", "S Gupta", "A Gupta"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.05189", "abstract": "We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a goal they could not individually. Our key idea is that a good guiding principle for intrinsic motivation in synergistic tasks is to take actions which affect the world in ways that would not be achieved if the agents were acting on their own. Thus, we propose to incentivize agents to take (joint) actions whose effects cannot be predicted via a composition"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.05189", "author_id": ["rNcmwggAAAAJ", "06rffEkAAAAJ", "1HO5UacAAAAJ", "bqL73OkAAAAJ"], "url_scholarbib": "/scholar?q=info:ng7W38RGCKUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntrinsic%2BMotivation%2BFor%2BEncouraging%2BSynergistic%2BBehavior%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ng7W38RGCKUJ&ei=mthXYoO6Ho2EmgH6u5u4BA&json=", "num_citations": 11, "citedby_url": "/scholar?cites=11891832627454676638&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ng7W38RGCKUJ:scholar.google.com/&scioq=Intrinsic+Motivation+For+Encouraging+Synergistic+Behavior&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.05189"}, "Precision Gating: Improving Neural Network Efficiency With Dynamic Dual-precision Activations": {"container_type": "Publication", "bib": {"title": "Precision gating: Improving neural network efficiency with dynamic dual-precision activations", "author": ["Y Zhang", "R Zhao", "W Hua", "N Xu", "GE Suh"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in a higher precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. Our experiments indicate that PG achieves excellent results on CNNs, including statically"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07136", "author_id": ["XrUhMgwAAAAJ", "8dswaWgAAAAJ", "KsykV2cAAAAJ", "", "neO3vFYAAAAJ"], "url_scholarbib": "/scholar?q=info:WO32w-W9xU0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPrecision%2BGating:%2BImproving%2BNeural%2BNetwork%2BEfficiency%2BWith%2BDynamic%2BDual-precision%2BActivations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WO32w-W9xU0J&ei=nNhXYveNNM2Ny9YPqPyUgAs&json=", "num_citations": 15, "citedby_url": "/scholar?cites=5604094105865350488&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WO32w-W9xU0J:scholar.google.com/&scioq=Precision+Gating:+Improving+Neural+Network+Efficiency+With+Dynamic+Dual-precision+Activations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07136"}, "Learning To Link": {"container_type": "Publication", "bib": {"title": "Learning to link", "author": ["MF Balcan", "T Dick", "M Lang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.00533", "abstract": "In this work, we study data-driven algorithm selection and metric learning for clustering   We design efficient learning algorithms which receive samples from an application-specific"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.00533", "author_id": ["LWlN_BUAAAAJ", "j30Gau4AAAAJ", "ZTTMZGUAAAAJ"], "url_scholarbib": "/scholar?q=info:ifUxYr8ELHIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BLink%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ifUxYr8ELHIJ&ei=n9hXYuuSKI2EmgH6u5u4BA&json=", "num_citations": 12, "citedby_url": "/scholar?cites=8226955839331759497&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ifUxYr8ELHIJ:scholar.google.com/&scioq=Learning+To+Link&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.00533"}, "A Closer Look At The Approximation Capabilities Of Neural Networks": {"container_type": "Publication", "bib": {"title": "A closer look at the approximation capabilities of neural networks", "author": ["KFE Chong"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06505", "abstract": "The universal approximation theorem, in one of its most general versions, says that if we consider only continuous activation functions $\\sigma $, then a standard feedforward neural network with one hidden layer is able to approximate any continuous multivariate function $ f $ to any given approximation threshold $\\varepsilon $, if and only if $\\sigma $ is non-polynomial. In this paper, we give a direct algebraic proof of the theorem. Furthermore we shall explicitly quantify the number of hidden units required for approximation. Specifically, if"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06505", "author_id": ["JewaBYEAAAAJ"], "url_scholarbib": "/scholar?q=info:4w4BufS3boIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCloser%2BLook%2BAt%2BThe%2BApproximation%2BCapabilities%2BOf%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4w4BufS3boIJ&ei=qthXYputHZGJmwGIxre4DA&json=", "num_citations": 10, "citedby_url": "/scholar?cites=9398651734072561379&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4w4BufS3boIJ:scholar.google.com/&scioq=A+Closer+Look+At+The+Approximation+Capabilities+Of+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06505"}, "Remixmatch: Semi-supervised Learning With Distribution Matching And Augmentation Anchoring": {"container_type": "Publication", "bib": {"title": "Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring", "author": ["D Berthelot", "N Carlini", "ED Cubuk", "A Kurakin"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The goal of a semi-supervised learning algorithm is to learn  Typical ways of achieving this  include training against \u201cguessed semi-supervised learning methods relevant to ReMixMatch,"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.09785", "author_id": ["46--eogAAAAJ", "q4qDvAoAAAAJ", "Mu_8iOEAAAAJ", "nCh4qyMAAAAJ"], "url_scholarbib": "/scholar?q=info:GMTy55wU9h8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRemixmatch:%2BSemi-supervised%2BLearning%2BWith%2BDistribution%2BMatching%2BAnd%2BAugmentation%2BAnchoring%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GMTy55wU9h8J&ei=rdhXYqbkEJaM6rQPlISayA8&json=", "num_citations": 392, "citedby_url": "/scholar?cites=2303050923585487896&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GMTy55wU9h8J:scholar.google.com/&scioq=Remixmatch:+Semi-supervised+Learning+With+Distribution+Matching+And+Augmentation+Anchoring&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.09785"}, "On Robustness Of Neural Ordinary Differential Equations": {"container_type": "Publication", "bib": {"title": "On robustness of neural ordinary differential equations", "author": ["H Yan", "J Du", "VYF Tan", "J Feng"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.05513", "abstract": "Neural ordinary differential equations (ODEs) have been attracting increasing attention in various research domains recently. There have been some works studying optimization issues and approximation capabilities of neural ODEs, but their robustness is still yet unclear. In this work, we fill this important gap by exploring robustness properties of neural ODEs both empirically and theoretically. We first present an empirical study on the robustness of the neural ODE-based networks (ODENets) by exposing them to inputs with"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.05513", "author_id": ["xRz9vN4AAAAJ", "WrJKEzEAAAAJ", "dJoAVvAAAAAJ", "Q8iay0gAAAAJ"], "url_scholarbib": "/scholar?q=info:lHDXoPUkSrQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BRobustness%2BOf%2BNeural%2BOrdinary%2BDifferential%2BEquations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lHDXoPUkSrQJ&ei=r9hXYqWMMMiBy9YP18Gi8As&json=", "num_citations": 61, "citedby_url": "/scholar?cites=12991236712487678100&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:lHDXoPUkSrQJ:scholar.google.com/&scioq=On+Robustness+Of+Neural+Ordinary+Differential+Equations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.05513"}, "A Generalized Training Approach For Multiagent Learning": {"container_type": "Publication", "bib": {"title": "A generalized training approach for multiagent learning", "author": ["P Muller", "S Omidshafiei", "M Rowland", "K Tuyls"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime wherein Nash equilibria are tractably computable. In moving from two-player zero-sum games to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12823", "author_id": ["mvb2bX0AAAAJ", "nm5wMNUAAAAJ", "-0U84zMAAAAJ", "cxRqeVwAAAAJ"], "url_scholarbib": "/scholar?q=info:Oh9w9c_wrdQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BGeneralized%2BTraining%2BApproach%2BFor%2BMultiagent%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Oh9w9c_wrdQJ&ei=s9hXYqfUDJaM6rQPlISayA8&json=", "num_citations": 39, "citedby_url": "/scholar?cites=15325169882978328378&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Oh9w9c_wrdQJ:scholar.google.com/&scioq=A+Generalized+Training+Approach+For+Multiagent+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12823"}, "On Identifiability In Transformers": {"container_type": "Publication", "bib": {"title": "On identifiability in transformers", "author": ["G Brunner", "Y Liu", "D Pascual", "O Richter"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Transformer architecture by investigating two of its core components: self-attention and  contextual embeddings. In particular, we study the identifiability  weights are not identifiable. We"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.04211", "author_id": ["ldsZ_WsAAAAJ", "", "WcmmNSIAAAAJ", "dKfpL8oAAAAJ"], "url_scholarbib": "/scholar?q=info:nztE40qQhw8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BIdentifiability%2BIn%2BTransformers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nztE40qQhw8J&ei=tdhXYt3cOYvEmgH7846QCg&json=", "num_citations": 96, "citedby_url": "/scholar?cites=1119021683739736991&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:nztE40qQhw8J:scholar.google.com/&scioq=On+Identifiability+In+Transformers&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.04211"}, "Abstract Diagrammatic Reasoning With Multiplex Graph Networks": {"container_type": "Publication", "bib": {"title": "Abstract diagrammatic reasoning with multiplex graph networks", "author": ["D Wang", "M Jamnik", "P Lio"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2006.11197", "abstract": "reasoning, particularly in the visual domain, is a complex human ability, but it remains a challenging problem for artificial neural learning systems. In this work we propose MXGNet, a multilayer graph neural network for multi-panel diagrammatic reasoning tasks. MXGNet combines three powerful concepts, namely, object-level representation, graph neural networks and multiplex graphs, for solving visual reasoning tasks. MXGNet first extracts object-level representations for each element in all panels of the diagrams, and then"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2006.11197", "author_id": ["8532hHAAAAAJ", "", "3YrWf7EAAAAJ"], "url_scholarbib": "/scholar?q=info:N46t_NM50skJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAbstract%2BDiagrammatic%2BReasoning%2BWith%2BMultiplex%2BGraph%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N46t_NM50skJ&ei=udhXYrikB5WMy9YPt8OamA0&json=", "num_citations": 35, "citedby_url": "/scholar?cites=14542749729372868151&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:N46t_NM50skJ:scholar.google.com/&scioq=Abstract+Diagrammatic+Reasoning+With+Multiplex+Graph+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2006.11197"}, "Contrastive Learning Of Structured World Models": {"container_type": "Publication", "bib": {"title": "Contrastive learning of structured world models", "author": ["T Kipf", "E van der Pol", "M Welling"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1911.12247", "abstract": "A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.12247", "author_id": ["83HL5FwAAAAJ", "564o-vIAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:eZpIgxeouZkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContrastive%2BLearning%2BOf%2BStructured%2BWorld%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eZpIgxeouZkJ&ei=w9hXYtHMJsiBy9YP18Gi8As&json=", "num_citations": 142, "citedby_url": "/scholar?cites=11077069577434733177&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:eZpIgxeouZkJ:scholar.google.com/&scioq=Contrastive+Learning+Of+Structured+World+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.12247.pdf?ref=https://githubhelp.com"}, "Estimating Gradients For Discrete Random Variables By Sampling Without Replacement": {"container_type": "Publication", "bib": {"title": "Estimating gradients for discrete random variables by sampling without replacement", "author": ["W Kool", "H van Hoof", "M Welling"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.06043", "abstract": "We derive an unbiased estimator for expectations over discrete random variables based on sampling without replacement, which reduces variance as it avoids duplicate samples. We show that our estimator can be derived as the Rao-Blackwellization of three different estimators. Combining our estimator with REINFORCE, we obtain a policy gradient estimator and we reduce its variance using a built-in control variate which is obtained without additional model evaluations. The resulting estimator is closely related to other gradient"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06043", "author_id": ["DLCKZqUAAAAJ", "9owUkLYAAAAJ", "8200InoAAAAJ"], "url_scholarbib": "/scholar?q=info:6rDTyFwYJnkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEstimating%2BGradients%2BFor%2BDiscrete%2BRandom%2BVariables%2BBy%2BSampling%2BWithout%2BReplacement%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6rDTyFwYJnkJ&ei=zdhXYtWwHMiBy9YP18Gi8As&json=", "num_citations": 34, "citedby_url": "/scholar?cites=8729691714489659626&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6rDTyFwYJnkJ:scholar.google.com/&scioq=Estimating+Gradients+For+Discrete+Random+Variables+By+Sampling+Without+Replacement&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06043"}, "Gradient-based Neural Dag Learning": {"container_type": "Publication", "bib": {"title": "Gradient-based neural dag learning", "author": ["S Lachapelle", "P Brouillard", "T Deleu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while avoiding the combinatorial nature of the problem. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.02226", "author_id": ["uxHoJp8AAAAJ", "qm9aIqUAAAAJ", "nLNwh-wAAAAJ"], "url_scholarbib": "/scholar?q=info:FQjE9USnipEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient-based%2BNeural%2BDag%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FQjE9USnipEJ&ei=0NhXYsWZFoySyATlkbrQCA&json=", "num_citations": 70, "citedby_url": "/scholar?cites=10487378596908501013&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FQjE9USnipEJ:scholar.google.com/&scioq=Gradient-based+Neural+Dag+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.02226"}, "Differentially Private Meta-learning": {"container_type": "Publication", "bib": {"title": "Differentially private meta-learning", "author": ["J Li", "M Khodak", "S Caldas", "A Talwalkar"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.05830", "abstract": "Crucially, although there are various threat models and degrees of DP one could  consider in the meta-learning setting (as we outline in Section 2), we balance the well-documented"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.05830", "author_id": ["JDS2BnIAAAAJ", "gIH9P-8AAAAJ", "u15oBdQAAAAJ", "TW7U1W0AAAAJ"], "url_scholarbib": "/scholar?q=info:_TB2VQHu-dQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentially%2BPrivate%2BMeta-learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_TB2VQHu-dQJ&ei=09hXYpS9CciBy9YP18Gi8As&json=", "num_citations": 58, "citedby_url": "/scholar?cites=15346558894737862909&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_TB2VQHu-dQJ:scholar.google.com/&scioq=Differentially+Private+Meta-learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.05830"}, "Incremental Rnn: A Dynamical View.": {"container_type": "Publication", "bib": {"title": "Deep incremental rnn for learning sequential data: A lyapunov stable dynamical system", "author": ["Z Zhang", "G Wu", "Y Li", "Y Yue"], "pub_year": "2021", "venue": "2021 IEEE International \u2026", "abstract": "viewing neural networks from a dynamical systems perspective as pointwise affine maps.  However, the theoretical results are adapted from dynamical  a novel deep incremental RNN ("}, "filled": false, "gsrank": 1, "pub_url": "https://ieeexplore.ieee.org/abstract/document/9679058/", "author_id": ["2yqx3oIAAAAJ", "D4ZrjeAAAAAJ", "ICOWtt0AAAAJ", ""], "url_scholarbib": "/scholar?q=info:WZF9ox3ChDsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIncremental%2BRnn:%2BA%2BDynamical%2BView.%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WZF9ox3ChDsJ&ei=1thXYu-KAs2Ny9YPqPyUgAs&json=", "num_citations": 2, "citedby_url": "/scholar?cites=4288766177716310361&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WZF9ox3ChDsJ:scholar.google.com/&scioq=Incremental+Rnn:+A+Dynamical+View.&hl=en&as_sdt=0,33", "eprint_url": "http://users.wpi.edu/~yli15/Includes/21_ICDM21_TinyRNN.pdf"}, "Augmenting Non-collaborative Dialog Systems With Explicit Semantic And Strategic Dialog History": {"container_type": "Publication", "bib": {"title": "Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History", "author": ["Y Zhou", "Y Tsvetkov", "AW Black", "Z Yu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.13425", "abstract": "We study non-collaborative dialogs, where two agents have a conflict of interest but must strategically communicate to reach an agreement (eg, negotiation). This setting poses new challenges for modeling dialog history because the dialog's outcome relies not only on the semantic intent, but also on tactics that convey the intent. We propose to model both semantic and tactic history using finite state transducers (FSTs). Unlike RNN, FSTs can explicitly represent dialog history through all the states traversed, facilitating interpretability"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.13425", "author_id": ["omQIqKwAAAAJ", "SEDPkrsAAAAJ", "Es-YRKMAAAAJ", "jee2Dy0AAAAJ"], "url_scholarbib": "/scholar?q=info:o9z6m-KjIsgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAugmenting%2BNon-collaborative%2BDialog%2BSystems%2BWith%2BExplicit%2BSemantic%2BAnd%2BStrategic%2BDialog%2BHistory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o9z6m-KjIsgJ&ei=3thXYqHiB8mUywTMkLbABQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=14421269150468594851&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:o9z6m-KjIsgJ:scholar.google.com/&scioq=Augmenting+Non-collaborative+Dialog+Systems+With+Explicit+Semantic+And+Strategic+Dialog+History&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.13425"}, "Maximum Likelihood Constraint Inference For Inverse Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Maximum likelihood constraint inference for inverse reinforcement learning", "author": ["DRR Scobee", "SS Sastry"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.05477", "abstract": "While most approaches to the problem of Inverse Reinforcement Learning (IRL) focus on estimating a reward function that best explains an expert agent's policy or demonstrated behavior on a control task, it is often the case that such behavior is more succinctly represented by a simple reward combined with a set of hard constraints. In this setting, the agent is attempting to maximize cumulative rewards subject to these given constraints on their behavior. We reformulate the problem of IRL on Markov Decision Processes (MDPs)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.05477", "author_id": ["", "KgZxzjsAAAAJ"], "url_scholarbib": "/scholar?q=info:4aRK9VAOqsYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaximum%2BLikelihood%2BConstraint%2BInference%2BFor%2BInverse%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4aRK9VAOqsYJ&ei=4thXYp7CN8LZmQHc1ovQAg&json=", "num_citations": 16, "citedby_url": "/scholar?cites=14315270106426025185&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:4aRK9VAOqsYJ:scholar.google.com/&scioq=Maximum+Likelihood+Constraint+Inference+For+Inverse+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.05477"}, "Watch, Try, Learn: Meta-learning From Demonstrations And Rewards": {"container_type": "Publication", "bib": {"title": "Watch, try, learn: Meta-learning from demonstrations and reward", "author": ["A Zhou", "E Jang", "D Kappler", "A Herzog"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.03352", "author_id": ["6S9C8XoAAAAJ", "", "_WLInT0AAAAJ", "jrfFYAIAAAAJ"], "url_scholarbib": "/scholar?q=info:-oEiZMCwO7IJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWatch,%2BTry,%2BLearn:%2BMeta-learning%2BFrom%2BDemonstrations%2BAnd%2BRewards%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-oEiZMCwO7IJ&ei=7dhXYtT7NpLeyQTms5KQBg&json=", "num_citations": 25, "citedby_url": "/scholar?cites=12843053102737293818&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-oEiZMCwO7IJ:scholar.google.com/&scioq=Watch,+Try,+Learn:+Meta-learning+From+Demonstrations+And+Rewards&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.03352.pdf?ref=https://githubhelp.com"}, "Fast Neural Network Adaptation Via Parameters Remapping": {"container_type": "Publication", "bib": {"title": "Fast neural network adaptation via parameter remapping and architecture search", "author": ["J Fang", "Y Sun", "K Peng", "Q Zhang", "Y Li", "W Liu"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02525", "author_id": ["-JcFoOoAAAAJ", "EGewoUAAAAAJ", "", "pCY-bikAAAAJ", "nh5a4LQAAAAJ", "D7jDk7gAAAAJ"], "url_scholarbib": "/scholar?q=info:Ipo9FybFMdQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFast%2BNeural%2BNetwork%2BAdaptation%2BVia%2BParameters%2BRemapping%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ipo9FybFMdQJ&ei=8dhXYuivI46pywSdh6agAg&json=", "num_citations": 29, "citedby_url": "/scholar?cites=15290218977288886818&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Ipo9FybFMdQJ:scholar.google.com/&scioq=Fast+Neural+Network+Adaptation+Via+Parameters+Remapping&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02525"}, "Deep Double Descent: Where Bigger Models And More Data Hurt": {"container_type": "Publication", "bib": {"title": "Deep double descent: Where bigger models and more data hurt", "author": ["P Nakkiran", "G Kaplun", "Y Bansal", "T Yang"], "pub_year": "2021", "venue": "Journal of Statistical \u2026", "abstract": "We show that a variety of modern deep learning tasks exhibit a'double-descent'phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our"}, "filled": false, "gsrank": 1, "pub_url": "https://iopscience.iop.org/article/10.1088/1742-5468/ac3a74/meta", "author_id": ["zithBbUAAAAJ", "y4BzFYsAAAAJ", "uj1OljkAAAAJ", ""], "url_scholarbib": "/scholar?q=info:aSGU28AtUooJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BDouble%2BDescent:%2BWhere%2BBigger%2BModels%2BAnd%2BMore%2BData%2BHurt%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aSGU28AtUooJ&ei=9NhXYtzNGYvMsQK69Y7ABg&json=", "num_citations": 361, "citedby_url": "/scholar?cites=9967079231665217897&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aSGU28AtUooJ:scholar.google.com/&scioq=Deep+Double+Descent:+Where+Bigger+Models+And+More+Data+Hurt&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02292"}, "Quantifying Point-prediction Uncertainty In Neural Networks Via Residual Estimation With An I/o Kernel": {"container_type": "Publication", "bib": {"title": "Quantifying point-prediction uncertainty in neural networks via residual estimation with an i/o kernel", "author": ["X Qiu", "E Meyerson", "R Miikkulainen"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.00588", "abstract": "Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (ie risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.00588", "author_id": ["SET9oYsAAAAJ", "RT_LBQ4AAAAJ", "2SmbjHAAAAAJ"], "url_scholarbib": "/scholar?q=info:dp0W9sAjVI8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuantifying%2BPoint-prediction%2BUncertainty%2BIn%2BNeural%2BNetworks%2BVia%2BResidual%2BEstimation%2BWith%2BAn%2BI/o%2BKernel%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dp0W9sAjVI8J&ei=-NhXYqrbCcWemAHB5baIBQ&json=", "num_citations": 30, "citedby_url": "/scholar?cites=10327919157136760182&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dp0W9sAjVI8J:scholar.google.com/&scioq=Quantifying+Point-prediction+Uncertainty+In+Neural+Networks+Via+Residual+Estimation+With+An+I/o+Kernel&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.00588"}, "Learning To Retrieve Reasoning Paths Over Wikipedia Graph For Question Answering": {"container_type": "Publication", "bib": {"title": "Learning to retrieve reasoning paths over wikipedia graph for question answering", "author": ["A Asai", "K Hashimoto", "H Hajishirzi", "R Socher"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.10470", "author_id": ["gqB4u_wAAAAJ", "gVi99BIAAAAJ", "LOV6_WIAAAAJ", "FaOcyfMAAAAJ"], "url_scholarbib": "/scholar?q=info:xRAFgeISjYoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BRetrieve%2BReasoning%2BPaths%2BOver%2BWikipedia%2BGraph%2BFor%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xRAFgeISjYoJ&ei=-9hXYtPoDYySyATlkbrQCA&json=", "num_citations": 167, "citedby_url": "/scholar?cites=9983656712986759365&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xRAFgeISjYoJ:scholar.google.com/&scioq=Learning+To+Retrieve+Reasoning+Paths+Over+Wikipedia+Graph+For+Question+Answering&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.10470.pdf?ref=https://githubhelp.com"}, "Truth Or Backpropaganda? An Empirical Investigation Of Deep Learning Theory": {"container_type": "Publication", "bib": {"title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "author": ["M Goldblum", "J Geiping", "A Schwarzschild"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we:(1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples;(2) show that small-norm parameters are not optimal for generalization;(3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.00359", "author_id": ["pGDKzuUAAAAJ", "206vNCEAAAAJ", "WNvQ7AcAAAAJ"], "url_scholarbib": "/scholar?q=info:UcdGn_NiygkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTruth%2BOr%2BBackpropaganda%253F%2BAn%2BEmpirical%2BInvestigation%2BOf%2BDeep%2BLearning%2BTheory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UcdGn_NiygkJ&ei=_thXYty6BM6E6rQPz8uiuAc&json=", "num_citations": 24, "citedby_url": "/scholar?cites=705485090125694801&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:UcdGn_NiygkJ:scholar.google.com/&scioq=Truth+Or+Backpropaganda%3F+An+Empirical+Investigation+Of+Deep+Learning+Theory&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.00359"}, "Simple And Effective Regularization Methods For Training On Noisily Labeled Data With Generalization Guarantee": {"container_type": "Publication", "bib": {"title": "Simple and effective regularization methods for training on noisily labeled data with generalization guarantee", "author": ["W Hu", "Z Li", "D Yu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.11368", "abstract": "Over-parameterized deep neural networks trained by simple first-order methods are known to be able to fit any labeling of data. Such over-fitting ability hinders generalization when mislabeled training examples are present. On the other hand, simple regularization methods like early-stopping can often achieve highly nontrivial performance on clean test data in these scenarios, a phenomenon not theoretically understood. This paper proposes and analyzes two simple and intuitive regularization methods:(i) regularization by the distance"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11368", "author_id": ["ZybgAqkAAAAJ", "5vVjpBsAAAAJ", "KJLJstYAAAAJ"], "url_scholarbib": "/scholar?q=info:ht4sd83W3I8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimple%2BAnd%2BEffective%2BRegularization%2BMethods%2BFor%2BTraining%2BOn%2BNoisily%2BLabeled%2BData%2BWith%2BGeneralization%2BGuarantee%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ht4sd83W3I8J&ei=AdlXYqWHD4vEmgH7846QCg&json=", "num_citations": 26, "citedby_url": "/scholar?cites=10366396620256108166&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ht4sd83W3I8J:scholar.google.com/&scioq=Simple+And+Effective+Regularization+Methods+For+Training+On+Noisily+Labeled+Data+With+Generalization+Guarantee&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11368"}, "Directional Message Passing For Molecular Graphs": {"container_type": "Publication", "bib": {"title": "Directional message passing for molecular graphs", "author": ["J Klicpera", "J Gro\u00df", "S G\u00fcnnemann"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2003.03123", "abstract": "Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, eg in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2003.03123", "author_id": ["QqdUw8MAAAAJ", "", "npqoAWwAAAAJ"], "url_scholarbib": "/scholar?q=info:gU9LBlSbzSoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDirectional%2BMessage%2BPassing%2BFor%2BMolecular%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gU9LBlSbzSoJ&ei=BNlXYuyOFYySyATlkbrQCA&json=", "num_citations": 230, "citedby_url": "/scholar?cites=3084292105003814785&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gU9LBlSbzSoJ:scholar.google.com/&scioq=Directional+Message+Passing+For+Molecular+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2003.03123"}, "Meta-learning Deep Energy-based Memory Models": {"container_type": "Publication", "bib": {"title": "Meta-learning deep energy-based memory models", "author": ["S Bartunov", "JW Rae", "S Osindero"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the problem of learning associative memory--a system which is able to retrieve a remembered pattern based on its distorted or incomplete version. Attractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02720", "author_id": ["VDmoGxwAAAAJ", "s0YmMPcAAAAJ", "Jq8ZS5kAAAAJ"], "url_scholarbib": "/scholar?q=info:wWZZgN9Q6eQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2BDeep%2BEnergy-based%2BMemory%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wWZZgN9Q6eQJ&ei=CNlXYr3TBY2EmgH6u5u4BA&json=", "num_citations": 20, "citedby_url": "/scholar?cites=16494804031082424001&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wWZZgN9Q6eQJ:scholar.google.com/&scioq=Meta-learning+Deep+Energy-based+Memory+Models&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02720"}, "Autoq: Automated Kernel-wise Neural Network Quantization": {"container_type": "Publication", "bib": {"title": "Autoq: Automated kernel-wise neural network quantization", "author": ["Q Lou", "F Guo", "L Liu", "M Kim", "L Jiang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1902.05690", "abstract": "Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1902.05690", "author_id": ["SBYgXLoAAAAJ", "L6lqEq8AAAAJ", "L5dHk5cAAAAJ", "hEfnFKAAAAAJ", "-1sXorAAAAAJ"], "url_scholarbib": "/scholar?q=info:aVPOJBA4O4UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutoq:%2BAutomated%2BKernel-wise%2BNeural%2BNetwork%2BQuantization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aVPOJBA4O4UJ&ei=CtlXYs3GH82Ny9YPqPyUgAs&json=", "num_citations": 26, "citedby_url": "/scholar?cites=9600328672658477929&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aVPOJBA4O4UJ:scholar.google.com/&scioq=Autoq:+Automated+Kernel-wise+Neural+Network+Quantization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1902.05690"}, "Sqil: Imitation Learning Via Reinforcement Learning With Sparse Rewards": {"container_type": "Publication", "bib": {"title": "Sqil: Imitation learning via reinforcement learning with sparse rewards", "author": ["S Reddy", "AD Dragan", "S Levine"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.11108", "abstract": "Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL)"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.11108", "author_id": ["7GSWYLQAAAAJ", "UgHB5oAAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:72dIUFUjtb0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSqil:%2BImitation%2BLearning%2BVia%2BReinforcement%2BLearning%2BWith%2BSparse%2BRewards%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=72dIUFUjtb0J&ei=DdlXYtirL5mM6rQPjaOSEA&json=", "num_citations": 85, "citedby_url": "/scholar?cites=13669871093279123439&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:72dIUFUjtb0J:scholar.google.com/&scioq=Sqil:+Imitation+Learning+Via+Reinforcement+Learning+With+Sparse+Rewards&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.11108?ref=https://githubhelp.com"}, "Reformer: The Efficient Transformer": {"container_type": "Publication", "bib": {"title": "Reformer: The efficient transformer", "author": ["N Kitaev", "\u0141 Kaiser", "A Levskaya"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.04451", "abstract": "is both efficient to use  Reformer matches the results obtained with full Transformer but runs  much faster, especially on the text task, and with orders of magnitude better memory efficiency"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.04451", "author_id": ["", "JWmiQR0AAAAJ", "dN9QZfEAAAAJ"], "url_scholarbib": "/scholar?q=info:jYu0qEO9iOkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReformer:%2BThe%2BEfficient%2BTransformer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jYu0qEO9iOkJ&ei=ENlXYvPxKM2Ny9YPqPyUgAs&json=", "num_citations": 700, "citedby_url": "/scholar?cites=16827908105960721293&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jYu0qEO9iOkJ:scholar.google.com/&scioq=Reformer:+The+Efficient+Transformer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.04451.pdf?ref=https://githubhelp.com"}, "Mma Training: Direct Input Space Margin Maximization Through Adversarial Training": {"container_type": "Publication", "bib": {"title": "Mma training: Direct input space margin maximization through adversarial training", "author": ["GW Ding", "Y Sharma", "KYC Lui", "R Huang"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1812.02637", "abstract": "We study adversarial robustness of neural networks from a margin maximization perspective, where margins are defined as the distances from inputs to a classifier's decision boundary. Our study shows that maximizing margins can be achieved by minimizing the adversarial loss on the decision boundary at the\" shortest successful perturbation\", demonstrating a close connection between adversarial losses and the margins. We propose Max-Margin Adversarial (MMA) training to directly maximize the margins to achieve"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1812.02637", "author_id": ["f7AS33oAAAAJ", "AlGCn8wAAAAJ", "", "-tNbYyQAAAAJ"], "url_scholarbib": "/scholar?q=info:uyJGaeaYDiIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMma%2BTraining:%2BDirect%2BInput%2BSpace%2BMargin%2BMaximization%2BThrough%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uyJGaeaYDiIJ&ei=FNlXYq-8GJaM6rQPlISayA8&json=", "num_citations": 102, "citedby_url": "/scholar?cites=2454066962339603131&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:uyJGaeaYDiIJ:scholar.google.com/&scioq=Mma+Training:+Direct+Input+Space+Margin+Maximization+Through+Adversarial+Training&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1812.02637.pdf)%7B:target=%22_blank%22%7D"}, "Generalized Convolutional Forest Networks For Domain Generalization And Visual Recognition": {"container_type": "Publication", "bib": {"title": "Generalized convolutional forest networks for domain generalization and visual recognition", "author": ["J Ryu", "G Kwon", "MH Yang", "J Lim"], "pub_year": "2019", "venue": "International conference on \u2026", "abstract": "When constructing random forests, it is of prime importance to ensure high accuracy and low correlation of individual tree classifiers for good performance. Nevertheless, it is typically difficult for existing random forest methods to strike a good balance between these conflicting factors. In this work, we propose a generalized convolutional forest networks to learn a feature space to maximize the strength of individual tree classifiers while minimizing the respective correlation. The feature space is iteratively constructed by a probabilistic"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1lxVyStPH", "author_id": ["bfj6WyIAAAAJ", "", "p9-ohHsAAAAJ", "5_uwsGAAAAAJ"], "url_scholarbib": "/scholar?q=info:jT9wub72IHoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralized%2BConvolutional%2BForest%2BNetworks%2BFor%2BDomain%2BGeneralization%2BAnd%2BVisual%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jT9wub72IHoJ&ei=GNlXYrbkEZaM6rQPlISayA8&json=", "num_citations": 8, "citedby_url": "/scholar?cites=8800304970897309581&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jT9wub72IHoJ:scholar.google.com/&scioq=Generalized+Convolutional+Forest+Networks+For+Domain+Generalization+And+Visual+Recognition&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1lxVyStPH"}, "Cross-lingual Alignment Vs Joint Training: A Comparative Study And A Simple Unified Framework": {"container_type": "Publication", "bib": {"title": "Cross-lingual alignment vs joint training: A comparative study and a simple unified framework", "author": ["Z Wang", "J Xie", "R Xu", "Y Yang", "G Neubig"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning multilingual representations of text has proven a successful method for many cross-lingual transfer learning tasks. There are two main paradigms for learning such representations:(1) alignment, which maps different independently trained monolingual representations into a shared space, and (2) joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives jointly. In this paper, we first conduct direct comparisons of representations learned using both of these"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.04708", "author_id": ["GgD-B68AAAAJ", "", "HTp5S00AAAAJ", "MlZq4XwAAAAJ", "wlosgkoAAAAJ"], "url_scholarbib": "/scholar?q=info:BYGhH0-gJfcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCross-lingual%2BAlignment%2BVs%2BJoint%2BTraining:%2BA%2BComparative%2BStudy%2BAnd%2BA%2BSimple%2BUnified%2BFramework%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BYGhH0-gJfcJ&ei=GtlXYoSQJMLZmQHc1ovQAg&json=", "num_citations": 38, "citedby_url": "/scholar?cites=17808816563200033029&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:BYGhH0-gJfcJ:scholar.google.com/&scioq=Cross-lingual+Alignment+Vs+Joint+Training:+A+Comparative+Study+And+A+Simple+Unified+Framework&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.04708.pdf?ref=https://codemonkey.link"}, "Pure And Spurious Critical Points: A Geometric Study Of Linear Networks": {"container_type": "Publication", "bib": {"title": "Pure and spurious critical points: a geometric study of linear networks", "author": ["M Trager", "K Kohn", "J Bruna"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.01671", "abstract": "The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network's weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.01671", "author_id": ["g3sXAWkAAAAJ", "jk2kwWIAAAAJ", "L4bNmsMAAAAJ"], "url_scholarbib": "/scholar?q=info:p3669N0Y0SkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPure%2BAnd%2BSpurious%2BCritical%2BPoints:%2BA%2BGeometric%2BStudy%2BOf%2BLinear%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p3669N0Y0SkJ&ei=HdlXYqHILIvEmgH7846QCg&json=", "num_citations": 11, "citedby_url": "/scholar?cites=3013216967260274343&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:p3669N0Y0SkJ:scholar.google.com/&scioq=Pure+And+Spurious+Critical+Points:+A+Geometric+Study+Of+Linear+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.01671"}, "Ae-ot: A New Generative Model Based On Extended Semi-discrete Optimal Transport": {"container_type": "Publication", "bib": {"title": "Ae-ot: a new generative model based on extended semi-discrete optimal transport", "author": ["D An", "Y Guo", "N Lei", "Z Luo", "ST Yau", "X Gu"], "pub_year": "2019", "venue": "ICLR 2020", "abstract": "Generative adversarial networks (GANs) have attracted huge attention due to its capability to generate visual realistic images. However, most of the existing models suffer from the mode collapse or mode mixture problems. In this work, we give a theoretic explanation of the both problems by Figalli's regularity theory of optimal transportation maps. Basically, the generator compute the transportation maps between the white noise distributions and the data distributions, which are in general discontinuous. However, DNNs can only represent"}, "filled": false, "gsrank": 1, "pub_url": "https://par.nsf.gov/biblio/10185286", "author_id": ["v1rlw5QAAAAJ", "s9KR1ukAAAAJ", "mcbRXD8AAAAJ", "", "", "Y063_CIAAAAJ"], "url_scholarbib": "/scholar?q=info:8Jij_DOn4xYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAe-ot:%2BA%2BNew%2BGenerative%2BModel%2BBased%2BOn%2BExtended%2BSemi-discrete%2BOptimal%2BTransport%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8Jij_DOn4xYJ&ei=IdlXYqj_Bc2Ny9YPqPyUgAs&json=", "num_citations": 11, "citedby_url": "/scholar?cites=1649345730271484144&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:8Jij_DOn4xYJ:scholar.google.com/&scioq=Ae-ot:+A+New+Generative+Model+Based+On+Extended+Semi-discrete+Optimal+Transport&hl=en&as_sdt=0,33", "eprint_url": "https://par.nsf.gov/servlets/purl/10185286"}, "What Graph Neural Networks Cannot Learn: Depth Vs Width": {"container_type": "Publication", "bib": {"title": "What graph neural networks cannot learn: depth vs width", "author": ["A Loukas"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.03199", "abstract": "This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.03199", "author_id": ["-XGXJbQAAAAJ"], "url_scholarbib": "/scholar?q=info:e2Q4HnDQOukJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhat%2BGraph%2BNeural%2BNetworks%2BCannot%2BLearn:%2BDepth%2BVs%2BWidth%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e2Q4HnDQOukJ&ei=LdlXYp78JOHDywTjooCQBQ&json=", "num_citations": 117, "citedby_url": "/scholar?cites=16805974139448353915&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:e2Q4HnDQOukJ:scholar.google.com/&scioq=What+Graph+Neural+Networks+Cannot+Learn:+Depth+Vs+Width&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.03199"}, "Weakly Supervised Clustering By Exploiting Unique Class Count": {"container_type": "Publication", "bib": {"title": "Weakly supervised clustering by exploiting unique class count", "author": ["MU Oner", "HK Lee", "WK Sung"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.07647", "abstract": "A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count ($ ucc $), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect $ ucc $ classifier, perfect clustering of individual instances inside the bags is"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.07647", "author_id": ["ELQAk70AAAAJ", "", "KaCbE9MAAAAJ"], "url_scholarbib": "/scholar?q=info:CybUgBD9RfYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWeakly%2BSupervised%2BClustering%2BBy%2BExploiting%2BUnique%2BClass%2BCount%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CybUgBD9RfYJ&ei=NtlXYpWZLIvMsQK69Y7ABg&json=", "num_citations": 3, "citedby_url": "/scholar?cites=17745868154045998603&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:CybUgBD9RfYJ:scholar.google.com/&scioq=Weakly+Supervised+Clustering+By+Exploiting+Unique+Class+Count&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.07647"}, "Exploratory Not Explanatory: Counterfactual Analysis Of Saliency Maps For Deep Rl": {"container_type": "Publication", "bib": {"title": "Exploratory not explanatory: Counterfactual analysis of saliency maps for deep reinforcement learning", "author": ["A Atrey", "K Clary", "D Jensen"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.05743", "abstract": "Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.05743", "author_id": ["Jf_jES4AAAAJ", "EoTY9xAAAAAJ", "cjkgjggAAAAJ"], "url_scholarbib": "/scholar?q=info:24IsYUyX-mAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploratory%2BNot%2BExplanatory:%2BCounterfactual%2BAnalysis%2BOf%2BSaliency%2BMaps%2BFor%2BDeep%2BRl%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=24IsYUyX-mAJ&ei=OtlXYtLDFIvEmgH7846QCg&json=", "num_citations": 49, "citedby_url": "/scholar?cites=6988064126122361563&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:24IsYUyX-mAJ:scholar.google.com/&scioq=Exploratory+Not+Explanatory:+Counterfactual+Analysis+Of+Saliency+Maps+For+Deep+Rl&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.05743"}, "Sharing Knowledge In Multi-task Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Sharing knowledge in multi-task deep reinforcement learning", "author": ["C D'Eramo", "D Tateo", "A Bonarini", "M Restelli"], "pub_year": "2019", "venue": "International \u2026", "abstract": "We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkgpv2VFvr", "author_id": ["1Rt_86gAAAAJ", "LGnu3SEAAAAJ", "WgxMfwMAAAAJ", "xdgxRiEAAAAJ"], "url_scholarbib": "/scholar?q=info:xrtXM046ub0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSharing%2BKnowledge%2BIn%2BMulti-task%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xrtXM046ub0J&ei=PdlXYt_WCo2EmgH6u5u4BA&json=", "num_citations": 47, "citedby_url": "/scholar?cites=13671022251403099078&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:xrtXM046ub0J:scholar.google.com/&scioq=Sharing+Knowledge+In+Multi-task+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkgpv2VFvr"}, "Reinforcement Learning With Competitive Ensembles Of Information-constrained Primitives": {"container_type": "Publication", "bib": {"title": "Reinforcement learning with competitive ensembles of information-constrained primitives", "author": ["A Goyal", "S Sodhani", "J Binas", "XB Peng", "S Levine"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.10667", "author_id": ["krrh6OUAAAAJ", "ixp-vqMAAAAJ", "oD1W8a4AAAAJ", "FwxfQosAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:kvCHkIGuyckJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReinforcement%2BLearning%2BWith%2BCompetitive%2BEnsembles%2BOf%2BInformation-constrained%2BPrimitives%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kvCHkIGuyckJ&ei=QNlXYrv6Mo2EmgH6u5u4BA&json=", "num_citations": 19, "citedby_url": "/scholar?cites=14540344743441199250&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kvCHkIGuyckJ:scholar.google.com/&scioq=Reinforcement+Learning+With+Competitive+Ensembles+Of+Information-constrained+Primitives&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.10667"}, "Rtfm: Generalising To New Environment Dynamics Via Reading": {"container_type": "Publication", "bib": {"title": "Rtfm: Generalising to new environment dynamics via reading", "author": ["V Zhong", "T Rockt\u00e4schel", "E Grefenstette"], "pub_year": "2020", "venue": "ICLR", "abstract": "Obtaining policies that can generalise to new environments in reinforcement learning is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new environments. We propose a grounded policy learning problem, Read to Fight Monsters (RTFM), in which the agent must jointly reason over a language goal, relevant dynamics described in a document, and environment observations. We procedurally generate environment dynamics and"}, "filled": false, "gsrank": 1, "pub_url": "https://discovery.ucl.ac.uk/id/eprint/10101221/", "author_id": ["lT3YoNkAAAAJ", "mWBY8aIAAAAJ", "ezllEwMAAAAJ"], "url_scholarbib": "/scholar?q=info:fSfkFj3Vo9sJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRtfm:%2BGeneralising%2BTo%2BNew%2BEnvironment%2BDynamics%2BVia%2BReading%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fSfkFj3Vo9sJ&ei=RNlXYseQOsWemAHB5baIBQ&json=", "num_citations": 22, "citedby_url": "/scholar?cites=15826727973863827325&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:fSfkFj3Vo9sJ:scholar.google.com/&scioq=Rtfm:+Generalising+To+New+Environment+Dynamics+Via+Reading&hl=en&as_sdt=0,33", "eprint_url": "https://discovery.ucl.ac.uk/id/eprint/10101221/1/rtfm_generalising_to_new_environment_dynamics_via_reading.pdf"}, "Strategies For Pre-training Graph Neural Networks": {"container_type": "Publication", "bib": {"title": "Strategies for pre-training graph neural networks", "author": ["W Hu", "B Liu", "J Gomes", "M Zitnik", "P Liang"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.12265", "author_id": ["wAFMjfkAAAAJ", "l_IWUOAAAAAJ", "SE0_PNEAAAAJ", "YtUDgPIAAAAJ", "pouyVyUAAAAJ"], "url_scholarbib": "/scholar?q=info:MdLCe968tHgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStrategies%2BFor%2BPre-training%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MdLCe968tHgJ&ei=SNlXYscWi8SaAfvzjpAK&json=", "num_citations": 329, "citedby_url": "/scholar?cites=8697784444104397361&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MdLCe968tHgJ:scholar.google.com/&scioq=Strategies+For+Pre-training+Graph+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.12265"}, "Additive Powers-of-two Quantization: A Non-uniform Discretization For Neural Networks": {"container_type": "Publication", "bib": {"title": "Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks", "author": ["Y Li", "X Dong", "W Wang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.13144", "abstract": "quantization levels for a tiny range around the mean. To this end, we propose additive  Powers-of-Two (APoT) quantization  We introduce the APoT quantization scheme for the weights"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.13144", "author_id": ["3UzXL-AAAAAJ", "O8nBN64AAAAJ", "46Dd4v4AAAAJ"], "url_scholarbib": "/scholar?q=info:ODaZNgJIvNoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdditive%2BPowers-of-two%2BQuantization:%2BA%2BNon-uniform%2BDiscretization%2BFor%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ODaZNgJIvNoJ&ei=StlXYvLHLZWMy9YPt8OamA0&json=", "num_citations": 96, "citedby_url": "/scholar?cites=15761551970233038392&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ODaZNgJIvNoJ:scholar.google.com/&scioq=Additive+Powers-of-two+Quantization:+A+Non-uniform+Discretization+For+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.13144.pdf?ref=https://githubhelp.com"}, "Neural Outlier Rejection For Self-supervised Keypoint Learning": {"container_type": "Publication", "bib": {"title": "Neural outlier rejection for self-supervised keypoint learning", "author": ["J Tang", "H Kim", "V Guizilini", "S Pillai", "R Ambrus"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks. However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (ie InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10615", "author_id": ["rRuqIQQAAAAJ", "bJdq_tIAAAAJ", "UH9tP6QAAAAJ", "FuYln-oAAAAJ", "2xjjS3oAAAAJ"], "url_scholarbib": "/scholar?q=info:jfUsGczvbgsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BOutlier%2BRejection%2BFor%2BSelf-supervised%2BKeypoint%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jfUsGczvbgsJ&ei=T9lXYuiGJc2Ny9YPqPyUgAs&json=", "num_citations": 6, "citedby_url": "/scholar?cites=823859441730123149&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:jfUsGczvbgsJ:scholar.google.com/&scioq=Neural+Outlier+Rejection+For+Self-supervised+Keypoint+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10615"}, "Improved Sample Complexities For Deep Neural Networks And Robust Classification Via An All-layer Margin": {"container_type": "Publication", "bib": {"title": "Improved sample complexities for deep neural networks and robust classification via an all-layer margin", "author": ["C Wei", "T Ma"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound\u2013a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=HJe_yR4Fwr", "author_id": ["9OO7nhUAAAAJ", "i38QlUwAAAAJ"], "url_scholarbib": "/scholar?q=info:HdsIQ2Bgz8EJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproved%2BSample%2BComplexities%2BFor%2BDeep%2BNeural%2BNetworks%2BAnd%2BRobust%2BClassification%2BVia%2BAn%2BAll-layer%2BMargin%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HdsIQ2Bgz8EJ&ei=UtlXYtymKJmM6rQPjaOSEA&json=", "num_citations": 18, "citedby_url": "/scholar?cites=13965486936056978205&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:HdsIQ2Bgz8EJ:scholar.google.com/&scioq=Improved+Sample+Complexities+For+Deep+Neural+Networks+And+Robust+Classification+Via+An+All-layer+Margin&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=HJe_yR4Fwr"}, "R\u00e9nyi Fair Inference": {"container_type": "Publication", "bib": {"title": "R\\'enyi Fair Inference", "author": ["S Baharlouei", "M Nouiehed", "A Beirami"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "In this paper, we use R\u00e9nyi correlation as a measure of fairness of  to fair classification and  fair clustering problems. To demonstrate the performance of the proposed R\u00e9nyi fair inference"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.12005", "author_id": ["a-yq6EAAAAAJ", "", "VuKWbMMAAAAJ"], "url_scholarbib": "/scholar?q=info:MB5Yf_HUnXkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DR%25C3%25A9nyi%2BFair%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MB5Yf_HUnXkJ&ei=VdlXYrfcFY2EmgH6u5u4BA&json=", "num_citations": 33, "citedby_url": "/scholar?cites=8763394583621541424&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:MB5Yf_HUnXkJ:scholar.google.com/&scioq=R%C3%A9nyi+Fair+Inference&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.12005"}, "Enhancing Adversarial Defense By K-winners-take-all": {"container_type": "Publication", "bib": {"title": "Enhancing adversarial defense by k-winners-take-all", "author": ["C Xiao", "P Zhong", "C Zheng"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.10510", "abstract": "We propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), we advocate the use of k-Winners-Take-All (k-WTA) activation, a C0 discontinuous function that purposely invalidates the neural network model's gradient at densely distributed input data points. The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead. Our proposal"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10510", "author_id": ["QghjQNYAAAAJ", "l4ffN_QAAAAJ", "-0rEuLgAAAAJ"], "url_scholarbib": "/scholar?q=info:txuqc6O6XKUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnhancing%2BAdversarial%2BDefense%2BBy%2BK-winners-take-all%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=txuqc6O6XKUJ&ei=V9lXYoHPOovMsQK69Y7ABg&json=", "num_citations": 46, "citedby_url": "/scholar?cites=11915603925298453431&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:txuqc6O6XKUJ:scholar.google.com/&scioq=Enhancing+Adversarial+Defense+By+K-winners-take-all&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10510"}, "Sub-policy Adaptation For Hierarchical Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Sub-policy adaptation for hierarchical reinforcement learning", "author": ["AC Li", "C Florensa", "I Clavera", "P Abbeel"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.05862", "abstract": "Hierarchical reinforcement learning is a promising approach to tackle long-horizon decision-making problems with sparse rewards. Unfortunately, most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task. Leaving the skills fixed can lead to significant sub-optimality in the transfer setting. In this work, we propose a novel algorithm to discover a set of skills, and continuously adapt them along with the higher level even when training on a new task. Our main contributions"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.05862", "author_id": ["bOitqMUAAAAJ", "7_7op_IAAAAJ", "yABlzrsAAAAJ", "vtwH6GkAAAAJ"], "url_scholarbib": "/scholar?q=info:VJblSozHuYcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSub-policy%2BAdaptation%2BFor%2BHierarchical%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VJblSozHuYcJ&ei=Y9lXYuvUMZaM6rQPlISayA8&json=", "num_citations": 40, "citedby_url": "/scholar?cites=9780067471177651796&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:VJblSozHuYcJ:scholar.google.com/&scioq=Sub-policy+Adaptation+For+Hierarchical+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.05862"}, "Adversarial Lipschitz Regularization": {"container_type": "Publication", "bib": {"title": "Adversarial lipschitz regularization", "author": ["D Terj\u00e9k"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.05681", "abstract": "with a regularization term penalizing the violation of the Lipschitz  Inspired by Virtual Adversarial  Training, we propose a  parallel between Lipschitz regularization and adversarial training"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.05681", "author_id": ["5EObP3EAAAAJ"], "url_scholarbib": "/scholar?q=info:GsEYuAXD5k0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BLipschitz%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GsEYuAXD5k0J&ei=ZtlXYpnHI8WemAHB5baIBQ&json=", "num_citations": 21, "citedby_url": "/scholar?cites=5613388414894784794&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:GsEYuAXD5k0J:scholar.google.com/&scioq=Adversarial+Lipschitz+Regularization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.05681"}, "U-gat-it: Unsupervised Generative Attentional Networks With Adaptive Layer-instance Normalization For Image-to-image Translation": {"container_type": "Publication", "bib": {"title": "U-gat-it: Unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation", "author": ["J Kim", "M Kim", "H Kang", "K Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1907.10830", "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1907.10830", "author_id": ["WtjDugkAAAAJ", "", "", "i79faMoAAAAJ"], "url_scholarbib": "/scholar?q=info:6LXMAU7wM5AJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DU-gat-it:%2BUnsupervised%2BGenerative%2BAttentional%2BNetworks%2BWith%2BAdaptive%2BLayer-instance%2BNormalization%2BFor%2BImage-to-image%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6LXMAU7wM5AJ&ei=adlXYqu8OcLZmQHc1ovQAg&json=", "num_citations": 227, "citedby_url": "/scholar?cites=10390912983102174696&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6LXMAU7wM5AJ:scholar.google.com/&scioq=U-gat-it:+Unsupervised+Generative+Attentional+Networks+With+Adaptive+Layer-instance+Normalization+For+Image-to-image+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1907.10830"}, "Editable Neural Networks": {"container_type": "Publication", "bib": {"title": "Editable neural networks", "author": ["A Sinitsin", "V Plokhotnyuk", "D Pyrkin", "S Popov"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "that we call Editable Training. This approach involves training neural networks in such a  way that the trained parameters can be easily edited afterwards. Editable Training employs"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2004.00345", "author_id": ["VCM7XscAAAAJ", "", "", ""], "url_scholarbib": "/scholar?q=info:dhKWs8jv5oYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEditable%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dhKWs8jv5oYJ&ei=bdlXYuPTCZaM6rQPlISayA8&json=", "num_citations": 12, "citedby_url": "/scholar?cites=9720720491011248758&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:dhKWs8jv5oYJ:scholar.google.com/&scioq=Editable+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2004.00345"}, "Meta Reinforcement Learning With Autonomous Inference Of Subtask Dependencies": {"container_type": "Publication", "bib": {"title": "Meta reinforcement learning with autonomous inference of subtask dependencies", "author": ["S Sohn", "H Woo", "J Choi", "H Lee"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.00248", "abstract": "We propose and address a novel few-shot RL problem, where a task is characterized by a subtask graph which describes a set of subtasks and their dependencies that are unknown to the agent. The agent needs to quickly adapt to the task over few episodes during adaptation phase to maximize the return in the test phase. Instead of directly learning a meta-policy, we develop a Meta-learner with Subtask Graph Inference (MSGI), which infers the latent parameter of the task by interacting with the environment and maximizes the return"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.00248", "author_id": ["3Q8LdcYAAAAJ", "3NsVg4IAAAAJ", "UX-H08cAAAAJ", "fmSHtE8AAAAJ"], "url_scholarbib": "/scholar?q=info:Fm74T8oQNdcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta%2BReinforcement%2BLearning%2BWith%2BAutonomous%2BInference%2BOf%2BSubtask%2BDependencies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Fm74T8oQNdcJ&ei=b9lXYoXcOo2EmgH6u5u4BA&json=", "num_citations": 18, "citedby_url": "/scholar?cites=15507319353031290390&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Fm74T8oQNdcJ:scholar.google.com/&scioq=Meta+Reinforcement+Learning+With+Autonomous+Inference+Of+Subtask+Dependencies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.00248"}, "Assemblenet: Searching For Multi-stream Neural Connectivity In Video Architectures": {"container_type": "Publication", "bib": {"title": "Assemblenet: Searching for multi-stream neural connectivity in video architectures", "author": ["MS Ryoo", "AJ Piergiovanni", "M Tan"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.13209", "author_id": ["vcw0TJIAAAAJ", "Nt4lak0AAAAJ", "6POeyBoAAAAJ"], "url_scholarbib": "/scholar?q=info:Dotiqi0EvDgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAssemblenet:%2BSearching%2BFor%2BMulti-stream%2BNeural%2BConnectivity%2BIn%2BVideo%2BArchitectures%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Dotiqi0EvDgJ&ei=c9lXYsbYGYvMsQK69Y7ABg&json=", "num_citations": 76, "citedby_url": "/scholar?cites=4088147155924192014&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Dotiqi0EvDgJ:scholar.google.com/&scioq=Assemblenet:+Searching+For+Multi-stream+Neural+Connectivity+In+Video+Architectures&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.13209"}, "Jacobian Adversarially Regularized Networks For Robustness": {"container_type": "Publication", "bib": {"title": "Jacobian adversarially regularized networks for robustness", "author": ["A Chan", "Y Tay", "YS Ong", "J Fu"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.10185", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10185", "author_id": ["ald869IAAAAJ", "VBclY_cAAAAJ", "h9oWOsEAAAAJ", "66osleIAAAAJ"], "url_scholarbib": "/scholar?q=info:WIWe1QFHInMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DJacobian%2BAdversarially%2BRegularized%2BNetworks%2BFor%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WIWe1QFHInMJ&ei=dtlXYpKGEM2Ny9YPqPyUgAs&json=", "num_citations": 44, "citedby_url": "/scholar?cites=8296271536774350168&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:WIWe1QFHInMJ:scholar.google.com/&scioq=Jacobian+Adversarially+Regularized+Networks+For+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10185"}, "Multi-agent Interactions Modeling With Correlated Policies": {"container_type": "Publication", "bib": {"title": "Multi-agent interactions modeling with correlated policies", "author": ["M Liu", "M Zhou", "W Zhang", "Y Zhuang", "J Wang"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "In multi-agent systems, complex interacting behaviors arise due to the high correlations among agents. However, previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures. In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents' policies, which can recover agents' policies that can"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.03415", "author_id": ["O_4qYW4AAAAJ", "xuW4NIYAAAAJ", "Qzss0GEAAAAJ", "ny9KAREAAAAJ", ""], "url_scholarbib": "/scholar?q=info:v8pFjgp1shcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-agent%2BInteractions%2BModeling%2BWith%2BCorrelated%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v8pFjgp1shcJ&ei=edlXYuSpIJGJmwGIxre4DA&json=", "num_citations": 7, "citedby_url": "/scholar?cites=1707555896923900607&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:v8pFjgp1shcJ:scholar.google.com/&scioq=Multi-agent+Interactions+Modeling+With+Correlated+Policies&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.03415"}, "Prediction Poisoning: Towards Defenses Against Dnn Model Stealing Attacks": {"container_type": "Publication", "bib": {"title": "Prediction poisoning: Towards defenses against dnn model stealing attacks", "author": ["T Orekondy", "B Schiele", "M Fritz"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.10908", "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications eg, cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (ie, inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.10908", "author_id": ["sbsFpScAAAAJ", "z76PBfYAAAAJ", "4V1nNm4AAAAJ"], "url_scholarbib": "/scholar?q=info:R1o2Xw60a4wJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPrediction%2BPoisoning:%2BTowards%2BDefenses%2BAgainst%2BDnn%2BModel%2BStealing%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=R1o2Xw60a4wJ&ei=fNlXYsGrEciBy9YP18Gi8As&json=", "num_citations": 59, "citedby_url": "/scholar?cites=10118378961637890631&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:R1o2Xw60a4wJ:scholar.google.com/&scioq=Prediction+Poisoning:+Towards+Defenses+Against+Dnn+Model+Stealing+Attacks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.10908"}, "Freelb: Enhanced Adversarial Training For Language Understanding": {"container_type": "Publication", "bib": {"title": "Freelb: Enhanced adversarial training for language understanding", "author": ["C Zhu", "Y Cheng", "Z Gan", "S Sun", "T Goldstein", "J Liu"], "pub_year": "2019", "venue": "NA", "abstract": "Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Fk_47QnlVy", "author_id": ["m-om5O8AAAAJ", "ORPxbV4AAAAJ", "E64XWyMAAAAJ", "2dyg3WgAAAAJ", "KmSuVtgAAAAJ", ""], "url_scholarbib": "/scholar?q=info:QpCRql-GFzkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFreelb:%2BEnhanced%2BAdversarial%2BTraining%2BFor%2BLanguage%2BUnderstanding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QpCRql-GFzkJ&ei=f9lXYrOLC5WMy9YPt8OamA0&json=", "num_citations": 40, "citedby_url": "/scholar?cites=4113904530067918914&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:QpCRql-GFzkJ:scholar.google.com/&scioq=Freelb:+Enhanced+Adversarial+Training+For+Language+Understanding&hl=en&as_sdt=0,33"}, "Deformable Kernels: Adapting Effective Receptive Fields For Object Deformation": {"container_type": "Publication", "bib": {"title": "Deformable kernels: Adapting effective receptive fields for object deformation", "author": ["H Gao", "X Zhu", "S Lin", "J Dai"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.02940", "abstract": "Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02940", "author_id": ["cwXx4EQAAAAJ", "02RXI00AAAAJ", "", "SH_-B_AAAAAJ"], "url_scholarbib": "/scholar?q=info:aFPZheJwbf0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeformable%2BKernels:%2BAdapting%2BEffective%2BReceptive%2BFields%2BFor%2BObject%2BDeformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aFPZheJwbf0J&ei=gtlXYsm3NZGJmwGIxre4DA&json=", "num_citations": 31, "citedby_url": "/scholar?cites=18261376182267761512&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:aFPZheJwbf0J:scholar.google.com/&scioq=Deformable+Kernels:+Adapting+Effective+Receptive+Fields+For+Object+Deformation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02940"}, "Incorporating Bert Into Neural Machine Translation": {"container_type": "Publication", "bib": {"title": "Incorporating bert into neural machine translation", "author": ["J Zhu", "Y Xia", "L Wu", "D He", "T Qin", "W Zhou", "H Li"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The recently proposed BERT has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.06823", "author_id": ["FvGy0LQAAAAJ", "GS5wRxYAAAAJ", "RD5kSG0AAAAJ", "orVoz4IAAAAJ", "Bl4SRU0AAAAJ", "8s1JF8YAAAAJ", "7sFMIKoAAAAJ"], "url_scholarbib": "/scholar?q=info:KiBSpVIfOCcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIncorporating%2BBert%2BInto%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KiBSpVIfOCcJ&ei=hdlXYu_7G5WMy9YPt8OamA0&json=", "num_citations": 208, "citedby_url": "/scholar?cites=2826043205996388394&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:KiBSpVIfOCcJ:scholar.google.com/&scioq=Incorporating+Bert+Into+Neural+Machine+Translation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.06823"}, "Distance-based Learning From Errors For Confidence Calibration": {"container_type": "Publication", "bib": {"title": "Distance-based learning from errors for confidence calibration", "author": ["C Xing", "S Arik", "Z Zhang", "T Pfister"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.01730", "abstract": "Deep neural networks (DNNs) are poorly calibrated when trained in conventional ways. To improve confidence calibration of DNNs, we propose a novel training method, distance-based learning from errors (DBLE). DBLE bases its confidence estimation on distances in the representation space. In DBLE, we first adapt prototypical learning to train classification models. It yields a representation space where the distance between a test sample and its ground truth class center can calibrate the model's classification performance. At inference"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.01730", "author_id": ["tAUdLM0AAAAJ", "-EZBCBAAAAAJ", "lGrbH60AAAAJ", "ahSpJOAAAAAJ"], "url_scholarbib": "/scholar?q=info:9vjRAO0o_W4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistance-based%2BLearning%2BFrom%2BErrors%2BFor%2BConfidence%2BCalibration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9vjRAO0o_W4J&ei=iNlXYvrjHs2Ny9YPqPyUgAs&json=", "num_citations": 18, "citedby_url": "/scholar?cites=7997593511665989878&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:9vjRAO0o_W4J:scholar.google.com/&scioq=Distance-based+Learning+From+Errors+For+Confidence+Calibration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.01730"}, "Dividemix: Learning With Noisy Labels As Semi-supervised Learning": {"container_type": "Publication", "bib": {"title": "Dividemix: Learning with noisy labels as semi-supervised learning", "author": ["J Li", "R Socher", "SCH Hoi"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2002.07394", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07394", "author_id": ["MuUhwi0AAAAJ", "FaOcyfMAAAAJ", "JoLjflYAAAAJ"], "url_scholarbib": "/scholar?q=info:u7DNtr23FqYJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDividemix:%2BLearning%2BWith%2BNoisy%2BLabels%2BAs%2BSemi-supervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=u7DNtr23FqYJ&ei=i9lXYpCXOIySyATlkbrQCA&json=", "num_citations": 257, "citedby_url": "/scholar?cites=11967955085227307195&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:u7DNtr23FqYJ:scholar.google.com/&scioq=Dividemix:+Learning+With+Noisy+Labels+As+Semi-supervised+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07394.pdf%C3%AF%C2%BC%E2%80%B0%C3%A3%E2%82%AC%E2%80%9A"}, "Reanalysis Of Variance Reduced Temporal Difference Learning": {"container_type": "Publication", "bib": {"title": "Reanalysis of variance reduced temporal difference learning", "author": ["T Xu", "Z Wang", "Y Zhou", "Y Liang"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.01898", "abstract": "Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning, but the vanilla TD can substantially suffer from the inherent optimization variance. A variance reduced TD (VRTD) algorithm was proposed by Korda and La (2015), which applies the variance reduction technique directly to the online TD learning with Markovian samples. In this work, we first point out the technical errors in the analysis of VRTD in Korda and La (2015), and then provide a mathematically solid analysis"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.01898", "author_id": ["g0Rc2skAAAAJ", "DkcabN8AAAAJ", "4fK8bYIAAAAJ", "lGgLAiIAAAAJ"], "url_scholarbib": "/scholar?q=info:_165sKSh3zMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReanalysis%2BOf%2BVariance%2BReduced%2BTemporal%2BDifference%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_165sKSh3zMJ&ei=kNlXYtSuA5WMy9YPt8OamA0&json=", "num_citations": 30, "citedby_url": "/scholar?cites=3737883944452447999&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_165sKSh3zMJ:scholar.google.com/&scioq=Reanalysis+Of+Variance+Reduced+Temporal+Difference+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.01898"}, "Population-guided Parallel Policy Search For Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Population-guided parallel policy search for reinforcement learning", "author": ["W Jung", "G Park", "Y Sung"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.02907", "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02907", "author_id": ["72La2OEAAAAJ", "", "-9D2k3UAAAAJ"], "url_scholarbib": "/scholar?q=info:USLoJ8YL07UJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPopulation-guided%2BParallel%2BPolicy%2BSearch%2BFor%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=USLoJ8YL07UJ&ei=m9lXYp2TDZmM6rQPjaOSEA&json=", "num_citations": 17, "citedby_url": "/scholar?cites=13101828686651859537&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:USLoJ8YL07UJ:scholar.google.com/&scioq=Population-guided+Parallel+Policy+Search+For+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02907"}, "Decoupling Representation And Classifier For Long-tailed Recognition": {"container_type": "Publication", "bib": {"title": "Decoupling representation and classifier for long-tailed recognition", "author": ["B Kang", "S Xie", "M Rohrbach", "Z Yan", "A Gordo"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, eg, by loss re-weighting, data re-sampling, or transfer learning from head-to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.09217", "author_id": ["NmHgX-wAAAAJ", "Y2GtJkAAAAAJ", "3kDtybgAAAAJ", "JFEHAwIAAAAJ", "UgK1my4AAAAJ"], "url_scholarbib": "/scholar?q=info:Vl4thwH2Bx8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecoupling%2BRepresentation%2BAnd%2BClassifier%2BFor%2BLong-tailed%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Vl4thwH2Bx8J&ei=ntlXYu2fD4ySyATlkbrQCA&json=", "num_citations": 328, "citedby_url": "/scholar?cites=2236026226436038230&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Vl4thwH2Bx8J:scholar.google.com/&scioq=Decoupling+Representation+And+Classifier+For+Long-tailed+Recognition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.09217.pdf?ref=https://githubhelp.com"}, "Projection Based Constrained Policy Optimization": {"container_type": "Publication", "bib": {"title": "Projection-based constrained policy optimization", "author": ["TY Yang", "J Rosca", "K Narasimhan"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "We consider the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm, Projection-Based Constrained Policy Optimization (PCPO). This is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. We theoretically analyze PCPO and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2010.03152", "author_id": ["g-hQdY8AAAAJ", "feURfe4AAAAJ", "euc0GX4AAAAJ"], "url_scholarbib": "/scholar?q=info:m-3JxptFs44J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProjection%2BBased%2BConstrained%2BPolicy%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m-3JxptFs44J&ei=otlXYpETxZ6YAcHltogF&json=", "num_citations": 61, "citedby_url": "/scholar?cites=10282638909574344091&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:m-3JxptFs44J:scholar.google.com/&scioq=Projection+Based+Constrained+Policy+Optimization&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2010.03152"}, "Nas Evaluation Is Frustratingly Hard": {"container_type": "Publication", "bib": {"title": "Nas evaluation is frustratingly hard", "author": ["A Yang", "PM Esperan\u00e7a", "FM Carlucci"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.12522", "abstract": "Our first contribution is a benchmark of 8 NAS methods on 5  Surprisingly, we find that  many NAS techniques struggle to  of each component in the NAS pipeline. These experiments"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.12522", "author_id": ["SlMKN6IAAAAJ", "ralB4sUAAAAJ", "wJsmNzsAAAAJ"], "url_scholarbib": "/scholar?q=info:plRDKA5cFK0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNas%2BEvaluation%2BIs%2BFrustratingly%2BHard%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=plRDKA5cFK0J&ei=ptlXYoJXzY3L1g-o_JSACw&json=", "num_citations": 119, "citedby_url": "/scholar?cites=12471694483970544806&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:plRDKA5cFK0J:scholar.google.com/&scioq=Nas+Evaluation+Is+Frustratingly+Hard&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.12522"}, "Backpack: Packing More Into Backprop": {"container_type": "Publication", "bib": {"title": "Backpack: Packing more into backprop", "author": ["F Dangel", "F Kunstner", "P Hennig"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.10985", "abstract": "Automatic differentiation frameworks are optimized for exactly one thing: computing the average mini-batch gradient. Yet, other quantities such as the variance of the mini-batch gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same time as the gradient. While these quantities are of great interest to researchers and practitioners, current deep-learning software does not support their automatic calculation. Manually implementing them is burdensome, inefficient if done naively, and the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.10985", "author_id": ["9hlJ9W0AAAAJ", "EhpYjPAAAAAJ", "UeG5w08AAAAJ"], "url_scholarbib": "/scholar?q=info:FJ2DEgScw4MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBackpack:%2BPacking%2BMore%2BInto%2BBackprop%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FJ2DEgScw4MJ&ei=qdlXYs6xJJLeyQTms5KQBg&json=", "num_citations": 45, "citedby_url": "/scholar?cites=9494603980731555092&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:FJ2DEgScw4MJ:scholar.google.com/&scioq=Backpack:+Packing+More+Into+Backprop&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.10985"}, "Principled Weight Initialization For Hypernetworks": {"container_type": "Publication", "bib": {"title": "Principled weight initialization for hypernetworks", "author": ["O Chang", "L Flokas", "H Lipson"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "Hypernetworks are meta neural networks that generate weights for a main neural network in an end-to-end differentiable manner. Despite extensive applications ranging from multi-task learning to Bayesian deep learning, the problem of optimizing hypernetworks has not been studied to date. We observe that classical weight initialization methods like Glorot & Bengio (2010) and He et al.(2015), when applied directly on a hypernet, fail to produce weights for the mainnet in the correct scale. We develop principled techniques for weight initialization in"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=H1lma24tPB&noteId;=SttNyKHM11", "author_id": ["eEVU18YAAAAJ", "YCV6U3sAAAAJ", "F_Go4V4AAAAJ"], "url_scholarbib": "/scholar?q=info:wrL-tBh7byIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPrincipled%2BWeight%2BInitialization%2BFor%2BHypernetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wrL-tBh7byIJ&ei=rtlXYsbSK4ySyATlkbrQCA&json=", "num_citations": 26, "citedby_url": "/scholar?cites=2481337265750454978&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wrL-tBh7byIJ:scholar.google.com/&scioq=Principled+Weight+Initialization+For+Hypernetworks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=H1lma24tPB"}, "Lamol: Language Modeling For Lifelong Language Learning": {"container_type": "Publication", "bib": {"title": "Lamol: Language modeling for lifelong language learning", "author": ["FK Sun", "CH Ho", "HY Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.03329", "abstract": "Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling. LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity. Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples. When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.03329", "author_id": ["sfEwE4gAAAAJ", "", "DxLO11IAAAAJ"], "url_scholarbib": "/scholar?q=info:kT0YED6vW-QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLamol:%2BLanguage%2BModeling%2BFor%2BLifelong%2BLanguage%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kT0YED6vW-QJ&ei=stlXYvShHsS4ywTtzb_QDA&json=", "num_citations": 48, "citedby_url": "/scholar?cites=16454938344621096337&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:kT0YED6vW-QJ:scholar.google.com/&scioq=Lamol:+Language+Modeling+For+Lifelong+Language+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.03329"}, "Disentangling Factors Of Variations Using Few Labels": {"container_type": "Publication", "bib": {"title": "Disentangling factors of variation using few labels", "author": ["F Locatello", "M Tschannen", "S Bauer", "G R\u00e4tsch"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al.(2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.01258", "author_id": ["wQanfTIAAAAJ", "TSj_8nYAAAAJ", "O-oICE8AAAAJ", "tQuQ1FwAAAAJ"], "url_scholarbib": "/scholar?q=info:s7n2bhPwzVQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDisentangling%2BFactors%2BOf%2BVariations%2BUsing%2BFew%2BLabels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s7n2bhPwzVQJ&ei=tdlXYuqMO5aM6rQPlISayA8&json=", "num_citations": 87, "citedby_url": "/scholar?cites=6110804235668339123&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:s7n2bhPwzVQJ:scholar.google.com/&scioq=Disentangling+Factors+Of+Variations+Using+Few+Labels&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.01258"}, "Self-adversarial Learning With Comparative Discrimination For Text Generation": {"container_type": "Publication", "bib": {"title": "Self-adversarial learning with comparative discrimination for text generation", "author": ["W Zhou", "T Ge", "K Xu", "F Wei", "M Zhou"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.11691", "abstract": "Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.11691", "author_id": ["UebIjuQAAAAJ", "", "", "G-V1VpwAAAAJ", "a0w5c0gAAAAJ"], "url_scholarbib": "/scholar?q=info:JesUvD6mfacJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-adversarial%2BLearning%2BWith%2BComparative%2BDiscrimination%2BFor%2BText%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JesUvD6mfacJ&ei=xtlXYoKaFM6E6rQPz8uiuAc&json=", "num_citations": 14, "citedby_url": "/scholar?cites=12068985364796468005&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:JesUvD6mfacJ:scholar.google.com/&scioq=Self-adversarial+Learning+With+Comparative+Discrimination+For+Text+Generation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.11691.pdf?ref=https://githubhelp.com"}, "Compressive Transformers For Long-range Sequence Modelling": {"container_type": "Publication", "bib": {"title": "Compressive transformers for long-range sequence modelling", "author": ["JW Rae", "A Potapenko", "SM Jayakumar"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.05507", "author_id": ["s0YmMPcAAAAJ", "AE9oqLwAAAAJ", "rJUAY8QAAAAJ"], "url_scholarbib": "/scholar?q=info:OFeqfwYZ5HQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompressive%2BTransformers%2BFor%2BLong-range%2BSequence%2BModelling%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OFeqfwYZ5HQJ&ei=y9lXYsO3MZGJmwGIxre4DA&json=", "num_citations": 164, "citedby_url": "/scholar?cites=8422884718792038200&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:OFeqfwYZ5HQJ:scholar.google.com/&scioq=Compressive+Transformers+For+Long-range+Sequence+Modelling&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.05507"}, "Towards Verified Robustness Under Text Deletion Interventions": {"container_type": "Publication", "bib": {"title": "Towards Verified Robustness under Text Deletion Interventions", "author": ["J Welbl", "PS Huang", "R Stanforth", "S Gowal"], "pub_year": "2020", "venue": "NA", "abstract": "Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behaviour is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. We develop a novel technique for formal verification of this"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/pdf?id=g-ffEqAyiG4", "author_id": ["SaHRjy4AAAAJ", "4oJB32YAAAAJ", "", "7wclGnQAAAAJ"], "url_scholarbib": "/scholar?q=info:2IbzSIMuk7QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BVerified%2BRobustness%2BUnder%2BText%2BDeletion%2BInterventions%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2IbzSIMuk7QJ&ei=z9lXYrePAciBy9YP18Gi8As&json=", "num_citations": 1, "citedby_url": "/scholar?cites=13011794889803007704&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2IbzSIMuk7QJ:scholar.google.com/&scioq=Towards+Verified+Robustness+Under+Text+Deletion+Interventions&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=g-ffEqAyiG4"}, "Learning Execution Through Neural Code Fusion": {"container_type": "Publication", "bib": {"title": "Learning execution through neural code fusion", "author": ["Z Shi", "K Swersky", "D Tarlow", "P Ranganathan"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "As the performance of computer systems stagnates due to the end of Moore's Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks (GNNs) to learn representations of source code, these representations do not understand how code dynamically executes. In this work, we propose a new approach to use GNNs to learn fused representations of general source code and its execution. Our approach defines a multi-task"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.07181", "author_id": ["w2I-wNQAAAAJ", "IrixA8MAAAAJ", "oavgGaMAAAAJ", "S3gQoMgAAAAJ"], "url_scholarbib": "/scholar?q=info:wX88_vaRWQoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BExecution%2BThrough%2BNeural%2BCode%2BFusion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wX88_vaRWQoJ&ei=1NlXYrSJOsLZmQHc1ovQAg&json=", "num_citations": 18, "citedby_url": "/scholar?cites=745787703319887809&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:wX88_vaRWQoJ:scholar.google.com/&scioq=Learning+Execution+Through+Neural+Code+Fusion&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.07181"}, "Training Recurrent Neural Networks Online By Learning Explicit State Variables": {"container_type": "Publication", "bib": {"title": "Training recurrent neural networks online by learning explicit state variables", "author": ["S Nath", "V Liu", "A Chan", "X Li", "A White"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Recurrent neural networks (RNNs) allow an agent to construct a state-representation from a stream of experience, which is essential in partially observable problems. However, there are two primary issues one must overcome when training an RNN: the sensitivity of the learning algorithm's performance to truncation length and and long training times. There are variety of strategies to improve training in RNNs, the mostly notably Backprop Through Time (BPTT) and by Real-Time Recurrent Learning. These strategies, however, are typically"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=SJgmR0NKPr", "author_id": ["fNjl1AwAAAAJ", "0BWc_FwAAAAJ", "lmQmYPgAAAAJ", "71RErloAAAAJ", "1GqGhcsAAAAJ"], "url_scholarbib": "/scholar?q=info:LgaV-G7K018J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BRecurrent%2BNeural%2BNetworks%2BOnline%2BBy%2BLearning%2BExplicit%2BState%2BVariables%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LgaV-G7K018J&ei=2NlXYoWcNsiBy9YP18Gi8As&json=", "num_citations": 3, "citedby_url": "/scholar?cites=6905085231654831662&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LgaV-G7K018J:scholar.google.com/&scioq=Training+Recurrent+Neural+Networks+Online+By+Learning+Explicit+State+Variables&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=SJgmR0NKPr"}, "At Stability's Edge: How To Adjust Hyperparameters To Preserve Minima Selection In Asynchronous Training Of Neural Networks?": {"container_type": "Publication", "bib": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "author": ["N Giladi", "MS Nacson", "E Hoffer", "D Soudry"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.12340", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.12340", "author_id": ["cDwy5pwAAAAJ", "wrozdTYAAAAJ", "iEfTH7AAAAAJ", "AEBWEm8AAAAJ"], "url_scholarbib": "/scholar?q=info:ziwVueElop4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAt%2BStability%2527s%2BEdge:%2BHow%2BTo%2BAdjust%2BHyperparameters%2BTo%2BPreserve%2BMinima%2BSelection%2BIn%2BAsynchronous%2BTraining%2BOf%2BNeural%2BNetworks%253F%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ziwVueElop4J&ei=3NlXYp3PFpWMy9YPt8OamA0&json=", "num_citations": 8, "citedby_url": "/scholar?cites=11430740455622782158&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:ziwVueElop4J:scholar.google.com/&scioq=At+Stability%27s+Edge:+How+To+Adjust+Hyperparameters+To+Preserve+Minima+Selection+In+Asynchronous+Training+Of+Neural+Networks%3F&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.12340"}, "Estimating Counterfactual Treatment Outcomes Over Time Through Adversarially Balanced Representations": {"container_type": "Publication", "bib": {"title": "Estimating counterfactual treatment outcomes over time through adversarially balanced representations", "author": ["I Bica", "AM Alaa", "J Jordon", "M van der Schaar"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.04083", "author_id": ["mnU3HpcAAAAJ", "_pv1sEcAAAAJ", "jiyonF0AAAAJ", "DZ3S--MAAAAJ"], "url_scholarbib": "/scholar?q=info:Wr_zYrEh_tgJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEstimating%2BCounterfactual%2BTreatment%2BOutcomes%2BOver%2BTime%2BThrough%2BAdversarially%2BBalanced%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Wr_zYrEh_tgJ&ei=39lXYqKkNZLeyQTms5KQBg&json=", "num_citations": 53, "citedby_url": "/scholar?cites=15635972002030010202&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Wr_zYrEh_tgJ:scholar.google.com/&scioq=Estimating+Counterfactual+Treatment+Outcomes+Over+Time+Through+Adversarially+Balanced+Representations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.04083"}, "Adversarial Policies: Attacking Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Adversarial policies: Attacking deep reinforcement learning", "author": ["A Gleave", "M Dennis", "C Wild", "N Kant", "S Levine"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.10615", "author_id": ["lBunDH0AAAAJ", "WXXu26AAAAAJ", "", "eSgXTkkAAAAJ", "8R35rCwAAAAJ"], "url_scholarbib": "/scholar?q=info:6wsbrRMAtRAJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BPolicies:%2BAttacking%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6wsbrRMAtRAJ&ei=6tlXYt2zPM6E6rQPz8uiuAc&json=", "num_citations": 167, "citedby_url": "/scholar?cites=1203868559900085227&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:6wsbrRMAtRAJ:scholar.google.com/&scioq=Adversarial+Policies:+Attacking+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.10615"}, "Relational State-space Model For Stochastic Multi-object Systems": {"container_type": "Publication", "bib": {"title": "Relational State-Space Model for Stochastic Multi-Object Systems", "author": ["F Yang", "L Chen", "F Zhou", "Y Gao", "W Cao"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.04050", "abstract": "Real-world dynamical systems often consist of multiple stochastic subsystems that interact with each other. Modeling and forecasting the behavior of such dynamics are generally not easy, due to the inherent hardness in understanding the complicated interactions and evolutions of their constituents. This paper introduces the relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects. By"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.04050", "author_id": ["Bygi7noAAAAJ", "Vxi9eakAAAAJ", "Ihj2Rw8AAAAJ", "TxUuDcwAAAAJ", "pKLKtLUAAAAJ"], "url_scholarbib": "/scholar?q=info:u0p0_b6Hr_QJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelational%2BState-space%2BModel%2BFor%2BStochastic%2BMulti-object%2BSystems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=u0p0_b6Hr_QJ&ei=7dlXYpjIO4ySyATlkbrQCA&json=", "num_citations": 3, "citedby_url": "/scholar?cites=17631460320544574139&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:u0p0_b6Hr_QJ:scholar.google.com/&scioq=Relational+State-space+Model+For+Stochastic+Multi-object+Systems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.04050"}, "Neural Symbolic Reader: Scalable Integration Of Distributed And Symbolic Representations For Reading Comprehension": {"container_type": "Publication", "bib": {"title": "Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension", "author": ["X Chen", "C Liang", "AW Yu", "D Zhou", "D Song"], "pub_year": "2019", "venue": "International \u2026", "abstract": "Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning. In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, eg, BERT, to encode the passage and question, and a programmer, eg, LSTM, to generate a program that is executed to produce the answer. Compared to previous works, NeRd is"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=ryxjnREFwH", "author_id": ["d4W1UT0AAAAJ", "ILQ8_ekAAAAJ", "-hW6cvgAAAAJ", "UwLsYw8AAAAJ", "84WzBlYAAAAJ"], "url_scholarbib": "/scholar?q=info:-JP4UP0NuiUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BSymbolic%2BReader:%2BScalable%2BIntegration%2BOf%2BDistributed%2BAnd%2BSymbolic%2BRepresentations%2BFor%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-JP4UP0NuiUJ&ei=9dlXYo-TEIvEmgH7846QCg&json=", "num_citations": 38, "citedby_url": "/scholar?cites=2718500706707870712&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-JP4UP0NuiUJ:scholar.google.com/&scioq=Neural+Symbolic+Reader:+Scalable+Integration+Of+Distributed+And+Symbolic+Representations+For+Reading+Comprehension&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=ryxjnREFwH"}, "Inductive Representation Learning On Temporal Graphs": {"container_type": "Publication", "bib": {"title": "Inductive representation learning on temporal graphs", "author": ["D Xu", "C Ruan", "E Korpeoglu", "S Kumar"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07962", "author_id": ["-jl6A84AAAAJ", "T3EUphwAAAAJ", "BFRRLJUAAAAJ", "H960LkUAAAAJ"], "url_scholarbib": "/scholar?q=info:Tme1pEoebl0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInductive%2BRepresentation%2BLearning%2BOn%2BTemporal%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Tme1pEoebl0J&ei=-9lXYqXbLs2Ny9YPqPyUgAs&json=", "num_citations": 104, "citedby_url": "/scholar?cites=6732351798905235278&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Tme1pEoebl0J:scholar.google.com/&scioq=Inductive+Representation+Learning+On+Temporal+Graphs&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07962"}, "One-shot Pruning Of Recurrent Neural Networks By Jacobian Spectrum Evaluation": {"container_type": "Publication", "bib": {"title": "One-shot pruning of recurrent neural networks by jacobian spectrum evaluation", "author": ["MS Zhang", "B Stadie"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.00120", "abstract": "Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.00120", "author_id": ["Y1y2jdoAAAAJ", "lEV5F5kAAAAJ"], "url_scholarbib": "/scholar?q=info:tsrykvbQZV8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOne-shot%2BPruning%2BOf%2BRecurrent%2BNeural%2BNetworks%2BBy%2BJacobian%2BSpectrum%2BEvaluation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tsrykvbQZV8J&ei=ANpXYuGYD5LeyQTms5KQBg&json=", "num_citations": 18, "citedby_url": "/scholar?cites=6874130163696847542&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:tsrykvbQZV8J:scholar.google.com/&scioq=One-shot+Pruning+Of+Recurrent+Neural+Networks+By+Jacobian+Spectrum+Evaluation&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.00120"}, "Training Generative Adversarial Networks From Incomplete Observations Using Factorised Discriminators": {"container_type": "Publication", "bib": {"title": "Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators", "author": ["D Stoller", "S Ewert", "S Dixon"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1905.12660", "abstract": "Generative adversarial networks (GANs) have shown great success in applications such as image generation and inpainting. However, they typically require large datasets, which are often not available, especially in the context of prediction tasks such as image segmentation that require labels. Therefore, methods such as the CycleGAN use more easily available unlabelled data, but do not offer a way to leverage additional labelled data for improved performance. To address this shortcoming, we show how to factorise the joint data"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1905.12660", "author_id": ["Ozxm6UsAAAAJ", "idpI0w0AAAAJ", "9GvyqXEAAAAJ"], "url_scholarbib": "/scholar?q=info:gbYM86NvcnIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BGenerative%2BAdversarial%2BNetworks%2BFrom%2BIncomplete%2BObservations%2BUsing%2BFactorised%2BDiscriminators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gbYM86NvcnIJ&ei=B9pXYrm3AYvMsQK69Y7ABg&json=", "num_citations": 1, "citedby_url": "/scholar?cites=8246776617616848513&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gbYM86NvcnIJ:scholar.google.com/&scioq=Training+Generative+Adversarial+Networks+From+Incomplete+Observations+Using+Factorised+Discriminators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1905.12660"}, "Sampling-free Learning Of Bayesian Quantized Neural Networks": {"container_type": "Publication", "bib": {"title": "Sampling-free learning of bayesian quantized neural networks", "author": ["J Su", "M Cvitkovic", "F Huang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.02992", "abstract": "Bayesian learning of model parameters in neural networks is important in scenarios where estimates with well-calibrated uncertainty are important. In this paper, we propose Bayesian quantized networks (BQNs), quantized neural networks (QNNs) for which we learn a posterior distribution over their discrete parameters. We provide a set of efficient algorithms for learning and prediction in BQNs without the need to sample from their parameters or activations, which not only allows for differentiable learning in QNNs, but also reduces the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.02992", "author_id": ["", "n_Nh2M0AAAAJ", "mRI6AogAAAAJ"], "url_scholarbib": "/scholar?q=info:XsHmm-cMyloJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSampling-free%2BLearning%2BOf%2BBayesian%2BQuantized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XsHmm-cMyloJ&ei=C9pXYuT6HIvMsQK69Y7ABg&json=", "num_citations": 3, "citedby_url": "/scholar?cites=6542055597601636702&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XsHmm-cMyloJ:scholar.google.com/&scioq=Sampling-free+Learning+Of+Bayesian+Quantized+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.02992"}, "Comparing Fine-tuning And Rewinding In Neural Network Pruning": {"container_type": "Publication", "bib": {"title": "Comparing fine-tuning and rewinding in neural network pruning", "author": ["A Renda", "J Frankle", "M Carbin"], "venue": "NA", "pub_year": "NA", "abstract": ""}, "filled": false, "gsrank": 1, "author_id": ["4BCuJ2AAAAAJ", "", ""], "url_scholarbib": "/scholar?q=info:an9k3ZXMMjkJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComparing%2BFine-tuning%2BAnd%2BRewinding%2BIn%2BNeural%2BNetwork%2BPruning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=an9k3ZXMMjkJ&ei=DtpXYrq2M4vMsQK69Y7ABg&json=", "num_citations": 15, "citedby_url": "/scholar?cites=4121581553033969514&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:an9k3ZXMMjkJ:scholar.google.com/&scioq=Comparing+Fine-tuning+And+Rewinding+In+Neural+Network+Pruning&hl=en&as_sdt=0,33"}, "Mogrifier Lstm": {"container_type": "Publication", "bib": {"title": "Mogrifier lstm", "author": ["G Melis", "T Ko\u010disk\u00fd", "P Blunsom"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.01792", "abstract": "Neither did we observe a significant change in the gap between the LSTM and the  Mogrifier in the tied and untied embeddings settings, which would be expected if recovery was"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.01792", "author_id": ["TbLw2lcAAAAJ", "LuLM2EoAAAAJ", "eJwbbXEAAAAJ"], "url_scholarbib": "/scholar?q=info:AfwlqWFsXUcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMogrifier%2BLstm%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AfwlqWFsXUcJ&ei=EtpXYp_9FYvEmgH7846QCg&json=", "num_citations": 53, "citedby_url": "/scholar?cites=5142385516232440833&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AfwlqWFsXUcJ:scholar.google.com/&scioq=Mogrifier+Lstm&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.01792"}, "Sign-opt: A Query-efficient Hard-label Adversarial Attack": {"container_type": "Publication", "bib": {"title": "Sign-opt: A query-efficient hard-label adversarial attack", "author": ["M Cheng", "S Singh", "P Chen", "PY Chen", "S Liu"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access: the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (> 20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.10773", "author_id": ["_LkC1yoAAAAJ", "EF9yr9UAAAAJ", "", "jxwlCUUAAAAJ", "C7dO_UgAAAAJ"], "url_scholarbib": "/scholar?q=info:cW0A3C-MMDwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSign-opt:%2BA%2BQuery-efficient%2BHard-label%2BAdversarial%2BAttack%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cW0A3C-MMDwJ&ei=FtpXYvCPHcWemAHB5baIBQ&json=", "num_citations": 83, "citedby_url": "/scholar?cites=4337120578340154737&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:cW0A3C-MMDwJ:scholar.google.com/&scioq=Sign-opt:+A+Query-efficient+Hard-label+Adversarial+Attack&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.10773"}, "Understanding Generalization In Recurrent Neural Networks": {"container_type": "Publication", "bib": {"title": "Understanding generalization in recurrent neural networks", "author": ["Z Tu", "F He", "D Tao"], "pub_year": "2019", "venue": "International Conference on Learning \u2026", "abstract": "In this work, we develop the theory for analyzing the generalization performance of recurrent neural networks. We first present a new generalization bound for recurrent neural networks based on matrix 1-norm and Fisher-Rao norm. The definition of Fisher-Rao norm relies on a structural lemma about the gradient of RNNs. This new generalization bound assumes that the covariance matrix of the input data is positive definite, which might limit its use in practice. To address this issue, we propose to add random noise to the input data and prove"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=rkgg6xBYDH", "author_id": ["", "QSx-Yu0AAAAJ", "RwlJNLcAAAAJ"], "url_scholarbib": "/scholar?q=info:gVea2UKyHKIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BGeneralization%2BIn%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gVea2UKyHKIJ&ei=GtpXYteVBpWMy9YPt8OamA0&json=", "num_citations": 13, "citedby_url": "/scholar?cites=11681407533680580481&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:gVea2UKyHKIJ:scholar.google.com/&scioq=Understanding+Generalization+In+Recurrent+Neural+Networks&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=rkgg6xBYDH"}, "Denoising And Regularization Via Exploiting The Structural Bias Of Convolutional Generators": {"container_type": "Publication", "bib": {"title": "Denoising and regularization via exploiting the structural bias of convolutional generators", "author": ["R Heckel", "M Soltanolkotabi"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.14634", "abstract": "Convolutional Neural Networks (CNNs) have emerged as highly successful tools for image generation, recovery, and restoration. A major contributing factor to this success is that convolutional networks impose strong prior assumptions about natural images. A surprising experiment that highlights this architectural bias towards natural images is that one can remove noise and corruptions from a natural image without using any training data, by simply fitting (via gradient descent) a randomly initialized, over-parameterized convolutional"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.14634", "author_id": ["ZWV0I7cAAAAJ", "narJyMAAAAAJ"], "url_scholarbib": "/scholar?q=info:-5rq6k1tYqMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDenoising%2BAnd%2BRegularization%2BVia%2BExploiting%2BThe%2BStructural%2BBias%2BOf%2BConvolutional%2BGenerators%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-5rq6k1tYqMJ&ei=HtpXYvilC8WemAHB5baIBQ&json=", "num_citations": 33, "citedby_url": "/scholar?cites=11773092557321050875&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-5rq6k1tYqMJ:scholar.google.com/&scioq=Denoising+And+Regularization+Via+Exploiting+The+Structural+Bias+Of+Convolutional+Generators&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.14634"}, "Memo: A Deep Network For Flexible Combination Of Episodic Memories": {"container_type": "Publication", "bib": {"title": "Memo: A deep network for flexible combination of episodic memories", "author": ["A Banino", "AP Badia", "R K\u00f6ster", "MJ Chadwick"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the memory-based reasoning neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning--the appreciation of distant relationships among elements"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.10913", "author_id": ["QD-sf3IAAAAJ", "DcWRJW4AAAAJ", "eEGGCiUAAAAJ", "odkRSW4AAAAJ"], "url_scholarbib": "/scholar?q=info:5LhYRVFEdHoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemo:%2BA%2BDeep%2BNetwork%2BFor%2BFlexible%2BCombination%2BOf%2BEpisodic%2BMemories%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5LhYRVFEdHoJ&ei=IdpXYqehLZWMy9YPt8OamA0&json=", "num_citations": 21, "citedby_url": "/scholar?cites=8823752685772126436&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:5LhYRVFEdHoJ:scholar.google.com/&scioq=Memo:+A+Deep+Network+For+Flexible+Combination+Of+Episodic+Memories&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.10913"}, "Stochastic Weight Averaging In Parallel: Large-batch Training That Generalizes Well": {"container_type": "Publication", "bib": {"title": "Stochastic weight averaging in parallel: Large-batch training that generalizes well", "author": ["V Gupta", "SA Serrano", "D DeCoste"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.02312", "abstract": "We propose Stochastic Weight Averaging in Parallel (SWAP), an algorithm to accelerate DNN training. Our algorithm uses large mini-batches to compute an approximate solution quickly and then refines it by averaging the weights of multiple models computed independently and in parallel. The resulting models generalize equally well as those trained with small mini-batches but are produced in a substantially shorter time. We demonstrate the reduction in training time and the good generalization performance of the resulting models"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02312", "author_id": ["c1xeyA0AAAAJ", "", "wVIG7l8AAAAJ"], "url_scholarbib": "/scholar?q=info:mK8kRHDXA6YJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BWeight%2BAveraging%2BIn%2BParallel:%2BLarge-batch%2BTraining%2BThat%2BGeneralizes%2BWell%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mK8kRHDXA6YJ&ei=JNpXYrLcJsmUywTMkLbABQ&json=", "num_citations": 12, "citedby_url": "/scholar?cites=11962641912405733272&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:mK8kRHDXA6YJ:scholar.google.com/&scioq=Stochastic+Weight+Averaging+In+Parallel:+Large-batch+Training+That+Generalizes+Well&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02312"}, "Defending Against Physically Realizable Attacks On Image Classification": {"container_type": "Publication", "bib": {"title": "Defending against physically realizable attacks on image classification", "author": ["T Wu", "L Tong", "Y Vorobeychik"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.09552", "abstract": "We study the problem of defending deep neural network approaches for image classification from physically realizable attacks. First, we demonstrate that the two most scalable and effective methods for learning robust models, adversarial training with PGD attacks and randomized smoothing, exhibit very limited effectiveness against three of the highest profile physical attacks. Next, we propose a new abstract adversarial model, rectangular occlusion attacks, in which an adversary places a small adversarially crafted rectangle in an image"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.09552", "author_id": ["dt0eV8CPx3AC", "w362AJMAAAAJ", "ptI-HHkAAAAJ"], "url_scholarbib": "/scholar?q=info:azf1SYy-mBoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDefending%2BAgainst%2BPhysically%2BRealizable%2BAttacks%2BOn%2BImage%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=azf1SYy-mBoJ&ei=K9pXYrjxFcWemAHB5baIBQ&json=", "num_citations": 56, "citedby_url": "/scholar?cites=1916491151191652203&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:azf1SYy-mBoJ:scholar.google.com/&scioq=Defending+Against+Physically+Realizable+Attacks+On+Image+Classification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.09552"}, "Controlling Generative Models With Continuous Factors Of Variations": {"container_type": "Publication", "bib": {"title": "Controlling generative models with continuous factors of variations", "author": ["A Plumerault", "HL Borgne", "C Hudelot"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.10238", "abstract": "Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.10238", "author_id": ["", "ZlKEgWYAAAAJ", "gFlAh6MAAAAJ"], "url_scholarbib": "/scholar?q=info:D8btXluvw30J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DControlling%2BGenerative%2BModels%2BWith%2BContinuous%2BFactors%2BOf%2BVariations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D8btXluvw30J&ei=LtpXYtTFJM6E6rQPz8uiuAc&json=", "num_citations": 53, "citedby_url": "/scholar?cites=9062279682169095695&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:D8btXluvw30J:scholar.google.com/&scioq=Controlling+Generative+Models+With+Continuous+Factors+Of+Variations&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.10238"}, "Intrinsically Motivated Discovery Of Diverse Patterns In Self-organizing Systems": {"container_type": "Publication", "bib": {"title": "Intrinsically motivated discovery of diverse patterns in self-organizing systems", "author": ["C Reinke", "M Etcheverry", "PY Oudeyer"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1908.06663", "abstract": "In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify interesting patterns"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1908.06663", "author_id": ["LdGa3YMAAAAJ", "DjONosMAAAAJ", "gCqGj4sAAAAJ"], "url_scholarbib": "/scholar?q=info:1T4MXneMVSMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIntrinsically%2BMotivated%2BDiscovery%2BOf%2BDiverse%2BPatterns%2BIn%2BSelf-organizing%2BSystems%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1T4MXneMVSMJ&ei=MtpXYuW_EcWemAHB5baIBQ&json=", "num_citations": 8, "citedby_url": "/scholar?cites=2546095608654741205&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:1T4MXneMVSMJ:scholar.google.com/&scioq=Intrinsically+Motivated+Discovery+Of+Diverse+Patterns+In+Self-organizing+Systems&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1908.06663"}, "Influence-based Multi-agent Exploration": {"container_type": "Publication", "bib": {"title": "Influence-based multi-agent exploration", "author": ["T Wang", "J Wang", "Y Wu", "C Zhang"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.05512", "abstract": "Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.05512", "author_id": ["-AR1yc4AAAAJ", "RpomSmsAAAAJ", "dusV5HMAAAAJ", "LjxqXycAAAAJ"], "url_scholarbib": "/scholar?q=info:Rzke9ClEICsJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInfluence-based%2BMulti-agent%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Rzke9ClEICsJ&ei=NdpXYvv2DYvEmgH7846QCg&json=", "num_citations": 46, "citedby_url": "/scholar?cites=3107558689865611591&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Rzke9ClEICsJ:scholar.google.com/&scioq=Influence-based+Multi-agent+Exploration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.05512.pdf?ref=https://githubhelp.com"}, "Graphzoom: A Multi-level Spectral Approach For Accurate And Scalable Graph Embedding": {"container_type": "Publication", "bib": {"title": "Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding", "author": ["C Deng", "Z Zhao", "Y Wang", "Z Zhang", "Z Feng"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper we propose GraphZoom, a multi-level framework for improving both accuracy and"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.02370", "author_id": ["jOFdYeUAAAAJ", "9Huh6vgAAAAJ", "", "x05pUHsAAAAJ", ""], "url_scholarbib": "/scholar?q=info:TJ-ts4u-6JUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraphzoom:%2BA%2BMulti-level%2BSpectral%2BApproach%2BFor%2BAccurate%2BAnd%2BScalable%2BGraph%2BEmbedding%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TJ-ts4u-6JUJ&ei=ONpXYqSeHY2EmgH6u5u4BA&json=", "num_citations": 61, "citedby_url": "/scholar?cites=10802093213472366412&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:TJ-ts4u-6JUJ:scholar.google.com/&scioq=Graphzoom:+A+Multi-level+Spectral+Approach+For+Accurate+And+Scalable+Graph+Embedding&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.02370"}, "Asymptotics Of Wide Networks From Feynman Diagrams": {"container_type": "Publication", "bib": {"title": "Asymptotics of wide networks from feynman diagrams", "author": ["E Dyer", "G Gur-Ari"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.11304", "abstract": "Understanding the asymptotic behavior of wide networks is of considerable interest. In this work, we present a general method for analyzing this large width behavior. The method is an adaptation of Feynman diagrams, a standard tool for computing multivariate Gaussian integrals. We apply our method to study training dynamics, improving existing bounds and deriving new results on wide network evolution during stochastic gradient descent. Going beyond the strict large width limit, we present closed-form expressions for higher-order terms"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.11304", "author_id": ["LWeVRdUAAAAJ", "mx8P4QUAAAAJ"], "url_scholarbib": "/scholar?q=info:zikkCI_QSDUJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAsymptotics%2BOf%2BWide%2BNetworks%2BFrom%2BFeynman%2BDiagrams%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zikkCI_QSDUJ&ei=PNpXYt4LjqnLBJ2HpqAC&json=", "num_citations": 59, "citedby_url": "/scholar?cites=3839547995068836302&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:zikkCI_QSDUJ:scholar.google.com/&scioq=Asymptotics+Of+Wide+Networks+From+Feynman+Diagrams&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.11304"}, "Graph Constrained Reinforcement Learning For Natural Language Action Spaces": {"container_type": "Publication", "bib": {"title": "Graph constrained reinforcement learning for natural language action spaces", "author": ["P Ammanabrolu", "M Hausknecht"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2001.08837", "abstract": "Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.08837", "author_id": ["2yaiWZ8AAAAJ", "lutJce0AAAAJ"], "url_scholarbib": "/scholar?q=info:7Dyob_LsFdEJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BConstrained%2BReinforcement%2BLearning%2BFor%2BNatural%2BLanguage%2BAction%2BSpaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7Dyob_LsFdEJ&ei=QNpXYrOiBMiBy9YP18Gi8As&json=", "num_citations": 43, "citedby_url": "/scholar?cites=15066208654437399788&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:7Dyob_LsFdEJ:scholar.google.com/&scioq=Graph+Constrained+Reinforcement+Learning+For+Natural+Language+Action+Spaces&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.08837"}, "Neural Stored-program Memory": {"container_type": "Publication", "bib": {"title": "Neural stored-program memory", "author": ["H Le", "T Tran", "S Venkatesh"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.08862", "abstract": "By introducing a meta network to moderate the operations of the program memory, our  model, henceforth referred to as Neural Stored-program Memory (NSM), can learn to switch the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.08862", "author_id": ["q2HbxngAAAAJ", "zvspVLwAAAAJ", "AEkRUQcAAAAJ"], "url_scholarbib": "/scholar?q=info:LBCE-Ooen90J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BStored-program%2BMemory%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LBCE-Ooen90J&ei=QtpXYtyPIZaM6rQPlISayA8&json=", "num_citations": 22, "citedby_url": "/scholar?cites=15969516798219653164&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:LBCE-Ooen90J:scholar.google.com/&scioq=Neural+Stored-program+Memory&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.08862"}, "Spikegrad: An Ann-equivalent Computation Model For Implementing Backpropagation With Spikes": {"container_type": "Publication", "bib": {"title": "Spikegrad: An ann-equivalent computation model for implementing backpropagation with spikes", "author": ["JC Thiele", "O Bichler", "A Dupret"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.00851", "abstract": "Event-based neuromorphic systems promise to reduce the energy consumption of deep learning tasks by replacing expensive floating point operations on dense matrices by low power sparse and asynchronous operations on spike events. While these systems can be trained increasingly well using approximations of the back-propagation algorithm, these implementations usually require high precision errors for training and are therefore incompatible with the typical communication infrastructure of neuromorphic circuits. In this"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.00851", "author_id": ["cqwhQswAAAAJ", "mocqL0kAAAAJ", ""], "url_scholarbib": "/scholar?q=info:diPsd2FulB4J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpikegrad:%2BAn%2BAnn-equivalent%2BComputation%2BModel%2BFor%2BImplementing%2BBackpropagation%2BWith%2BSpikes%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=diPsd2FulB4J&ei=RdpXYpzsMoySyATlkbrQCA&json=", "num_citations": 18, "citedby_url": "/scholar?cites=2203507482593862518&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:diPsd2FulB4J:scholar.google.com/&scioq=Spikegrad:+An+Ann-equivalent+Computation+Model+For+Implementing+Backpropagation+With+Spikes&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.00851"}, "Reconstructing Continuous Distributions Of 3d Protein Structure From Cryo-em Images": {"container_type": "Publication", "bib": {"title": "Reconstructing continuous distributions of 3D protein structure from cryo-EM images", "author": ["ED Zhong", "T Bepler", "JH Davis", "B Berger"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1909.05215", "abstract": "Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7} $ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1909.05215", "author_id": ["17tGlqAAAAAJ", "Roxjki8AAAAJ", "DjR64IwAAAAJ", "bYjKaowAAAAJ"], "url_scholarbib": "/scholar?q=info:SN_mXkw32WoJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReconstructing%2BContinuous%2BDistributions%2BOf%2B3d%2BProtein%2BStructure%2BFrom%2BCryo-em%2BImages%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SN_mXkw32WoJ&ei=SNpXYougJZaM6rQPlISayA8&json=", "num_citations": 38, "citedby_url": "/scholar?cites=7699245839115804488&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SN_mXkw32WoJ:scholar.google.com/&scioq=Reconstructing+Continuous+Distributions+Of+3d+Protein+Structure+From+Cryo-em+Images&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1909.05215.pdf?ref=https://githubhelp.com"}, "Extracting And Leveraging Feature Interaction Interpretations": {"container_type": "Publication", "bib": {"title": "Recommender systems leveraging multimedia content", "author": ["Y Deldjoo", "M Schedl", "P Cremonesi", "G Pasi"], "pub_year": "2020", "venue": "ACM Computing Surveys \u2026", "abstract": "In fact, scientific research related to the analysis of  items based on an analysis of the features  extracted from the item itself representation structure named feature interaction graph (FIG)."}, "filled": false, "gsrank": 1, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3407190", "author_id": ["-C_x_hUAAAAJ", "TQR8qIEAAAAJ", "dTSOPCMAAAAJ", "5KWMh6wAAAAJ"], "url_scholarbib": "/scholar?q=info:SchUJ4TIi4MJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExtracting%2BAnd%2BLeveraging%2BFeature%2BInteraction%2BInterpretations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SchUJ4TIi4MJ&ei=S9pXYubaD8WemAHB5baIBQ&json=", "num_citations": 51, "citedby_url": "/scholar?cites=9478890310652446793&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:SchUJ4TIi4MJ:scholar.google.com/&scioq=Extracting+And+Leveraging+Feature+Interaction+Interpretations&hl=en&as_sdt=0,33", "eprint_url": "https://www.researchgate.net/profile/Yashar-Deldjoo/publication/344454095_Recommender_Systems_Leveraging_Multimedia_Content/links/5f775999299bf1b53e098667/Recommender-Systems-Leveraging-Multimedia-Content.pdf"}, "Gradient Regularization For Quantization Robustness": {"container_type": "Publication", "bib": {"title": "Gradient  Regularization for Quantization Robustness", "author": ["M Alizadeh", "A Behboodi", "M van Baalen"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "regularization technique for robustness to quantization noise. We first propose an appropriate  model for quantization  of the output perturbation caused by quantization. When the linear"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2002.07520", "author_id": ["YmGyDhcAAAAJ", "", "a-Au4JUAAAAJ"], "url_scholarbib": "/scholar?q=info:XMvQyfWPiXQJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient%2BRegularization%2BFor%2BQuantization%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XMvQyfWPiXQJ&ei=TtpXYpTjC4vEmgH7846QCg&json=", "num_citations": 26, "citedby_url": "/scholar?cites=8397401266024663900&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:XMvQyfWPiXQJ:scholar.google.com/&scioq=Gradient+Regularization+For+Quantization+Robustness&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2002.07520"}, "Masked Based Unsupervised Content Transfer": {"container_type": "Publication", "bib": {"title": "A hierarchical reinforced sequence operation method for unsupervised text style transfer", "author": ["C Wu", "X Ren", "F Luo", "X Sun"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1906.01833", "abstract": "in text style transfer, ie, fluency, style polarity, and content  content preservation, we adopt  a reconstruction reward and a self-supervised reconstruction loss. We introduce a mask-based"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1906.01833", "author_id": ["WFKit_4AAAAJ", "", "1s79Z5cAAAAJ", "tpXiQkYAAAAJ"], "url_scholarbib": "/scholar?q=info:AyK-alUsAY0J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMasked%2BBased%2BUnsupervised%2BContent%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AyK-alUsAY0J&ei=UdpXYtXHHo2EmgH6u5u4BA&json=", "num_citations": 40, "citedby_url": "/scholar?cites=10160450979699237379&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:AyK-alUsAY0J:scholar.google.com/&scioq=Masked+Based+Unsupervised+Content+Transfer&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1906.01833.pdf?ref=https://githubhelp.com"}, "Bayesian Meta Sampling For Fast Uncertainty Adaptation": {"container_type": "Publication", "bib": {"title": "Bayesian meta sampling for fast uncertainty adaptation", "author": ["Z Wang", "Y Zhao", "P Yu", "R Zhang"], "pub_year": "2019", "venue": "\u2026 Conference on Learning \u2026", "abstract": "Meta learning has been making impressive progress for fast model adaptation. However, limited work has been done on learning fast uncertainty adaption for Bayesian modeling. In this paper, we propose to achieve the goal by placing meta learning on the space of probability measures, inducing the concept of meta sampling for fast uncertainty adaption. Specifically, we propose a Bayesian meta sampling framework consisting of two main components: a meta sampler and a sample adapter. The meta sampler is constructed by"}, "filled": false, "gsrank": 1, "pub_url": "https://openreview.net/forum?id=Bkxv90EKPB", "author_id": ["Lca2K60AAAAJ", "9zmGBugAAAAJ", "-V7TJhwAAAAJ", "-seCWbAAAAAJ"], "url_scholarbib": "/scholar?q=info:H4GENkypwKwJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2BMeta%2BSampling%2BFor%2BFast%2BUncertainty%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H4GENkypwKwJ&ei=VNpXYpzzOsWemAHB5baIBQ&json=", "num_citations": 9, "citedby_url": "/scholar?cites=12448135514849313055&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:H4GENkypwKwJ:scholar.google.com/&scioq=Bayesian+Meta+Sampling+For+Fast+Uncertainty+Adaptation&hl=en&as_sdt=0,33", "eprint_url": "https://openreview.net/pdf?id=Bkxv90EKPB"}, "A Simple Randomization Technique For Generalization In Deep Reinforcement Learning": {"container_type": "Publication", "bib": {"title": "Network randomization: A simple technique for generalization in deep reinforcement learning", "author": ["K Lee", "K Lee", "J Shin", "H Lee"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1910.05396", "abstract": "of developing a simple and plausible method applicable to training deep RL agents.  is to  develop a simple randomization technique for improving the generalization ability across tasks"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1910.05396", "author_id": ["92M8xv4AAAAJ", "6wwWRdEAAAAJ", "m3eDp7kAAAAJ", ""], "url_scholarbib": "/scholar?q=info:_K7iybCE8lMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BSimple%2BRandomization%2BTechnique%2BFor%2BGeneralization%2BIn%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_K7iybCE8lMJ&ei=WNpXYoLlJZGJmwGIxre4DA&json=", "num_citations": 80, "citedby_url": "/scholar?cites=6049043144348184316&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:_K7iybCE8lMJ:scholar.google.com/&scioq=A+Simple+Randomization+Technique+For+Generalization+In+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1910.05396"}, "Mutual Information Gradient Estimation For Representation Learning": {"container_type": "Publication", "bib": {"title": "Mutual information gradient estimation for representation learning", "author": ["L Wen", "Y Zhou", "L He", "M Zhou", "Z Xu"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2005.01123", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2005.01123", "author_id": ["jwHflLcAAAAJ", "", "cGVDQD0AAAAJ", "LXwCIisAAAAJ", "gF0H9nEAAAAJ"], "url_scholarbib": "/scholar?q=info:0cad8sw9SSIJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMutual%2BInformation%2BGradient%2BEstimation%2BFor%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0cad8sw9SSIJ&ei=XNpXYur9ApGJmwGIxre4DA&json=", "num_citations": 16, "citedby_url": "/scholar?cites=2470573821042476753&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:0cad8sw9SSIJ:scholar.google.com/&scioq=Mutual+Information+Gradient+Estimation+For+Representation+Learning&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2005.01123"}, "Neural Network Branching For Neural Network Verification": {"container_type": "Publication", "bib": {"title": "Neural network branching for neural network verification", "author": ["J Lu", "MP Kumar"], "pub_year": "2019", "venue": "arXiv preprint arXiv:1912.01329", "abstract": "Formal verification of neural networks is essential for their deployment in safety-critical areas. Many available formal verification methods have been shown to be instances of a unified Branch and Bound (BaB) formulation. We propose a novel framework for designing an effective branching strategy for BaB. Specifically, we learn a graph neural network (GNN) to imitate the strong branching heuristic behaviour. Our framework differs from previous methods for learning to branch in two main aspects. Firstly, our framework directly treats the"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1912.01329", "author_id": ["", "BfmcfEAAAAAJ"], "url_scholarbib": "/scholar?q=info:Mi9dRdOKTi8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BNetwork%2BBranching%2BFor%2BNeural%2BNetwork%2BVerification%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Mi9dRdOKTi8J&ei=XtpXYveqMY6pywSdh6agAg&json=", "num_citations": 31, "citedby_url": "/scholar?cites=3408814607972511538&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:Mi9dRdOKTi8J:scholar.google.com/&scioq=Neural+Network+Branching+For+Neural+Network+Verification&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1912.01329"}, "Explanation By Progressive Exaggeration": {"container_type": "Publication", "bib": {"title": "Explanation by progressive exaggeration", "author": ["S Singla", "B Pollack", "J Chen"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "the explanation, especially for end users (eg, physicians). We argue that the explanation  should  Our proposed method falls into the local explanation paradigm. Our approach is model"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/1911.00483", "author_id": ["U3TPVDYAAAAJ", "zDJ3_8AAAAAJ", "sZ9QsNIAAAAJ"], "url_scholarbib": "/scholar?q=info:pqogXv-M7ccJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExplanation%2BBy%2BProgressive%2BExaggeration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pqogXv-M7ccJ&ei=YdpXYonXGeHDywTjooCQBQ&json=", "num_citations": 46, "citedby_url": "/scholar?cites=14406325811451832998&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:pqogXv-M7ccJ:scholar.google.com/&scioq=Explanation+By+Progressive+Exaggeration&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/1911.00483"}, "Provable Robustness Against All Adversarial -perturbations For": {"container_type": "Publication", "bib": {"title": "Householder Activations for Provable Robustness against Adversarial Attacks", "author": ["S Singla", "S Singla", "S Feizi"], "pub_year": "2021", "venue": "arXiv preprint arXiv:2108.04062", "abstract": "nontrivial provable robustness against adversarial attacks, we further boost their robustness   improvements in both the standard and provable robust accuracy over the prior works (gain"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2108.04062", "author_id": ["", "", "lptAmrMAAAAJ"], "url_scholarbib": "/scholar?q=info:-_1wBtmWTPcJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProvable%2BRobustness%2BAgainst%2BAll%2BAdversarial%2B-perturbations%2BFor%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-_1wBtmWTPcJ&ei=ZNpXYsScMZGJmwGIxre4DA&json=", "num_citations": 1, "citedby_url": "/scholar?cites=17819783684458348027&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:-_1wBtmWTPcJ:scholar.google.com/&scioq=Provable+Robustness+Against+All+Adversarial+-perturbations+For&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2108.04062"}, "Spatially Parallel Attention And Component Extraction For Scene Decomposition": {"container_type": "Publication", "bib": {"title": "Space: Unsupervised object-oriented scene representation via spatial attention and decomposition", "author": ["Z Lin", "YF Wu", "SV Peri", "W Sun", "G Singh", "F Deng"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": ", called Spatially Parallel Attention and Component Extraction (SPACE),  unifies the benefits  of spatial-attention and scene-mixture  Second, we introduce a spatially parallel multi-object"}, "filled": false, "gsrank": 1, "pub_url": "https://arxiv.org/abs/2001.02407", "author_id": ["BiyrJeMAAAAJ", "Fv2A650AAAAJ", "AaY4U-wAAAAJ", "", "lXpFxDwAAAAJ", ""], "url_scholarbib": "/scholar?q=info:2u8dpcFadu8J:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpatially%2BParallel%2BAttention%2BAnd%2BComponent%2BExtraction%2BFor%2BScene%2BDecomposition%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2u8dpcFadu8J&ei=aNpXYtmHCJWMy9YPt8OamA0&json=", "num_citations": 85, "citedby_url": "/scholar?cites=17255078810062024666&as_sdt=5,33&sciodt=0,33&hl=en", "url_related_articles": "/scholar?q=related:2u8dpcFadu8J:scholar.google.com/&scioq=Spatially+Parallel+Attention+And+Component+Extraction+For+Scene+Decomposition&hl=en&as_sdt=0,33", "eprint_url": "https://arxiv.org/pdf/2001.02407"}}